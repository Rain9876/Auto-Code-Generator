,Generated Code,Actual Code
0,"def get_parent_from_obj(self, obj): obj.get_parent(self, obj.get_parent): obj.get_parent(self, obj.get_parent): obj.get_parent(self, obj.get_parent): obj.get_parent(self.get_parent): obj.get_parent(self.get_parent): obj.get_parent(self.get_parent): obj.get_parent(self.get_parent): obj.get_parent(self.get_parent): obj.get_parent(self.get_parent): obj.get_parent(self.get_parent): obj.get_parent(self.get_parent): obj.get_parent(self.get_parent): obj.get_parent(self.get_parent(self.get_parent(self.get_parent(self.get_parent(self.get_parent(self.get_parent(self.get_parent(self.get_parent(self.get_parent(self.get_parent(self.get","def get_parent(obj): names = obj.__qualname__.split('.')[:-1] if 'locals>' in names: raise ValueError('cannot get parent from locals object.') module = sys.modules[obj.__module__] parent = module while names: parent = getattr(parent, names.pop(0)) return parent"
1,"def start_qt4_event_loop(self, qt4_event_loop, qt4_event_loop, qt4_event_loop, qt4_event_loop, qt4_event_loop, qt4_event_loop, qt4_event_loop, qt4_event_loop, qt4_event_loop, qt4_event_loop, qt4_event_loop) qt4_event_loop, qt4_event_loop, qt4_event_loop, qt4_event_loop, qt4_event_loop, qt4_event_loop, qt4_event_loop, qt4_event_loop, qt4_event_loop, qt4_event_loop, qt4_event_loop, qt4_event_loop, qt4_event_loop, qt4_event_loop, qt4_event_",def start_event_loop_qt4(app=None): if app is None: app = get_app_qt4(['']) if not is_event_loop_running_qt4(app): app._in_event_loop = True app.exec_() app._in_event_loop = False else: app._in_event_loop = True
2,def resume currently scheduled program - resume currently scheduled program - resume currently scheduled program - resume currently scheduled program - resume currently scheduled program - resume currently scheduled program - resume currently scheduled program - resume currently scheduled program - resume currently scheduled program,"def resume_program(self, index, resume_all=False): body = ""selection"":  ""selectionType"": ""thermostats"", ""selectionMatch"": self.thermostats[index]['identifier'], ""functions"": [""type"": ""resumeProgram"", ""params"":  ""resumeAll"": resume_all ] log_msg_action = ""resume program"" return self.make_request(body, log_msg_action)"
3,"def get_spectraltype(self, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt, spt = spt = spt = spt = spt = spt = spt = spt = spt = spt = spt = spt = spt = spt = spt","def vminiFromSpt(spt): if spt in _sptToVminiVabsDictionary: return _sptToVminiVabsDictionary[spt][0] else: message=""Unknown spectral type. Allowed values are: "" for key in _sptToVminiVabsDictionary.keys(): message += key+"" "" raise Exception(message)"
4,def ________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________,"def blank_dc(self, n_coarse_chan): if n_coarse_chan  1: logger.warning('Coarse channel number  1, unable to blank DC bin.') return None if not n_coarse_chan % int(n_coarse_chan) == 0: logger.warning('Selection does not contain an interger number of coarse channels, unable to blank DC bin.') return None n_coarse_chan = int(n_coarse_chan) n_chan = self.data.shape[-1] n_chan_per_coarse = int(n_chan / n_coarse_chan) mid_chan = int(n_chan_per_coarse / 2) for ii in range(n_coarse_chan): ss = ii*n_chan_per_coarse self.data[..., ss+mid_chan] = np.median(self.data[..., ss+mid_chan+5:ss+mid_chan+10])"
5,"def create_api_version(self, api_version, api_version): api_version = self.api_version(api_version, self.api_version, self.api_version, self.api_version, self.api_version, self.api_version)","def _create_client_impl(self, api_version): if api_version == v7_0_VERSION: from azure.keyvault.v7_0 import KeyVaultClient as ImplClient elif api_version == v2016_10_01_VERSION: from azure.keyvault.v2016_10_01 import KeyVaultClient as ImplClient else: raise NotImplementedError(""APIVersion  is not available"".format(api_version)) impl = ImplClient(credentials=self._credentials) impl.config = self.config if self._entered and hasattr(impl, '__enter__'): impl.__enter__() self._client_impls[api_version] = impl return impl"
6,"def run_sql(self, sql, autocommit): if self.sql(self.sql): self.sql(self.sql): self.sql(self.sql): self.sql(self.sql): self.sql(self.sql): self.sql(self.sql): self.sql(self.sql): self.sql(self.sql): self.sql): self.sql(self.sql): self.sql = self.sql)","def run(self, sql, autocommit=False, parameters=None): if isinstance(sql, basestring): sql = [sql] with closing(self.get_conn()) as conn: if self.supports_autocommit: self.set_autocommit(conn, autocommit) with closing(conn.cursor()) as cur: for s in sql: if parameters is not None: self.log.info("" with parameters "".format(s, parameters)) cur.execute(s, parameters) else: self.log.info(s) cur.execute(s) if not self.get_autocommit(conn): conn.commit()"
7,def volume(volume): volume(volume) = volume(volume) = volume(volume) = volume(volume) = volume(volume) = volume(volume) = volume(volume),"def volume_percentage_used(self, volume): volume = self._get_volume(volume) if volume is not None: total = int(volume[""size""][""total""]) used = int(volume[""size""][""used""]) if used is not None and used > 0 and  total is not None and total > 0: return round((float(used) / float(total)) * 100.0, 1)"
8,"def get_file_like_object(os_path, skip_sub_command, skip_sub_command, skip_sub_command, skip_sub_command, skip_sub_command, skip_sub_command, os_path, skip_sub_command, skip_sub_command, skip_sub_command, disk_closed_callback, disk_closed_callback, os_path, skip_sub_command, skip_sub_command, skip_sub_command, os_closed_callback, disk_closed_callback, disk_closed_callback, disk_closed_callback, disk_closed_callback, disk_closed_callback, disk_closed_callback, disk_closed_callback, disk_closed_callback, disk_closed_callback, disk_closed_callback, disk_closed_callback, disk_closed_callback, disk_closed_callback, disk_closed_callback, disk_closed_callback, disk_closed_callback, disk_closed_callback, disk_closed_callback, disk_closed_callback, disk_closed_callback, disk_closed_callback, disk_closed_call","def with_stdout(self, os_path=None, skip_sub_command=False, disk_closed_callback=None): sub_command = None if skip_sub_command else self.stdout_sub_command out, path = self._get_out_and_path( self.stdout, self.stdout_root, sub_command, os_path) try: if hasattr(out,'stdin'): yield out.stdin else: yield out finally: if hasattr(out,'stdin'): self._close(out.stdin) self._wait(out, path) self._close(out) if disk_closed_callback and path: disk_closed_callback(path)"
9,def import_foo(path/to/foo/bar.py): if path/to/foo/bar.py is path/to/foo/bar.py: if path/to/foo/bar.py is path/to/foo/bar.py: if path/to/foo/bar.py is path/to/foo/bar.py: if path/to/foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo,"def importFromPath(self, path, fqname): path_parts = os.path.normpath(os.path.abspath(path)).split(os.sep) name_parts = fqname.split('.') if path_parts[-1].startswith('__init__'): path_parts.pop() path_parts = path_parts[:-(len(name_parts))] dir_path = os.sep.join(path_parts) return self.importFromDir(dir_path, fqname)"
10,"def build_asset(self, preprocessors, compilers, postprocessors, postprocessors, preprocessors, compilers, postprocessors, preprocessors, compilers, postprocessors, preprocessors, compilers, postprocessors, preprocessors, preprocessors, compilers, postprocessors, preprocessors, preprocessors, postprocessors, preprocessors, preprocessors, postprocessors)",def processors(self): return self.preprocessors + list(reversed(self.compilers)) + self.postprocessors
11,"def convert_html_content_to_plaintext(self, html_content): self.convert(self.convert(html_content)) self.convert(html_content) self.convert(html_content) self.convert(html_content) self.convert(html_content) self.convert(html_content) self.convert(html_content)",def html_to_text(content): text = None h2t = html2text.HTML2Text() h2t.ignore_links = False text = h2t.handle(content) return text
12,"def get_next_sibling(self, children): if a name is provided, the next sibling with the given name is returned.get_next_sibling(next_sibling)","def next_sibling(self, name=None): if name is None: return XMLElement(lib.lsl_next_sibling(self.e)) else: return XMLElement(lib.lsl_next_sibling_n(self.e, str.encode(name)))"
13,"def _get_text(self, text): if self.get_text(self.get_text) self.get_text(self.get_text) self.get_text(self.get_text) self.get_text(self.get_text) self.get_text(self.get_text) self.get_text(self.get_text) self.get_text(self.get_text) self.get_text(self.get_text) self.get_text(self.get_text) self.get_text(self.get_text) self.get_text(self.get_text) self.get_text(self.get_text) self.get_text(self.get_text) self.get_text(self.get_text) self.get_text(self.get_text) self.get_text(self.get_text) self.get_text(self.get_text) self.get_text(self.get_text) self.get_text(self.get_text) self.get_text(self.get_text) self.get_text(self.get_text) self.get_text(self.get_text) self.get_text(self.get_text) self.get_text(self.",def text(self): if capybara.ignore_hidden_elements or capybara.visible_text_only: return self.visible_text else: return self.all_text
14,"def pick_start_name(self, start_name): if self.________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________","def summarizeFailedRuns(): global g_summary_dict_all onlyFiles = [x for x in listdir(g_test_root_dir) if isfile(join(g_test_root_dir, x))] for f in onlyFiles: for fileStart in g_file_start: if (fileStart in f) and (os.path.getsize(f) > 10): fFullPath = os.path.join(g_test_root_dir, f) try: temp_dict = json.load(open(fFullPath,'r')) for ind in range(len(temp_dict[""TestName""])): addFailedTests(g_summary_dict_all, temp_dict, ind) except: continue break"
15,"def get_album(self, market): if self.album(album): self.album(album): self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.album(album) self.","async def total_albums(self, *, market: str = None) -> int: data = await self.__client.http.artist_albums(self.id, limit=1, offset=0, market=market) return data['total']"
16,"def self.get_agentstate(self, agentstate): self.get_agentstate(agentstate) self.get_agentstate(agentstate) self.get_agentstate(agentstate) self.get_agentstate(agentstate) self.get_agentstate(agentstate) self.get_agentstate(agentstate) self.get_agentstate(agentstate) self.get_agentstate(agentstate) self.get_agentstate(agentstate) self.get_agentstate(agentstate) self.get_agentstate) self.get_agentstate(agentstate) self.get_agentstate(agentstate) self.get_agentstate) self.get_agentstate(agentstate) self.get_agentstate(agentstate) self.get_agentstate(agentstate) self.get_agentstate(agentstate) self.get_agentstate) self.get_agentstate(agentstate) self.get_agentstate(agentstate) self.get_agentstate(agentstate) self.get_agentstate(agentstate) self.get_agentstate(agentstate) self.get_agentstate","def internal_state(): data ='services':  'capture': ServiceStatus.str(get_service_status(Service.CAPTURE)), 'ingest': ServiceStatus.str(get_service_status(Service.INGEST)),'schedule': ServiceStatus.str(get_service_status(Service.SCHEDULE)), 'agentstate': ServiceStatus.str(get_service_status(Service.AGENTSTATE))   return make_response(jsonify('meta': data))"
17,"def disease_obj(self, alias_genes, hgnc_id, alias_symbol): disease_obj(self, alias_genes, hgnc_id, hpo_terms, hpo_terms, hpo_terms, hpo_terms, hpo_terms, hpo_terms, hpo_terms, hpo_terms, hpo_terms, hpo_terms, hpo_terms, hpo_terms, hpo_terms, hpo_terms, hpo_terms, hpo_terms, hpo_terms, hpo_terms, hpo_terms, hpo_terms, hpo_terms, hpo_terms, hpo_terms, hpo_terms, hpo_terms, hpo_terms, hpo_terms, hpo_terms, hpo_terms, hpo_","def build_disease_term(disease_info, alias_genes=): try: disease_nr = int(disease_info['mim_number']) except KeyError: raise KeyError(""Diseases has to have a disease number"") except ValueError: raise KeyError(""Diseases nr has to be integer"") disease_id = ""0:1"".format('OMIM', disease_nr) LOG.debug(""Building disease term %s"", disease_id) try: description = disease_info['description'] except KeyError: raise KeyError(""Diseases has to have a description"") disease_obj = DiseaseTerm( disease_id=disease_id, disease_nr=disease_nr, description=description, source='OMIM' ) inheritance_models = disease_info.get('inheritance') if inheritance_models: disease_obj['inheritance'] = list(inheritance_models) hgnc_ids = set() for hgnc_symbol in disease_info.get('hgnc_symbols', []): # if hgnc_symbol in alias_genes: if alias_genes[hgnc_symbol]['true']: hgnc_ids.add(alias_genes[hgnc_symbol]['true']) else: for hgnc_id in alias_genes[hgnc_symbol]['ids']: hgnc_ids.add(hgnc_id) else: LOG.debug(""Gene symbol %s could not be found in database"", hgnc_symbol) disease_obj['gen"
18,"def generate_report(self, account_id): self.report(report_id) self.report(report_id) self.report(report_id) self.report(report_id) self.report(report_id) self.report(report_id) self.report(report_id) self.report(report_id) self.report(report_id) self.report(report_id) self.report_id) self.report_id) self.report_id) self.report(report_id) self.report_id) self.report(report_id) self.report_id) self.report_id) self.report_id) self.report_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_i","def create_report(self, report_type, account_id, term_id=None, params=): if term_id is not None: params[""enrollment_term_id""] = term_id url = ACCOUNTS_API.format(account_id) + ""/reports/"".format( report_type) body = ""parameters"": params data = self._post_resource(url, body) data[""account_id""] = account_id return Report(data=data)"
19,"def POST_JSON_DATA_OBJECT(self, server, data object): self.get_data(data object) self.get_data(data object) self.get_data(data object) self.get_data(data object) self.get_data(data object) self.get_data(data object) self.get_data(data object) self.get_data(data object) self.get_data(data object) self.get_data(data object) self.get_data(data object) self.get_data(data object) self.get_data(data object) self.get_data(data object) self.get_data(data object) self.get_data(data object) self.get_data(data object) self.get_data(data object) self.get_data(data) self.get_data(data) self.get_data(data) self.get_data(data) self.get_data(data) self.get_data(data) self.get_data(data) self.get_data(data) self.get_data(data) self.get_data(data) self.get_data(data) self.get_data(data) self.get_data(data) self.get_data(data) self.get","def rundata(self, strjson): d = json.loads(strjson) return self.api.data.post(d)"
20,"def validation_helper(self, instance, schema, prefix, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs)","def schema_validate(instance, schema, exc_class, *prefix, **kwargs): try: jsonschema.validate(instance, schema) except jsonschema.ValidationError as exc: path = '/'.join((a if isinstance(a, six.string_types) else '[%d]' % a) for a in itertools.chain(prefix, exc.path)) message = 'Failed to validate ""%s"": %s' % (path, exc.message) raise exc_class(message, **kwargs)"
21,"def configure_pool(self, pool, storage): self.configure_pool(self.configure_pool) self.configure_pool(self.configure_pool) self.configure_pool(self.configure_pool) self.configure_pool(self.configure_pool) self.configure_pool(self.configure_pool) self.configure_pool(self.configure_pool) self.configure_pool(self.configure_pool) self.configure_pool(self.configure_pool) self.configure_pool) self.configure_pool) self.configure_pool(self.configure_pool) self.configure_pool) self.configure_pool(self.configure_pool) self.configure_pool) self.configure_pool(self.configure_pool) self.configure_pool) self.configure_pool(self.configure_pool) self.configure_pool) self.configure_pool(self.configure_pool) self.configure_pool(self.configure_pool) self.configure_pool) self.configure_pool(self.configure_pool) self.configure_pool) self.configure_pool(self.configure_pool) self.configure_pool","def _configure_pool(kwargs): _pool_single_run.storage_service = kwargs['storage_service'] _configure_niceness(kwargs) _configure_logging(kwargs, extract=False)"
22,"def delete_hook(self, pre, post, delete_hooks): self.get_delteview(self.get_delteview(self.get_delteview(self.get_delteview(self.get_delteview(self.get_delteview(self.get_delteview))) self.get_delteview(self.get_delteview) self.get_delteview(self.get_delteview) self.get_delteview(self.get_delteview) self.get_delteview(self.get_delteview) self.get_delteview) self.get_delteview(self.get_delteview) self.get_delteview) self.get_delteview(self.get_delteview) self.get_delteview(self.get_delteview) self.get_delteview(self.get_delteview) self.get_delteview(self.get_delteview) self.get_delteview(self.get_delteview) self.get_delteview(self.get_delteview) self.get_delteview(self.get_delteview) self.get_delteview(self.get_delteview","def delete(self, request, *args, **kwargs): self.object = self.get_object() success_url = self.get_success_url() self.pre_delete(self.object) self.object.delete() self.post_delete(self.object) return HttpResponseRedirect(success_url)"
23,"def grant_type=refresh_token(self, grant_type=refresh_token, client_id=refresh_token, client_secret=refresh_token, client_secret=refresh_token, client_secret=refresh_token, format=json, format=json): grant_type = grant_type = grant_type = grant_type = grant_type = grant_type = grant_type = grant_type = grant_type = grant_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret = client_secret","def _refresh_access_token(self): resource = ""https://na1.salesforce.com/services/oauth2/token"" fields = dict(grant_type=""refresh_token"", refresh_token=self.refresh_token, client_id=self.auth.client_id, client_secret=self.auth.client_secret, format=""json"") status, data = self._handle_response(""POST"", resource, fields=fields, refresh_access_token=False) if ""access_token"" in data: self.access_token = data[""access_token""] if callable(self.access_token_refreshed_callback): self.access_token_refreshed_callback(self.access_token) return True return False"
24,"def compare_pil_image(im1: first_pil_image, im2: second_pil_image, im2: second_pil_image, im1: second_pil_image, im2: second_pil_image, im2: second_pil_image, im2: second_pil_image, im2: second_pil_image, im2: second_pil_image, im2: second_pil_image, im2: second_pil_image, im1: second_pil_image, im2: second_pil_image, im2: first_pil_image, im2: second_pil_image, im2: second_pil_image, im2: first_pil_image, im2: second_pil_image, im2: second_pil_image, im1: second_pil_image, im2: first_pil_image, im2: second_pil_image, im2: second_pil_image, im1: second_pil_image, im2: first_pil_image, im2: second_pil_image, im2: second_pil_image, im1: second_pil_image, im2: second_pil_image,","def compute_ssim(image1, image2, gaussian_kernel_sigma=1.5, gaussian_kernel_width=11): gaussian_kernel_1d = get_gaussian_kernel( gaussian_kernel_width, gaussian_kernel_sigma) return SSIM(image1, gaussian_kernel_1d).ssim_value(image2)"
25,"def update_an_existing_login(self, pseudonyms, pseudonyms, pseudonyms, pseudonyms, pseudonyms, pseudonyms, pseudonyms, pseudonyms, pseudonyms, pseudonyms, pseudonyms, pseudonyms, pseudonyms, pseudonyms, pseudonyms, pseudonyms)","def update_user_login(self, login, account_id=None): if account_id is None: account_id = self._canvas_account_id if account_id is None: raise MissingAccountID login_id = login.login_id url = ACCOUNTS_API.format(account_id) + ""/logins/"".format(login_id) data = self._put_resource(url, login.put_data()) return Login(data=data)"
26,"def transpose(self, QuantumChannel): return self.transpose(self.transpose(self.transpose(self.transpose)))","def transpose(self): kraus_l, kraus_r = self._data kraus_l = [k.T for k in kraus_l] if kraus_r is not None: kraus_r = [k.T for k in kraus_r] return Kraus((kraus_l, kraus_r), input_dims=self.output_dims(), output_dims=self.input_dims())"
27,"def normalize_average(self, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db) db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, db, d","def normalize_spl_by_average(self, db): arr = self.to_numpy_array().copy() if len(arr) == 0: raise ValueError(""Cannot normalize the SPL of an empty AudioSegment"") def rms(x): return np.sqrt(np.mean(np.square(x))) desired_rms = P_REF_PCM * ((10 ** (db/20.0)) - 1E-9) # max_ntries = 50 res_rms = 0.0 ntries = 0 factor = 0.1 left = 0.0 right = desired_rms while (ntries  max_ntries) and not util.isclose(res_rms, desired_rms, abs_tol=0.1): res_rms = rms(arr * factor) if res_rms  desired_rms: left = factor else: right = factor factor = 0.5 * (left + right) ntries += 1 dtype_dict = 1: np.int8, 2: np.int16, 4: np.int32 dtype = dtype_dict[self.sample_width] new_seg = from_numpy_array(np.array(arr * factor, dtype=dtype), self.frame_rate) return new_seg"
28,"def save(self, session.save(obj1, obj2,...), session.save(obj1, obj2,...), session.save(obj1, obj2,...), session.save(obj1, obj2,...), session.save(obj1, obj2,...), session.save(obj1, obj2,...), session.save(obj1, obj1, obj2,...), session.save(obj1, obj2,...), session.save(obj1, obj1, obj2,...), session.save(obj1, obj2,...), session.save(obj1, obj2,...), session.save(obj1, obj1, obj2,...), session.save(obj1, obj1, obj2,...), session.save(obj1, obj1, obj2,...), session.save(obj1, obj2,","def save(self, *objects, **kwargs): from rom import Model full = kwargs.get('full') all = kwargs.get('all') force = kwargs.get('force') changes = 0 items = deque() items.extend(objects) while items: o = items.popleft() if isinstance(o, (list, tuple)): items.extendleft(reversed(o)) elif isinstance(o, Model): if not o._deleted and (all or o._modified): changes += o.save(full, force) else: raise ORMError( ""Cannot save an object that is not an instance of a Model (you provided %r)""%( o,)) return changes"
29,"def write_meta(self, meta, ucd,description): self.write_meta(self, meta.yaml) self.write_meta(self, meta.yaml) self.write_meta(self, meta.yaml) self.write_meta(self.write_meta) self.write_meta(self.write_meta) self.write_meta(self.write_meta) self.write_meta(self.write_meta) self.write_meta) self.write_meta(self.write_meta) self.write_meta(self.write_meta) self.write_meta(self.write_meta) self.write_meta) self.write_meta(self.write_meta) self.write_meta(self.write_meta) self.write_meta(self.write_meta) self.write_meta(self.write_meta) self.write_meta(self.write_meta) self.write_meta(self.write_meta) self.write_meta(self.write_meta) self.write_meta(self.write_meta) self.write_meta(self.write","def write_meta(self): path = os.path.join(self.get_private_dir(create=True), ""meta.yaml"") units = key: str(value) for key, value in self.units.items() meta_info = dict(description=self.description, ucds=self.ucds, units=units, descriptions=self.descriptions, ) vaex.utils.write_json_or_yaml(path, meta_info)"
30,def _convert_AST_to_python_source_code(self): if self._convert(self._convert(self._convert(self._convert(self._convert(self._convert(self._convert(self._convert(self._convert(self._convert(self._convert(self._convert(self._convert(self._convert(self._convert(self._convert(self._convert(self._convert(self._convert(self._convert(self._convert(self._convert(self._convert))))))))))),"def _ast_option_group_to_code(self, option_group, **kwargs): lines = [""option(""] lines.extend(self._indent(self._ast_to_code(option_group.expression))) lines.append("")"") return lines"
31,"def use_discount(self, discount): self.discount(discount) self.discount(discount) self.discount(discount) self.discount(discount) self.discount(discount) self.discount(discount) self.discount(discount) self.discount(discount) self.discount(discount) self.discount(discount) self.discount(discount) self.discount(discount) self.discount(discount) self.discount) self.discount) self.discount(discount) self.discount) self.discount(discount) self.discount) self.discount) self.discount(discount) self.discount) self.discount) self.discount(discount) self.discount) self.discount(discount) self.discount) self.discount(discount) self.discount) self.discount(discount) self.discount) self.discount(discount) self.discount(discount)","def discount_status(request, form): discounts = form.cleaned_data[""discount""] items = commerce.DiscountItem.objects.filter( Q(discount__in=discounts), ).select_related(""cart"", ""product"", ""product__category"") items = group_by_cart_status( items, [""discount""], [""discount"", ""discount__description""], ) headings = [ ""Discount"", ""Paid"", ""Reserved"", ""Unreserved"", ""Refunded"", ] data = [] for item in items: data.append([ item[""discount__description""], item[""total_paid""], item[""total_reserved""], item[""total_unreserved""], item[""total_refunded""], ]) return ListReport(""Usage by item"", headings, data)"
32,"def use(self, 'used', 'total', 'used', 'used', 'used', 'used', 'used', 'used', 'used', 'used', 'used', 'used', 'used', 'used', 'used', 'used', 'used', 'used', 'used', 'used', 'used', 'used', 'total', 'used', 'used', 'used', 'used', 'used', 'used', 'used', 'used', 'total', 'used', 'total', 'total', 'total', 'total', 'total', 'total', 'total', 'total', 'total', 'total', 'total', 'total', 'total', 'total', 'total', 'total', 'total', 'total', 'total', 'total', 'total'","def usage_percent(used, total, _round=None): try: ret = (used / total) * 100 except ZeroDivisionError: ret = 0 if _round is not None: return round(ret, _round) else: return ret"
33,nats_name_scope: nats_name_scope: bits_name_scope: bits_name_scope: bits_name_scope: bits_name_scope: bits_name_scope: bits_name_scope: bits_name_scope: bits_name_scope: bits_name_scope: bits_name_scope: bits_name_scope: bits_name_scope: bits_name_scope: bits_name_scope: bits_name_scope: nats_name_scope: bits_name_scope: bits_name_scope: bits_name_scope: bits_name_scope: bits_name_scope: bits_name_scope: bits_name_scope: bits_name_scope: bits_name_scope: bits_name_scope: bits_name_scope: bits_name_scope: bits_name_scope: bits_name_scope: bits_name_scope: bits_name_scope: bits_name_scope: bits_name_scope: bits_name_scope: nats_name_scope: bits_name_scope: bits_name_scope: bits_name_scope: nats_name_scope: bits_name_scope: bits_name_scope: bits_name_scope: bits_name_scope: nats_name_scope,"def summarize_mean_in_nats_and_bits(inputs, units, name, nats_name_scope=""nats"", bits_name_scope=""bits_per_dim""): mean = tf.reduce_mean(input_tensor=inputs) with tf.compat.v1.name_scope(nats_name_scope): tf.compat.v2.summary.scalar( name, mean, step=tf.compat.v1.train.get_or_create_global_step()) with tf.compat.v1.name_scope(bits_name_scope): tf.compat.v2.summary.scalar( name, mean / units / tf.math.log(2.), step=tf.compat.v1.train.get_or_create_global_step())"
34,"def get_restaurant_type(city_id, city_id, city_id, city_id, city_id, city_id, city_id, city_id, city_id, city_id, city_id, city_id, city_id, city_id, city_id, city_id, city_id, city_id, city_id, city_id, city_id, city_id, city_id, city_id, city_id, city_id, city_id)","def getEstablishments(self, city_id, **kwargs): params = ""city_id"": city_id optional_params = [""lat"", ""lon""] for key in optional_params: if key in kwargs: params[key] = kwargs[key] establishments = self.api.get(""/establishments"", params) return establishments"
35,"def mute_volume(self, volume, volume, volume): if volume is not mute, volume is not mute.mute_volume(volume, volume, volume, volume, volume)","def mute_volume(self, mute): if mute: self._device.mute_on() else: self._device.mute_off()"
36,"def parse_all_parse(self, parse_all_parse(self, parse_all_parse(self, parse_all_parse(self, parse_all_parse(self, parse_all_parse(self, parse_all_parse(self, parse_all_parse(self, parse_all_parse(self, parse_all_parse_all_parse_all_parse_all_parse_all_parse_all_parse_all_parse_all_parse_all_parse_parse_parse_parse_parse_parse_parse_parse_parse_parse_parse_parse_parse_parse_parse_parse_parse_parse_parse_parse_parse_parse_parse_parse_parse_parse_parse_parse_parse_parse_parse_parse_parse_parse","def parse(self): for tag in self.soup.findAll('span'): self.create_italic(tag) self.create_strong(tag) self.create_underline(tag) self.unwrap_span(tag) for tag in self.soup.findAll('a'): self.remove_comments(tag) self.check_next(tag) if self.soup.body: for tag in self.soup.body.findAll(): self.remove_empty(tag) self.remove_inline_comment(tag) self.parse_attrs(tag) for token, target in self.tokens: self.find_token(tag, token, target) self.remove_blacklisted_tags(tag)"
37,"def build_config(self, SDK config): config = self.jinja2 config = self.jinja2 config = self.jinja2 config = self.jinja2 config = self.jinja2 config = self.jinja2 config = self.jinja2 config = self.jinja2","def build_config(config : Dict[str, Any]) -> Dict[str, str]: result = config.copy() is_stable = result.pop(""is_stable"", False) if is_stable: result[""classifier""] = ""Development Status :: 5 - Production/Stable"" else: result[""classifier""] = ""Development Status :: 4 - Beta"" package_name = result[""package_name""] result[""package_nspkg""] = result.pop( ""package_nspkg"", package_name[:package_name.rindex('-')]+""-nspkg"" ) result['is_arm'] = result.pop(""is_arm"", True) result['need_msrestazure'] = result.pop(""need_msrestazure"", True) package_parts = result[""package_nspkg""][:-len('-nspkg')].split('-') result['nspkg_names'] = [ ""."".join(package_parts[:i+1]) for i in range(len(package_parts)) ] result['init_names'] = [ ""/"".join(package_parts[:i+1])+""/__init__.py"" for i in range(len(package_parts)) ] return result"
38,"def create_room(self, title, team_id, team_id, request_parameters, request_parameters, request_parameters, request_parameters, request_parameters, request_parameters, request_parameters, request_parameters, request_parameters, request_parameters, request_parameters, request_parameters, request_parameters, request_parameters, request_parameters): if request_parameter","def create(self, title, teamId=None, **request_parameters): check_type(title, basestring) check_type(teamId, basestring) post_data = dict_from_items_with_values( request_parameters, title=title, teamId=teamId, ) json_data = self._session.post(API_ENDPOINT, json=post_data) return self._object_factory(OBJECT_TYPE, json_data)"
39,"def provide_lowercase_extension(filename, versionchanged, versionchanged, versionchanged, versionchanged, versionchanged, versionchanged, versionchanged, versionchanged, versionchanged, versionchanged, versionchanged, versionchanged, versionchanged, versionchanged, versionchanged, versionchanged, versionchanged, versionchanged, versionchanged, versionchanged, versionchanged, versionchanged, versionchanged, versionchanged, versionchanged, versionchanged, versionchanged, versionchanged, versionchanged, versionchanged)","def lowercase_ext(filename): if '.' in filename: main, ext = os.path.splitext(filename) return main + ext.lower() return filename"
40,"def check(self, len(self, len(self, len(self, len(self, len(self, len(self, len(self, len(self, len(self, len(self, len(self, len(self, len(self, len(self, len(self, len(self, len(self, len(self, len(self, len(self)))))))))))))))","def _is_len_call(node): return ( isinstance(node, astroid.Call) and isinstance(node.func, astroid.Name) and node.func.name == ""len"" )"
41,def _substitute(self): self.substitute(self.substitute(self.substitute(self.substitute(self.substitute(self.substitute(self.substitute(self.substitute(self.substitute(self.substitute(self.substitute(self.substitute(self.substitute(self.substitute)))) self.substitute(self.substitute(self.substitute)) self.substitute) self.substitute(self.substitute) self.substitute) self.substitute(self.substitute) self.substitute) self.substitute(self.substitute) self.substitute) self.substitute(self.substitute(self.substitute) self.substitute) self.substitute(self.substitute) self.substitute(self.substitute) self.substitute(self.substitute) self.substitute(self.substitute) self.substitute(self.sub,"def sub_symbols(pattern, code, symbol): return pattern.replace('', code).replace('', symbol)"
42,"def apply_filter(self.separate, self.bkgsub, self.calibrated, self.calibrated, self.calibrated, self.calibrated, self.calibrated, self.calibrated, self.calibrated, self.calibrated, self.calibrated, self.calibrated, self.calibrated, self.calibrated, self.calibrated, self.calibrated, self.calibrated, self.calibrated, self.separate, self.separate, self.separate)","def bkg_calc_weightedmean(self, analytes=None, weight_fwhm=None, n_min=20, n_max=None, cstep=None, bkg_filter=False, f_win=7, f_n_lim=3, focus_stage='despiked'): if analytes is None: analytes = self.analytes self.bkg = Bunch() elif isinstance(analytes, str): analytes = [analytes] if weight_fwhm is None: weight_fwhm = 600 self.get_background(n_min=n_min, n_max=n_max, bkg_filter=bkg_filter, f_win=f_win, f_n_lim=f_n_lim, focus_stage=focus_stage) if 'calc' not in self.bkg.keys(): if cstep is None: cstep = weight_fwhm / 20 elif cstep > weight_fwhm: warnings.warn(""ncstep should be less than weight_fwhm. Your backgroundsn"" + ""might not behave as expected.n"") bkg_t = np.linspace(0, self.max_time, self.max_time // cstep) self.bkg['calc'] = Bunch() self.bkg['calc']['uTime'] = bkg_t mean, std, stderr = gauss_weighted_stats(self.bkg['raw'].uTime, self.bkg['raw'].loc[:, analytes].values, self.bkg['calc']['uTime'], fwhm=weight_fwhm) for i, a in enumerate(analytes): self.bkg['calc'][a] ='me"
43,"def check_input_dimension(self, input dimension): if self.get_qubit_subsystem(input dimension): self.get_qubit_subsystem(input dimension) self.get_qubit_subsystem(input dimension) self.get_qubit_subsystem(input dimension) self.get_qubit_subsystem(input dimension) self.get_qubit_subsystem(input dimension) self.get_qubit_subsystem(input dimension) self.get_qubit_subsystem(input dimension) self.get_qubit_subsystem(input dimension) self.get_qbit_subsystem(input dimension) self.get_qbit_subsystem(input dimension) self.get_qbit_subsystem(input dimension) self.get_qbit_subsystem(input dimension) self.get_qbit_subsystem(input dimension) self.get_qbit_subsystem(input dimension) self.get_qbit_subsystem(input dimension) self.get_qbit_subsystem(input dimension) self.get_qbit_subsystem(input dimension) self.get_qbit_subsystem(input dimension) self.get_qbit_subsystem(input dimension) self","def _automatic_dims(cls, dims, size): if dims is None: dims = size elif np.product(dims)!= size: raise QiskitError(""dimensions do not match size."") if isinstance(dims, (int, np.integer)): num_qubits = int(np.log2(dims)) if 2 ** num_qubits == size: return num_qubits * (2,) return (dims,) return tuple(dims)"
44,"def numWorkers(self, cores, disk, leaderMemory, leaderMemory, numWorkers, numWorkers, cores, disk, disk): if self.___________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________","def spawn_spark_cluster(job, numWorkers, cores=None, memory=None, disk=None, overrideLeaderIP=None): if numWorkers  1: raise ValueError(""Must have more than one worker. %d given."" % numWorkers) leaderService = SparkService(cores=cores, memory=memory, disk=disk, overrideLeaderIP=overrideLeaderIP) leaderIP = job.addService(leaderService) for i in range(numWorkers): job.addService(WorkerService(leaderIP, cores=cores, disk=disk, memory=memory), parentService=leaderService) return leaderIP"
45,"def move_dimension(x_perm, source_idx, dest_idx, dest_idx, x_perm, x_perm, source_idx, dest_idx, source_idx, dest_idx, source_idx, source_idx, source_idx, source_idx, source_idx, source_idx, source_idx, source_idx, source_idx, source_idx, source_idx, source_idx, source_idx, source_idx, source_idx)","def move_dimension(x, source_idx, dest_idx): ndims = prefer_static_rank(x) dtype = dtype_util.common_dtype([source_idx, dest_idx], preferred_dtype=tf.int32) source_idx = tf.convert_to_tensor(value=source_idx, dtype=dtype) dest_idx = tf.convert_to_tensor(value=dest_idx, dtype=dtype) source_idx = pick_scalar_condition(source_idx  0, ndims + source_idx, source_idx) dest_idx = pick_scalar_condition(dest_idx  0, ndims + dest_idx, dest_idx) def move_left_permutation(): return prefer_static_value( tf.concat([ tf.range(0, dest_idx, dtype=dtype), [source_idx], tf.range(dest_idx, source_idx, dtype=dtype), tf.range(source_idx + 1, ndims, dtype=dtype) ], axis=0)) def move_right_permutation(): return prefer_static_value( tf.concat([ tf.range(0, source_idx, dtype=dtype), tf.range(source_idx + 1, dest_idx + 1, dtype=dtype), [source_idx], tf.range(dest_idx + 1, ndims, dtype=dtype) ], axis=0)) def x_permuted(): return tf.transpose( a=x, perm=prefer_static.cond(source_idx  dest_idx,"
46,def path: if path is not path: if path is not path: if path is not path: if path is not path: if path is not path: if path is not path: if path is not path: if path is not path: if path is not path is not path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path,def ensure_directory(path): dirname = os.path.dirname(path) if not os.path.isdir(dirname): os.makedirs(dirname)
47,"def dump_default_args(self, default_args, default_args, default_args, default_args, default_args, default_args, default_args, default_args, default_args, default_args, default_args, default_args, default_args, default_args, default_args, default_args, default_args, default_args, default_args)","def _dump_arg_defaults(kwargs): if current_app: kwargs.setdefault('cls', current_app.json_encoder) if not current_app.config['JSON_AS_ASCII']: kwargs.setdefault('ensure_ascii', False) kwargs.setdefault('sort_keys', current_app.config['JSON_SORT_KEYS']) else: kwargs.setdefault('sort_keys', True) kwargs.setdefault('cls', JSONEncoder)"
48,"def change_game_state(self, win/loss): if win/loss is not found: if win/loss is not found: if win/loss is not found: if win/loss is not found: if win/loss is not found: if win/loss is not found: if win/loss is not found: if win/loss is not found: if win/loss is not found: if win/loss is not found",def check_game_end(self): if self.player in self.crashes.union(self.robots): return self.end_game('You Died!') elif not self.robots: return self.end_game('You Win!') else: return self
49,"def create_alert(self, name, description, scope): if self.alert(self.alert(self.alert)): self.alert(self.alert) self.alert(self.alert) self.alert(self.alert) self.alert(self.alert) self.alert(self.alert) self.alert(self.alert) self.alert(self.alert) self.alert(self.alert) self.alert(self.alert) self.alert(self.alert) self.alert(self.alert) self.alert(self.alert) self.alert(self.alert) self.alert(self.alert) self.alert(self.alert) self.alert(self.alert) self.alert(self.alert) self.alert_alert_alert_alert_alert_alert_alert_alert_alert_alert_alert_alert_alert_aler","def add_alert(self, name, description=None, scope="""", triggers='failed': True, 'unscanned': True, enabled=False, notification_channels=[]): alert =  'name': name, 'description': description, 'triggers': triggers,'scope': scope, 'enabled': enabled, 'autoscan': True, 'notificationChannelIds': notification_channels,  url = self.url + '/api/scanning/v1/alerts' data = json.dumps(alert) res = requests.post(url, headers=self.hdrs, data=data, verify=self.ssl_verify) if not self._checkResponse(res): return [False, self.lasterr] return [True, res.json()]"
50,def _draw_background(self): if self.draw(self.draw_background): self.draw_background(self.draw_background) self.draw_background(self.draw_background) self.draw_background(self.draw_background) self.draw_background(self.draw_background) self.draw_background(self.draw_background) self.draw_background(self.draw_background) self.draw_background(self.draw_background) self.draw_background(self.draw_background) self.draw_background() self.draw_background() self.draw_background() self.draw_background() self.draw_background() self.draw_background() self.draw_background() self.draw_background() self.draw_background() self.draw_background() self.draw_background() self.draw_background() self.draw_background() self.draw_background() self.draw_background() self.draw_background() self.draw_background() self.draw_background() self.draw_background() self.draw_background() self.draw_background() self.draw_background() self.draw_background(),"def on_redraw(self): if self.bg is not None: if not self.bg.initialized: self.bg.init_bg() self.bg.initialized=True self.bg.redraw_bg() super(Widget,self).on_redraw()"
51,"def search_result(self, query): if self.result(self.result(self.result(self.result(self.result(self.result(self.result(self.result(self.result(self.result(self.result(self.result(self.result(self.result(self.result(self.result(self.result(self.result(self.result(self.result(self.result(self.result))))))","def search(self, query): results = self.session.query(Domain).filter(Domain.name.ilike('%%%s%%' % query)).all() return results"
52,"def delete_account(self, path, body, body): if self.delete_account = self.delete_account = self.delete_account = self.delete_account = self.delete_account = self.delete_account = self.delete_account = self.delete_account = self.delete_account = self.delete_account = self.delete_account = self.delete_account = self.delete_account = self.delete_account = self.delete_account = self.delete_account = self.delete_account","def cli_delete(context, path, body=None, recursive=False, yes_empty_account=False, yes_delete_account=False, until_empty=False): path = path.lstrip('/') if path else '' if not path: if yes_empty_account: cli_empty_account( context, yes_empty_account=yes_empty_account, until_empty=until_empty) if yes_delete_account: with context.client_manager.with_client() as client: status, reason, headers, contents = client.delete_account( headers=context.headers, query=context.query, cdn=context.cdn, body=body, yes_i_mean_delete_the_account=yes_delete_account) if status // 100!= 2: if status == 404 and context.ignore_404: return raise ReturnCode( 'deleting account: %s %s' % (status, reason)) elif '/' not in path.rstrip('/'): path = path.rstrip('/') if recursive: cli_empty_container(context, path, until_empty=until_empty) with context.client_manager.with_client() as client: status, reason, headers, contents = client.delete_container( path, headers=context.headers, query=context.query, cdn=context.cdn, body=body) if status // 100!= 2: if status == 404 and context.ignore_404: return raise ReturnCode( 'deleting container %r: %s %s' % (path, status, reason)) else: with context.client_manager.with_client() as client: status, reason, header"
53,def csiszar_f_divergences(vmco): if csiszar_f_divergences = csiszar_f_divergences = csiszar_f_divergences = csiszar_f_divergences = csiszar_f_divergences = csiszar_f_divergences = csiszar_f_divergences = csiszar_f_divergences = csiszar_f_divergences = csiszar_f_divergences = csiszar_f_divergences = csiszar_f_divergences = csiszar_f_divergences = csiszar_f_divergences = csiszar_f_divergences = csiszar_f_divergences = csiszar_f_divergences = csiszar_f_divergences = csiszar_f_divergences = csiszar_f_divergences = csiszar_f_divergences = csis,"def csiszar_vimco(f, p_log_prob, q, num_draws, num_batch_draws=1, seed=None, name=None): with tf.compat.v1.name_scope(name, ""csiszar_vimco"", [num_draws, num_batch_draws]): if num_draws  2: raise ValueError(""Must specify num_draws > 1."") stop = tf.stop_gradient x = stop(q.sample(sample_shape=[num_draws, num_batch_draws], seed=seed)) logqx = q.log_prob(x) logu = p_log_prob(x) - logqx f_log_avg_u, f_log_sooavg_u = [f(r) for r in csiszar_vimco_helper(logu)] dotprod = tf.reduce_sum( input_tensor=logqx * stop(f_log_avg_u - f_log_sooavg_u), axis=0) f_log_avg_u += dotprod - stop(dotprod) return tf.reduce_mean(input_tensor=f_log_avg_u, axis=0)"
54,"def replace_svg_content(svg_content, corrected_svg, corrected_svg, corrected_svg, corrected_svg, corrected_svg, corrected_svg, corrected_svg, corrected_svg, corrected_svg, corrected_svg, corrected_svg, corrected_svg, corrected_svg, corrected_svg, corrected_svg, corrected_svg, corrected_svg, corrected_svg, corrected_svg, corrected_svg, corrected_svg)","def replace_chars_for_svg_code(svg_content): result = svg_content svg_char = [ ('&', '&amp;'), ('>', '&gt;'), ('', '&lt;'), ('""', '&quot;'), ] for c, entity in svg_char: result = result.replace(c, entity) return result"
55,"def find_cell(self, v_merge): if v_merge(self, v_merge, v_merge, v_merge, v_merge, v_merge, v_merge, v_merge, v_merge, v_merge, v_merge, v_merge, v_merge, v_merge, v_merge, v_merge): v_merge, v_merge, v_merge, v_merge, v_merge, v_merge, v_merge, v_merge, v_merge, v_merge, v_merge, v_merge, v_merge, v_merge, td, td, td, td, td, td, td, td, td, td, td, td, td, td, td, td, td, td, td, td, td, td, td, td, td","def get_td_at_index(tr, index): current = 0 for td in tr.xpath('.//w:tc', namespaces=tr.nsmap): if index == current: return td current += get_grid_span(td)"
56,"def transform_feature(z, n_samples, length = n_samples, length = n_samples, length = n_samples, length = n_samples, length = n_samples, length = n_samples, length = n_samples): if z = n_samples, z = n_samples, z = n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples,","def transform(self, Z): mapper = self.broadcast(super(SparkDictVectorizer, self).transform, Z.context) dtype = sp.spmatrix if self.sparse else np.ndarray return Z.transform(mapper, column='X', dtype=dtype)"
57,def ________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________,"def mlsd(conn, path="""", facts=None): facts = facts or [] if facts: conn.sendcmd(""OPTS MLST "" + "";"".join(facts) + "";"") if path: cmd = ""MLSD %s"" % path else: cmd = ""MLSD"" lines = [] conn.retrlines(cmd, lines.append) for line in lines: facts_found, _, name = line.rstrip(ftplib.CRLF).partition(' ') entry =  for fact in facts_found[:-1].split("";""): key, _, value = fact.partition(""="") entry[key.lower()] = value yield (name, entry)"
58,"def aggregation(self, cart_status): aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation = aggregation","def _items(self, cart_status, category=None): if not isinstance(cart_status, Iterable): cart_status = [cart_status] status_query = ( Q(productitem__cart__status=status) for status in cart_status ) in_cart = Q(productitem__cart__user=self.user) in_cart = in_cart & reduce(operator.__or__, status_query) quantities_in_cart = When( in_cart, then=""productitem__quantity"", ) quantities_or_zero = Case( quantities_in_cart, default=Value(0), ) products = inventory.Product.objects if category: products = products.filter(category=category) products = products.select_related(""category"") products = products.annotate(quantity=Sum(quantities_or_zero)) products = products.filter(quantity__gt=0) out = [] for prod in products: out.append(ProductAndQuantity(prod, prod.quantity)) return out"
59,"def return_circuit(self, barrier): if self.circuit(self.circuit): self.circuit(self.circuit): self.circuit(self.circuit)","def run(self, dag): final_op_types = ['measure', 'barrier'] final_ops = [] for candidate_node in dag.named_nodes(*final_op_types): is_final_op = True for _, child_successors in dag.bfs_successors(candidate_node): if any(suc.type == 'op' and suc.name not in final_op_types for suc in child_successors): is_final_op = False break if is_final_op: final_ops.append(candidate_node) if not final_ops: return dag barrier_layer = DAGCircuit() for qreg in dag.qregs.values(): barrier_layer.add_qreg(qreg) for creg in dag.cregs.values(): barrier_layer.add_creg(creg) final_qubits = set(final_op.qargs[0] for final_op in final_ops) barrier_layer.apply_operation_back( Barrier(len(final_qubits)), list(final_qubits), []) ordered_final_nodes = [node for node in dag.topological_op_nodes() if node in set(final_ops)] for final_node in ordered_final_nodes: barrier_layer.apply_operation_back(final_node.op, final_node.qargs, final_node.cargs) for final_op in final_ops: dag.remove_op_node(final_op) dag.extend_back(barrier_layer) adjacent_pass = MergeAdjacentBarriers() return adjacent_pass.run(dag)"
60,"def process_all_ways(self, process_all_ways): self.process_all_ways(self.process_all_ways) self.process_all_ways(self.process_all_ways) self.process_all_ways(self.process_all_ways) self.process_all_ways(self.process_all_ways) self.process_all_ways(self.process_all_ways) self.process_all_ways(self.process_all_ways) self.process_all_ways(self.process_all_ways) self.process_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_way_all_","def way(self, w): if w.id not in self.way_ids: return way_points = [] for n in w.nodes: try: way_points.append(Point(n.location.lon, n.location.lat)) except o.InvalidLocationError: logging.debug('InvalidLocationError at way %s node %s', w.id, n.ref) self.ways[w.id] = Way(w.id, way_points)"
61,def force_reload(self): self.reload(self.reload(self.reload)) self.reload(self.reload) self.reload(self.reload) self.reload(self.reload) self.reload(self.reload) self.reload(self.reload) self.reload(self.reload) self.reload(self.reload) self.reload(self.reload) self.reload(self.reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_reload_,"def load(self): self._check_open() try: data = json.load(self.file, **self.load_args) except ValueError: data =  if not isinstance(data, dict): raise ValueError('Root JSON type must be dictionary') self.clear() self.update(data)"
62,"def action_generate(self, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator): action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator, action_generator,","def generate_action(args): controller = args.get('controller>') action = args.get('action>') with_template = args.get('-t') current_path = os.getcwd() logger.info('Start generating action.') controller_file_path = os.path.join(current_path, 'application/controllers', controller + '.py') if not os.path.exists(controller_file_path): logger.warning(""The controller %s does't exist."" % controller) return if with_template: action_source_path = os.path.join(dirname(abspath(__file__)), 'templates/action.py') else: action_source_path = os.path.join(dirname(abspath(__file__)), 'templates/action_without_template.py') with open(action_source_path, 'r') as action_source_file: with open(controller_file_path, 'a') as controller_file: for action_line in action_source_file: new_line = action_line.replace('#controller', controller).  replace('#action', action) controller_file.write(new_line) logger.info(""Updated: %s"" % _relative_path(controller_file_path)) if with_template: assets_dir_path = os.path.join(current_path, 'application/pages/%s/%s' % (controller, action)) _mkdir_p(assets_dir_path) action_html_template_path = os.path.join(dirname(abspath(__file__)), 'templates/action.html') action_html_path = os.path.join(assets_dir_path"
63,def obj_default_arguments(obj_default_arguments): return obj_default_arguments(obj_default_arguments) return obj_default_arguments(obj_default_arguments) return obj_default_arguments(obj_default_arguments),"def _default_arguments(self, obj): if not (inspect.isfunction(obj) or inspect.ismethod(obj)): if inspect.isclass(obj): obj = (getattr(obj,'__init__',None) or getattr(obj,'__new__',None)) elif hasattr(obj, '__call__'): obj = obj.__call__ try: args,_,_1,defaults = inspect.getargspec(obj) if defaults: return args[-len(defaults):] except TypeError: pass return []"
64,"def connect_graph(self, graph): if graph is not connected, if graph is not connected: return True if graph is not connected: return False if graph is not connected: return False if graph is not connected: return False if graph is not connected: return False if graph is not connected: return False if graph is not connected: return False if graph is not connected: return False if graph is not connected: return False",def is_connected(self): try: return nx.is_weakly_connected(self.graph) except nx.exception.NetworkXException: return False
65,"def tag: if tag is :class:TextElement, if tag is :class:TextElement, if tag is :class:TextElement, if tag is :class:TextElement, if tag is :class:TextElement, if tag is :class:TextElement","def add_element(self, tag): if tag is TextElement and self.last_tag is TextElement: return self.last_tag = tag if tag not in self.tags: self.tags[tag] = 1 else: self.tags[tag] += 1"
66,"def read(self, amt, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content): decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content, decode_content","def stream(self, amt=2**16, decode_content=None): if self.chunked: for line in self.read_chunked(amt, decode_content=decode_content): yield line else: while not is_fp_closed(self._fp): data = self.read(amt=amt, decode_content=decode_content) if data: yield data"
67,"def extract_monitor(self, subrun_list, network_dict, subrun_list, subrun_list, subrun_list, network_dict, subrun_list, subrun_list, network_dict, subrun_list, subrun_list, network_dict, subrun_list, subrun_list, subrun_list, network_dict, subrun_list, subrun_list, subrun_list, network_dict, subrun_list, subrun_list, network_dict, network_dict, subrun_list, network_dict, subrun_list, network_dict, subrun_list, network_dict, subrun_list, subrun_list, network_dict, subrun_list, network_dict, subrun_list, network_dict, subrun_list, network_dict, network_dict, network_dict, network_dict, network_dict, network_dict, network_dict, network_dict, network_dict, network_dict, network_dict, network_dict, network_dict, network_dict, network_dict, network_dict, network_dict, network_dict, network_dict, network_dict, network_dict, network_dict, network_dict, network_dict, network_dict, network_dict, network_dict, network_dict, network","def analyse(self, traj, network, current_subrun, subrun_list, network_dict): if len(subrun_list)==0: traj.f_add_result(Brian2MonitorResult,'monitors.spikes_e', self.spike_monitor, comment = 'The spiketimes of the excitatory population') traj.f_add_result(Brian2MonitorResult,'monitors.V', self.V_monitor, comment = 'Membrane voltage of four neurons from 2 clusters') traj.f_add_result(Brian2MonitorResult,'monitors.I_syn_e', self.I_syn_e_monitor, comment = 'I_syn_e of four neurons from 2 clusters') traj.f_add_result(Brian2MonitorResult,'monitors.I_syn_i', self.I_syn_i_monitor, comment = 'I_syn_i of four neurons from 2 clusters') print('Plotting') if traj.parameters.analysis.make_plots: self._print_graphs(traj)"
68,"def download(self, bucket, key, filename, expected_size, expected_size, expected_size, expected_size, expected_size, expected_size, expected_size, expected_size, expected_size, expected_size, expected_size, expected_size, expected_size, expected_size, expected_size, expected_size, expected_size, expected_size, expected_size)","def download_file(self, bucket, key, filename, extra_args=None, expected_size=None): self._start_if_needed() if extra_args is None: extra_args =  self._validate_all_known_args(extra_args) transfer_id = self._transfer_monitor.notify_new_transfer() download_file_request = DownloadFileRequest( transfer_id=transfer_id, bucket=bucket, key=key, filename=filename, extra_args=extra_args, expected_size=expected_size, ) logger.debug( 'Submitting download file request: %s.', download_file_request) self._download_request_queue.put(download_file_request) call_args = CallArgs( bucket=bucket, key=key, filename=filename, extra_args=extra_args, expected_size=expected_size) future = self._get_transfer_future(transfer_id, call_args) return future"
69,"def fetch_commits(from_date, to_date, branches): if from_date is None: if from_date is None: if from_date is None: if from_date is None: if from_date is None: if from_date is None: if from_date is None: if from_date is None: if from_date is None: if new_items' is None: if 'new_items' is None: if 'new_items' is None: 'new_items' is None: 'new_items' is None: 'new_items' is None: 'new_items' is None: 'new_items' is None: 'new_items' is None: 'new_items' is None: 'new_items' is None: 'new_items' is None: 'new_items' = 'new_items' = 'new_items' = 'new_items' = 'new_items","def fetch(self, category=CATEGORY_COMMIT, from_date=DEFAULT_DATETIME, to_date=DEFAULT_LAST_DATETIME, branches=None, latest_items=False, no_update=False): if not from_date: from_date = DEFAULT_DATETIME if not to_date: to_date = DEFAULT_LAST_DATETIME kwargs =  'from_date': from_date, 'to_date': to_date, 'branches': branches, 'latest_items': latest_items, 'no_update': no_update  items = super().fetch(category, **kwargs) return items"
70,"def _path(path, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method): if not method, method, method, method, method, method, method, type_cast, type_cast, type_cast, type_cast, type_cast, type_cast, type_cast, type_cast, type_cast, type_cast","def s_url(self, path, method=None, type_cast=None): if not type_cast: type_cast =  def decorator(function): self.s_add(path, function, method, type_cast) return function return decorator"
71,"def import_line(self, xml.d, xml.dom, xml.dom, xml.dom, xml.dom, xml.dom, xml.dom, xml.dom, xml.dom, xml.dom, xml.dom, xml.dom, xml.dom, xml.dom, xml.dom, xml.dom, xml.dom)","def module_completion(line): words = line.split(' ') nwords = len(words) if nwords == 3 and words[0] == 'from': return ['import '] if nwords  3 and (words[0] in ['import','from']) : if nwords == 1: return get_root_modules() mod = words[1].split('.') if len(mod)  2: return get_root_modules() completion_list = try_import('.'.join(mod[:-1]), True) return ['.'.join(mod[:-1] + [el]) for el in completion_list] if nwords >= 3 and words[0] == 'from': mod = words[1] return try_import(mod)"
72,"def delta_step(self, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step): delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_step, delta_","def _runSeqProcesses(self) -> Generator[None, None, None]: updates = [] for proc in self._seqProcsToRun: try: outContainer = self._outputContainers[proc] except KeyError: outContainer = None proc(self, outContainer) if outContainer is not None: updates.append(outContainer) self._seqProcsToRun = UniqList() self._runSeqProcessesPlaned = False for cont in updates: for sigName, sig in cont._all_signals: newVal = getattr(cont, sigName) if newVal is not None: v = self._conflictResolveStrategy(newVal) updater, _ = v sig.simUpdateVal(self, updater) setattr(cont, sigName, None) return yield"
73,def ____________________________________________________________________________________________________________________________________________________________________________________________________________________________________,def events(): db = get_session() upcoming_events = db.query(UpcomingEvent).order_by(UpcomingEvent.start) recorded_events = db.query(RecordedEvent).order_by(RecordedEvent.start.desc()) result = [event.serialize() for event in upcoming_events] result += [event.serialize() for event in recorded_events] return make_data_response(result)
74,"def edit_properties(self, property): self.edit_properties(properties): self.edit_properties(properties) self.edit_properties(properties) self.edit_properties(properties) self.edit_properties(properties) self.edit_properties(properties) self.edit_properties(properties) self.edit_properties(properties) self.edit_properties(properties)","def edit(self, name=""""): ""Programatically select a (default) property to start editing it"" for name in (name, ""label"", ""value"", ""text"", ""title"", ""filename"", ""name""): prop = self.pg.GetPropertyByName(name) if prop is not None: break self.Parent.SetFocus() self.Parent.Raise() self.pg.SetFocus() wx.CallLater(250, self.select, prop.GetName())"
75,def measure_q[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c[0] -> c,"def qasm(self): name_param = self.name if self.params: name_param = ""%s(%s)"" % (name_param, "","".join( [str(i) for i in self.params])) return self._qasmif(name_param)"
76,"def create_record(self, str name, list values): if self.create_record(self.create_record, change_info): self.create_record(self.create_record, change_info)","def create_srv_record(self, name, values, ttl=60): self._halt_if_already_deleted() values = locals() del values['self'] return self._add_record(SRVResourceRecordSet, **values)"
77,"def wrap_new_fn(self, new_fn, original_fn, new_fn, new_fn, original_fn, new_fn, new_fn, new_fn, new_fn, original_fn, new_fn, new_fn, original_fn, new_fn, new_fn, new_fn, new_fn, new_fn, new_fn, new_fn, new_fn, new_fn, new_fn, new_fn, new_fn, new_fn, new_fn, new_fn, new_fn, new_fn, new_fn, new_fn)","def _copy_docstring(original_fn, new_fn): original_spec = tf_inspect.getfullargspec(original_fn) new_spec = tf_inspect.getfullargspec(new_fn) if original_spec!= new_spec: raise ValueError( 'Arg specs do not match: original=, new=, fn='.format( original_spec, new_spec, original_fn)) @decorator.decorator def wrap(wrapped_fn, *args, **kwargs): del wrapped_fn return new_fn(*args, **kwargs) return wrap(original_fn)"
78,"def get_dingding_endpoint(self, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint): endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint, endpoint","def _get_endpoint(self): conn = self.get_connection(self.http_conn_id) token = conn.password if not token: raise AirflowException('Dingding token is requests but get nothing,''check you conn_id configuration.') return 'robot/send?access_token='.format(token)"
79,"def expression(self, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult, op_mult","def expression(self, text): self._attempting(text) return alternation([ concatenation([ self.number, self.op_mult, self.expression ], ignore_whitespace=True), concatenation([ self.expression_terminal, self.op_mult, self.number, option( concatenation([ self.operator, self.expression ], ignore_whitespace=True) ) ], ignore_whitespace=True), concatenation([ self.expression_terminal, self.op_add, option( concatenation([ self.operator, self.expression ], ignore_whitespace=True) ) ], ignore_whitespace=True), concatenation([ self.expression_terminal, option( concatenation([ self.operator, self.expression ], ignore_whitespace=True) ) ], ignore_whitespace=True) ])(text).retyped(TokenType.expression)"
80,"def fetch_latools_reference(self, name): if self._______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________","def load_reference_data(name=None): base_url = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vQJfCeuqrtFFMAeSpA9rguzLAo9OVuw50AHhAULuqjMJzbd3h46PK1KjF69YiJAeNAAjjMDkJK7wMpG/pub?gid=:&single=true&output=csv' gids = 'culture_reference': '0', 'culture_test': '1170065442', 'downcore_reference': '190752797', 'downcore_test': '721359794', 'iolite_reference': '483581945', 'zircon_reference': '1355554964' if name is None: out =  for nm, gid in gids.items(): url = base_url.format(gid) tmp = pd.read_csv(url, header=[0], index_col=[0, 1]) tmp.index.names = ['sample','rep'] tmp.columns.names = ['analyte'] tmp.sort_index(1, inplace=True) out[nm] = tmp else: gid = gids[name] url = base_url.format(gid) out = pd.read_csv(url, index_col=[0, 1]) out.columns.names = ['analyte'] out.sort_index(1, inplace=True) return out"
81,"def _list_price(self, list_price): return self.get_list_price(list_price): return self.get_list_price(list_price)","def list_price(self): price = self._safe_get_element_text('ItemAttributes.ListPrice.Amount') currency = self._safe_get_element_text( 'ItemAttributes.ListPrice.CurrencyCode') if price: return float(price) / 100, currency else: return None, None"
82,"def create_folder(self, folder_id): if self.folder_id = self.folder_id = self.folder_id = self.folder_id = self.folder_id = self.folder_id = self.folder_id = self.folder_id = self.folder_id = self.folder_id = self.folder_id","def mkdir(self, name=None, folder_id='me/skydrive', metadata=dict()): metadata = metadata.copy() if name: metadata['name'] = name return self(folder_id, data=metadata, method='post', auth_header=True)"
83,"def mantissa(self, snum, snum, rtype, snum, snum, rtype, snum, snum, rtype, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum,",def peng_mant(snum): rsnum = snum.rstrip() return float(snum if snum[-1].isdigit() else snum[:-1])
84,"def compute_edge_ngram(self, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram): ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram, ngram","def compute_edge_ngrams(token, min=None): if min is None: min = config.MIN_EDGE_NGRAMS token = token[:config.MAX_EDGE_NGRAMS + 1] return [token[:i] for i in range(min, len(token))]"
85,"def insert_worker(self, insert_worker, insert_worker, insert_worker, insert_worker, insert_worker, insert_worker, insert_worker, insert_worker, insert_worker, insert_worker, insert_worker, insert_worker, insert_worker, insert_worker, insert_worker, insert_worker, insert_worker, insert_worker, insert_worker, insert_worker, insert_worker, insert_worker_worker_worker_worker_worker_worker_worker_worker_worker_worker_worker_worker)","def run_benchmark(): stopping = threading.Event() workers = [ InsertWorker(stopping) for _ in range(NUM_WORKERS) ] print('Launching %d workers' % NUM_WORKERS) [ worker.start() for worker in workers ] time.sleep(WORKLOAD_TIME) print('Stopping workload') stopping.set() [ worker.join() for worker in workers ] with get_connection() as conn: count = conn.get(""SELECT COUNT(*) AS count FROM %s"" % TABLE).count print(""%d rows inserted using %d workers"" % (count, NUM_WORKERS)) print(""%.1f rows per second"" % (count / float(WORKLOAD_TIME)))"
86,"def Translate_search_query(self, Mopidy, Spotify): if self.get_search_query(self.get_search_query): self.get_search_query(self.get_search_query) self.get_search_query(self.get_search_query) self.get_search_query(self.get_search_query) self.get_search_query(self.get_search_query) self.get_search_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_query_que","def sp_search_query(query): result = [] for (field, values) in query.items(): field = SEARCH_FIELD_MAP.get(field, field) if field is None: continue for value in values: if field == 'year': value = _transform_year(value) if value is not None: result.append('%s:%d' % (field, value)) elif field == 'any': result.append('""%s""' % value) else: result.append('%s:""%s""' % (field, value)) return''.join(result)"
87,"def from_block(self, from_address, to_address): if self.__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________","def trace_filter(self, from_block: int = 1, to_block: Optional[int] = None, from_address: Optional[List[str]] = None, to_address: Optional[List[str]] = None, after: Optional[int] = None, count: Optional[int] = None) -> List[Dict[str, any]]: assert from_address or to_address, 'You must provide at least from_address or to_address' parameters =  if from_block: parameters['fromBlock'] = '0x%x' % from_block if to_block: parameters['toBlock'] = '0x%x' % to_block if from_address: parameters['fromAddress'] = from_address if to_address: parameters['toAddress'] = to_address if after: parameters['after'] = after if count: parameters['count'] = count try: return self._decode_traces(self.slow_w3.parity.traceFilter(parameters)) except ParityTraceDecodeException as exc: logger.warning('Problem decoding trace: %s - Retrying', exc) return self._decode_traces(self.slow_w3.parity.traceFilter(parameters))"
88,"def group_object(self, grouper, list): group_object(self, grouper, list): group_object(self, grouper, list): group_object(self, grouper, list): group_object(self, grouper, list): group_object(self, grouper, list): group_object(self, grouper, list): group_object(self, grouper, list): group_object(self, grouper, grouper, list): group_object(self, grouper, list): group_object(self, grouper, list): group_object(self, group_object): group_object): group_object) group_object) group_object(self, group_object) group_object) group_object(self, group_object) group_object) group_object) group_object(self, group_object) group_object) group_object(self, group_object) group_object) group_object(self, group_object) group_object) group_object(self, group_object) group_object(self, group_object) group_object(self, group_object) group_object) group_object(self, group_object) group_object(self, group_object) group_object(self, group_object)","def do_groupby(environment, value, attribute): expr = make_attrgetter(environment, attribute) return sorted(map(_GroupTuple, groupby(sorted(value, key=expr), expr)))"
89,"def hardware_information(self, CPU count, CPU count): if self.________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________","def local_hardware_info(): results =  'os': platform.system(),'memory': psutil.virtual_memory().total / (1024 ** 3), 'cpus': psutil.cpu_count(logical=False) or 1  return results"
90,"def set_token(self, token): self.get_token(token) self.get_token(token) self.get_token(token) self.get_token(token) self.get_token(token) self.get_token(token) self.get_token(token) self.get_token(token) self.get_token(token) self.get_token(token) self.get_token(token) self.get_token(token(token) self.get_token(token) self.get_token(token) self.get_token(token) self.get_token(token) self.get_token(token) self.get_token(token) self.get_token(token) self.get_token(token) self.get_token(token) self.get_token(token) self.get_token(token) self.get_token(token) self.get_token(token) self.get_token(token) self.get_token(token) self.get_token(token) self.get_","def set_token(self, token): if token: auth = HTTPBasicAuth(token, '') self._token = token self.authenticated = True self.session.auth = auth logger.debug(""Using session token: %s"", token) else: self._token = None self.authenticated = False self.session.auth = None logger.debug(""Session token/auth reinitialised"")"
91,def remove_dead_code(self): if self.dead_code(self.dead_code): self.dead_code(self.dead_code) self.dead_code(self.dead_code) self.dead_code(self.dead_code) self.dead_code(self.dead_code) self.dead_code(self.dead_code) self.dead_code(self.dead_code) self.dead_code) self.dead_code(self.dead_code) self.dead_code) self.dead_code(self.dead_code) self.dead_code(self.dead_code) self.dead_code) self.dead_code(self.dead_code) self.dead_code) self.dead_code(self.dead_code) self.dead_code(self.dead_code) self.dead_code(self.dead_code) self.dead_code) self.dead_code(self.dead_code) self.de,"def visit_While(self, node: ast.While) -> Optional[ast.AST]: new_node = self.generic_visit(node) assert isinstance(new_node, ast.While) return ast.copy_location( ast.While( test=new_node.test, body=_filter_dead_code(new_node.body), orelse=_filter_dead_code(new_node.orelse), ), new_node, )"
92,"def dispatches_request(self, request, postprocessing, exception catching, error handling): self.get_request(request): self.get_request(request) self.get_request(request) self.get_request(request) self.get_request(request) self.get_request(request) self.get_request(request) self.get_request(request) self.get_request(request) self.get_request) self.get_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_re","def full_dispatch_request(self): self.try_trigger_before_first_request_functions() try: request_started.send(self) rv = self.preprocess_request() if rv is None: rv = self.dispatch_request() except Exception as e: rv = self.handle_user_exception(e) response = self.make_response(rv) response = self.process_response(response) request_finished.send(self, response=response) return response"
93,"def delete_date(self, date): if self.date(dates): self.date(dates): self.date(dates): self.date(dates): self.date(dates): self.date(dates)","def dates_to_delete(dates, years=0, months=0, weeks=0, days=0, firstweekday=SATURDAY, now=None): dates = set(dates) return dates - dates_to_keep(dates, years=years, months=months, weeks=weeks, days=days, firstweekday=firstweekday, now=now)"
94,"def responser(self, _answer, _answer, _answer, _answer, _answer, _answer, _answer, _answer, _answer, _answer, _answer, _answer, _answer, _answer, _answer, _answer, _answer, _answer, _answer, _answer, _answer, _answer, _answer, _answer, _answer, _answer)","def _runResponder(self, responder, request, command, identifier): key = failure.trap(*command.allErrors) response =  ""_error_code"": command.allErrors[key], ""_error_description"": str(failure.value), ""_error"": identifier  return response d.addCallbacks(_addIdentifier, _serializeFailure) return d"
95,"def caching_request(self, requests): if self.caching_request(self.caching_request): self.caching_request(self.caching_request) self.caching_request(self.caching_request) self.caching_request(self.caching_request) self.caching_request(self.caching_request) self.caching_request(self.caching_request) self.caching_request(self.caching_request) self.caching_request(self.caching_request) self.caching_request(self.caching_request) self.caching_request(self.caching_request) self.caching_request(self.caching_request) self.caching_request(self.caching_request) self.caching_request(self.caching_request) self.caching_request(self.caching_request) self.caching_request(self.caching_request) self.caching_request(self.caching_request) self.caching_request(self.caching_request) self.caching_request(self.ca","def cache_response(self, request, response, body=None): if response.status not in [200, 203, 300, 301]: return response_headers = CaseInsensitiveDict(response.headers) cc_req = self.parse_cache_control(request.headers) cc = self.parse_cache_control(response_headers) cache_url = self.cache_url(request.url) no_store = cc.get('no-store') or cc_req.get('no-store') if no_store and self.cache.get(cache_url): self.cache.delete(cache_url) if self.cache_etags and 'etag' in response_headers: self.cache.set( cache_url, self.serializer.dumps(request, response, body=body), ) elif response.status == 301: self.cache.set( cache_url, self.serializer.dumps(request, response) ) elif 'date' in response_headers: if cc and cc.get('max-age'): if int(cc['max-age']) > 0: self.cache.set( cache_url, self.serializer.dumps(request, response, body=body), ) elif 'expires' in response_headers: if response_headers['expires']: self.cache.set( cache_url, self.serializer.dumps(request, response, body=body), )"
96,"def migrate_data(self, target_fields, validation_params, metadata, commit_mode, kwargs): kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs: kwargs:","def migrate(self, target, follow=True, **kwargs): from solvebio import Dataset from solvebio import DatasetMigration if isinstance(target, Dataset): target_id = target.id else: target_id = target limit = kwargs.pop('limit', None) if not limit and self._limit  float('inf'): limit = self._limit params = self._build_query(limit=limit) params.pop('offset', None) params.pop('ordering', None) migration = DatasetMigration.create( source_id=self._dataset_id, target_id=target_id, source_params=params, client=self._client, **kwargs) if follow: migration.follow() return migration"
97,"def retrieve_raw_data(self, uri, payload, headers): return self.get_archiveError(self.get_archiveError)","def retrieve(self, uri, payload, headers): hashcode = self.make_hashcode(uri, payload, headers) logger.debug(""Retrieving entry %s with %s %s %s in %s"", hashcode, uri, payload, headers, self.archive_path) self._db.row_factory = sqlite3.Row try: cursor = self._db.cursor() select_stmt = ""SELECT data ""  ""FROM "" + self.ARCHIVE_TABLE + "" ""  ""WHERE hashcode =?"" cursor.execute(select_stmt, (hashcode,)) row = cursor.fetchone() cursor.close() except sqlite3.DatabaseError as e: msg = ""data retrieval error; cause: %s"" % str(e) raise ArchiveError(cause=msg) if row: found = pickle.loads(row['data']) else: msg = ""entry %s not found in archive %s"" % (hashcode, self.archive_path) raise ArchiveError(cause=msg) return found"
98,"def convert_timestamp(self, value): valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError: valueError:","def timestamp_YmdHMS(value): i = int(value) S = i M = S//100 H = M//100 d = H//100 m = d//100 Y = m//100 return int(calendar.timegm(( Y % 10000, m % 100, d % 100, H % 100, M % 100, S % 100, 0, 0, 0) ))"
99,"def check_convergence_criteria(self, convergence_criteria, convergence_criteria, convergence_criteria, convergence_criteria, convergence_criteria, convergence_criteria, convergence_criteria, convergence_criteria, convergence_criteria, convergence_criteria, convergence_criteria)","def _check_convergence(current_position, next_position, current_objective, next_objective, next_gradient, grad_tolerance, f_relative_tolerance, x_tolerance): grad_converged = norm(next_gradient, dims=1) = grad_tolerance x_converged = norm(next_position - current_position, dims=1) = x_tolerance f_converged = (norm(next_objective - current_objective, dims=0) = f_relative_tolerance * current_objective) return grad_converged | x_converged | f_converged"
100,"def delete_layer(self, layer): if layer is not 0: if layer is not 0: if layer is not 0: if layer is not 0: if layer is not 0: if layer is not 0: if layer is not 0: if layer is not 0: if layer is not 0: if layer is not 0: if layer is not 0: if layer is not 0: if layer is not 0: if layer is not 0: if layer is not","def delete_lower(script, layer_num=None): if layer_num is None: layer_num = script.current_layer() if layer_num!= 0: change(script, 0) for i in range(layer_num): delete(script, 0) return None"
101,def bash_script(self): if self.get(self.get(self.get(self.get))): self.get(self.get(self.get(self.get))) self.get(self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get(self.get) self.get(self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.,"def create_custom_exec_script(self):.format(cmds=cmds) with open(cfg_path, 'w') as fds: fds.write(customs_script) return cfg_path"
102,"def support(self, csr, csc, bsr, dia sparse matrices, csr, csc, bsr, dia sparse matrices, dia sparse matrices, csr, csc, bsr, dia sparse matrices, csr, csc, bsr, dia sparse matrices, dia sparse matrices, dia sparse matrices, csr, dia sparse matrices)","def _supports(self, item): if SparseParameter._is_supported_matrix(item): return True else: return super(SparseResult, self)._supports(item)"
103,"def reformat_annotations(self, annotations): if self.get_annotations(annotations): if self.get_annotations(annotations): return self.get_annotations(annotations)","def annotation_list_builder(annotations, host): return [ create_annotation(int(timestamp * 1000000), key, host) for key, timestamp in annotations.items() ]"
104,"def split_tuple(self, directory, tuple): if tuple(dir, tuple): if tuple(dir, tuple): if tuple(dir, tuple): tuple(dir, tuple): tuple(dir, tuple): tuple(dir, tuple): tuple(dir, tuple): tuple(dir, tuple): tuple(dir, tuple): tuple(dir, tuple): tuple): tuple(dir, tuple): tuple(dir, tuple): tuple(dir, tuple): tuple(dir, tuple): tuple(dir, tuple): tuple(dir, tuple): tuple(dir, tuple): tuple(dir, tuple): tuple(dir, tuple): tuple(dir, tuple): tuple) tuple",def calculate(directory): if sys.platform == 'windows': slash = '' elif sys.platform == 'linux': slash = '/' else: print('#Error. Unknown platform.') return print('Files in the current directory and their md5-hashes:n') for i in range(len(directory[2])): full_path = directory[0]+slash+directory[2][i] print(full_path) size(full_path) print(md5(full_path))
105,"def padded_data(self, block_size, style): if self.pkcs7: self.pkcs7: self.pkcs7: self.pkcs7: self.pkcs7: self.pkcs7: self.pkcs7: self.pkcs7: self.pkcs7: self.pkcs7: self.pkcs7: self.pkcs7: self.pkcs7: self.pkcs7: self.pkcs7: self.pkcs7: self.pkcs7: self.pkcs7: self.pkcs7: self.pkcs7: self.pkcs7: self.pkcs7: self.pkcs7: self.pkcs7: self.pkcs7: self.pkcs7: self.pkcs7: self.pkcs7: self.pkcs7: self.pkcs7: self.pkcs7: self.pkcs7: self.pkcs7: self.pkcs7: self.pkcs7: self","def unpad(padded_data, block_size, style='pkcs7'): pdata_len = len(padded_data) if pdata_len % block_size: raise ValueError(""Input data is not padded"") if style in ('pkcs7', 'x923'): padding_len = bord(padded_data[-1]) if padding_len1 or padding_len>min(block_size, pdata_len): raise ValueError(""Padding is incorrect."") if style == 'pkcs7': if padded_data[-padding_len:]!=bchr(padding_len)*padding_len: raise ValueError(""PKCS#7 padding is incorrect."") else: if padded_data[-padding_len:-1]!=bchr(0)*(padding_len-1): raise ValueError(""ANSI X.923 padding is incorrect."") elif style == 'iso7816': padding_len = pdata_len - padded_data.rfind(bchr(128)) if padding_len1 or padding_len>min(block_size, pdata_len): raise ValueError(""Padding is incorrect."") if padding_len>1 and padded_data[1-padding_len:]!=bchr(0)*(padding_len-1): raise ValueError(""ISO 7816-4 padding is incorrect."") else: raise ValueError(""Unknown padding style"") return padded_data[:-padding_len]"
106,"def fetch_tweets(self, category, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs)","def fetch_items(self, category, **kwargs): since_id = kwargs['since_id'] max_id = kwargs['max_id'] geocode = kwargs['geocode'] lang = kwargs['lang'] entities = kwargs['include_entities'] tweets_type = kwargs['result_type'] logger.info(""Fetching tweets %s from %s to %s"", self.query, str(since_id), str(max_id) if max_id else '--') tweets_ids = [] min_date = None max_date = None group_tweets = self.client.tweets(self.query, since_id=since_id, max_id=max_id, geocode=geocode, lang=lang, include_entities=entities, result_type=tweets_type) for tweets in group_tweets: for i in range(len(tweets)): tweet = tweets[i] tweets_ids.append(tweet['id']) if tweets[-1] == tweet: min_date = str_to_datetime(tweets[-1]['created_at']) if tweets[0] == tweet and not max_date: max_date = str_to_datetime(tweets[0]['created_at']) yield tweet logger.info(""Fetch process completed: %s (unique %s) tweets fetched, from %s to %s"", len(tweets_ids), len(list(set(tweets_ids))), min_date, max_date)"
107,"def new_schedule(self, parent, child): self.schedule(self.schedule): self.schedule(self.schedule) self.schedule(self.schedule) self.schedule(self.schedule) self.schedule(self.schedule) self.schedule(self.schedule) self.schedule(self.schedule) self.schedule(self.schedule) self.schedule) self.schedule) self.schedule(self.schedule) self.schedule) self.schedule) self.schedule(self.schedule) self.schedule) self.schedule) self.schedule(self.schedule) self.schedule) self.schedule(self.schedule) self.schedule) self.schedule(self.schedule) self.schedule) self.schedule(self.schedule) self.schedule) self.schedule(self.schedule) self.","def insert(parent: ScheduleComponent, time: int, child: ScheduleComponent, name: str = None) -> Schedule: return union(parent, (time, child), name=name)"
108,"def settle_modes(self, settle_modes): self.set_modes(self.set_modes): self.set_modes(self.set_modes) self.set_modes(self.set_modes) self.set_modes(self.set_modes) self.set_modes(self.set_modes) self.set_modes(self.set_modes) self.set_modes(self.set_modes) self.set_modes) self.set_modes(self.set_modes) self.set_modes) self.set_modes(self.set_modes) self.set_modes) self.set_modes(self.set_modes) self.set_modes) self.set_modes(self.set_modes) self.set_modes(self.set_modes) self.set_modes) self.set_modes(self.set_modes) self.set_modes(self.set_modes) self.set_modes(self.set_modes) self.set_modes(self.set_modes) self.set_modes) self.set_modes(self.set_modes) self.set",def _get_remote_settle_modes(pn_link): modes =  snd = pn_link.remote_snd_settle_mode if snd == proton.Link.SND_UNSETTLED: modes['snd-settle-mode'] = 'unsettled' elif snd == proton.Link.SND_SETTLED: modes['snd-settle-mode'] ='settled' if pn_link.remote_rcv_settle_mode == proton.Link.RCV_SECOND: modes['rcv-settle-mode'] ='second' return modes
109,"def r(self, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj): obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj,","def wave_vectors(obj): rexdesc = pexdoc.pcontracts.get_exdesc() if not isinstance(obj, list) or (isinstance(obj, list) and not obj): raise ValueError(exdesc) if any([not (isinstance(item, tuple) and len(item) == 2) for item in obj]): raise ValueError(exdesc) indep_vector, dep_vector = zip(*obj) if _check_increasing_real_numpy_vector(np.array(indep_vector)): raise ValueError(exdesc) if _check_real_numpy_vector(np.array(dep_vector)): raise ValueError(exdesc)"
110,"def load_test_data_failure(self, test_data_failure, test_data_failure, test_data_failure, test_data_failure, test_data_failure, test_data_failure)","def records(): import uuid from invenio_records.api import Record from invenio_pidstore.models import PersistentIdentifier, PIDStatus create_test_user() indexer = RecordIndexer() with db.session.begin_nested(): rec_uuid = uuid.uuid4() pid1 = PersistentIdentifier.create('recid', '1', object_type='rec', object_uuid=rec_uuid, status=PIDStatus.REGISTERED) Record.create( 'title': 'Registered', 'description': 'This is an awesome description', 'control_number': '1', 'access_right':'restricted', 'access_conditions': 'fuu', 'owners': [1, 2],'recid': 1, id_=rec_uuid) indexer.index_by_id(pid1.object_uuid) db.session.commit() sleep(3)"
111,"def striplog(self, tops): if self.get_intervals(self.get_intervals): self.get_intervals(self.get_intervals) self.get_intervals(self.get_intervals) self.get_intervals(self.get_intervals) self.get_intervals(self.get_intervals) self.get_intervals(self.get_intervals) self.get_intervals(self.get_intervals)","def __intervals_from_tops(self, tops, values, basis, components, field=None, ignore_nan=True): length = float(basis.size) start, stop = basis[0], basis[-1] tops = [start + (p/(length-1)) * (stop-start) for p in tops] bases = tops[1:] + [stop] list_of_Intervals = [] for i, t in enumerate(tops): v, c, d = values[i], [],  if ignore_nan and np.isnan(v): continue if (field is not None): d = field: v if components is not None: try: c = [deepcopy(components[int(v)])] except IndexError: c = [] if c and (c[0] is None): c = [] interval = Interval(t, bases[i], data=d, components=c) list_of_Intervals.append(interval) return list_of_Intervals"
112,"def patch_list(self, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list): patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_list, patch_","def patches_after(self, patch): return [line.get_patch() for line in self._patchlines_after(patch) if line.get_patch()]"
113,"def start_container(self, hdfs, job): self.container(self.container) self.container(self.container) self.container(self.container) self.container(self.container) self.container(self.container) self.container(self.container) self.container(self.container) self.container(self.container) self.container(self.container) self.container(self.container) self.container(self.container) self.container(self.container) self.container(self.container) self.container(self.container) self.container) self.container) self.container(self.container) self.container(self.container) self.container(self.container) self.container(self.container) self.container(self.container) self.container(self.container) self.container(self.container) self.container(self.container) self.containe","def start(self, job): self.sparkContainerID = dockerCheckOutput(job=job, defer=STOP, workDir=os.getcwd(), tool=""quay.io/ucsc_cgl/apache-spark-worker:1.5.2"", dockerParameters=[""--net=host"", ""-d"", ""-v"", ""/mnt/ephemeral/:/ephemeral/:rw"", ""-e"", """"SPARK_MASTER_IP="" + self.masterIP + "":"" + _SPARK_MASTER_PORT + """""", ""-e"", ""SPARK_LOCAL_DIRS=/ephemeral/spark/local"", ""-e"", ""SPARK_WORKER_DIR=/ephemeral/spark/work""], parameters=[self.masterIP + "":"" + _SPARK_MASTER_PORT])[:-1] self.__start_datanode(job) hdfs_down = True retries = 0 while hdfs_down and (retries  5): _log.info(""Sleeping 30 seconds before checking HDFS startup."") time.sleep(30) clusterID = """" try: clusterID = subprocess.check_output([""docker"", ""exec"", self.hdfsContainerID, ""grep"", ""clusterID"", ""-R"", ""/opt/apache-hadoop/logs""]) except: pass if ""Incompatible"" in clusterID: _log.warning(""Hadoop Datanode failed to start with: %s"", clusterID) _log.warning(""Retrying container startup, retry #%d."", retries) retries += 1 _log.warning(""Removing ephemeral hdfs directory."") subprocess.check_call([""docker"", ""exec"", self.hdfsContainerID"
114,"def versioned_URI(self, version, URI): self.get_versioned_URI = self.get_versioned_URI = self.get_versioned_URI = self.get_versioned_URI = self.get_versioned_URI = self.get_versioned_URI = self.get_versioned_URI","def class_url(cls): base = 'v0'.format(getattr(cls, 'RESOURCE_VERSION', '1')) return ""/0/1"".format(base, class_to_api_name(cls.class_name()))"
115,"def hdf5(self, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id) anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id, anno_id,","def from_hdf5(hdf5, anno_id=None): if anno_id is None: return from_hdf5(hdf5, list(hdf5.keys())[0]) anno_id = str(anno_id) if anno_id not in list(hdf5.keys()): raise ValueError(""ID  is not in this file. Options are: "".format( anno_id, "", "".join(list(hdf5.keys())) )) anno = hdf5[anno_id] try: r = AnnotationType.get_class(anno['ANNOTATION_TYPE'][0])() except: raise InvalidRAMONError(""This is not a valid RAMON type."") metadata = anno['METADATA'] r.author = metadata['AUTHOR'][0] r.confidence = metadata['CONFIDENCE'][0] r.status = metadata['STATUS'][0] r.id = anno_id if type(r) in [RAMONNeuron, RAMONSynapse]: r.segments = metadata['SEGMENTS'][()] if 'KVPAIRS' in metadata: kvs = metadata['KVPAIRS'][()][0].split() if len(kvs)!= 0: for i in kvs: k, v = str(i).split(',') r.kvpairs[str(k)] = str(v) else: r.kvpairs =  if issubclass(type(r), RAMONVolume): if 'CUTOUT' in anno: r.cutout = anno['CUTOUT'][()] if 'XYZOFFSET' in anno: r.cutout = anno['XYZOFF"
116,"def save_file(self, filesystem): if self.save(filesystem): self.save(filesystem): self.save(filesystem) self.save(filesystem) self.save(filesystem) self.save(filesystem) self.save(filesystem) self.save(filesystem) self.save(filesystem) self.save(filesystem) self.save(filesystem) self.save(filesystem) self.save(filesystem) self.save(filesystem) self.save(filesystem) self.save(filesystem) self.save(filesystem) self.save(filesystem) self.save(filesystem) self.save(filesystem) self.save(filesystem) self.save(filesystem) self.save(filesystem) self.save(filesystem) self.save(filesystem) self.save(filesystem) self.save(filesystem) self.save(filesystem) self.save(filesystem) self.save(filesystem) self.save(filesystem) self.sa","def save(self, temp_file, filename, obj): if not (self.storage_type and self.bucket_name): ret = self._save_local(temp_file, filename, obj) else: if self.storage_type!='s3': raise ValueError('Storage type ""%s"" is invalid, the only supported storage type (apart from default local storage) is s3.' % self.storage_type) ret = self._save_s3(temp_file, filename, obj) if self.field_name: setattr(obj, self.field_name, ret) if self.storage_type =='s3': if self.storage_type_field: setattr(obj, self.storage_type_field, self.storage_type) if self.bucket_name_field: setattr(obj, self.bucket_name_field, self.bucket_name) else: if self.storage_type_field: setattr(obj, self.storage_type_field, '') if self.bucket_name_field: setattr(obj, self.bucket_name_field, '') return ret"
117,def post_estimates(self): self.post_estimates(self.post_estimates): self.post_estimates(self.post_estimates) self.post_estimates(self.estimates) self.post_estimates(self.estimates) self.estimates(self.estimates) self.estimates(self.estimates) self.estimates(self.estimates) self.estimates(self.estimates) self.estimates(self.estimates) self.estimates(self.estimates) self.estimates(self.estimates) self.estimates) self.estimates(self.estimates) self.estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates_estimates,"def _postprocess(self, est_idxs, est_labels): if self.in_bound_idxs is not None: F = self._preprocess() est_labels = U.synchronize_labels(self.in_bound_idxs, est_idxs, est_labels, F.shape[0]) est_idxs = self.in_bound_idxs est_idxs, est_labels = U.remove_empty_segments(est_idxs, est_labels) assert len(est_idxs) - 1 == len(est_labels), ""Number of boundaries ""  ""(%d) and number of labels(%d) don't match"" % (len(est_idxs), len(est_labels)) est_idxs = np.asarray(est_idxs, dtype=int) return est_idxs, est_labels"
118,"def check_if_hgnc_alias(self, alias): if self.hgnc_alias(self.hgnc_alias) return self.hgnc_alias(self.hgnc_alias)","def to_hgnc(self, hgnc_alias, build='37'): result = self.hgnc_genes(hgnc_symbol=hgnc_alias, build=build) if result: for gene in result: return gene['hgnc_symbol'] else: return None"
119,"def state_fidelity(state1, state2): fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity = fidelity =","def state_fidelity(state1, state2): s1 = np.array(state1) s2 = np.array(state2) if s1.ndim == 1 and s2.ndim == 1: return np.abs(s2.conj().dot(s1)) ** 2 elif s1.ndim == 1: return np.abs(s1.conj().dot(s2).dot(s1)) elif s2.ndim == 1: return np.abs(s2.conj().dot(s1).dot(s2)) s1sq = _funm_svd(s1, np.sqrt) s2sq = _funm_svd(s2, np.sqrt) return np.linalg.norm(s1sq.dot(s2sq), ord='nuc') ** 2"
120,"def download_url_repository(self, url, vcs infos): if self.get_url_repository(url, vcs infos): self.get_url_repository(url): self.get_url_repository(url, vcs infos)","def unpack(self, location): if os.path.exists(location): rmtree(location) self.obtain(location)"
121,"def retry_interval(block, timeout, retry_interval, retry_interval, retry_interval, retry_interval, extra_predicate, extra_predicate, raw_predicate, extra_predicate, extra_predicate, extra_predicate, extra_predicate, extra_predicate, extra_predicate, extra_predicate, extra_predicate, extra_predicate, raw_predicate, extra_predicate, extra_predicate, extra_predicate, extra_predicate, extra_predicate, extra_predicate, extra_predicate, raw_predicate, extra_predicate, extra_predicate, extra_predicate, extra_predicate, extra_predicate, extra_predicate, extra_predicate, extra_predicate, extra_predicate, extra_predicate, extra_predicate, extra_predicate, extra_predicate, extra_predicate, extra_predicate, extra_predicate, extra_predicate, extra_predicate, extra_predicate, extra_predicate, extra_predicate, extra_predicate","def start(self, block=False, timeout=None, retry_interval=0.5, extra_predicate=None): start = time.time() while 1: task_handler = self._dequeue_task(extra_predicate) if task_handler is None and block: if timeout is not None and (time.time() - start) > timeout: break time.sleep(retry_interval * (random.random() + 0.1)) else: break return task_handler"
122,"def render_ditaa(self, ditaa, ditaa, ditaa, ditaa): def render_ditaa(ditaa, ditaa, ditaa, ditaa, ditaa, ditaa, ditaa, ditaa, ditaa, ditaa, ditaa, ditaa, ditaa, ditaa, ditaa, ditaa))","def render_ditaa(self, code, options, prefix='ditaa'): hashkey = code.encode('utf-8') + str(options) +  str(self.builder.config.ditaa) +  str(self.builder.config.ditaa_args) infname = '%s-%s.%s' % (prefix, sha(hashkey).hexdigest(), ""ditaa"") outfname = '%s-%s.%s' % (prefix, sha(hashkey).hexdigest(), ""png"") inrelfn = posixpath.join(self.builder.imgpath, infname) infullfn = path.join(self.builder.outdir, '_images', infname) outrelfn = posixpath.join(self.builder.imgpath, outfname) outfullfn = path.join(self.builder.outdir, '_images', outfname) if path.isfile(outfullfn): return outrelfn, outfullfn ensuredir(path.dirname(outfullfn)) if isinstance(code, unicode): code = code.encode('utf-8') ditaa_args = [self.builder.config.ditaa] ditaa_args.extend(self.builder.config.ditaa_args) ditaa_args.extend(options) ditaa_args.extend( [infullfn] ) ditaa_args.extend( [outfullfn] ) f = open(infullfn, 'w') f.write(code) f.close() try: self.builder.warn(ditaa_args) p = Popen(ditaa_args, stdout=PIPE"
123,"def create_set(self, members): self.set(self.set): self.set(self.set): self.set(self.set)","def s(*members: T, meta=None) -> Set[T]: return Set(pset(members), meta=meta)"
124,"def statement_list(self, statement_list, statement_list, statement_list, statement_list, statement_list, statement_list, statement_list, statement_list, statement_list, statement_list, statement_list, statement_list, statement_list)","def p_statement_list_2(self, p): p[0] = StatementListNode() if p[1] is not None: p[0].children.insert(0, p[1])"
125,def find_path_list(path_list): if path_list is None: if path_list is None: if path_list is None: if path_list is None: if path_list is None: if path_list is None: if path_list is None: if path_list is None: if path_list is None: if path_list is None: if path_list is None: if path_list is None: if path_list is None: if path_list is None: if path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list = path_list =,"def scan_path(root='.', recurse=False, pattern='*'): path_list = [] for path in Walk(root=root, recurse=recurse, pattern=pattern): path_list.append(path) return path_list"
126,"def delete_token(project_name, dataset_name, channel_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name,","def delete_token(self, token_name, project_name, dataset_name): return self.resources.delete_token(token_name, project_name, dataset_name)"
127,"def get_text(self, url): if self.get_text(url): return self.get_text(url) return self.get_text(url)",def read(self): if self.connection.has_changed(): image_path = self.connection.download_image() image = Image.open(image_path) self.text_cache = pytesseract.image_to_string(image) image.close() return self.text_cache
128,"def nextflow_config_string(self, nextflow_config_string): if self.config_string(self.config_string): self.config_string(self.config_string) self.config_string(self.config_string) self.config_string(self.config_string) self.config_string(self.config_string) self.config_string(self.config_string) self.config_string(self.config_string) self.config_string = self.config_string","def _get_manifest_string(self): config_str = """" config_str += 'ntname = """"'.format(self.pipeline_name) config_str += 'ntmainScript = """"'.format(self.nf_file) return config_str"
129,"def record_consumption(self, amt, time_at_consumption, time_at_consumption, time_at_consumption, time_at_consumption, time_at_consumption, time_at_consumption, time_at_consumption, time_at_consumption, time_at_consumption, time_at_consumption, time_at_consumption, time_at_consumption, time_at_consumption, time_at_consumption, time_at_consumption, time_at_consumption)","def record_consumption_rate(self, amt, time_at_consumption): if self._last_time is None: self._last_time = time_at_consumption self._current_rate = 0.0 return self._current_rate = self._calculate_exponential_moving_average_rate( amt, time_at_consumption) self._last_time = time_at_consumption"
130,"def parse_gene(self, gene): self.parse_gene(gene): self.parse_gene(gene): self.parse_gene(gene)","def gene(store, hgnc_id): res = 'builds': '37': None, '38': None,'symbol': None, 'description': None, 'ensembl_id': None,'record': None for build in res['builds']: record = store.hgnc_gene(hgnc_id, build=build) if record: record['position'] = ""this[chromosome]:this[start]-this[end]"".format(this=record) res['aliases'] = record['aliases'] res['hgnc_id'] = record['hgnc_id'] res['description'] = record['description'] res['builds'][build] = record res['symbol'] = record['hgnc_symbol'] res['description'] = record['description'] res['entrez_id'] = record.get('entrez_id') res['pli_score'] = record.get('pli_score') add_gene_links(record, int(build)) res['omim_id'] = record.get('omim_id') res['incomplete_penetrance'] = record.get('incomplete_penetrance',False) res['inheritance_models'] = record.get('inheritance_models',[]) for transcript in record['transcripts']: transcript['position'] = (""this[chrom]:this[start]-this[end]"".format(this=transcript)) add_tx_links(transcript, build) for phenotype in record.get('pheno"
131,"def list_external_files(self, record): if self.external_files(self.external_files) return self.external_files(self.external_files) return self.external_files(self.external_files) return self.external_files(self.external_files)","def get_external_files(self): return [v for v in self._data.values() if isinstance(v, ExternalFile)]"
132,"def func(self, callback, failReason, failReason): if self.func(self.func): self.func(self.func) self.func(self.func) self.func(self.func) self.func(self.func) self.func(self.func) self.func(self.func) self.func(self.func) self.func(self.func) self.func(self.func) self.func(self.func) self.func(self.func","def replicated(*decArgs, **decKwargs): def replicatedImpl(func): def newFunc(self, *args, **kwargs): if kwargs.pop('_doApply', False): return func(self, *args, **kwargs) else: if isinstance(self, SyncObj): applier = self._applyCommand funcName = self._getFuncName(func.__name__) funcID = self._methodToID[funcName] elif isinstance(self, SyncObjConsumer): consumerId = id(self) funcName = self._syncObj._getFuncName((consumerId, func.__name__)) funcID = self._syncObj._methodToID[(consumerId, funcName)] applier = self._syncObj._applyCommand else: raise SyncObjException(""Class should be inherited from SyncObj or SyncObjConsumer"") callback = kwargs.pop('callback', None) if kwargs: cmd = (funcID, args, kwargs) elif args and not kwargs: cmd = (funcID, args) else: cmd = funcID sync = kwargs.pop('sync', False) if callback is not None: sync = False if sync: asyncResult = AsyncResult() callback = asyncResult.onResult timeout = kwargs.pop('timeout', None) applier(pickle.dumps(cmd), callback, _COMMAND_TYPE.REGULAR) if sync: res = asyncResult.event.wait"
133,"def Generate_and_view_documentation(self, documentation): self.get_documentation(documentation) self.get_documentation(documentation) self.get_documentation(documentation) self.get_documentation(documentation) self.get_documentation(documentation) self.get_documentation(documentation) self.get_documentation(documentation) self.get_documentation(documentation) self.get_documentation(documentation) self.get_documentation) self.get_documentation_documentation_documentation_documentation_documentation_documentation_documentation_documentation_documentation_documentation_documentation_documentation_documentation_documentation_documentation_documentation_documentation_documentation_documentation_documentation_documentation_documentation_documentation_documentation_documentation_documentation_documentation_documentation_documentation_documentation_documentation_documentation_documentation_documentation_documentation_documentation_documentation_documentation","def run(self): cmds = (self.clean_docs_cmd, self.html_docs_cmd, self.view_docs_cmd) self.call_in_sequence(cmds)"
134,def load_qt_bindings(self): if self.load_qt_bindings(self.load_qt_bindings): self.load_qt_bindings(self.load_qt_bindings): self.load_qt_bindings(self.load_qt_bindings): self.load_qt_bindings(self.load_qt_bindings),"def import_qtcore(): has_ida = False try: import idaapi has_ida = True except ImportError: has_ida = False if has_ida: old_path = sys.path[:] try: ida_python_path = os.path.dirname(idaapi.__file__) sys.path.insert(0, ida_python_path) if idaapi.IDA_SDK_VERSION >= 690: from PyQt5 import QtCore return QtCore else: from PySide import QtCore return QtCore finally: sys.path = old_path else: try: from PyQt5 import QtCore return QtCore except ImportError: pass try: from PySide import QtCore return QtCore except ImportError: pass raise ImportError(""No module named PySide or PyQt"")"
135,def kl_dirichlet_dirichlet(d1 || d2): kl_dirichlet_dirichlet(d1 || d2): kl_dirichlet_dirichlet(d1 || d2)): kl_dirichlet_dirichlet(d1 || d2): kl_dirichlet_dirichlet(d1 || d2): kl_dirichlet_dirichlet_dirichlet_dirichlet_dirichlet_dirichlet_dirichlet_dirichlet_dirichlet_dirichlet_dirichlet): kl_dirichlet_dirichlet_dirichlet_dirichlet_dirichlet_dirichlet_dirichlet_dirichlet_dirichlet_dirichlet_dirichlet_dirichlet_dirichlet_dirichlet_dirichlet_dirichlet_dirichlet_dirichlet_dirichlet_dirichlet_dirichlet_dirichlet_dirichlet_dirichlet_d1: d2: d1: d2: d2: d1: d2: d1: d2: d2: d1: d2:,"def _kl_dirichlet_dirichlet(d1, d2, name=None): with tf.name_scope(name or ""kl_dirichlet_dirichlet""): digamma_sum_d1 = tf.math.digamma( tf.reduce_sum(input_tensor=d1.concentration, axis=-1, keepdims=True)) digamma_diff = tf.math.digamma(d1.concentration) - digamma_sum_d1 concentration_diff = d1.concentration - d2.concentration return ( tf.reduce_sum(input_tensor=concentration_diff * digamma_diff, axis=-1) - tf.math.lbeta(d1.concentration) + tf.math.lbeta(d2.concentration))"
136,"def _extract_attribute(self, attribute): self.get_attribute(attribute) self.get_attribute(attribute) self.get_attribute(attribute) self.get_attribute(attribute) self.get_attribute(attribute) self.get_attribute(attribute) self.get_attribute(attribute) self.get_attribute(attribute) self.get_attribute(attribute) self.get_attribute) self.get_attribute(attribute) self.get_attribute(attribute) self.get_attribute) self.get_attribute(attribute) self.get_attribute(attribute) self.get_attribute(attribute) self.get_attribute(attribute) self.get_attribute(attribute) self.get_attribute(attribute) self.get_attribute(attribute) self.get_attribute(attribu","def _process_template_attribute(self, template_attribute): attributes =  if len(template_attribute.names) > 0: raise exceptions.ItemNotFound( ""Attribute templates are not supported."" ) for attribute in template_attribute.attributes: name = attribute.attribute_name.value if not self._attribute_policy.is_attribute_supported(name): raise exceptions.InvalidField( ""The 0 attribute is unsupported."".format(name) ) if self._attribute_policy.is_attribute_multivalued(name): values = attributes.get(name, list()) if (not attribute.attribute_index) and len(values) > 0: raise exceptions.InvalidField( ""Attribute index missing from multivalued attribute."" ) values.append(attribute.attribute_value) attributes.update([(name, values)]) else: if attribute.attribute_index: if attribute.attribute_index.value!= 0: raise exceptions.InvalidField( ""Non-zero attribute index found for "" ""single-valued attribute."" ) value = attributes.get(name, None) if value: raise exceptions.IndexOutOfBounds( ""Cannot set multiple instances of the "" ""0 attribute."".format(name) ) else: attributes.update([(name, attribute.attribute_value)]) return attributes"
137,"def get_container(self, type, value): if self.get_container(self.get_container): self.get_container(self.get_container): self.get_container(self.get_container) self.get_container(self.get_container) self.get_container(self.get_container) self.get_container(self.get_container) self.get_container(self.get_container) self.get_container(self.get_container) self.get_container(self.get_container) self.get_container_container_container_container_container_container_container_container_container_container_container_container_container_container_container_container_container_container_container_container_container_container_container_container_container_container_container_container_container_container_container_container_container_","def list_containers(self): results = [] for image in self._bucket.list_blobs(): if image.metadata is not None: if ""type"" in image.metadata: if image.metadata['type'] == ""container"": results.append(image) if len(results) == 0: bot.info(""No containers found, based on metadata type:container"") return results"
138,"def reionize(self, mol, mol, mol): self.reionize(mol) self.reionize(mol) self.reionize(mol) self.reionize(mol) self.reionize(mol) self.reionize(mol) self.reionize(mol) self.reionize(mol) self.reionize(mol) self.reionize(mol) self.reionize(mol) self.reionize(mol) self.reionize(mol) self.reionize(mol) self.reionize(mol) self.reionize(mol) self.reionize(mol) self.reionize(mol) self.reionize(mol) self.reionize(mol) self.reionize(mol) self.reionize(mol) self.reionize(mol) self.reionize(mol) self.reionize(mol) self.reionize(mol) self.reionize(mol) self.reionize(mol) self.reionize(mol) self.reionize(mol) self.reionize(mol)","def reionize(self, mol): log.debug('Running Reionizer') start_charge = Chem.GetFormalCharge(mol) for cc in self.charge_corrections: for match in mol.GetSubstructMatches(cc.smarts): atom = mol.GetAtomWithIdx(match[0]) log.info('Applying charge correction %s (%s %+d)', cc.name, atom.GetSymbol(), cc.charge) atom.SetFormalCharge(cc.charge) current_charge = Chem.GetFormalCharge(mol) charge_diff = Chem.GetFormalCharge(mol) - start_charge if not current_charge == 0: while charge_diff > 0: ppos, poccur = self._strongest_protonated(mol) if ppos is None: break log.info('Ionizing %s to balance previous charge corrections', self.acid_base_pairs[ppos].name) patom = mol.GetAtomWithIdx(poccur[-1]) patom.SetFormalCharge(patom.GetFormalCharge() - 1) if patom.GetNumExplicitHs() > 0: patom.SetNumExplicitHs(patom.GetNumExplicitHs() - 1) patom.UpdatePropertyCache() charge_diff -= 1 already_moved = set() while True: ppos, poccur = self._strongest_protonated(mol) ipos, ioccur = self._weakest_ionized(mol) if ioccur and poccur and ppos  ipos: if poccur[-1] == ioccur[-1]: log.warning('Aborted reionization due to unexpected situation') break key = tuple(sorted([poccur[-1],"
139,def return true if function does not return true if function does not return true if function does not return true if function does not return true if function does not return true if function does not return true if function does not return true if function does not return true if function does not return true if function does not return true if function does not return true if function does not return true if function does not return true if function does not return true if function does not return true if function does,"def is_error(node: astroid.node_classes.NodeNG) -> bool: for child_node in node.get_children(): if isinstance(child_node, astroid.Raise): return True return False"
140,"def find_streams_for_vk.com(vk.com, vk.com, vk.com, vk.com, vk.com, vk.com, vk.com, vk.com, vk.com, vk.com, vk.com, vk.com, vk.com, vk.com, vk.com, vk.com, vk.com, vk.com, vk.com, vk.com, vk.com)","def _get_streams(self): self.session.http.headers.update('User-Agent': useragents.IPHONE_6) url = self.follow_vk_redirect(self.url) m = self._url_re.match(url) if not m: log.error('URL is not compatible: 0'.format(url)) return video_id = m.group('video_id') log.debug('video ID: 0'.format(video_id)) params =  'act':'show_inline', 'al': '1', 'video': video_id,  res = self.session.http.post(self.API_URL, params=params) for _i in itertags(res.text, 'iframe'): if _i.attributes.get('src'): iframe_url = update_scheme(self.url, _i.attributes['src']) log.debug('Found iframe: 0'.format(iframe_url)) for s in self.session.streams(iframe_url).items(): yield s for _i in itertags(res.text,'source'): if _i.attributes.get('type') == 'application/vnd.apple.mpegurl': video_url = _i.attributes['src'] if video_url.startswith('https://vk.com/'): continue streams = HLSStream.parse_variant_playlist(self.session, video_url) if not streams: yield 'live', HLSStream(self.session, video_url) else: for s in streams.items(): yield s elif _i.attributes.get('type') ="
141,"def calculate_mdr_mutual_information(self, X, Y, labels): if self.mdr_mutual_information = self.mdr_mutual_information = self.mdr_mutual_information = self.mdr_mutual_information = self.mdr_mutual_information = self.mdr_mutual_information = self.mdr_mutual_information = self.mdr_mutual_information = self.mdr_mutual_information = self.mdr_mutual_information = self.mdr_mutual_information = self.mdr_mutual_information","def mdr_mutual_information(X, Y, labels, base=2): return mutual_information(_mdr_predict(X, Y, labels), labels, base=base)"
142,def gate_sdg(sdg): gate_sdg = gate_sdg = gate_sdg = gate_sdg = gate_sdg = gate_sdg = gate_sdg = gate_sdg = gate_sdg = gate_sdg = gate_sdg,"def _define(self): definition = [] q = QuantumRegister(1, ""q"") rule = [ (U1Gate(-pi/2), [q[0]], []) ] for inst in rule: definition.append(inst) self.definition = definition"
143,"def find_stack_frame(self, stack_frame): self.stack_frame(self.stack_frame) self.stack_frame(self.stack_frame) self.stack_frame(self.stack_frame) self.stack_frame(self.stack_frame) self.stack_frame(self.stack_frame) self.stack_frame(self.stack_frame) self.stack_frame(self.stack_frame) self.stack_frame(self.stack_frame) self.stack_frame(self.stack_frame) self.stack_frame(self.stack_frame) self.stack_frame(self.stack_frame) self.stack_frame(self.stack_frame) self.stack_frame(self.stack_frame) self.stack_frame(self.stack_frame) self.stack_frame(self.stack_frame) self.stack_frame(self.stack_frame) self.stack_frame(self.stack_frame) self.stack_frame(self.","def findCaller(self, stack_info=False): f = logging.currentframe() if f is not None: f = f.f_back rv = ""(unknown file)"", 0, ""(unknown function)"" while hasattr(f, ""f_code""): co = f.f_code filename = os.path.normcase(co.co_filename) if filename == logging._srcfile or filename == self._srcfile: f = f.f_back continue rv = (co.co_filename, f.f_lineno, co.co_name) if stack_info: sio = io.StringIO() sio.write('Stack (most recent call last):n') traceback.print_stack(f, file=sio) sinfo = sio.getvalue() if sinfo[-1] == 'n': sinfo = sinfo[:-1] sio.close() break return rv"
144,def play_with_full_bag_of_tiles(self): if self.play_with_full_bag_of_tiles(self.play_with_full_bag_of_tiles): self.play_with_full_bag_of_tiles(self.play_with_full_bag_of_tiles): self.play_with_full_bag_of_tiles(self.play_with_full_bag_of_tiles(self.play_with_full_bag_of_tiles),"def valid_scrabble_word(word): letters_in_bag =  ""a"": 9, ""b"": 2, ""c"": 2, ""d"": 4, ""e"": 12, ""f"": 2, ""g"": 3, ""h"": 2, ""i"": 9, ""j"": 1, ""k"": 1, ""l"": 4, ""m"": 2, ""n"": 6, ""o"": 8, ""p"": 2, ""q"": 1, ""r"": 6, ""s"": 4, ""t"": 6, ""u"": 4, ""v"": 2, ""w"": 2, ""x"": 1, ""y"": 2, ""z"": 1, ""_"": 2,  for letter in word: if letter == ""?"": continue try: letters_in_bag[letter] -= 1 except KeyError: return False if letters_in_bag[letter]  0: letters_in_bag[""_""] -= 1 if letters_in_bag[""_""]  0: return False return True"
145,"def extract(self, arguments): self.get(self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get(self.get) self.get(self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.",def get_arguments(self): ApiCli.get_arguments(self) if self.args.pluginName is not None: self.pluginName = self.args.pluginName
146,"def cmd_line_args(self, args): args = self.args(args) if self.args(args)","def populate_requirement_set(requirement_set, args, options, finder, session, name, wheel_cache): for req in args: requirement_set.add_requirement( InstallRequirement.from_line( req, None, isolated=options.isolated_mode, wheel_cache=wheel_cache ) ) for req in options.editables: requirement_set.add_requirement( InstallRequirement.from_editable( req, default_vcs=options.default_vcs, isolated=options.isolated_mode, wheel_cache=wheel_cache ) ) found_req_in_file = False for filename in options.requirements: for req in parse_requirements( filename, finder=finder, options=options, session=session, wheel_cache=wheel_cache): found_req_in_file = True requirement_set.add_requirement(req) if not (args or options.editables or found_req_in_file): opts = 'name': name if options.find_links: msg = ('You must give at least one requirement to''%(name)s (maybe you meant ""pip %(name)s''%(links)s""?)' % dict(opts, links=' '.join(options.find_links))) else: msg = ('You must give at least one requirement''to %(name)s (see ""pip help %(name)s"")' % opts) logger.warning(msg)"
147,"def _finally_invoke(self, 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally', 'finally","def _atexit(self): self.log.debug(""Application._atexit"") if self._atexit_func: self._atexit_func(self)"
148,"def disable_report(self, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id) id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, i","def disable_report(self, reportid): reportid = reportid.upper() self._reports_state[reportid] = False"
149,def create_folder(self): if self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name = self.folder_name =,"def create_folder(self, folder): folder_metadata =  'name': os.path.basename(folder),'mimeType': 'application/vnd.google-apps.folder'  created = self._service.files().create(body=folder_metadata, fields='id').execute() return created"
150,"def raise_ServerException(self, scheduled_queries, dicts): if self.logentries_api.exceptions.ServerException = self.logentries_api.exceptions.ServerException = self.logentries_api.exceptions.ServerException = self.logentries_api.exceptions.ServerException = self.logentries_api.exceptions.ServerException = self.logentries_api.exceptions.ServerException",def list_scheduled_queries(self): url = 'https://logentries.com/rest/account_id/api/scheduled_queries/'.format( account_id=self.account_id) return self._api_get(url=url).get('scheduled_searches')
151,def write_dependencies_as_dot(self): if self.dot(self.dot): self.dot(self.dot) self.dot(self.dot) self.dot(self.dot) self.dot(self.dot) self.dot(self.dot) self.dot(self.dot) self.dot(self.dot) self.dot) self.dot) self.dot(self.dot) self.dot(self.dot) self.dot(self.dot) self.dot) self.dot) self.dot(self.dot) self.dot) self.dot(self.dot) self.dot) self.dot(self.dot) self.dot(self.dot) self.dot) self.dot(self.dot) self.dot) self.dot(self.dot) self.dot(self.dot) self.dot(self.dot) self.d,"def _dependencies_graph(filename, dep_info): done =  printer = DotBackend(filename[:-4], rankdir=""LR"") printer.emit('URL=""."" node[shape=""box""]') for modname, dependencies in sorted(dep_info.items()): done[modname] = 1 printer.emit_node(modname) for depmodname in dependencies: if depmodname not in done: done[depmodname] = 1 printer.emit_node(depmodname) for depmodname, dependencies in sorted(dep_info.items()): for modname in dependencies: printer.emit_edge(modname, depmodname) printer.generate(filename)"
152,"def fast_access(self, fast_access, copy, fast_access, fast_access, copy): if fast_access is true and copy false: if fast_access is false: if copy is false: if fast_access is true: if copy is false: if fast_access is true: if copy is false: if fast_access is true: if copy is false: if fast_access is true: if copy is false: if copy is false: if fast_access returns","def f_get_results(self, fast_access=False, copy=True): return self._return_item_dictionary(self._results, fast_access, copy)"
153,"def get_public_key(self, public_key): self.get_public_key(public_key) self.get_public_key(public_key) self.get_public_key(public_key) self.get_public_key(public_key) self.get_public_key(public_key) self.get_public_key(public_key) self.get_public_key(public_key) self.get_public_key(public_key) self.get_public_key","def get_pubkey(self): pkey = PKey.__new__(PKey) pkey._pkey = _lib.X509_REQ_get_pubkey(self._req) _openssl_assert(pkey._pkey!= _ffi.NULL) pkey._pkey = _ffi.gc(pkey._pkey, _lib.EVP_PKEY_free) pkey._only_public = True return pkey"
154,def interpolation(self): self.interpolation(self.interpolation) self.interpolation(self.interpolation) self.interpolation(self.interpolation) self.interpolation(self.interpolation) self.interpolation(self.interpolation) self.interpolation(self.interpolation) self.interpolation(self.interpolation) self.interpolation(self.interpolation) self.interpolation(self.interpolation) self.interpolation(self.interpolation) self.interpolation(self.interpolation) self.interpolation(self.interpolation) self.interpolation(self.interpolation) self.interpolation(self.interpolation) self.interpolation(self.interpolation) self.interpolation) self.interpolation(self.interpolation) self.interpolation) self.interpolation(self.interpolation) self.interpolation(self.interpolation) self.interpolation(self.interpolation) self.interpolation(self.interpolation) self.interpolation(self.interpolation) self.interpolation(self.interpolation) self.interpolation(self.interpolation) self.,"def _interp_regular_1d_grid_impl(x, x_ref_min, x_ref_max, y_ref, axis=-1, batch_y_ref=False, fill_value='constant_extension', fill_value_below=None, fill_value_above=None, grid_regularizing_transform=None, name=None): with tf.compat.v1.name_scope( name, values=[ x, x_ref_min, x_ref_max, y_ref, axis, fill_value, fill_value_below, fill_value_above ]): allowed_fv_st = ('constant_extension', 'extrapolate') for fv in (fill_value, fill_value_below, fill_value_above): if isinstance(fv, str) and fv not in allowed_fv_st: raise ValueError( 'A fill value () was not an allowed string ()'.format( fv, allowed_fv_st)) need_separate_fills = ( fill_value_above is not None or fill_value_below is not None or fill_value == 'extrapolate' ) if need_separate_fills and fill_value_above is None: fill_value_above = fill_value if need_separate_fills and fill_value_below is None: fill_value_below = fill_value dtype = dtype_util.common_dtype([x, x_ref_min, x_ref_max, y_ref], preferred_dtype=tf.float32) x = tf.convert_to_tensor(value=x, name='x', dtype=dtype) x_ref_min = tf.convert_to_tensor( value=x_ref"
155,def get_directory_plugin_names(ida_settings.get_directory_plugin_names.get_directory_plugin_names.get_directory_plugin_names.get_directory_plugin_names.get_directory_plugin_names.get_directory_plugin_names.get_directory_plugin_names.get_directory_plugin_names.get_directory_plugin_names.get_directory_plugin_names.get_directory_plugin_names.get_directory_plugin_names.get_directory_plugin_names.get_directory_plugin_names.get_directory_plugin_names.get_directory_plugin_names.get_names.get_names.get_names.get_names.get_names.get_names.get_names.get_names.get_names.get_names.get_names.get_names.get_names.get_names.get_names.get_names.get_names.get,"def get_directory_plugin_names(config_directory=None): ensure_ida_loaded() return QtCore.QSettings(get_directory_config_path(directory=config_directory), QtCore.QSettings.IniFormat).childGroups()[:]"
156,"def decorator(self, codes): if self.getToken(self.getToken) self.getToken(self.getToken) self.getToken(self.getToken) self.getToken(self.getToken) self.getToken(self.getToken) self.getToken(self.getToken) self.getToken(self.getToken) self.getToken(self.getToken) self.getToken(self.getToken) self.getToken(self.getToken) self.getToken(self.getToken) self.getToken(self.getToken) self.getToken(self.getToken) self.getToken(self.getToken) self.getToken(self.getToken) self.getToken(self.getToken) self.getToken(self.getToken) self.getToken(self.getToken) self.getToken(self.getToken) self.getToken(self.getToken) self.getToken(self.getToken) self.getToken(self.getToken) self.getToken(self.getToken) self.getToken(self.getToken","def handle(*codes, **kwargs): regToken = kwargs.get(""regToken"", False) subscribe = kwargs.get(""subscribe"") def decorator(fn): @functools.wraps(fn) def wrapper(self, *args, **kwargs): try: return fn(self, *args, **kwargs) except SkypeApiException as e: if isinstance(e.args[1], requests.Response) and e.args[1].status_code in codes: conn = self if isinstance(self, SkypeConnection) else self.conn if regToken: conn.getRegToken() if subscribe: conn.endpoints[subscribe].subscribe() return fn(self, *args, **kwargs) raise return wrapper return decorator"
157,"def load_case(self, vcf_files, bam_files, vcf_files, vcf_files, vcf_files, bam_files, vcf_files, vcf_files, vcf_files, vcf_files, vcf_files, vcf_files, vcf_files, vcf_files, vcf_files, vcf_files, vcf_files, vcf_files)","def case(context, vcf, vcf_sv, vcf_cancer, vcf_str, owner, ped, update, config, no_variants, peddy_ped, peddy_sex, peddy_check): adapter = context.obj['adapter'] if config is None and ped is None: LOG.warning(""Please provide either scout config or ped file"") context.abort() config_raw = yaml.load(config) if config else  try: config_data = parse_case_data( config=config_raw, ped=ped, owner=owner, vcf_snv=vcf, vcf_sv=vcf_sv, vcf_str=vcf_str, vcf_cancer=vcf_cancer, peddy_ped=peddy_ped, peddy_sex=peddy_sex, peddy_check=peddy_check ) except SyntaxError as err: LOG.warning(err) context.abort() LOG.info(""Use family %s"" % config_data['family']) try: case_obj = adapter.load_case(config_data, update) except Exception as err: LOG.error(""Something went wrong during loading"") LOG.warning(err) context.abort()"
158,"def get_checksum(self, address, checksum, checksum): return self.get_checksum(self, address, checksum)","def with_valid_checksum(self): return Address( trytes=self.address + self._generate_checksum(), balance=self.balance, key_index=self.key_index, security_level=self.security_level, )"
159,def load_weights(self): if self.load_weights(self.load_weights): self.load_weights(self.load_weights) self.load_weights(self.load_weights) self.load_weights(self.load_weights) self.load_weights(self.load_weights) self.load_weights(self.load_weights) self.load_weights(self.load_weights) self.load_weights(self.load_weights) self.load_weights) self.load_weights(self.load_weights) self.load_weights) self.load_weights(self.load_weights) self.load_weights) self.load_weights(self.load_weights) self.load_weights) self.load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_load_,"def _pretrained_initializer(varname, weight_file, embedding_weight_file=None): weight_name_map =  for i in range(2): for j in range(8): root = 'RNN_/RNN/MultiRNNCell/Cell'.format(i, j) weight_name_map[root + '/rnn/lstm_cell/kernel'] =  root + '/LSTMCell/W_0' weight_name_map[root + '/rnn/lstm_cell/bias'] =  root + '/LSTMCell/B' weight_name_map[root + '/rnn/lstm_cell/projection/kernel'] =  root + '/LSTMCell/W_P_0' varname_in_file = varname[5:] if varname_in_file.startswith('RNN'): varname_in_file = weight_name_map[varname_in_file] if varname_in_file == 'embedding': with h5py.File(embedding_weight_file, 'r') as fin: embed_weights = fin[varname_in_file][...] weights = np.zeros( (embed_weights.shape[0] + 1, embed_weights.shape[1]), dtype=DTYPE ) weights[1:, :] = embed_weights else: with h5py.File(weight_file, 'r') as fin: if varname_in_file == 'char_embed': char_embed_weights = fin[varname_in_file][...] weights = np.zeros( (char_embed_weights.shape[0] + 1, char_embed_weights.shape[1]), dtype"
160,def _fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_fetch_summary_f,def __fetch_summary(self): raw_summary = self.client.summary() summary = json.loads(raw_summary) summary['fetched_on'] = str(datetime_utcnow()) yield summary
161,def _get_entity(self): if self.get_entity(self.get_entity): self.get_entity(self.get_entity): self.get_entity(self.get_entity): self.get_entity(self.get_entity): self.get_entity(self.get_entity),"def forget(self, obj): self._init() self.known.pop(obj._pk, None) self.wknown.pop(obj._pk, None)"
162,"def download(path, str, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack, unpack,","def download(path='.', url=None, unpack=False): if url is None: url = 'https://github.com/neurosynth/neurosynth-data/blob/master/current_data.tar.gz?raw=true' if os.path.exists(path) and os.path.isdir(path): basename = os.path.basename(url).split('?')[0] filename = os.path.join(path, basename) else: filename = path f = open(filename, 'wb') u = urlopen(url) file_size = int(u.headers[""Content-Length""][0]) print(""Downloading the latest Neurosynth files: 0 bytes: 1"".format( url, file_size)) bytes_dl = 0 block_size = 8192 while True: buffer = u.read(block_size) if not buffer: break bytes_dl += len(buffer) f.write(buffer) p = float(bytes_dl) / file_size status = r""0 [1:.2%]"".format(bytes_dl, p) status = status + chr(8) * (len(status) + 1) sys.stdout.write(status) f.close() if unpack: import tarfile tarfile.open(filename, 'r:gz').extractall(os.path.dirname(filename))"
163,def register_dispatchers_for_streams_for_streams_for_streams_for_streams_for_streams_for_streams_for_streams_for_streams_for_streams_for_streams_for_streams_for_streams_for_streams_for_streams_for_streams_for_streams_for_streams_for_streams_for_streams_for_streams_for_streams_for_stream_dispatchers_for_stream_dispatchers_dispatchers_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatcher_dispatch,"def start(self): self.shell.exit_now = False if self.control_stream: self.control_stream.on_recv(self.dispatch_control, copy=False) def make_dispatcher(stream): def dispatcher(msg): return self.dispatch_shell(stream, msg) return dispatcher for s in self.shell_streams: s.on_recv(make_dispatcher(s), copy=False)"
164,"def add_extra_conditions(self, othertable.id, extra_conditions): self.add_extra_conditions(self.add_extra_conditions) self.add_extra_conditions(self.add_extra_conditions) self.add_extra_conditions(self.add_extra_conditions) self.add_extra_conditions(self.add_extra_conditions) self.add_extra_conditions(self.add_extra_conditions) self.add_extra_conditions(self.add_extra_conditions) self.add_extra_conditions(self.add_conditions) self.add_conditions(self.add_conditions) self.add_conditions(self.add_conditions) self.add_conditions_conditions_conditions_conditions_conditions_conditions_conditions_conditions_conditions_conditions_conditions_conditions_conditions_conditions_conditions_conditions_conditions_conditions_conditions_conditions_conditions_conditions_conditions_conditions_conditions_conditions_conditions_conditions_conditions_conditions_conditions_conditions_conditions_conditions_conditions_conditions_conditions_conditions_conditions_conditions_","def add_join_conditions(self, conditions: Dict[str, Any]) -> None: alias = self.get_initial_alias() opts = self.get_meta() for name, value in conditions.items(): parts = name.split(LOOKUP_SEP) join_info = self.setup_joins(parts, opts, alias, allow_many=True) self.trim_joins(join_info[1], join_info[3], join_info[4]) target_table = join_info[3][-1] field = join_info[1][-1] join = self.alias_map.get(target_table) if not join: raise SuspiciousOperation(( 'Cannot add an extra join condition for ""%s"", there's no''existing join to add it to.' ) % target_table) if not isinstance(join, ConditionalJoin): self.alias_map[target_table] = ConditionalJoin.from_join(join) join = self.alias_map[target_table] join.add_condition(field, value)"
165,"def copy_file(self, folder): self.copy_file(object): self.copy_file(object): self.copy_file(object): self.copy_file(object): self.copy_file(object): self.copy_file(object)","def copy(self, obj_id, folder_id, move=False): return self( obj_id, method='copy' if not move else'move', data=dict(destination=folder_id), auth_header=True )"
166,"def compute_roughe_n(self, reference_sentences, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, roughe_n, roughe_n, roughe_n, roughe_n, roughe_n, roughe_n, roughe_n, roughe_n, roughe_n)","def rouge_n(evaluated_sentences, reference_sentences, n=2): if len(evaluated_sentences) = 0 or len(reference_sentences) = 0: raise ValueError(""Collections must contain at least 1 sentence."") evaluated_ngrams = _get_word_ngrams(n, evaluated_sentences) reference_ngrams = _get_word_ngrams(n, reference_sentences) reference_count = len(reference_ngrams) evaluated_count = len(evaluated_ngrams) overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams) overlapping_count = len(overlapping_ngrams) return f_r_p_rouge_n(evaluated_count, reference_count, overlapping_count)"
167,"def write_xml(self, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit) Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit, Xunit,","def report(self, stream): self.stats['encoding'] = self.encoding self.stats['total'] = (self.stats['errors'] + self.stats['failures'] + self.stats['passes'] + self.stats['skipped']) self.error_report_file.write( u'?xml version=""1.0"" encoding=""%(encoding)s""?>' u'testsuite name=""nosetests"" tests=""%(total)d""'u'errors=""%(errors)d"" failures=""%(failures)d""'u'skip=""%(skipped)d"">' % self.stats) self.error_report_file.write(u''.join([self._forceUnicode(e) for e in self.errorlist])) self.error_report_file.write(u'/testsuite>') self.error_report_file.close() if self.config.verbosity > 1: stream.writeln(""-"" * 70) stream.writeln(""XML: %s"" % self.error_report_file.name)"
168,"def process_word(self, data, tags): if self.process_word = self.process_word = self.process_word = self.process_word = self.process_word = self.process_word = self.process_word = self.process_word = self.process_word = self.process_word = self.process_word = self.process_word = self.process_word = self.process_word = self.process_word = self.process_word = self.process_word = self.process_word = self.process_word = self.process_word","def preprocess_data(data: List[Tuple[List[str], List[str]]], to_lower: bool = True, append_case: str = ""first"") -> List[Tuple[List[Tuple[str]], List[str]]]: new_data = [] for words, tags in data: new_words = [process_word(word, to_lower=to_lower, append_case=append_case) for word in words] new_tags = tags new_data.append((new_words, new_tags)) return new_data"
169,"def get_client(self, client): self.get_client(client): self.get_client(client): self.get_client(client): self.get_client(client)","def get_client(self): client = None try: client = self.clients.get(block=False) except queue.Empty: pass if not client: self.client_id += 1 kwargs = dict(self.kwargs) kwargs['verbose_id'] = kwargs.get('verbose_id', '') + str(self.client_id) client = self.client_class(*self.args, **kwargs) return client"
170,"def get_constant_load_parts(self, constant_load_parts): self.get_constant_load_parts() self.get_constant_load_parts() self.get_constant_load_parts() self.get_constant_load_parts() self.get_constant_load_parts() self.get_constant_load_parts() self.get_constant_load_parts() self.get_constant_load_parts(self.get_constant_load_parts) self.get_constant_load_parts() self.get_constant_load_parts() self.get_constant_load_parts() self.get_constant_load_parts() self.get_constant_load_parts() self.get_constant_load_parts() self.get_constant_load_parts() self.get_constant_load_parts() self.get_constant_load_parts() self.get_constant_load_parts() self.get_constant_load_parts() self.get_constant_load_parts_parts_parts_parts_parts_parts_parts_parts_parts_part","def get_float_rps_list(self): int_rps = range(int(self.minrps), int(self.maxrps) + 1) step_duration = float(self.duration) / len(int_rps) rps_list = [(rps, int(step_duration)) for rps in int_rps] return rps_list"
171,"def reject_sample_shape(self, dim, sample_shape, seed, weights): if self.sample_shape = self.sample_shape = self.sample_shape = self.sample_shape = self.sample_shape = self.sample_shape = self.sample_shape = self.sample_shape = self.sample_shape","def correlation_matrix_volume_rejection_samples( det_bounds, dim, sample_shape, dtype, seed): with tf.compat.v1.name_scope(""rejection_sampler""): rej_proposals = _uniform_correlation_like_matrix( dim, sample_shape, dtype, seed=seed) rej_proposal_volume = 2. ** (dim * (dim - 1) / 2.) rej_weights = rej_proposal_volume * _psd_mask( rej_proposals) * _det_large_enough_mask(rej_proposals, det_bounds) return rej_weights, rej_proposal_volume"
172,"def get_session_number(self, session_number): self.get_session_number(session_number) self.get_session_number(session_number) self.get_session_number(session_number) self.get_session_number(session_number) self.get_session_number(session_number) self.get_session_number(session_number) self.get_session_number(session_number) self.get_session_number) self.get_session_number(session_number) self.get_session_number(session_number) self.get_session_number) self.get_session_number(session_number) self.get_session_number) self.get_session_number(session_number) self.get_session_number(session_number) self.get_session_number(session_number) self.get_session_number(session_number) self.get_session_number(session_number) self.get_session_number(session_number) self.get_session_number(session_number) self.get_session_number(session_number) self.get",def _init_db(self): %self.table) self._db.commit()
173,"def build_html_content(self, js): js = self.js(js) self.js(js) self.js(js) self.js(js) self.js(js) self.js(js) self.js(js) self.js(js) self.js(js) self.js(js) self.js(js) self.js(js) self.js = self.js = self.js = self.js = self.js = self.js = self.js = self.js = self.js = self.js = self.js = self.js = self.js = self.js = self.js = self.js = self.js = self.js = self.js = self.js = self.js = self.js = self.js = self.js = self.js = self.js = self.js = self.js = self.js = self.js = self.js = self.js = self.js = self.js = self.js = self.js = self.js",def buildcontent(self): self.buildcontainer() self.buildjschart() self.htmlcontent = self.template_chart_nvd3.render(chart=self)
174,"def create_notebook(self, id, name, id, worksheets): id = self.notebook(name, id, worksheets) self.notebook(name, id, worksheets) self.notebook(name, id, worksheets) self.notebook(name, id, worksheets) self.notebook(name, id, worksheets) self.notebook(name, id, worksheets) self.notebook(name, id) self.notebook(name, id) self.notebook(name, id) self.notebook(name, id) self.notebook(name, id) self.notebook(name, id) self.notebook(name, id) self.notebook(name, id) self.notebook(name, id) self.notebook(name, id) self.notebook(name, id) self.notebook(name, id) self.notebook(name, id) self.notebook(name, id) self.notebook(name, id) self.notebook(name, id) self.notebook(name, id) self","def new_notebook(name=None, metadata=None, worksheets=None): nb = NotebookNode() nb.nbformat = nbformat nb.nbformat_minor = nbformat_minor if worksheets is None: nb.worksheets = [] else: nb.worksheets = list(worksheets) if metadata is None: nb.metadata = new_metadata() else: nb.metadata = NotebookNode(metadata) if name is not None: nb.metadata.name = unicode(name) return nb"
175,"def calculate_log_loss(y_true, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted)","def sk_log_loss(y_true: Union[List[List[float]], List[List[int]], np.ndarray], y_predicted: Union[List[List[float]], List[List[int]], np.ndarray]) -> float: return log_loss(y_true, y_predicted)"
176,"def _voting(self, voting): if self.voting(voting): self.voting(voting): self.voting(voting)","def roy_voting(self): url = '/awards/awards_.html'.format(sportsref.nba.BASE_URL, self.yr) doc = pq(sportsref.utils.get_html(url)) table = doc('table#roy') df = sportsref.utils.parse_table(table) return df"
177,"def connect_to_Riemann(self, Riemann, Riemann, Riemann, Riemann, Riemann, Riemann, Riemann, Riemann, Riemann, Riemann, Riemann, Riemann, Riemann, Riemann, Riemann, Riemann, Riemann, Riemann, Riemann, Riemann, Riemann, Riemann, Riemann, Riemann, Riemann, Riemann, Riemann, Riemann, Riemann, Riemann, Riemann)","def createClient(self): server = self.config.get('server', 'localhost') port = self.config.get('port', 5555) failover = self.config.get('failover', False) self.factory = riemann.RiemannClientFactory(server, failover=failover) if failover: initial = random.choice(server) else: initial = server log.msg('Connecting to Riemann on %s:%s' % (initial, port)) if self.tls: if SSL: self.connector = reactor.connectSSL(initial, port, self.factory, ClientTLSContext(self.key, self.cert)) else: log.msg('[FATAL] SSL support not available!' 'Please install PyOpenSSL. Exiting now') reactor.stop() else: self.connector = reactor.connectTCP(initial, port, self.factory) d = defer.Deferred() def cb(): if hasattr(self.factory, 'proto') and self.factory.proto: self.t.start(self.inter) d.callback(None) else: reactor.callLater(0.01, cb) cb() return d"
178,"def get_exception_info(self, traceback, traceback, traceback, traceback, traceback, traceback, traceback, traceback, traceback, traceback, traceback, traceback, traceback, traceback, traceback, traceback, traceback, traceback, traceback, traceback, traceback, traceback, traceback, traceback, traceback, traceback, traceback, traceback, traceback, traceback, traceback, traceback, traceback, traceback, traceback, traceback, traceback, traceback, traceback, traceback, traceback,","def get_current_traceback(ignore_system_exceptions=False, show_hidden_frames=False, skip=0): exc_type, exc_value, tb = sys.exc_info() if ignore_system_exceptions and exc_type in system_exceptions: raise for x in range_type(skip): if tb.tb_next is None: break tb = tb.tb_next tb = Traceback(exc_type, exc_value, tb) if not show_hidden_frames: tb.filter_hidden_frames() return tb"
179,"def start_pool_single_run(self, pool_single_run): if self.single_run(self.single_run): self.single_run(self.single_run): self.single_run(self.single_run): self.single_run(self.single_run)",def _pool_single_run(kwargs): wrap_mode = kwargs['wrap_mode'] traj = kwargs['traj'] traj.v_storage_service = _pool_single_run.storage_service if wrap_mode == pypetconstants.WRAP_MODE_LOCAL: traj.v_storage_service.free_references() return _sigint_handling_single_run(kwargs)
180,"def add_ext(filepath, ext, check_if_exists, ext): if filepath doesn't have ext, add_ext(filepath, ext, check_if_exists, ext, check_if_exists, ext, ext, ext, check_if_exists, ext, ext, ext, ext, ext, ext, ext, ext, ext, ext, filepath, ext, ext, check_if_exists, check_if_exists, ext, check_if_exists, ext, ext, filepath, ext, check_if_exists, check_if_exists, ext, filepath, ext, filepath, ext, filepath, ext, filepath, ext, filepath, ext, filepath, ext, filepath, ext, filepath, ext, ext, filepath, ext, filepath, ext, ext, ext, ext, ext, ext, ext, ext, ext, ext, ext, ext, ext, ext, ext, ext, ext, ext, ext, ext, ext, ext, ext,","def add_extension_if_needed(filepath, ext, check_if_exists=False): if not filepath.endswith(ext): filepath += ext if check_if_exists: if not os.path.exists(filepath): err = 'File not found:'+ filepath log.error(err) raise IOError(err) return filepath"
181,"def split_symbol(self, symbol): if self.symbol(self.symbol): self.symbol(self.symbol) self.symbol(self.symbol) self.symbol(self.symbol) self.symbol(self.symbol) self.symbol(self.symbol) self.symbol(self.symbol) self.symbol(self.symbol) self.symbol(self.symbol) self.symbol) self.symbol) self.symbol(self.symbol) self.symbol) self.symbol(self.symbol) self.symbol) self.symbol(self.symbol) self.symbol) self.symbol(self.symbol) self.symbol) self.symbol(self.symbol) self.symbol(self.symbol) self.symbol) self.symbol(self.symbol) self.symbol) self.","def parse(self, symbol: str) -> (str, str): symbol_parts = symbol.split("":"") namespace = None mnemonic = symbol if len(symbol_parts) > 1: namespace = symbol_parts[0] mnemonic = symbol_parts[1] self.namespace = namespace self.mnemonic = mnemonic return namespace, mnemonic"
182,"def load_table(self, filepath, delimiter, field_dict): self.load_table(filepath, delimiter): self.load_table(filepath) self.load_table(filepath) self.load_table(filepath) self.load_table(filepath) self.load_table(filepath) self.load_table(filepath) self.load_table(filepath) self.load_table(filepath) self.load_table(filepath) self.load_table(filepath) self.load_table(filepath) self.load_table(file_dict) self.load_table(file_dict) self.load_table(file_dict) self.load_table(file_dict) self.load_table(file_dict) self.load_table(file_dict) self.load_table(file_dict) self.load_table(file_dict) self.load_table(file_dict) self.load_table(file_dict) self.load_table(file_dict) self.load_table(file_dict) self.load_table(file_dict) self.load_table(file_dict) self.load_table(file_dict) self.load_table(file_dict) self.load_table(","def load_file( self, filepath, table, delimiter="","", field_dict=None, create=True, overwrite=True, partition=None, recreate=False, tblproperties=None): hql = '' if recreate: hql += ""DROP TABLE IF EXISTS table;n"".format(table=table) if create or recreate: if field_dict is None: raise ValueError(""Must provide a field dict when creating a table"") fields = "",n "".join( [k +'' + v for k, v in field_dict.items()]) hql += ""CREATE TABLE IF NOT EXISTS table (nfields)n"".format( table=table, fields=fields) if partition: pfields = "",n "".join( [p + "" STRING"" for p in partition]) hql += ""PARTITIONED BY (pfields)n"".format(pfields=pfields) hql += ""ROW FORMAT DELIMITEDn"" hql += ""FIELDS TERMINATED BY 'delimiter'n"".format(delimiter=delimiter) hql += ""STORED AS textfilen"" if tblproperties is not None: tprops = "", "".join( [""'0'='1'"".format(k, v) for k, v in tblproperties.items()]) hql += ""TBLPROPERTIES(tprops)n"".format(tprops=tprops) hql += "";"" self.log.info(hql) self.run_cli(hql) hql = ""LOAD DATA LOCAL INPATH 'filepath' ""."
183,"def get_local_certificate(self, local_certificate): self.get_local_certificate(local_certificate) self.get_local_certificate(local_certificate) self.get_local_certificate(local_certificate) self.get_local_certificate(local_certificate) self.get_local_certificate(local_certificate) self.get_local_certificate(local_certificate) self.get_local_certificate) self.get_local_certificate) self.get_local_certificate(local_certificate) self.get_local_certificate(local_certificate) self.get_certificate) self.get_certificate(local_certificate) self.get_certificate) self.get_certificate(local_certificate) self.get_certificate(local_certificate) self.get_certificate(local_certificate) self.get_certificate(local_certificate) self.get_certificate) self.get_cer",def get_certificate(self): cert = _lib.SSL_get_certificate(self._ssl) if cert!= _ffi.NULL: _lib.X509_up_ref(cert) return X509._from_raw_x509_ptr(cert) return None
184,"def calculate_range(self, part_size, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_size, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_index, part_size, part_size, part_size, num_parts, num_parts, num_parts, num_parts, num_parts, num_parts, num_","def calculate_range_parameter(part_size, part_index, num_parts, total_size=None): start_range = part_index * part_size if part_index == num_parts - 1: end_range = '' if total_size is not None: end_range = str(total_size - 1) else: end_range = start_range + part_size - 1 range_param = 'bytes=%s-%s' % (start_range, end_range) return range_param"
185,"def run_unit_test_suite(self, support library, Python version): self.test_suite(self.test_suite): self.test_suite(self.test_suite) self.test_suite(self.test_suite) self.test_suite(self.test_suite) self.test_suite(self.test_suite) self.test_suite(self.test_suite) self.test_suite(self.test_suite) self.test_suite) self.test_suite(self.test_suite) self.test_suite) self.test_suite(self.test_suite) self.test_suite) self.test_suite) self.test_suite(self.test_suite) self.test_suite) self.test_suite) self.test_suite(self.test_suite) self.test_suite) self.test_suite(self.test_suite) self.test_suite) self.test_suite(self.test_suite) self.test_suite(self.test_suite) self.test_suite(self.test_suite) self.test_suite(self.test_s","def compatibility(session, install): session.install('-e', '.[dev]') session.install(install) _run_tests(session)"
186,"def get_credits(self, language, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response) append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append","def tv_credits(self, **kwargs): path = self._get_id_path('tv_credits') response = self._GET(path, kwargs) self._set_attrs_to_values(response) return response"
187,"def pop_bind(self, ns): if self.get_bind(ns): self.get_bind(ns): self.get_bind(ns): self.get_bind(ns): self.get_bind(ns)","def remove_ns_bindings(ns_var_name: str = NS_VAR_NAME, ns_var_ns: str = NS_VAR_NS): ns_var_sym = sym.Symbol(ns_var_name, ns=ns_var_ns) ns_var = Maybe(Var.find(ns_var_sym)).or_else_raise( lambda: RuntimeException( f""Dynamic Var sym.Symbol(ns_var_name, ns=ns_var_ns) not bound!"" ) ) try: yield finally: ns_var.pop_bindings() logger.debug(f""Reset bindings for ns_var_sym to ns_var.value"")"
188,"def get_data(self, package_data, data_files): self.get_data(data_files) self.get_data(data_files) self.get_data(data_files) self.get_data(data_files) self.get_data(data_files) self.get_data(data_files) self.get_data(data_files) self.get_data(data_files) self.get_data(data_files) self.get_data(data_files) self.get_data(data_files) self.get_data(data_files) self.get_data(data_files) self.get_data(data_files) self.get_data(data_files) self.get_data(data_files) self.get_data(data_files) self.get_data(data_files) self.get_data(data_files) self.get_data(data_files) self.get_data(data_files) self.get_data(data_files) self.get_data(data_files) self.get_data(data_files) self.get_data(data_files) self.get_data(","def _get_file_handler(package_data_spec, data_files_spec): class FileHandler(BaseCommand): def run(self): package_data = self.distribution.package_data package_spec = package_data_spec or dict() for (key, patterns) in package_spec.items(): package_data[key] = _get_package_data(key, patterns) self.distribution.data_files = _get_data_files( data_files_spec, self.distribution.data_files ) return FileHandler"
189,"def read_context(self, turn_context): self.cache(turn_context) self.cache(turn_context) self.cache(turn_context) self.cache(turn_context) self.cache(turn_context) self.cache(turn_context) self.cache(turn_context) self.cache(turn_context) self.cache(turn_context) self.cache(turn_context) self.cache(turn_context) self.cache(turn_context) self.cache(turn_context)","async def load(self, turn_context: TurnContext, force: bool = False) -> None: if turn_context == None: raise TypeError('BotState.load(): turn_context cannot be None.') cached_state = turn_context.turn_state.get(self._context_service_key) storage_key = self.get_storage_key(turn_context) if (force or not cached_state or not cached_state.state) : items = await self._storage.read([storage_key]) val = items.get(storage_key) turn_context.turn_state[self._context_service_key] = CachedBotState(val)"
190,"def select_instance_pool(self, where_eq(), order_by(), filter_functions): if self.instance_pool(self.instance_pool): self.instance_pool(self.instance_pool) self.instance_pool(self.instance_pool) self.instance_pool(self.instance_pool) self.instance_pool(self.instance_pool) self.instance_pool(self.instance_pool(self.instance_pool(self.instance_pool) self.instance_pool(self.instance_pool) self.instance_pool(self.instance_pool) self.instance_pool(self.instance_pool) self.instance_pool(self.instance_pool) self.instance_pool(self.instance_pool) self.instance_pool(self.instance_pool) self.instance_pool) self.instance_pool(self.instance_pool) self.instance_pool(self.instance_pool) self.instance_pool(self.instance_pool) self.instance_pool(self.instance_pool) self.instance_pool(self.instance_pool) self.instance_pool(self.","def select_one(self, *args): s = apply_query_operators(self.storage, args) return next(iter(s), None)"
191,"def index(self, 0s, 1s, 0s, 1s, 0s, 1s, 0s, 1s, 0s, 1s, 0s, 1s, 0s, 1s, 0s, 1s, 0s, 1s, 0s, 1s, 0s, 1s, 0s, 1s, 0s, 1s, 0s, 1s, 0s, 0s, 1s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s, 0s,","def bit_string_index(text): n = len(text) k = text.count(""1"") if text.count(""0"")!= n - k: raise VisualizationError(""s must be a string of 0 and 1"") ones = [pos for pos, char in enumerate(text) if char == ""1""] return lex_index(n, k, ones)"
192,"def seq_pair(self, seq_pair, multiplicity, profiles): seq_pair = seq_pair, seq_pair = seq_pair, seq_pair = seq_pair, multiplicity = seq_pair, profiles = seq_pair, multiplicity = seq_pair, profiles = seq_pair, multiplicity = seq_pair, profiles = seq_pair, multiplicity = seq_pair, multiplicity = seq_pair, multiplicity = seq_pair = seq_pair = seq_pair = seq_pair = seq_pair = seq_pair = seq_pair = seq_pair = seq_pair = seq_pair = seq_pair.seq_pair.seq_pair.seq_pair.seq_pair.seq_pair.seq_pair.seq_pair.seq_pair.seq_pair.seq_pair.seq_pair.seq_pair.seq_pair.seq_pair.seq_p","def optimal_t_compressed(self, seq_pair, multiplicity, profiles=False, tol=1e-10): if profiles: res = -1.0*self.prob_t_profiles(seq_pair, multiplicity,t**2, return_log=True) return res else: return -1.0*self.prob_t_compressed(seq_pair, multiplicity,t**2, return_log=True) try: from scipy.optimize import minimize_scalar opt = minimize_scalar(_neg_prob, bounds=[-np.sqrt(ttconf.MAX_BRANCH_LENGTH),np.sqrt(ttconf.MAX_BRANCH_LENGTH)], args=(seq_pair, multiplicity), tol=tol) new_len = opt[""x""]**2 if'success' not in opt: opt['success'] = True self.logger(""WARNING: the optimization result does not contain a'success' flag:""+str(opt),4, warn=True) except: import scipy print('legacy scipy', scipy.__version__) from scipy.optimize import fminbound new_len = fminbound(_neg_prob, -np.sqrt(ttconf.MAX_BRANCH_LENGTH),np.sqrt(ttconf.MAX_BRANCH_LENGTH), args=(seq_pair, multiplicity)) new_len = new_len**2 opt='success':True if new_len >.9 * ttconf.MAX_BRANCH_LENGTH: self.logger(""WARNING: GTR.optimal_t_compressed -- The branch length seems to be very long!"", 4,"
193,"def get_format(self, description): self.get_format(self.get_format(self.get_format)) self.get_format(self.get_format) self.get_format(self.get_format) self.get_format(self.get_format) self.get_format(self.get_format) self.get_format(self.get_format) self.get_format(self.get_format) self.get_format(self.get_format) self.get_format(self.get_format) self.get_format(self.get_format) self.get_format(self.get_format) self.get_format(self.get_format) self.get_format(self.get_format) self.get_format(self.get_format) self.get_format(self.get_format) self.get_format(self.get_format) self.get_format(self.get_format) self.get_format(self.get_format) self.get_format(self.get_format(self.get_format(self.get_format) self.get_format(self.get_format) self.get_format(self.get_format(self.get_format(self.get_format(self.get_format","def parse_header_format(description): description = description.strip('""') keyword = 'Format:' before_keyword, keyword, after_keyword = description.partition(keyword) return after_keyword.strip()"
194,"def _validate_public_key(key_type, bits, bits): if key_type is None: if key_type is None: if key_type is None: if key_type is None: if key_type is None: if key_type is None: if key_type is None: if key_type is None: if key_type is None: if key_type is None: if key_type is None: if key_type is None: if key_type = if key_type = if key_type = if key_type = if key_type = if key_type = if key_type = if key_type = if key_type = if key_type = if key_type = if key_type = if key_type = if key_type = if key_type = if key_type = if key_type = if key_type = if key_type = if key_type = if key_type = if key_type = if key_type = if key_type = if key_type = if key_type = if key_type = if key_type = if key_type = if key_type = if key","def parse(self, keydata=None): if keydata is None: if self.keydata is None: raise ValueError(""Key data must be supplied either in constructor or to parse()"") keydata = self.keydata else: self.reset() self.keydata = keydata if keydata.startswith(""---- BEGIN SSH2 PUBLIC KEY ----""): key_type = None pubkey_content = """".join([line for line in keydata.split(""n"") if "":"" not in line and ""----"" not in line]) else: key_parts = self._split_key(keydata) key_type = key_parts[0] pubkey_content = key_parts[1] self._decoded_key = self.decode_key(pubkey_content) current_position, unpacked_key_type = self._unpack_by_int(self._decoded_key, 0) if key_type is not None and key_type!= unpacked_key_type.decode(): raise InvalidTypeError(""Keytype mismatch: %s!= %s"" % (key_type, unpacked_key_type)) self.key_type = unpacked_key_type key_data_length = self._process_key(self._decoded_key[current_position:]) current_position = current_position + key_data_length if current_position!= len(self._decoded_key): raise MalformedDataError(""Leftover data: %s bytes"" % (len(self._decoded_key) - current_position)) if self.disallow_options and self.options: raise InvalidOptionsError(""Options are disallowed."")"
195,def ________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________,"def skus(self): api_version = self._get_api_version('skus') if api_version == '2017-06-01': from.v2017_06_01.operations import SkusOperations as OperationClass elif api_version == '2017-10-01': from.v2017_10_01.operations import SkusOperations as OperationClass elif api_version == '2018-02-01': from.v2018_02_01.operations import SkusOperations as OperationClass elif api_version == '2018-03-01-preview': from.v2018_03_01_preview.operations import SkusOperations as OperationClass elif api_version == '2018-07-01': from.v2018_07_01.operations import SkusOperations as OperationClass else: raise NotImplementedError(""APIVersion  is not available"".format(api_version)) return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))"
196,"def report_metadata(self, org_name, org_email, report_metadata, report_metadata, report_metadata, report_metadata, report_metadata, report_metadata, report_metadata, report_metadata, report_metadata, report_metadata, report_metadata, report_metadata, report_metadata, report_metadata, report_metadata, report_metadata, report_metadata, report_metadata, report_metadata, report_metadata)",def strip_metadata(report): report['org_name'] = report['report_metadata']['org_name'] report['org_email'] = report['report_metadata']['org_email'] report['report_id'] = report['report_metadata']['report_id'] report.pop('report_metadata') return report
197,def detect_soapysd(self): if self.sapysd(self.sapysd): self.sapysd(self.sapysd): self.sapysd(self.sapysd) self.sapysd(self.sapysd) self.sapysd(self.sapysd) self.sapysd(self.sapysd) self.sapysd(self.sapysd) self.sapysd(self.sapysd) self.sapysd(self.sapysd(self.sapysd) self.sapysd(self.sapysd) self.sapysd(self.sapysd) self.sapysd(self.sapysd(self.sapysd(self.sapysd) self.sapysd(self.sapysd) self.sapysd(self.sapysd(self,"def detect_devices(soapy_args=''): devices = simplesoapy.detect_devices(soapy_args, as_string=True) text = [] text.append('Detected SoapySDR devices:') if devices: for i, d in enumerate(devices): text.append(' '.format(d)) else: text.append(' No devices found!') return (devices, 'n'.join(text))"
198,"def plot_title(self, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title): if figsize = None, figsize = None, figsize = None, figsize = None, figsize = None, figsize = None, figsize = None, figsize = None, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, title, figsize = None, figsize = None, figsize = None, figsize = None, figsize = None, figsize = None, figsize = None, figsize = None, figsize = None, figsize = None, figsize = None, figsize = None, figsize = None, figsize = None, figsize = None, figsize = None, figsize = None, figsize =","def plot_bloch_multivector(rho, title='', figsize=None): if not HAS_MATPLOTLIB: raise ImportError('Must have Matplotlib installed.') rho = _validate_input_state(rho) num = int(np.log2(len(rho))) width, height = plt.figaspect(1/num) fig = plt.figure(figsize=(width, height)) for i in range(num): ax = fig.add_subplot(1, num, i + 1, projection='3d') pauli_singles = [ Pauli.pauli_single(num, i, 'X'), Pauli.pauli_single(num, i, 'Y'), Pauli.pauli_single(num, i, 'Z') ] bloch_state = list( map(lambda x: np.real(np.trace(np.dot(x.to_matrix(), rho))), pauli_singles)) plot_bloch_vector(bloch_state, ""qubit "" + str(i), ax=ax, figsize=figsize) fig.suptitle(title, fontsize=16) plt.close(fig) return fig"
199,"def handle_subscribe(self, data, ts, ts, ts, data, ts, data, ts, data, ts, data, ts, data, ts, data, ts, data, ts, data, ts, data, ts, ts, data, ts, ts, data, ts, data, ts, data, ts, data, ts, data, ts, data, ts, data, ts, data, ts, data, ts, data, ts, data, ts, data, ts, data, ts, data, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts,","def _response_handler(self, event, data, ts): self.log.debug(""_response_handler(): Passing %s to client.."", data) self.pass_to_client(event, data, ts)"
200,"def configure(self, database='foo', configure(database='foo', configure(database='foo', configure(database='foo', configure(database='foo', configure(database='foo', configure(database='foo', configure(database='foo', configure(database='foo', configure(database='foo')))))))))))))))))))))))))))))))))))))))))","def configure(module=None, prefix='MONGODB_', **kwargs): if module is not None and isinstance(module, types.ModuleType): attrs = ((attr.replace(prefix, '').lower(), value) for attr, value in vars(module).items() if attr.startswith(prefix)) _Options._configure(**dict(attrs)) elif kwargs: _Options._configure(**kwargs)"
201,"def target_mentions(self, target_mentions, chain_mentions, target_mentions, target_mentions, target_mentions, target_mentions, chain_mentions, target_mentions, target_mentions, chain_mentions, target_mentions, target_mentions, chain_mentions, target_mentions, target_mentions, chain_mentions, target_mentions, chain_mentions, target_mentions, chain_mentions, target_mentions, chain_mentions, target_mentions, target_mentions, chain_mentions, target_mentions, chain_mentions, target_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions,","def ALL_mentions(target_mentions, chain_mentions): found_all = True for name in target_mentions: found_one = False for chain_ment in chain_mentions: if name in chain_ment: found_one = True break if not found_one: found_all = False break return found_all"
202,"def create_contact_from_user_input(user_input, contact_name, contact_name, contact_name, contact_name, contact_name, contact_name, contact_name, contact_name, contact_name, contact_name, contact_name, contact_name, contact_name, contact_name, contact_name, contact_name, contact_name, contact_name, contact_name, contact_name, contact_name, contact_name, contact_name, contact_name, contact_name, contact_name, contact_name, contact_name, contact_name, contact_name, contact_name, contact_name, contact_name, contact_name, contact_name, contact_name)","def from_user_input(cls, address_book, user_input, supported_private_objects, version, localize_dates): contact = cls(address_book, None, supported_private_objects, version, localize_dates) contact._process_user_input(user_input) return contact"
203,"def wait_for_status(self, request_id, wait_for_status, sleep_interval, sleep_interval, wait_for_status, wait_for_status, wait_for_status, sleep_interval, sleep_interval, wait_for_status, wait_for_status, sleep_interval, sleep_interval, progress_callback, success_callback, failure_callback, failure_callback, failure_callback)","def wait_for_operation_status(self, request_id, wait_for_status='Succeeded', timeout=30, sleep_interval=5, progress_callback=wait_for_operation_status_progress_default_callback, success_callback=wait_for_operation_status_success_default_callback, failure_callback=wait_for_operation_status_failure_default_callback): loops = timeout // sleep_interval + 1 start_time = time.time() for _ in range(int(loops)): result = self.get_operation_status(request_id) elapsed = time.time() - start_time if result.status == wait_for_status: if success_callback is not None: success_callback(elapsed) return result elif result.error: if failure_callback is not None: ex = AzureAsyncOperationHttpError(_ERROR_ASYNC_OP_FAILURE, result.status, result) failure_callback(elapsed, ex) return result else: if progress_callback is not None: progress_callback(elapsed) time.sleep(sleep_interval) if failure_callback is not None: ex = AzureAsyncOperationHttpError(_ERROR_ASYNC_OP_TIMEOUT, result.status, result) failure_callback(elapsed, ex) return result"
204,def user_agent(user_agent): if user_agent(user_agent): return user_agent(user_agent) if user_agent(user_agent): return user_agent(user_agent),"def user_agent(): data =  ""installer"": ""name"": ""pip"", ""version"": pip.__version__, ""python"": platform.python_version(), ""implementation"":  ""name"": platform.python_implementation(),,  if data[""implementation""][""name""] == 'CPython': data[""implementation""][""version""] = platform.python_version() elif data[""implementation""][""name""] == 'PyPy': if sys.pypy_version_info.releaselevel == 'final': pypy_version_info = sys.pypy_version_info[:3] else: pypy_version_info = sys.pypy_version_info data[""implementation""][""version""] = ""."".join( [str(x) for x in pypy_version_info] ) elif data[""implementation""][""name""] == 'Jython': data[""implementation""][""version""] = platform.python_version() elif data[""implementation""][""name""] == 'IronPython': data[""implementation""][""version""] = platform.python_version() if sys.platform.startswith(""linux""): distro = dict(filter( lambda x: x[1], zip([""name"", ""version"", ""id""], platform.linux_distribution()), )) libc = dict(filter( lambda x: x[1], zip([""lib"", ""version""], platform.libc_ver()), )) if libc: distro[""libc""] = libc if distr"
205,def new_dag_run(self): if self.dag_run(self.dag_run): self.dag_run(self.dag_run): self.dag_run(self.dag_run),"def trigger_dag(dag_id): data = request.get_json(force=True) run_id = None if 'run_id' in data: run_id = data['run_id'] conf = None if 'conf' in data: conf = data['conf'] execution_date = None if 'execution_date' in data and data['execution_date'] is not None: execution_date = data['execution_date'] try: execution_date = timezone.parse(execution_date) except ValueError: error_message = ( 'Given execution date,, could not be identified''as a date. Example date format: 2015-11-16T14:34:15+00:00'.format(execution_date)) _log.info(error_message) response = jsonify('error': error_message) response.status_code = 400 return response try: dr = trigger.trigger_dag(dag_id, run_id, conf, execution_date) except AirflowException as err: _log.error(err) response = jsonify(error="""".format(err)) response.status_code = err.status_code return response if getattr(g, 'user', None): _log.info(""User %s created %s"", g.user, dr) response = jsonify(message=""Created "".format(dr)) return response"
206,"def get_root_of_namespace(self, namespace): self.get_root(namespace) self.get_root(namespace) self.get_root(namespace) self.get_root(namespace) self.get_root(namespace) self.get_root(namespace) self.get_root(namespace) self.get_root(namespace) self.get_root(namespace) self.get_root(namespace) self.get_root(namespace) self.get_root(namespace) self.get_root(namespace) self.get_root(namespace) self.get_root(namespace) self.get_root(namespace) self.get_root(namespace) self.get_root(namespace) self.get_root(namespace) self.get_root(namespace) self.get_root(namespace) self.get_root(namespace) self.get_root(namespace) self.get_root(namespace) self.get_root(namespace) self.get_root(namespace) self.get_root(namespace) self.get_root(namespace) self.get_root(namespace) self.get_root(namespace) self.get_root(namespace) self.get",def getBaseNameScope(cls): s = NameScope(False) s.setLevel(1) s[0].update(cls._keywords_dict) return s
207,"def write_job_description(self, job_description, job_description, job_description, job_description, job_description, job_description, job_description, job_description, job_description, job_description, job_description, job_description, job_description, job_description, job_description, job_description, job_description, job_description, job_description, job_description, job_description, job_description, job_description, job_description, job_description)","def write(self, filename): txt = self.tostring() with open(filename, 'w') as f: f.write(txt)"
208,def calculate_wisible_length(self): self.get_wisible_length(self.get_wisible_length) self.get_wisible_length(self.get_wisible_length) self.get_wisible_length(self.get_wisible_length) self.get_wisible_length(self.get_wisible_length) self.get_wisible_length(self.get_wisible_length) self.get_wisible_length(self.get_wisible_length) self.get_wisible_length) self.get_wisible_length(self.get_wisible_length) self.get_wisible_length(self.get_wisible_length) self.get_wisible_length(self.get_wisible_length) self.get_wisible_length) self.get_wisible_length(self.get_wisible_length) self.get_wisible_length(self.get_wisible_length) self.get_wisible_length(self.get_wisible_length) self.get_wisible_length(self.get_wisible_length) self.get_wis,"def clean_len(self, line): if isinstance(line, basestring): return len(self.screen.markup.clean_markup(line)) elif isinstance(line, tuple) or isinstance(line, list): markups = self.screen.markup.get_markup_vars() length = 0 for i in line: if i not in markups: length += len(i) return length"
209,"def produce_file(self, rtype, rtype): if self.produce_file(self.produce_file): self.produce_file(self.produce_file) self.produce_file(self.produce_file) self.produce_file(self.produce_file) self.produce_file(self.produce_file) self.produce_file(self.produce_file) self.produce_file(self.produce_file) self.produce_file) self.produce_file(self.produce_file) self.produce_file(self.produce_file) self.produce_file) self.produce_file(self.produce_file) self.produce_file) self.produce_file(self.produce_file) self.produce_file) self.produce_file(self.produce_file) self.produce_file) self.produce_file(self.produce_file) self.produce_file(self.produce_file) self.produce_file) self.produce_file(self.produce_file) self.produce_file) self.produce_file(self.produce_file) self.produce_file","def _do_not_produce_file(self): if ( Inactive().is_present() and self.domain_status in [ PyFunceble.STATUS[""official""][""down""], PyFunceble.STATUS[""official""][""invalid""], ] and PyFunceble.INTERN[""to_test""] not in PyFunceble.INTERN[""extracted_list_to_test""] ): return True return False"
210,"def delete_selected_objects(self, object): delete_selected_objects(object) delete_objects(object) delete_objects(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_object(object) delete_","def delete(self, event): ""delete all of the selected objects"" for obj in self.selection: if obj: if DEBUG: print ""deleting"", obj.name obj.destroy() self.selection = [] self.inspector.load_object()"
211,def Equality test - Equality test - Equality test Equality test Equality test Equality test Equality test Equality test Equality test Equality test Equality test Equality test Equality test Equality test Equality test Equality test Equality test Equality test Equality test Equality test Equality test Equality test Equality test Equality test Equality test Equality test Equality test Equality test Equality test Equality test Equality test Equality test Equality test Equality test Equality test Equality,"def _is_equal(self, test_color): if test_color is None: ans = False elif test_color.color_type!= self.color_type: ans = False elif self.name is not None and self.name == test_color.name: ans = True elif (self.red == test_color.red and self.blue == test_color.blue and self.green == test_color.green): ans = True else: ans = False return ans"
212,"def fetch_question(self, category): self.get_question(category): self.get_question(category) self.get_question(category) self.get_question(category) self.get_question(category) self.get_question(category) self.get_question(category) self.get_question(category) self.get_question(category) self.get_question(category)","def fetch(self, category=CATEGORY_QUESTION, offset=DEFAULT_OFFSET): if not offset: offset = DEFAULT_OFFSET kwargs = ""offset"": offset items = super().fetch(category, **kwargs) return items"
213,"def save_screenshot(self, args, kwargs): if self.save_path(path, args, kwargs): self.save_screenshot(path, args, kwargs): self.save_screenshot(path, args, kwargs)","def save_screenshot(self, path=None, **kwargs): path = _prepare_path(path, ""png"") self.driver.save_screenshot(path, **kwargs) return path"
214,def disconnect_satelintegra(self): if self.satelintegra(self.satelintegra): self.satelintegra(self.satelintegra): self.satelintegra(self.satelintegra) self.satelintegra(self.satelintegra) self.satelintegra(self.satelintegra) self.satelintegra(self.satelintegra) self.satelintegra(self.satelintegra) self.satelintegra) self.satelintegra_satelintegra_satelintegra_satelintegra_satelintegra_satelintegra_satelintegra_satelintegra_satelintegra_satelintegra_satelintegra_satelintegra_satelintegra_satelintegra_satelintegra_satelintegra_satelintegra_satelintegra_satelintegra_satelintegra_satelintegra_satelintegra_satelintegra),async def keep_alive(self): while True: await asyncio.sleep(self._keep_alive_timeout) if self.closed: return data = generate_query(b'xEEx01x01') await self._send_data(data)
215,def python_data_frame(data_frame): if python_data_frame(data_frame): if python_data_frame(data_frame): if python_data_frame(data_frame): python_data_frame(data_frame): python_data_frame(data_frame): python_data_frame(data_frame): python_data_frame(data_frame): python_data_frame(data_frame): python_data_frame(data_frame): python_data_frame(data_frame): python_data_frame(data_frame): python_data_frame(data_frame): python_data_frame(data_frame): python_data_frame(data_frame): python_data_frame(data_frame): python_data_frame(data_frame): python_data_frame(data_frame): python_data_frame(data_frame): python_,"def as_data_frame(self): if can_use_pandas(): import pandas pandas.options.display.max_colwidth = 70 return pandas.DataFrame(self._cell_values, columns=self._col_header) return self"
216,"def isModerator(self, teamId, personEmail, isModerator, isModerator, isModerator, isModerator, isModerator, isModerator, isModerator, isModerator, isModerator, isModerator, isModerator, isModerator, isModerator, isModerator, isModerator, isModerator, isModerator, isModerator, isModerator, isModerator, isModerator)","def create(self, teamId, personId=None, personEmail=None, isModerator=False, **request_parameters): check_type(teamId, basestring, may_be_none=False) check_type(personId, basestring) check_type(personEmail, basestring) check_type(isModerator, bool) post_data = dict_from_items_with_values( request_parameters, teamId=teamId, personId=personId, personEmail=personEmail, isModerator=isModerator, ) json_data = self._session.post(API_ENDPOINT, json=post_data) return self._object_factory(OBJECT_TYPE, json_data)"
217,"def reST_index_file(self.rst_extension, relative_to, relative_to, relative_to, relative_to, relative_to, relative_to, relative_to, relative_to, relative_to, relative_to, relative_to, relative_to, relative_to, relative_to, relative_to, relative_to, relative_to, relative_to, relative_to, relative_to, relative_to, relative_to, relative_to): relative_to, relative_to, relative_to, relative_to, relative_to, relative_to, relative_to, relative_to, relative_to, relative_to, relative_to, relative_to, relative_to, relative_to, relative_to, relative_to, relative_to, relative_to, relative_to)","def write_index(self, outdir, froot='gen', relative_to=None): if self.written_modules is None: raise ValueError('No modules written') path = os.path.join(outdir, froot+self.rst_extension) if relative_to is not None: relpath = outdir.replace(relative_to + os.path.sep, '') else: relpath = outdir idx = open(path,'wt') w = idx.write w('.. AUTO-GENERATED FILE -- DO NOT EDIT!nn') w('.. toctree::nn') for f in self.written_modules: w(' %sn' % os.path.join(relpath,f)) idx.close()"
218,def correspond_float_point(self): return self.float_point(self.float_point),"def real(self, nested_scope=None): op = self.children[0].name expr = self.children[1] dispatch = 'sin': sympy.sin, 'cos': sympy.cos, 'tan': sympy.tan, 'asin': sympy.asin, 'acos': sympy.acos, 'atan': sympy.atan, 'exp': sympy.exp, 'ln': sympy.log,'sqrt': sympy.sqrt  if op in dispatch: arg = expr.real(nested_scope) return dispatch[op](arg) else: raise NodeException(""internal error: undefined external"")"
219,def _tempo_coordinates(self): if self.coordinates(self.coordinates): self.coordinates(self.coordinates) self.coordinates(self.coordinates) self.coordinates(self.coordinates) self.coordinates,"def __coord_tempo(n, sr=22050, hop_length=512, **_kwargs): basis = core.tempo_frequencies(n+2, sr=sr, hop_length=hop_length)[1:] edges = np.arange(1, n+2) return basis * (edges + 0.5) / edges"
220,def load_grammar_file(file_path): if file_path is None: if file_path is None: if file_path is None: if file_path is None: if file_path is None: if file_path is None: if file_path is None: if file_path is None: if file_path is None: if file_path is None: if file_path is None: if file_path is None: if file_path is None: if file_path is None: if file_path is None: if file_path is None: if file_path is None: if file_path = if file_path = if file_path = if file_path = if file_path = if file_path = if file_path = if file_path = if file_path = if file_path = if file_path = if file_path = if file_path = if file_path = if file_path = if file_path = if file_path = if file_path = if file_path = if file_path = if file_path = if file_path = if file_path = if file_path = if file_path,"def load_grammar(self, path): if not os.path.exists(path): raise Exception(""path does not exist:!r"".format(path)) grammar_path = os.path.dirname(path) if grammar_path not in sys.path: sys.path.append(grammar_path) with open(path, ""r"") as f: data = f.read() code = compile(data, path, ""exec"") locals_ = ""GRAMFUZZER"": self, ""__file__"": path exec(code) in locals_ if ""TOP_CAT"" in locals_: cat_group = os.path.basename(path).replace("".py"", """") self.set_cat_group_top_level_cat(cat_group, locals_[""TOP_CAT""])"
221,"def append_timestamp(self, timestamp): self.get_timestamp(timestamp) self.get_timestamp(timestamp) self.get_timestamp(timestamp) self.get_timestamp(timestamp) self.get_timestamp(timestamp) self.get_timestamp(timestamp) self.get_timestamp(timestamp) self.get_timestamp) self.get_timestamp(timestamp) self.get_timestamp(timestamp) self.get_timestamp) self.get_timestamp(timestamp) self.get_timestamp) self.get_timestamp(timestamp) self.get_timestamp(timestamp) self.get_timestamp(timestamp) self.get_timestamp(timestamp) self.get_timestamp(timestamp) self.get_timestamp(timestamp) self.get_timestamp(timestamp) self.get_timestamp(timestamp) self.get","def _timestamp(self): if PyFunceble.CONFIGURATION[""inactive_database""]: if ( ""inactive_db"" in PyFunceble.INTERN and PyFunceble.INTERN[""file_to_test""] in PyFunceble.INTERN[""inactive_db""] and PyFunceble.INTERN[""inactive_db""][PyFunceble.INTERN[""file_to_test""]] ): database_keys = [ x for x in PyFunceble.INTERN[""inactive_db""][ PyFunceble.INTERN[""file_to_test""] ].keys() if x.isdigit() ] if database_keys: recent_date = max(database_keys) else: return int(PyFunceble.time()) if int(PyFunceble.time()) > int(recent_date) + self.one_day_in_seconds: return int(PyFunceble.time()) if int(PyFunceble.time())  int(recent_date) + self.days_in_seconds: return int(recent_date) return int(PyFunceble.time())"
222,"def convert_elementtree(self, raw_xml, __text__, __text__, __text__, __text__, __text__, __text__, __text__, __text__, __text__, __text__, __text__, __text__, __text__, __text__, __text__, __text__, __text__, __text__, __text__, __text__, __text__, __text__, __text__, __text__, __text__, __text__, __text__, __text__, __text__, __text__, __text__, __text__, __text__, __text__, __text__, __text__, __text__, __text__, __text__, __text__, _","def xml_to_dict(raw_xml): def node_to_dict(node): d =  d.update(node.items()) text = getattr(node, 'text', None) if text is not None: d['__text__'] = text childs =  for child in node: childs.setdefault(child.tag, []).append(node_to_dict(child)) d.update(childs.items()) return d purged_xml = remove_invalid_xml_chars(raw_xml) try: tree = xml.etree.ElementTree.fromstring(purged_xml) except xml.etree.ElementTree.ParseError as e: cause = ""XML stream %s"" % (str(e)) raise ParseError(cause=cause) d = node_to_dict(tree) return d"
223,"def report_fn(code_unit, analysis): report_fn = report_fn(code_unit, analysis)","def report_files(self, report_fn, morfs, directory=None): self.find_code_units(morfs) if not self.code_units: raise CoverageException(""No data to report."") self.directory = directory if self.directory and not os.path.exists(self.directory): os.makedirs(self.directory) for cu in self.code_units: try: report_fn(cu, self.coverage._analyze(cu)) except NoSource: if not self.config.ignore_errors: raise except NotPython: if cu.should_be_python() and not self.config.ignore_errors: raise"
224,"def iterate_over_keys(self, field, field): self.get_keys(field, field) self.get_keys(field, field) self.get_keys(field, field) self.get_keys(field, field) self.get_keys(field, field) self.get_keys(field, field) self.get_keys(field, field) self.get_keys(field, field) self.get_keys(field, field) self.get_keys(field) self.get_keys(field) self.get_keys(field) self.get_keys(field) self.get_keys(field) self.get_keys(field) self.get_keys(field) self.get_keys(field) self.get_keys(field) self.get_keys(field) self.get_keys(field) self.get_keys(field) self.get_keys(field) self.get_keys(field) self.get_keys(field) self.get_keys(field) self.get_keys(field) self.get_keys(field) self.get_keys(field) self.get_keys(field) self.","def _iterate_uniqueness_keys(self, field): uniqueness = getattr(field, 'uniqueness', None) if not uniqueness: return for keys in uniqueness: composed_keys = self._compose_keys(keys) yield composed_keys"
225,"def add_function(self, event_type include wildcard): if event_type include wildcard, if event_type include wildcard, if event_type include wildcard, if event_type include wildcard, if event_type include wildcard, if event_type include wildcard, if event_type include wildcard, if event_type include wildcard, if event_type include wildcard, if event_type include wildcard, if event_type include wildcard","def swift(*arg): check_event_type(Openstack.Swift, *arg) event_type = arg[0] def decorator(func): if event_type.find(""*"")!= -1: event_type_pattern = pre_compile(event_type) swift_customer_process_wildcard[event_type_pattern] = func else: swift_customer_process[event_type] = func log.info(""add function 0 to process event_type:1"".format(func.__name__, event_type)) @functools.wraps(func) def wrapper(*args, **kwargs): func(*args, **kwargs) return wrapper return decorator"
226,"def y_actual(y_actual, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_actual, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_","def _check_targets(y_actual, y_predicted): if len(y_actual)!= len(y_predicted): raise ValueError(""Row mismatch: [,]"".format(len(y_actual), len(y_predicted)))"
227,"def compute_season(self, season): self.get_season(season) self.get_season(season) self.get_season(season) self.get_season(season) self.get_season(season) self.get_season(season) self.get_season(season) self.get_season(season) self.get_season(season) self.get_season(season) self.get_season(season) self.get_season(season) self.get_season(season) self.get_season) self.get_season(season) self.get_season(season) self.get_season(season) self.get_season(season) self.get_season) self.get_season(season) self.get_season(season) self.get_season(season) self.get_season(season) self.get_season(season) self.get_season(season) self.get_season(season) self.get_season(season) self.get_season(season) self.get_season(season) self.get_season(season) self.get_season(season) self.get_season(season) self.get_season(season) self.get_season(season) self.get_season(season) self.get_season(season) self.get_","def build_is_last_day_of_season(num_steps_per_season): num_steps_per_cycle = np.sum(num_steps_per_season) changepoints = np.cumsum(np.ravel(num_steps_per_season)) - 1 def is_last_day_of_season(t): t_ = dist_util.maybe_get_static_value(t) if t_ is not None: step_in_cycle = t_ % num_steps_per_cycle return any(step_in_cycle == changepoints) else: step_in_cycle = tf.math.floormod(t, num_steps_per_cycle) return tf.reduce_any( input_tensor=tf.equal(step_in_cycle, changepoints)) return is_last_day_of_season"
228,"def process_queue(self, queue): if queue is not empty, process_queue(queue): if queue is not empty, process_queue(queue): if queue is empty, process_queue(queue): if queue is empty, process_queue(queue): if queue is empty, process_queue(queue): if queue is empty, process_queue(queue): if queue_queue): queue_queue = queue_queue = queue_queue = queue_queue = queue_queue_queue_queue_queue_queue_queue_queue_queue_queue_queue_queue_queue_queue_queue_queue_queue_queue_queue_queue_queue_queue_queue_queue_queue_queue_queue_queue_queue_queue_queue_queue_queue_queue_queue_queue_queue_queue",def _send_queue(self): while not LWLink.the_queue.empty(): self._send_reliable_message(LWLink.the_queue.get_nowait())
229,"def _longest_common_substring_from-more-than-two-strings_python(self, substrings, substrings, substrings, substrings, substrings, substrings, substrings, substrings, substrings, substrings, substrings, substrings, substrings, substrings, substrings, substrings, substrings, substrings, substrings, substrings, substrings, longest_common_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substrings_substring",def long_substr(data): substr = '' if len(data) > 1 and len(data[0]) > 0: for i in range(len(data[0])): for j in range(len(data[0])-i+1): if j > len(substr) and all(data[0][i:i+j] in x for x in data): substr = data[0][i:i+j] elif len(data) == 1: substr = data[0] return substr
230,"def alpha_character(self, alpha_character): alpha_character = alpha_character = alpha_character = alpha_character = alpha_character = alpha_character = alpha_character = alpha_character = alpha_character = alpha_character = alpha_character = alpha_character = alpha_character = alpha_character = alpha_character = alpha_character = alpha_character = alpha_character = alpha_character = alpha_character = digit = alpha_character = digit = alpha_character = digit = alpha_character = digit = alpha_character = digit = alpha_character = digit = alpha_character = alpha_character = digit = alpha_character = digit = alpha_character = digit = alpha_character = digit = alpha_character = digit = alpha_character = digit = alpha_character = digit = alpha_character = digit = alpha_character = digit = alpha_character = digit = alpha_character = digit = alpha_character = digit = alpha_character = digit = alpha_character = digit = alpha_character = digit = alpha_character = digit = alpha_character = digit =","def identifier(self, text): self._attempting(text) return concatenation([ alternation([ self.alpha_character, ""_"" ]), zero_or_more( alternation([ self.alpha_character, ""_"", self.digit ]) ) ], ignore_whitespace=False)(text).compressed(TokenType.identifier)"
231,"def _add_node(add_node, node_name, options): add_node(add_node, node_name, options) return add_node(add_node, node_name, options) return add_node(add_node, options)","def _proc_node_stmt(self, toks): opts = toks[1] dummy_node = Node(""dummy"") for key, value in opts.iteritems(): trait = dummy_node.trait(key) if trait is not None: if trait.is_trait_type( Float ): opts[key] = float( value ) elif trait.is_trait_type( Tuple ): opts[key] = tuple( [float(c) for c in value.split("","")] ) return super(GodotDataParser, self)._proc_node_stmt(toks)"
232,....,"def gen_rebinding_time(lease_time, elapsed=0): rebinding_time = int(lease_time) * REBIND_PERC - elapsed range_fuzz = int(lease_time) - rebinding_time logger.debug('rebinding fuzz range %s', range_fuzz) fuzz = random.uniform(-(range_fuzz), +(range_fuzz)) rebinding_time += fuzz logger.debug('Rebinding time %s.', rebinding_time) return rebinding_time"
233,"def send_request(self, send_request): self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request) self.request(self.request) self.request) self.request(self.request) self.request) self.request(self.request) self.request) self.request(self.request) self.request) self.request(self.request) self.request) self.request(self.request) self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request) self.request(self.request) self.request(self.request) self.request","def send(r, stream=False): r.send(stream=stream) return r.response"
234,"def guild_count(shard_count, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no, shard_no,","async def post_guild_count( self, shard_count: int = None, shard_no: int = None ): await self.http.post_guild_count(self.bot_id, self.guild_count(), shard_count, shard_no)"
235,def optik_optparse(self): if self.optparse(self.optparse): self.optparse(self.optparse) self.optparse(self.optparse) self.optparse(self.optparse) self.optparse(self.optparse) self.optparse(self.optparse) self.optparse(self.optparse) self.optparse_self.optparse_self.optparse_self.optparse_self.optparse_self.optparse_self.optparse_self.optparse_self.optparse_self.optparse_self.optparse_self.optparse_self.optparse_self.optparse_self.optparse_self.optparse_self.optparse_self.optparse_self.optparse_self.optparse_self.optparse_self.optparse_self.optparse_self.optparse_self.optparse_self.optparse_self.,"def optik_option(self, provider, opt, optdict): optdict = copy.copy(optdict) if ""action"" in optdict: self._nocallback_options[provider] = opt else: optdict[""action""] = ""callback"" optdict[""callback""] = self.cb_set_provider_option if ""default"" in optdict: if ( ""help"" in optdict and optdict.get(""default"") is not None and optdict[""action""] not in (""store_true"", ""store_false"") ): optdict[""help""] += "" [current: %default]"" del optdict[""default""] args = [""--"" + str(opt)] if ""short"" in optdict: self._short_options[optdict[""short""]] = opt args.append(""-"" + optdict[""short""]) del optdict[""short""] for key in list(optdict.keys()): if key not in self._optik_option_attrs: optdict.pop(key) return args, optdict"
236,"def raiseError(self, AttributeError, ValueError, ValueError, ValueError, AttributeError, ValueError, AttributeError, ValueError, ValueError, ValueError, ValueError, AttributeError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueError, ValueErr","def _check_scheme(self, config): try: scheme = config.get( escape_for_ini('keyring-setting'), escape_for_ini('scheme'), ) except (configparser.NoSectionError, configparser.NoOptionError): raise AttributeError(""Encryption scheme missing"") aesmode = scheme[-3:] if aesmode not in self._get_mode(): raise ValueError(""Encryption scheme invalid: %s"" % (aesmode)) self.aesmode = aesmode if scheme.startswith('PyCryptodome '): scheme = scheme[13:] if scheme!= self.scheme: raise ValueError(""Encryption scheme mismatch "" ""(exp.: %s, found: %s)"" % (self.scheme, scheme))"
237,"def frac(self, int/list frac): if self.split(int/list frac) self.split(int/list frac) self.split(int/list frac) self.split(int/list frac) self.split(int/list frac) self.split(int/list frac) self.split(int/list frac) self.split(int) self.split(int) self.split(int) self.split(int) self.split(int) self.split(int) self.split(int) self.split(int) self.split(int) self.split(int) self.split(int) self.split(int) self.split(int) self.split(int) self.split(int) self.split(int) self.split(int) self.split(int) self.split(int) self.split(int) self.split(int) self.","def split(self, frac): self = self.extract() if _issequence(frac): total = sum(frac) frac = [k / total for k in frac] else: assert frac = 1, ""fraction should be = 1"" frac = [frac, 1 - frac] offsets = np.round(np.cumsum(frac) * len(self)).astype(np.int64) start = 0 for offset in offsets: yield self[start:offset] start = offset"
238,"def run_protocol(self, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000, port 8000)","def main(): rollbar.init('ACCESS_TOKEN', environment='test', handler='twisted') factory = protocol.ServerFactory() factory.protocol = Echo reactor.listenTCP(8000, factory) reactor.run()"
239,def release_lock(self): self.lock(self.lock) self.lock(self.lock) self.lock(self.lock) self.lock(self.lock) self.lock(self.lock) self.lock(self.lock) self.lock(self.lock) self.lock(self.lock) self.lock(self.lock) self.lock(self.lock) self.lock(self.lock) self.lock(self.lock) self.lock(self.lock) self.lock,def reader_release(self): self._readers_mutex.acquire() self._readers -= 1 if self._readers == 0: self._access_mutex.release() self._readers_mutex.release()
240,"def upload_token(self, job_nr, upload_token, job_nr, job_nr, job_nr, job_nr, job_nr, job_nr, job_nr, job_nr, job_nr, job_nr, job_nr, job_nr, job_nr, job_nr, job_nr, job_nr, job_nr, job_nr, job_nr, job_nr, job_nr, job_nr, job_nr)","def new_job( self, task, person, tank, target_host, target_port, loadscheme=None, detailed_time=None, notify_list=None, trace=False): if not notify_list: notify_list = [] data =  'task': task, 'person': person, 'tank': tank, 'host': target_host, 'port': target_port, 'loadscheme': loadscheme, 'detailed_time': detailed_time, 'notify': notify_list  logger.debug(""Job create request: %s"", data) api_timeouts = self.api_timeouts() while True: try: response = self.__post( ""api/job/create.json"", data, trace=trace)[0] return response['job'], response['upload_token'] except (self.NotAvailable, self.StoppedFromOnline) as e: try: timeout = next(api_timeouts) logger.warn(""API error, will retry in %ss..."" % timeout) time.sleep(timeout) continue except StopIteration: logger.warn('Failed to create job on lunapark') raise self.JobNotCreated(e.message) except requests.HTTPError as e: raise self.JobNotCreated('Failed to create job on lunaparkn'.format(e.response.content)) except Exception as e: logger.warn('Failed to create job on lunapark') logger.warn(repr(e), ) raise self.JobNotCreated()"
241,"def _get_items(self, url, params, others, kwargs, erc, kwargs, erc, kwargs): if self.get_items(self.get_items): self.get_items(self.get_items): self.get_items(self.get_items)","def get_items(self, url, params=None, **kwargs): pages = self.get_pages(url, params=params, **kwargs) for json_page in pages: assert isinstance(json_page, dict) items = json_page.get('items') if items is None: error_message = ""'items' key not found in JSON data: ""  ""!r"".format(json_page) raise MalformedResponse(error_message) else: for item in items: yield item"
242,"def convert_value(self, value, value, value, value, value, value, value, value, value, value, value, value, value, value, value, value, value, value, value, value, value)","def maybebool(value): if isinstance(value, six.string_types) and value.lower() in booly: return asbool(value) return value"
243,"def set_proxy_host(self, host, port): self.proxy_authorization(self.proxy_authorization): self.proxy_authorization(self.proxy_authorization) self.proxy_authorization(self.proxy_authorization) self.proxy_authorization(self.proxy_authorization) self.proxy_authorization(self.proxy_authorization) self.proxy_authorization(self.proxy_authorization) self.proxy_authorization(self.proxy_authorization) self.proxy_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_authorization_","def set_proxy(self, host, port, user, password): self.proxy_host = host self.proxy_port = port self.proxy_user = user self.proxy_password = password"
244,"def cfg_ini(self, cfg_ini, cfg_ini, cfg_ini, cfg_ini, cfg_ini, cfg_ini, cfg_ini, cfg_ini, cfg_ini, cfg_ini, cfg_ini, cfg_ini, cfg_ini, cfg_ini, cfg_ini, cfg_ini, cfg_ini)","def parse_sections(cfg_ini): return [Section(section.lower(), guess_plugin(section.lower()), without_defaults(cfg_ini, section)) for section in cfg_ini.sections() if not re.match(CORE_SECTION_PATTERN, section.lower()) and section.lower() not in DEPRECATED_SECTIONS]"
245,"def subscribe(self, deviceId, commandId, commandId, commandId, commandId, commandId, commandId, commandId, commandId, commandId, commandId, commandId, commandId, commandId, commandId, commandId, commandId, commandId, commandId, commandId, commandId, commandId, commandId, commandId, commandId, commandId, commandId, qos, qos, qos, qos, qos, qos, qos, qos, qos, qos, qos, qos, qos, qos, qos, qos, qos, qos, qos, qos, qos, qos, qos, qos, qos, qos, qos, qos, qos, qos, qos, qos, qos, qos, qos, qos, qos,","def subscribeToDeviceCommands(self, typeId=""+"", deviceId=""+"", commandId=""+"", msgFormat=""+""): if self._config.isQuickstart(): self.logger.warning(""QuickStart applications do not support commands"") return 0 topic = ""iot-2/type/%s/id/%s/cmd/%s/fmt/%s"" % (typeId, deviceId, commandId, msgFormat) return self._subscribe(topic, 0)"
246,"def kernel_call_ast(self, kernel_call_ast, kernel_call_ast, kernel_call_ast, kernel_call_ast, kernel_call_ast, kernel_call_ast, kernel_call_ast, kernel_call_ast, kernel_call_ast, kernel_call_ast, kernel_call_ast, kernel_call_ast, kernel_call_ast): kernel_call_ast, kernel_call_ast, kernel_call_ast, kernel_call_ast, kernel_call_ast, kernel_call_ast, kernel_call_ast, kernel_call_ast, kernel_call_ast, kernel_call_ast, kernel_call_ast, kernel_call_ast, kernel_call_ast, kernel_call_ast, kernel_call_ast, kernel_call_ast, kernel_call_ast, kernel_call_ast, kernel_call_ast, kernel_call_ast, kernel_call_ast, kernel_call_ast, kernel_call_as","def _build_kernel_call(self, name='kernel'): return c_ast.FuncCall(name=c_ast.ID(name=name), args=c_ast.ExprList(exprs=[ c_ast.ID(name=d.name) for d in ( self._build_array_declarations()[0] + self._build_scalar_declarations() + self._build_const_declartions())]))"
247,"def ask_a_question(self, default, EOF): if default is not given, if default is not given, EOF is raised to prevent infinite loops._______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________","def ask_yes_no(prompt,default=None): answers = 'y':True,'n':False,'yes':True,'no':False ans = None while ans not in answers.keys(): try: ans = raw_input(prompt+' ').lower() if not ans: ans = default except KeyboardInterrupt: pass except EOFError: if default in answers.keys(): ans = default print() else: raise return answers[ans]"
248,"def mcubes(self, filename, str): if mcubes is None: mcubes is None: mcubes is None: mcubes is None: mcubes is None: mcubes is None: mcubes is None: mcubes is None: mcubes is None: mcubes is None: mcubes is None: mcubes is None: mcubes is None: mcubes = mcubes = mcubes = mcubes = mcubes = mcubes = mcubes = mcubes = mcubes = mcubes = mcubes = mcubes = mcubes = mcubes = mcubes = mcubes = mcubes = mcubes = mcubes = mcubes = mcubes = mcubes = mcubes = mcubes = mcubes = mcubes = mcubes = mcubes = mcubes =","def export_obj(filename, cutout, level=0): if "".obj"" not in filename: filename = filename + "".obj"" vs, fs = mcubes.marching_cubes(cutout, level) mcubes.export_obj(vs, fs, filename)"
249,"def length_of_element(self, box): return self.length(self.length) self.length(self.length) self.length(self.length) self.length(self.length) self.length(self.length) self.length(self.length) self.length(self.length) self.length(self.length) self.length(self.length) self.length(self.length) self.length(self.length) self.length(self.length) self.length","def length(self): return max(len(self.top), len(self.mid), len(self.bot))"
250,def start_cocoa_crcunLoop(self): if self.crcunLoop(self.crcunLoop): self.crcunLoop(self.crcunLoop) self.crcunLoop(self.crcunLoop) self.crcunLoop(self.crcunLoop) self.crcunLoop(self.crcunLoop) self.crcunLoop(self.crcunLoop) self.crcunLoop(self.crcunLoop) self.crcunLoop(self.crcunLoop) self.crcunLoop(self.crcunLoop) self.crcunLoop) self.crcunLoop(self.crcunLoop) self.crcunLoop(self.crcunLoop) self.crcunLoop(self.crcunLoop) self.crcunLoop(self.crcunLoop) self.crcunLoop(self.crcunLoop) self.crcunLoop(self.crcunLoop,"def loop_cocoa(kernel): if etype is KeyboardInterrupt: io.raw_print(""KeyboardInterrupt caught in CFRunLoop"") else: real_excepthook(etype, value, tb) def doi(): sys.excepthook = real_excepthook kernel.do_one_iteration() sys.excepthook = handle_int t = TimerMac(poll_interval) t.add_callback(doi) t.start() poller = zmq.Poller() if kernel.control_stream: poller.register(kernel.control_stream.socket, zmq.POLLIN) for stream in kernel.shell_streams: poller.register(stream.socket, zmq.POLLIN) while True: try: try: sys.excepthook = handle_int show.mainloop() sys.excepthook = real_excepthook poller.poll(10*poll_interval) kernel.do_one_iteration() except: raise except KeyboardInterrupt: io.raw_print(""KeyboardInterrupt caught in kernel"") finally: sys.excepthook = real_excepthook"
251,"def get_segmentation_mask(self, mask.size): if mask.size = self.segmentation_mask(self.segmentation_mask) self.segmentation_mask(segmentation_mask) self.segmentation_mask(segmentation_mask) self.segmentation_mask(segmentation_mask) self.segmentation_mask(segmentation_mask) self.segmentation_mask(segmentation_mask) self.segmentation_mask(segmentation_mask) self.segmentation_mask) self.segmentation_mask(segmentation_mask) self.segmentation_mask(segmentation_mask) self.segmentation_mask(segmentation_mask) self.mask(segmentation_mask) self.mask(segmentation_mask) self.mask(segmentation_mask) self.mask(segmentation_mask) self.mask","def _separate_masks(mask, threshold=0.025): try: ncpus = multiprocessing.cpu_count() except NotImplementedError: ncpus = 2 with multiprocessing.Pool(processes=ncpus) as pool: mask_ids = [id for id in np.unique(mask) if id!= 0] thresholds = [threshold * mask.size for _ in range(len(mask_ids))] masks = [mask for _ in range(len(mask_ids))] ms = pool.starmap(_separate_masks_task, zip(mask_ids, thresholds, masks)) return [m for m in ms if m is not None]"
252,"def append_function(self, target, items): self.append_function(self.append_function(self.append_function)): self.append_function(self.append_function) self.append_function(self.append_function) self.append_function(self.append_function) self.append_function(self.append_function) self.append_function(self.append_function) self.append_function(self.append_function) self.append_function(self.append_function) self.append_function) self.append_function(self.append_function) self.append_function) self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self","def _append_funcs(target, items): [target.append(item) for item in items if isfunction(item) or ismethod(item)]"
253,"def _feature_extraction_for_audio_segmentation(file_struct, file_struct, file_struct, file_struct, file_struct, file_struct, file_struct, file_struct, file_struct, file_struct, file_struct, file_struct, file_struct, file_struct, file_struct, file_struct, file_struct, file_struct, msaf.io.segmentation.feature_extraction_for_audio_segmentation_segmentation_segmentation_segmentation_segmentation_segmentation_segmentation_segmentation)","def features(file_struct, annot_beats=False, framesync=False): def compress_data(X, k): Xtemp = X.dot(X.T) if len(Xtemp) == 0: return None e_vals, e_vecs = np.linalg.eig(Xtemp) e_vals = np.maximum(0.0, np.real(e_vals)) e_vecs = np.real(e_vecs) idx = np.argsort(e_vals)[::-1] e_vals = e_vals[idx] e_vecs = e_vecs[:, idx] if k  len(e_vals): e_vals = e_vals[:k] e_vecs = e_vecs[:, :k] Z = np.sqrt(e_vals.max()) if Z > 0: e_vecs = e_vecs / Z return e_vecs.T.dot(X) def repetition(X, metric='euclidean'): R = librosa.segment.recurrence_matrix( X, k=2 * int(np.ceil(np.sqrt(X.shape[1]))), width=REP_WIDTH, metric=metric, sym=False).astype(np.float32) P = scipy.signal.medfilt2d(librosa.segment.recurrence_to_lag(R), [1, REP_FILTER]) P = P[P.any(axis=1)] return compress_data(P, N_REP) ######## pcp_obj = Features.select_"
254,def get_resource(self): if self.get_resource(self.get_resource): self.get_resource(self.get_resource) self.get_resource(self.get_resource) self.get_resource(self.get_resource) self.get_resource(self.get_resource) self.get_resource(self.get_resource) self.get_resource(self.get_resource) self.get_resource(self.get_resource) self.get_resource(self.get_resource) self.get_resource(self.get_resource) self.get_resource(self.get_resource) self.get_resource(self.get_resource) self.get_resource) self.get_resource(self.get_resource) self.get_resource) self.get_resource(self.get_resource) self.get_resource(self.get_resource) self.get_resource(self.get_resource) self.get_resource(self.get_resource) self.get_resource(self.get_resource) self.get_resource(self.get_resource) self.get_resource,"def get_resource(self, resource_name, name): try: logger.info(""Trying to get %s: '%s'"", resource_name, name) services_list = False if resource_name == 'host' and '/' in name: splitted_name = name.split('/') services_list = True name = splitted_name[0] params = 'where': json.dumps('name': name) if resource_name in ['host','service', 'user']: params = 'where': json.dumps('name': name, '_is_template': self.model) if resource_name =='service' and '/' in name: splitted_name = name.split('/') response2 = self.backend.get( 'host', params='where': json.dumps('name': splitted_name[0])) if response2['_items']: host = response2['_items'][0] logger.info(""Got host '%s' for the service '%s'"", splitted_name[0], splitted_name[1]) else: logger.warning(""Not found host '%s'!"", splitted_name[0]) return False params = 'where': json.dumps('name': splitted_name[1], 'host': host['_id'], '_is_template': self.model) if self.embedded and resource_name in self.embedded_resources: params.update('embedded': json.dumps(self.embedded_resources[resource_name])) response = self.backend.get(resource_name, params=params) if response"
255,"def check_object_name(self, bucket_name, object_name, bucket_name, object_name, bucket_name, object_name, bucket_name, object_name, bucket_name, object_name, bucket_name, object_name, bucket_name, object_name, bucket_name, object_name, bucket_name, object_name, bucket_name, object_name, bucket_name, object_name, bucket_name, object_name, bucket_name, object_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket","def exists(self, bucket_name, object_name): client = self.get_conn() bucket = client.get_bucket(bucket_name=bucket_name) blob = bucket.blob(blob_name=object_name) return blob.exists()"
256,"def extract_environment(self, env_extract_image, env_extract_tar, env_extract_image, env_extract_tar, env_extract_tar, env_extract_image, env_extract_tar, env_extract_image, env_extract_tar, env_extract_image, env_extract_image, env_extract_image, env_extract_tar, env_extract_image, env_extract_tar, env_extract_image, env_extract_image, env_extract_image, env_extract_tar, env_extract_image, env_extract_image, env_extract_image, env_extract_tar, env_extract_image, env_extract_image, env_extract_image, env_extract_tar, env_extract_image, env_extract_image,","def extract_env(self): environ = self._get_config('Env') if environ is not None: if not isinstance(environ, list): environ = [environ] lines = [] for line in environ: line = re.findall(""(?Pvar_name>.+?)=(?Pvar_value>.+)"", line) line = ['export %s=""%s""' % (x[0], x[1]) for x in line] lines = lines + line environ = ""n"".join(lines) bot.verbose3(""Found Docker container environment!"") return environ"
257,"def write_files(self, case_id, test_path, outpath, test_path, test_path, test_path, outpath, written_files, written_files, written_files, written_files, written_files, written_files, written_files, written_files, written_files, written_files, written_files, written_files, written_files, written_files, written_files, written_files, written_files, written_files, written_files, written_files, written_files)","def mt_report(context, case_id, test, outpath=None): LOG.info('exporting mitochondrial variants for case """"'.format(case_id)) adapter = context.obj['adapter'] query = 'chrom':'MT' case_obj = adapter.case(case_id=case_id) if not case_obj: LOG.warning('Could not find a scout case with id """". No report was created.'.format(case_id)) context.abort() samples = case_obj.get('individuals') mt_variants = list(adapter.variants(case_id=case_id, query=query, nr_of_variants= -1, sort_key='position')) if not mt_variants: LOG.warning('There are no MT variants associated to case  in database!'.format(case_id)) context.abort() today = datetime.datetime.now().strftime('%Y-%m-%d') if not outpath: outpath = str(os.getcwd()) written_files = 0 for sample in samples: sample_id = sample['individual_id'] sample_lines = export_mt_variants(variants=mt_variants, sample_id=sample_id) document_name = '.'.join([case_obj['display_name'], sample_id, today]) + '.xlsx' workbook = Workbook(os.path.join(outpath,document_name)) Report_Sheet = workbook.add_worksheet() if test and sample_lines and workbook: written_files +=1 continue row = 0 for col,field in enumerate(MT_EXPORT_HEADER): Report_Sheet"
258,"def traceback(ErrorKey, StackTraceEntry, [email recipients]): if StackTraceEntry(ErrorKey, StackTraceEntry, [email recipients]): if StackTraceEntry(ErrorKey, StackTraceEntry, [email recipients]): if StackTraceEntry(ErrorKey, StackTraceEntry, [email recipients]): if StackTraceEntry(ErrorKey, StackTraceEntry, [email recipients]): StackTraceEntry, [email recipients]): StackTraceEntry(ErrorKey, StackTraceEntry, [email recipients]): StackTraceEntry(ErrorKey, StackTraceEntry, [email recipients]): StackTraceEntry, StackTraceEntry, StackTraceEntry, StackTraceEntry, StackTraceEntry, StackTraceEntry, StackTraceEntry, StackTraceEntry, StackTraceEntry, StackTraceEntry, StackTraceEntry, StackTraceEntry, StackTraceEntry, StackTraceEntry","def _blame_line(self, traceback): key = None blamed_entry = None email_recipients = [] for stack_line in traceback: line_type = self._get_line_type(stack_line) if line_type == api_ttypes.LineType.THIRDPARTY_WHITELIST: return None, None, None, True elif line_type in [api_ttypes.LineType.DEFAULT, api_ttypes.LineType.KNOWN_ERROR]: filepath = self._get_basepath(stack_line.filename) entry = api_ttypes.CodeIdentifier(filepath, stack_line.function_name, stack_line.text) blamed_entry = entry key = api_ttypes.ErrorKey(filepath, stack_line.line_number, stack_line.function_name, stack_line.text) if filepath in self.watch_all_errors: email_recipients.extend(self.watch_all_errors[filepath]) return (key, blamed_entry, email_recipients, False)"
259,"def copy(self): self.copy(self.copy, self.copy, self.copy, self.copy, self.copy, self.copy, self.copy)",def copy(self): copy = self.__class__.__new__(self.__class__) copy._fwdm = self._fwdm.copy() copy._invm = self._invm.copy() copy._init_inv() return copy
260,"def send_response(self, message, headers): self.get_response(self.get_response(self.get_response(self.get_response(self.get_response(self.get_response(self.get_response(self.get_response(self.get_response(self.get_response(self.get_response(self.get_response(self.get_response(self.get_response_response_response_response_response_response_response_response_response_response_response_response_response_response))","def _send_response(self, http_code, message=None, headers=None): self.send_response(http_code, message) if headers: for header in headers: self.send_header(*header) self.end_headers()"
261,def connection_socket(socket): connection_socket(socket): connection_socket(socket) connection_socket(socket) connection_socket(socket) connection_socket(socket),"def client(self): if not self._client: self._client = socket.create_connection( (self.host, self.port), self.timeout) self.logger.debug('Client connected with guacd server (%s, %s, %s)' % (self.host, self.port, self.timeout)) return self._client"
262,"def remove_node(self, actual_node, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name, split_name","def _remove_along_branch(self, actual_node, split_name, recursive=False): if len(split_name) == 0: if actual_node.v_is_group and actual_node.f_has_children(): if recursive: for child in list(actual_node._children.keys()): actual_node.f_remove_child(child, recursive=True) else: raise TypeError('Cannot remove group %s it contains children. Please''remove with recursive=True.' % actual_node.v_full_name) self._delete_node(actual_node) return True name = split_name.popleft() if name in actual_node._links: if len(split_name)>0: raise RuntimeError('You cannot remove nodes while hopping over links!') actual_node.f_remove_link(name) else: child = actual_node._children[name] if self._remove_along_branch(child, split_name, recursive=recursive): del actual_node._children[name] if name in actual_node._groups: del actual_node._groups[name] elif name in actual_node._leaves: del actual_node._leaves[name] else: raise RuntimeError('You shall not pass!') del child return False"
263,"def run_fastqc(self, JobFunctionWrappingJob, JobFunctionWrappingJob, JobFunctionWrappingJob, JobFunctionWrappingJob, JobFunctionWrappingJob, JobFunctionWrappingJob, JobFunctionWrappingJob, JobFunctionWrappingJob, JobFunctionWrappingJob, JobFunctionWrappingJob, JobFunctionWrappingJob, JobFunctionWrappingJob, JobFunctionWrappingJob, JobFunctionWrappingJob, JobFunctionWrappingJob, JobFunctionWrappingJob, JobFunctionWrappingJob, JobFunctionWrappingJob, JobFunctionWrappingJob, JobFunctionWrappingJob, JobFunctionWrappingJob, JobFunctionWrappingJob, JobFunctionWrappingJob, JobFunctionWrappingJob, JobFunctionWrappingJob, JobFunctionWrappingJob, JobFun","def run_fastqc(job, r1_id, r2_id): work_dir = job.fileStore.getLocalTempDir() job.fileStore.readGlobalFile(r1_id, os.path.join(work_dir, 'R1.fastq')) parameters = ['/data/R1.fastq'] output_names = ['R1_fastqc.html', 'R1_fastqc.zip'] if r2_id: job.fileStore.readGlobalFile(r2_id, os.path.join(work_dir, 'R2.fastq')) parameters.extend(['-t', '2', '/data/R2.fastq']) output_names.extend(['R2_fastqc.html', 'R2_fastqc.zip']) dockerCall(job=job, tool='quay.io/ucsc_cgl/fastqc:0.11.5--be13567d00cd4c586edf8ae47d991815c8c72a49', workDir=work_dir, parameters=parameters) output_files = [os.path.join(work_dir, x) for x in output_names] tarball_files(tar_name='fastqc.tar.gz', file_paths=output_files, output_dir=work_dir) return job.fileStore.writeGlobalFile(os.path.join(work_dir, 'fastqc.tar.gz'))"
264,"def combUpdateDoneEv(self, combUpdateDoneEv, combUpdateDoneEv, combUpdateDoneEv, combUpdateDoneEv, combUpdateDoneEv, combUpdateDoneEv, combUpdateDoneEv, combUpdateDoneEv, combUpdateDoneEv, combUpdateDoneEv, combUpdateDoneEv, combUpdateDoneEv, combUpdateDoneEv, combUpdateDoneEv, combUpdateDoneEv, combUpdateDoneEv)","def _scheduleCombUpdateDoneEv(self) -> Event: assert not self._combUpdateDonePlaned, self.now cud = Event(self) cud.process_to_wake.append(self.__deleteCombUpdateDoneEv()) self._add_process(cud, PRIORITY_AGENTS_UPDATE_DONE) self._combUpdateDonePlaned = True self.combUpdateDoneEv = cud return cud"
265,"def worker_exit_lock(self, process_management_lock, worker_exit_lock): worker_exit_lock: worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker_exit_lock, worker","def _process_worker(call_queue, result_queue, initializer, initargs, processes_management_lock, timeout, worker_exit_lock, current_depth): if initializer is not None: try: initializer(*initargs) except BaseException: _base.LOGGER.critical('Exception in initializer:', exc_info=True) return global _CURRENT_DEPTH _CURRENT_DEPTH = current_depth _process_reference_size = None _last_memory_leak_check = None pid = os.getpid() mp.util.debug('Worker started with timeout=%s' % timeout) while True: try: call_item = call_queue.get(block=True, timeout=timeout) if call_item is None: mp.util.info(""Shutting down worker on sentinel"") except queue.Empty: mp.util.info(""Shutting down worker after timeout %0.3fs"" % timeout) if processes_management_lock.acquire(block=False): processes_management_lock.release() call_item = None else: mp.util.info(""Could not acquire processes_management_lock"") continue except BaseException as e: previous_tb = traceback.format_exc() try: result_queue.put(_RemoteTraceback(previous_tb)) except BaseException: print(previous_tb) sys.exit(1) if call_item is None: result_queue.put(pid) with worker_exit_lock: return try: r = call_item() except BaseException as e: exc = _ExceptionWithTraceback(e) result_queue.put(_ResultItem(call_item.work_id, exception=exc)) else: _sendback_result(result_queue, call_item.work_id, result="
266,"def selector(self, args, args): args = self.selector(self.selector) self.selector(self.selector) self.selector(self.selector) self.selector(self.selector) self.selector(self.selector) self.selector(self.selector) self.selector(self.selector) self.selector) self.selector(self.selector) self.selector) self.selector(self.selector) self.selector) self.selector) self.selector(self.selector) self.selector) self.selector(self.selector) self.selector) self.selector(self.selector) self.selector) self.selector(self.selector) self.selector(self.selector) self.selector(self.selector) self.selector(self.selector) self.selector","def Selector(fields): check_user_facing_fields_dict(fields, 'Selector') class _Selector(_ConfigSelector): def __init__(self): key = 'Selector.' + str(DictCounter.get_next_count()) super(_Selector, self).__init__( key=key, name=None, fields=fields, type_attributes=ConfigTypeAttributes(is_builtin=True), ) return _Selector"
267,"def set_exception_context(self, context=None, exception=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=None, context=N","def _check_bad_exception_context(self, node): cause = utils.safe_infer(node.cause) if cause in (astroid.Uninferable, None): return if isinstance(cause, astroid.Const): if cause.value is not None: self.add_message(""bad-exception-context"", node=node) elif not isinstance(cause, astroid.ClassDef) and not utils.inherit_from_std_ex( cause ): self.add_message(""bad-exception-context"", node=node)"
268,"def wait_for_cloud_to_be_up(self, wait_for_cloud_to_be_up(), wait_for_cloud_to_be_up(), wait_for_cloud_to_be_up(), wait_for_cloud_to_be_up(), wait_for_cloud_to_be_up(), wait_for_cloud_to_be_up(), wait_for_cloud_to_be_up())",def start(self): for node in self.nodes: node.start() for node in self.client_nodes: node.start()
269,"def fit(x, y, x, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y)","def fit_dataset(self, dataset, y, features=None, feature_type='features'): if feature_type == 'features': X = np.rot90(dataset.feature_table.data.toarray()) elif feature_type == 'voxels': X = np.rot90(dataset.image_table.data.toarray()) self.sk_classifier.fit(X, y)"
270,"def create_gui_object(self, filename, python struct): self.get_gui_object(filename, python struct) self.get_gui_object(filename) self.get_gui_object(filename) self.get_gui_object(filename) self.get_gui_object(filename) self.get_gui_object(filename) self.get_gui_object(filename) self.get_gui_object(filename) self.get_object(filename) self.get_object(filename) self.get_object(filename) self.get_object(filename) self.get_object(filename) self.get_object(filename) self.get_object(filename) self.get_object(filename) self.get_object(filename) self.get_object(filename) self.get_object(filename) self.get_object(filename) self.get_object(filename) self.get_object(filename) self.get_object(filename) self.get_object(filename) self.get_object(filename) self.get_object(filename) self.get_object(filename) self.get_object(filename) self","def load(controller=None, filename="""", name=None, rsrc=None): ""Create the GUI objects defined in the resource (filename or python struct)"" if not filename and not rsrc: if isinstance(controller, types.ClassType): mod_dict = util.get_class_module_dict(controller) elif isinstance(controller, types.ModuleType): mod_dict = controller.__dict__ elif isinstance(controller, Controller): mod_dict = util.get_class_module_dict(controller) else: mod_dict = util.get_caller_module_dict() if controller is None: controller = mod_dict if util.main_is_frozen(): if '__file__' in mod_dict: filename = os.path.split(mod_dict['__file__'])[1] else: filename = os.path.split(sys.argv[0])[-1] filename = os.path.join(util.get_app_dir(), filename) else: filename = mod_dict['__file__'] base, ext = os.path.splitext(filename) filename = base + "".rsrc.py"" if isinstance(filename, basestring): rsrc = parse(filename) ret = [] for win in rsrc: if not name or win['name'] == name: ret.append(build_window(win)) if ret and controller: connect(ret[0], controller) return ret[0] else: return ret"
271,"def Estimates K running X-means algorithm (Pelleg & Moore, 2000).","def estimate_K_xmeans(self, th=0.2, maxK = 10): means, labels = self.run_kmeans(self.X, self.init_K) stop = False curr_K = self.init_K while not stop: stop = True final_means = [] for k in range(curr_K): D = self.get_clustered_data(self.X, labels, k) if len(D) == 0 or D.shape[0] == 1: continue stdD = np.std(D, axis=0) #D = vq.whiten(D) D /= float(stdD) mean = D.mean(axis=0) half_means, half_labels = self.run_kmeans(D, K=2) bic1 = self.compute_bic(D, [mean], K=1, labels=np.zeros(D.shape[0]), R=D.shape[0]) bic2 = self.compute_bic(D, half_means, K=2, labels=half_labels, R=D.shape[0]) max_bic = np.max([np.abs(bic1), np.abs(bic2)]) norm_bic1 = bic1 / float(max_bic) norm_bic2 = bic2 / float(max_bic) diff_bic = np.abs(norm_bic1 - norm_bic2) #print ""diff_bic"", diff_bic if diff_bic > th: final_means.append(half_means[0] * stdD) final_means.append(half_means[1] * stdD) curr_K += 1 stop = False else: final_means.append(mean * stdD) final_means = np.asarray(final_means) #print ""Estimated K:"
272,"def get_extension(index, index): if index is not int(index): if index is not int(index): if index is not int(index): return if index is not int(index): return if index is not int(index): return if index is not int(index): return if index is not int(index): return if index is not int(index): return if index is not int(index)","def get_extension(self, index): ext = X509Extension.__new__(X509Extension) ext._extension = _lib.X509_get_ext(self._x509, index) if ext._extension == _ffi.NULL: raise IndexError(""extension index out of bounds"") extension = _lib.X509_EXTENSION_dup(ext._extension) ext._extension = _ffi.gc(extension, _lib.X509_EXTENSION_free) return ext"
273,"def load_connection_info(self.connection_file, self.connection_file, self.connection_file, self.connection_file, self.connection_file, self.connection_file, self.connection_file, self.connection_file, self.connection_file, self.connection_file, self.connection_file, self.connection_file, self.connection_file)",def load_connection_file(self): with open(self.connection_file) as f: cfg = json.loads(f.read()) self.ip = cfg['ip'] self.shell_port = cfg['shell_port'] self.stdin_port = cfg['stdin_port'] self.iopub_port = cfg['iopub_port'] self.hb_port = cfg['hb_port'] self.session.key = str_to_bytes(cfg['key'])
274,"def SolverInstance(self, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance) SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance, SolverInstance,","def Ainv(self): 'Returns a Solver instance' if not hasattr(self, '_Ainv'): self._Ainv = self.Solver(self.A) return self._Ainv"
275,"def create_token(project_name, dataset_name, token_name, dataset_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name,","def create_token(self, token_name, project_name, dataset_name, is_public): url = self.url() + '/nd/resource/dataset/'.format( dataset_name) + '/project/'.format(project_name) +  '/token//'.format(token_name) json =  ""token_name"": token_name, ""public"": is_public  req = self.remote_utils.post_url(url, json=json) if req.status_code is not 201: raise RemoteDataUploadError('Cout not upload :'.format(req.text)) if req.content == """" or req.content == b'': return True else: return False"
276,"def flat_map_nvim_io(self, map_nvim_io, flat_map_nvim_io, flat_map_nvim_io, flat_map_nvim_io, flat_map_nvim_io, flat_map_nvim_io, flat_map_nvim_io, flat_map_nvim_io, flat_map_nvim_io, flat_map_nvim_io, flat_map_nvim_io, flat_map_nvim_io, flat_map_nvim_io, flat_map_nvim_io, flat_map_nvim_io, flat_map_nvim_io, flat_map_nvim_io, flat_map_nvim_io, flat_map_nvim_io, flat_map_nvim_io, flat_map_nvim_io, flat_map_nvim_io, flat_map_nvim_io, flat_map_nvim_io, flat_map_nvim_io, flat_map_nvim_io,","def nvim_io_recover(self, io: NvimIORecover[A]) -> NvimIO[B]: return eval_step(self.vim)(io.map(lambda a: a))"
277,def SiteTree(self): self.get_site_tree(self.get_site_tree) self.get_site_tree(self.get_site_tree) self.get_site_tree(self.get_site_tree) self.get_site_tree(self.get_site_tree) self.get_site_tree(self.get_site_tree) self.get_site_tree(self.get_site_tree) self.get_site_tree(self.get_site_tree) self.get_site_tree) self.get_site_tree(self.get_site_tree) self.get_site_tree) self.get_site_tree(self.get_site_tree) self.get_site_tree) self.get_site_tree(self.get_site_tree) self.get_site_tree(self.get_site_tree) self.get_site_tree(self.get_site_tree) self.get_site_tree(self.get_site_tree) self.get_site_tree(self.get_site_tree) self.get_site_tree(self,"def get_sitetree(): sitetree = getattr(_THREAD_LOCAL, _THREAD_SITETREE, None) if sitetree is None: sitetree = SiteTree() setattr(_THREAD_LOCAL, _THREAD_SITETREE, sitetree) return sitetree"
278,"def parse_hpo(self, hpo): if self.hpo(self.hpo): self.hpo(self.hpo) self.hpo(self.hpo) self.hpo(self.hpo) self.hpo(self.hpo) self.hpo(self.hpo) self.hpo(self.hpo) self.hpo(self.hpo) self.hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_hpo_",def parse_hpo_obo(hpo_lines): term =  for line in hpo_lines: if len(line) == 0: continue line = line.rstrip() if line == '[Term]': if term: yield term term =  elif line.startswith('id'): term['hpo_id'] = line[4:] elif line.startswith('name'): term['description'] = line[6:] elif line.startswith('alt_id'): if 'aliases' not in term: term['aliases'] = [] term['aliases'].append(line[8:]) elif line.startswith('is_a'): if 'ancestors' not in term: term['ancestors'] = [] term['ancestors'].append(line[6:16]) if term: yield term
279,def terminate_dag_file_processor(self): if self.dag_file_processor(self.dag_processor(self.dag_processor(self.dag_processor(self.dag_processor(self.dag_processor(self.dag_processor))): self.dag_processor(self.dag_processor(self.dag_processor)) self.dag_processor(self.dag_processor) self.dag_processor(self.dag_processor) self.dag_processor) self.dag_processor(self.dag_processor) self.dag_processor(self.dag_processor) self.dag_processor(self.dag_processor) self.dag_processor(self.dag_processor) self.dag_processor) self.dag_processor(self.dag_processor) self.dag_processor(self.dag_processor) self.dag_processor(self.dag_processor) self.dag,"def terminate(self): self.log.info(""Sending termination message to manager."") self._child_signal_conn.send(DagParsingSignal.TERMINATE_MANAGER)"
280,"def uda(self, target, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, uda, ud",def build_uda_config_overrides(targets): from bugwarrior.services import get_service targets_udas =  for target in targets: targets_udas.update(get_service(target).ISSUE_CLASS.UDAS) return  'uda': targets_udas
281,"def save_content(self, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path","def save_content(self, file_path, encoding='utf-8'): if self.file_content_ is None: msg = 'Template content has not been updated.  Please fill the template before rendering it.' log.exception(msg) raise ValueError(msg) try: write_to_file(file_path, content=self.file_content_, encoding=encoding) except Exception as exc: msg = 'Document of type  got an error when  writing content.'.format(self.__class__) log.exception(msg) raise Exception(msg) from exc"
282,"def get_term_signal(self, signum, frame): if signum is not self._signal(self._signal): self._signal(self._signal) self._signal(self._signal) self._signal(self._signal) self._signal(self._signal) self._signal(self._signal) self._signal(self._signal) self._signal(self._signal) self._signal) self._signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal","def sigterm_handler(self, signum, frame): assert(self.state in ('WAITING', 'RUNNING', 'PAUSED')) logger.debug(""our state %s"", self.state) if self.state == 'WAITING': return self.ioloop.stop() if self.state == 'RUNNING': logger.debug('already running sending signal to child - %s', self.sprocess.pid) os.kill(self.sprocess.pid, signum) self.ioloop.stop()"
283,"def get_stats(self, short, keyword, short): return self.get_stats(self.get_stats(self.get_stats(self.get_stats)))","def url_stats(self, short): data = dict(action='url-stats', shorturl=short) jsondata = self._api_request(params=data) return _json_to_shortened_url(jsondata['link'])"
284,def debug_debug_debug(self): if self.debug(self.debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_,"def sigquit_handler(sig, frame): print(""Dumping stack traces for all threads in PID "".format(os.getpid())) id_to_name = dict([(th.ident, th.name) for th in threading.enumerate()]) code = [] for thread_id, stack in sys._current_frames().items(): code.append(""n.format(id_to_name.get(thread_id, """"), thread_id)) for filename, line_number, name, line in traceback.extract_stack(stack): code.append('File: """", line, in '.format(filename, line_number, name)) if line: code.append("" "".format(line.strip())) print(""n"".join(code))"
285,"def add_cer_to_service(service_name, data, certificate_format, password, cer_format): if cer_format is None: if cer_format is None: if cer_format is None: if cer_format is None: if cer_format is None: if cer_format is None: if cer_format is None: if cer_format is None: if cer_format is None: if cer_format is None: if cer_format = cer_format = cer_format = cer_format = cer_format = cer_format = cer_format = cer_format = cer_format = cer_format = cer_format = cer_format = cer_format = cer_format = cer_format = cer_format = cer_format = cer_format = cer_format = cer_format = cer_format = cer_format = cer_format = cer_format = cer_format = cer_format = cer_format = cer_format = cer_format = cer_format = cer_format = cer_format = cer_format = cer_format = cer_format = cer_format","def add_service_certificate(self, service_name, data, certificate_format, password=None): _validate_not_none('service_name', service_name) _validate_not_none('data', data) _validate_not_none('certificate_format', certificate_format) _validate_not_none('password', password) return self._perform_post( '/' + self.subscription_id + '/services/hostedservices/' + _str(service_name) + '/certificates', _XmlSerializer.certificate_file_to_xml( data, certificate_format, password), as_async=True)"
286,"def TempURL(self, url, url): if self.________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________","def generate_temp_url(method, url, seconds, key): method = method.upper() base_url, object_path = url.split('/v1/') object_path = '/v1/' + object_path expires = int(time.time() + seconds) hmac_body = '%sn%sn%s' % (method, expires, object_path) sig = hmac.new(key, hmac_body, hashlib.sha1).hexdigest() return '%s%s?temp_url_sig=%s&temp_url_expires=%s' % ( base_url, object_path, sig, expires)"
287,"def assess_optimal_trimmed_range(self, data_file, trim_nt, trim_nt, trim_nt, trim_nt, trim_nt, trim_nt, trim_nt, trim_nt, trim_nt, trim_nt, trim_nt, trim_nt, trim_nt, trim_nt, trim_nt, trim_nt, trim_nt, trim_nt, trim_nt, trim_nt, trim_nt, trim_nt, trim_nt)","def trim_range(data_file): logger.debug(""Starting trim range assessment"") target_nuc_bias = "">>Per base sequence content"" logger.debug(""Target string to start nucleotide bias assessment set to "" """".format(target_nuc_bias)) gather = False biased = [] with open(data_file) as fh: for line in fh: if line.startswith(target_nuc_bias): logger.debug(""Found target string at line: "".format(line)) next(fh) gather = True elif line.startswith("">>END_MODULE"") and gather: logger.debug(""Stopping parsing at line: "".format(line)) break elif gather: g, a, t, c = [float(x) for x in line.strip().split()[1:]] gc = (g + 0.1) / (c + 0.1) at = (a + 0.1) / (t + 0.1) if 0.8 = gc = 1.2 and 0.8 = at = 1.2: biased.append(False) else: biased.append(True) logger.debug(""Finished bias assessment with result: "".format(biased)) biased_5end, biased_3end = biased[:int(len(biased)/2)], biased[int(len(biased)/2):][::-1] logger.debug(""Getting optimal trim range from biased list"") trim_nt = [0, 0] trim_nt[0] = get_trim_index(biased_5end) logger.debug(""Optimal trim range at 5' end set to: "".format(trim_nt[0])) trim_nt[1] = len(biased) - get_trim_index(biased_3end) logger.debug(""Opti"
288,"def get_archived_after(backend_class, backend_args, backend_args, backend_args, backend_args, archived_after, archived_after, archived_after, backend_args, backend_args, backend_args, backend_args, backend_args, backend_args, backend_args, backend_args, backend_args, backend_args, backend_args, backend_args, backend_args, backend_args, backend_args, backend_args, backend_args, backend_args, backend_args, backend_args, backend_args, backend_args, backend_args, backend_args, backend_args) backend_args, backend_args, backend_args, backend_args, backend_args, backend_args, backend_args, backend_args, backend_args, backend_args, backend_args, backend_args, backend_args, backend_args, backend_args, backend_args","def fetch_from_archive(backend_class, backend_args, manager, category, archived_after): init_args = find_signature_parameters(backend_class.__init__, backend_args) backend = backend_class(**init_args) filepaths = manager.search(backend.origin, backend.__class__.__name__, category, archived_after) for filepath in filepaths: backend.archive = Archive(filepath) items = backend.fetch_from_archive() try: for item in items: yield item except ArchiveError as e: logger.warning(""Ignoring %s archive due to: %s"", filepath, str(e))"
289,"def self_other(self, self, qargs, qargs, front, qargs, qargs, qargs, qargs, qargs, qargs, front, qargs, qargs, qargs, qargs, qargs, qargs, qargs, qargs, qargs, front, qargs, qargs): qargs, qargs, qargs, qargs, qargs, qargs, qargs, qargs, qargs, qargs, qargs, qargs, qargs, qargs, qargs, qargs, qargs, qargs, qargs, qargs, qargs, qargs, qargs, qargs, qargs, qargs, qargs, qargs, qargs, qargs, qargs, qargs, qargs, qargs, qargs, qargs, qargs, q","def compose(self, other, qargs=None, front=False): if qargs is not None: return Kraus( SuperOp(self).compose(other, qargs=qargs, front=front)) if not isinstance(other, Kraus): other = Kraus(other) if front and self._input_dim!= other._output_dim: raise QiskitError( 'input_dim of self must match output_dim of other') if not front and self._output_dim!= other._input_dim: raise QiskitError( 'input_dim of other must match output_dim of self') if front: ka_l, ka_r = self._data kb_l, kb_r = other._data input_dim = other._input_dim output_dim = self._output_dim else: ka_l, ka_r = other._data kb_l, kb_r = self._data input_dim = self._input_dim output_dim = other._output_dim kab_l = [np.dot(a, b) for a in ka_l for b in kb_l] if ka_r is None and kb_r is None: kab_r = None elif ka_r is None: kab_r = [np.dot(a, b) for a in ka_l for b in kb_r] elif kb_r is None: kab_r = [np.dot(a, b) for a in ka_r for b in kb_l] else: kab_r = [np.dot(a, b) for a in ka_r for b in kb_r] return Kraus((kab_l,"
290,"def read_data(input_stream, input_stream, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError)","def read(self, input_stream, kmip_version=enums.KMIPVersion.KMIP_1_0): super(Credential, self).read(input_stream, kmip_version=kmip_version) local_stream = BytearrayStream(input_stream.read(self.length)) if self.is_tag_next(enums.Tags.CREDENTIAL_TYPE, local_stream): self._credential_type = primitives.Enumeration( enum=enums.CredentialType, tag=enums.Tags.CREDENTIAL_TYPE ) self._credential_type.read(local_stream, kmip_version=kmip_version) else: raise ValueError( ""Credential encoding missing the credential type."" ) if self.is_tag_next(enums.Tags.CREDENTIAL_VALUE, local_stream): if self.credential_type ==  enums.CredentialType.USERNAME_AND_PASSWORD: self._credential_value = UsernamePasswordCredential() elif self.credential_type == enums.CredentialType.DEVICE: self._credential_value = DeviceCredential() elif self.credential_type == enums.CredentialType.ATTESTATION: self._credential_value = AttestationCredential() else: raise ValueError( ""Credential encoding includes unrecognized credential "" ""type."" ) self._credential_value.read( local_stream, kmip_version=kmip_version ) else: raise ValueError( ""Credential encoding missing the credential value."" ) self.is_oversized(local_stream)"
291,"def raster_instance(path, mode, args): raster_instance(mode, args): raster_instance(mode, args): raster_instance(mode, args): raster_instance(mode, args): raster_instance(mode, args): raster_instance(mode, args): raster_instance(mode): raster_instance(mode): raster_instance(mode, args): raster_instance(mode, args): raster_instance(mode): raster_instance(mode): raster_instance(mode): raster_instance(mode): raster_instance(mode): raster_instance(mode): raster_instance(mode): raster_instance(mode): raster_instance(mode): raster_instance(mode): raster_instance(mode): raster_instance(mode): raster_in","def open(path, mode=gdalconst.GA_ReadOnly): path = getattr(path, 'name', path) try: return Raster(vsiprefix(path), mode) except AttributeError: try: imgdata = path.read() except AttributeError: raise TypeError('Not a file-like object providing read()') else: imgio = MemFileIO(delete=False) gdal.FileFromMemBuffer(imgio.name, imgdata) return Raster(imgio, mode) raise ValueError('Failed to open raster from ""%r""' % path)"
292,"def vldMask(val, typeObj, typeObj, typeObj, typeObj, typeObj, typeObj, typeObj, typeObj, typeObj, typeObj, val, typeObj, typeObj, typeObj, typeObj, typeObj, typeObj, typeObj, typeObj, typeObj): vldMask, typeObj, typeObj, typeObj, typeObj, typeObj, typeObj, typeObj, typeObj, typeObj, typeObj, typeObj, typeObj, typeObj, typeObj, typeObj, typeObj, typeObj, val, val, typeObj, typeObj, typeObj, val, val, val, val, val, val, val, val, val, val, val, val, val, val, val, val, val, val, val, val, val, val, val, val,","def fromPy(cls, val, typeObj, vldMask=None): assert isinstance(typeObj, Integer) vld = int(val is not None) if not vld: assert vldMask is None or vldMask == 0 val = 0 else: if vldMask == 0: val = False vld = 0 else: val = int(val) return cls(val, typeObj, vld)"
293,"def compute_lst(self, observation): if self.lst = self.lst(observation): self.lst = self.lst(observation)","def compute_lst(self): if self.header[b'telescope_id'] == 6: self.coords = gbt_coords elif self.header[b'telescope_id'] == 4: self.coords = parkes_coords else: raise RuntimeError(""Currently only Parkes and GBT supported"") if HAS_SLALIB: dut1 = 0.0 mjd = self.header[b'tstart'] tellong = np.deg2rad(self.coords[1]) last = s.sla_gmst(mjd) - tellong + s.sla_eqeqx(mjd) + dut1 if last  0.0 : last = last + 2.0*np.pi return last else: raise RuntimeError(""This method requires pySLALIB"")"
294,"def nodes_per_cloud(self, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, nodes_per_cloud, no","def scrape_cloudsize_from_stdout(self, nodes_per_cloud): retries = 60 while retries > 0: if self.terminated: return f = open(self.output_file_name, ""r"") s = f.readline() while len(s) > 0: if self.terminated: return match_groups = re.search(r""Cloud of size (d+) formed"", s) if match_groups is not None: size = match_groups.group(1) if size is not None: size = int(size) if size == nodes_per_cloud: f.close() return s = f.readline() f.close() retries -= 1 if self.terminated: return time.sleep(1) print("""") print(""ERROR: Too many retries starting cloud."") print("""") sys.exit(1)"
295,"def find_connection_file(filename, profile, profile): if profile is not specified, profile will be used if profile is not specified if profile is not specified if profile is not specified if profile is not specified if profile is not specified if profile is not specified if profile is not specified if profile is not specified if profile is not specified if profile is not specified if profile is not specified if profile is not specified if profile is not specified if profile is not specified if profile is not","def find_connection_file(filename, profile=None): from IPython.core.application import BaseIPythonApplication as IPApp try: return filefind(filename) except IOError: pass if profile is None: if IPApp.initialized(): app = IPApp.instance() profile_dir = app.profile_dir else: profile_dir = ProfileDir.find_profile_dir_by_name(get_ipython_dir(), 'default') else: profile_dir = ProfileDir.find_profile_dir_by_name(get_ipython_dir(), profile) security_dir = profile_dir.security_dir try: return filefind(filename, ['.', security_dir]) except IOError: pass if '*' in filename: pat = filename else: pat = '*%s*' % filename matches = glob.glob( os.path.join(security_dir, pat) ) if not matches: raise IOError(""Could not find %r in %r"" % (filename, security_dir)) elif len(matches) == 1: return matches[0] else: return sorted(matches, key=lambda f: os.stat(f).st_atime)[-1]"
296,"def remove_signals(self, signals): if self.signals(signals): self.signals(signals): self.signals(signals) self.signals(signals) self.signals(signals) self.signals(signals) self.signals(signals) self.signals(signals) self.signals(signals) self.signals(signals) self.signals(signals) self.signals(signals) self.signals) self.signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal_signal)",def _destroy(self): ctx = self._get_rtl_context() for i in self._inputs: i.endpoints.discard(self) for o in self._outputs: o.drivers.remove(self) ctx.statements.remove(self)
297,"def expect_list(self, timeout, searchwindowsize, self.timeout, self.searchwindowsize, self.searchwindowsize, self.searchwindowsize, self.searchwindowsize, self.searchwindowsize, self.searchwindowsize, self.searchwindowsize, self.searchwindowsize, self.searchwindowsize, self.searchwindowsize, self.searchwindowsize, self.searchwindowsize, self.searchwindowsize, self.searchwindowsize, self.searchwindowsize)","def expect_list(self, pattern_list, timeout = -1, searchwindowsize = -1): return self.expect_loop(searcher_re(pattern_list), timeout, searchwindowsize)"
298,"def getattr(self, c, match_only, str): if self.getattr(self.getattr): self.getattr(self.getattr): self.getattr(self.getattr)) self.getattr(self.getattr) self.getattr(self.getattr) self.getattr(self.getattr) self.getattr(self.getattr) self.getattr) self.getattr(self.getattr) self.getattr(self.getattr) self.getattr) self.getattr(self.getattr) self.getattr(self.getattr) self.getattr(self.getattr) self.getattr(self.getattr) self.getattr(self.getattr) self.getattr(self.getattr) self.getattr(self.getattr) self.getattr(self.getattr) self.getattr(self.getattr) self.getattr(self.","def get_colour(self, c, default='#eeeeee', match_only=None): return self.getattr(c=c, attr='colour', default=default, match_only=match_only)"
299,def def _attribute(attribute): if _attribute(attribute): if _attribute(attribute): if _attribute(attribute)): if _attribute(attribute): if _attribute(attribute)): if _attribute(attribute): if _attribute(attribute): if _attribute(attribute): if _attribute): if _attribute(attribute): if _attribute): if _attribute(attribute): if _attribute(attribute): if _attribute(attribute): if _attribute): if _attribute(attribute): if _attribute(attribute): if _attribute(attribute): if _attribute(attribute): if _attribute):,"def is_attribute_deprecated(self, attribute): rule_set = self._attribute_rule_sets.get(attribute) if rule_set.version_deprecated: if self._version >= rule_set.version_deprecated: return True else: return False else: return False"
300,"def df.unit(self, expression, default): return self.df.unit(expression, default)","def unit(self, expression, default=None): expression = _ensure_string_from_expression(expression) try: unit_or_quantity = eval(expression, expression_namespace, scopes.UnitScope(self)) unit = unit_or_quantity.unit if hasattr(unit_or_quantity, ""unit"") else unit_or_quantity return unit if isinstance(unit, astropy.units.Unit) else None except: try: return eval(expression, expression_namespace, scopes.UnitScope(self, 1.)).unit except: return default"
301,str->str = str->str = str->str = str->str = str->str = str->str = str->str,"def showroom_get_roomid_by_room_url_key(room_url_key): fake_headers_mobile =  'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Charset': 'UTF-8,*;q=0.5', 'Accept-Encoding': 'gzip,deflate,sdch', 'Accept-Language': 'en-US,en;q=0.8', 'User-Agent': 'Mozilla/5.0 (Linux; Android 4.4.2; Nexus 4 Build/KOT49H) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.114 Mobile Safari/537.36'  webpage_url = 'https://www.showroom-live.com/' + room_url_key html = get_content(webpage_url, headers = fake_headers_mobile) roomid = match1(html, r'room?room_id=(d+)') assert roomid return roomid"
302,"def search_path(self, PATH): return self.search_path(self.search_path)","def find_command(cmd, paths=None, pathext=None): if paths is None: paths = os.environ.get('PATH', '').split(os.pathsep) if isinstance(paths, six.string_types): paths = [paths] if pathext is None: pathext = get_pathext() pathext = [ext for ext in pathext.lower().split(os.pathsep) if len(ext)] if os.path.splitext(cmd)[1].lower() in pathext: pathext = [''] for path in paths: cmd_path = os.path.join(path, cmd) for ext in pathext: cmd_path_ext = cmd_path + ext if os.path.isfile(cmd_path_ext): return cmd_path_ext if os.path.isfile(cmd_path): return cmd_path raise BadCommand('Cannot find command %r' % cmd)"
303,"def delete_transfer_job(self, job_name, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id)","def delete_transfer_job(self, job_name, project_id): return ( self.get_conn().transferJobs().patch( jobName=job_name, body= PROJECT_ID: project_id, TRANSFER_JOB: STATUS1: GcpTransferJobsStatus.DELETED, TRANSFER_JOB_FIELD_MASK: STATUS1,, ).execute(num_retries=self.num_retries) )"
304,"def locate_unbalanced_start(self, unbalanced_start, unbalanced_start, unbalanced_start, unbalanced_start, unbalanced_start, unbalanced_start, unbalanced_start, unbalanced_start, unbalanced_start, unbalanced_start, unbalanced_start, unbalanced_start, unbalanced_start, unbalanced_start)","def locate_unbalanced_end(unbalanced_end, pre_delete, post_delete): while 1: if not unbalanced_end: break finding = unbalanced_end[-1] finding_name = finding.split()[0].strip('>/') if not pre_delete: break next = pre_delete[-1] if next is DEL_END or not next.startswith('/'): break name = next.split()[0].strip('>/') if name == 'ins' or name == 'del': break if name == finding_name: unbalanced_end.pop() post_delete.insert(0, pre_delete.pop()) else: break"
305,def main_program(self): if self.program(self.program): self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program) self.program(self.program) self.program(self.program) self.program) self.program(self.program) self.program(self.program) self.program) self.program(self.program) self.program(self.program) self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program) self.program(self.program) self.program(self.program) self,"def main(argv): global g_script_name g_script_name = os.path.basename(argv[0]) parse_config_file() parse_args(argv) url = 'https://0xdata.atlassian.net/rest/api/2/search?jql=sprint=""' + urllib.quote(g_sprint) + '""&maxResults=1000' r = requests.get(url, auth=(g_user, g_pass)) if (r.status_code!= 200): print(""ERROR: status code is "" + str(r.status_code)) sys.exit(1) j = r.json() issues = j[u'issues'] pm = PeopleManager() for issue in issues: pm.add(issue) pm.emit() print("""")"
306,"def lookup_zone(self, boto.route53.Route53Connection_zone): if self.get_zone(zone): self.get_zone(zone)","def lookup_zone(conn, zone): all_zones = conn.get_all_hosted_zones() for resp in all_zones['ListHostedZonesResponse']['HostedZones']: if resp['Name'].rstrip('.') == zone.rstrip('.'): return resp['Id'].replace('/hostedzone/', '') raise ZoneNotFoundError('zone %s not found in response' % zone)"
307,def detect_problem(self): if self.debug(self.debug): self.debug(self.debug) self.debug(self.debug) self.debug(self.debug) self.debug(self.debug) self.debug(self.debug) self.debug(self.debug) self.debug(self.debug) self.debug(self.debug) self.debug(self.debug) self.debug(self.debug(self.debug(self.debug) self.debug(self.debug) self.debug(self.debug) self.debug(self.debug) self.debug(self.debug) self.debug(self.debug) self.debug(self.debug) self.debug(self.debug) self.debug(self.debug) self.debug(self.debug) self.debug(self.debug) self.debug(self.debug) self.debug(self.debug) self.debug(self.debug) self.debug(self.debug) self.debug(self.debug) self.debug(self.debug) self.debug(self.,"def append(self, child): assert child not in self.parents() VNode.append(self, child)"
308,"def get_name(self, subset): if self.get_name(subset): self.get_name(subset) self.get_name(subset) self.get_name(subset) self.get_name(subset) self.get_name(subset) self.get_name(subset) self.get_name(subset) self.get_name(subset) self.get_name(subset) self.get_name(subset) self.get_name(subset) self.get_name(subset) self.get_name(subset) self.get_name(subset) self.get_name(subset) self.get_name(subset) self.get_name(subset) self.get_name(subset) self.get_name(subset) self.get_name(subset) self.get_name(subset) self.get_name(subset) self.get_name(subset) self.get_name(subset) self.get_name(subset) self.get_name(subset) self.get_name(subset) self.get_name(subset) self.get_name(subset) self.get_name(subset) self.get_name","def _get_samples(self, subset=None): if subset is None: samples = self.subsets['All_Samples'] else: try: samples = self.subsets[subset] except KeyError: raise KeyError((""Subset ':s' does not "".format(subset) + ""exist.nUse'make_subset' to create a"" + ""subset."")) return samples"
309,"def change_handler(self, 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', '","def _observe_block(self, change): if self.is_initialized and change['type'] == 'update': old_block = change['oldvalue'] for c in self.children: old_block.children.remove(c) c.set_parent(None) self.refresh_items()"
310,"def _generate_key(self, key): if self.get_key(key): self.get_key(key): self.get_key(key) self.get_key(key) self.get_key(key) self.get_key(key) self.get_key(key) self.get_key(key) self.get_key(key) self.get_key(key) self.get_key(key) self.get_key(key) self.get_key(key) self.get_key) self.get_key(key) self.get_key(key) self.get_key) self.get_key(key) self.get_key(key) self.get_key) self.get_key(key) self.get_key(key) self.get_key) self.get_key(key) self.get_key(key) self.get_key(key) self.get_key) self.get_key(key) self.get_key(key) self.get_key(key) self.get_key(key) self.get_key(key) self.get_key(key) self.get_key) self.get_key(key) self.get_key(key) self.get_","def lookup(self, key): assert self._mph key = convert_to_bytes(key) box = ffi.new('char[]', key) try: result = _cmph.cmph_search(self._mph, box, len(key)) return result finally: del box"
311,"def turn_striplog(self, source, dlm, abbreviations): if self.striplog(self.striplog): self.striplog(self.striplog) self.striplog(self.striplog) self.striplog(self.striplog) self.striplog(self.striplog) self.striplog(self.striplog) self.striplog(self.striplog) self.striplog(self.striplog) self.striplog) self.striplog(self.striplog) self.striplog(self.striplog) self.striplog) self.striplog(self.striplog) self.striplog(self.striplog) self.striplog) self.striplog(self.striplog) self.striplog) self.striplog(self.striplog) self.striplog) self.striplog(self.striplog) self.striplog(self.striplog) self.striplog(self.striplog) self.striplog(self.striplog) self.striplog) self.striplog(self.striplog) self.striplog(self.s","def from_las3(cls, string, lexicon=None, source=""LAS"", dlm=',', abbreviations=False): f = re.DOTALL | re.IGNORECASE regex = r'w+?_Data.+?n(.+?)(?:nn+|n*|n*$)' pattern = re.compile(regex, flags=f) text = pattern.search(string).group(1) s = re.search(r'.(.+?):?.+?source', string) if s: source = s.group(1).strip() return cls.from_descriptions(text, lexicon, source=source, dlm=dlm, abbreviations=abbreviations)"
312,"def fetch_page(self, category, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs)","def fetch_items(self, category, **kwargs): from_date = kwargs['from_date'] reviews_api = kwargs['reviews_api'] mediawiki_version = self.client.get_version() logger.info(""MediaWiki version: %s"", mediawiki_version) if reviews_api: if ((mediawiki_version[0] == 1 and mediawiki_version[1] >= 27) or mediawiki_version[0] > 1): fetcher = self.__fetch_1_27(from_date) else: logger.warning(""Reviews API only available in MediaWiki >= 1.27"") logger.warning(""Using the Pages API instead"") fetcher = self.__fetch_pre1_27(from_date) else: fetcher = self.__fetch_pre1_27(from_date) for page_reviews in fetcher: yield page_reviews"
313,"def parse_single_statement(self, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none, none.","def parse_statement(self): self._skip_whitespace_and_comments() if self._current_token.kind == tokenize.ENDMARKER: return None stmt_loc = self._current_location(ignore_char_num=True) binding_key_or_keyword = self._parse_selector() statement = None if self._current_token.value!= '=': if binding_key_or_keyword == 'import': module = self._parse_selector(scoped=False) statement = ImportStatement(module, stmt_loc) elif binding_key_or_keyword == 'include': str_loc = self._current_location() success, filename = self._maybe_parse_basic_type() if not success or not isinstance(filename, str): self._raise_syntax_error('Expected file path as string.', str_loc) statement = IncludeStatement(filename, stmt_loc) else: self._raise_syntax_error(""Expected '='."") else: self._advance_one_token() value = self.parse_value() scope, selector, arg_name = parse_binding_key(binding_key_or_keyword) statement = BindingStatement(scope, selector, arg_name, value, stmt_loc) assert statement, 'Internal parsing error.' if (self._current_token.kind!= tokenize.NEWLINE and self._current_token.kind!= tokenize.ENDMARKER): self._raise_syntax_error('Expected newline.') elif self._current_token.kind == tokenize.NEWLINE: self._advance_one_token() return statement"
314,"def attach_transaction_trytes(trytes, min_weight_magnitude, min_weight_magnitude, depth, min_weight_magnitude, min_weight_magnitude, min_weight_magnitude, min_weight_magnitude): if min_weight_magnitude is not provided, if min_weight_magnitude is not provided, if min_weight_magnitude is not provided, if min_weight_magnitude is used","def send_trytes(self, trytes, depth=3, min_weight_magnitude=None): if min_weight_magnitude is None: min_weight_magnitude = self.default_min_weight_magnitude return extended.SendTrytesCommand(self.adapter)( trytes=trytes, depth=depth, minWeightMagnitude=min_weight_magnitude, )"
315,"def validate_OCSP_data(self, callback, data): callback = self.validate_OCSP_data = self.validate_OCSP_data = self.validate_OCSP_data = self.validate_OCSP_data = self.validate_OCSP_data = self.validate_OCSP_data = self.validate_OCSP_data = self.validate_OCSP_data = self.validate_OCSP_data = self.validate_OCSP_data = self.validate_OCSP_data = self.validate_OCSP_data = self.validate_OCSP_data","def set_ocsp_client_callback(self, callback, data=None): helper = _OCSPClientCallbackHelper(callback) self._set_ocsp_callback(helper, data)"
316,"def save_aggregate_report(aggregate_report, index_suffix, index_suffix, index_suffix, index_suffix, index_suffix, index_suffix, index_suffix, index_suffix, index_suffix, index_suffix, index_suffix, index_suffix, index_suffix, index_suffix, index_suffix, index_suffix, index_suffix, index_suffix, index_suffix, index_suffix, index_suffix, index_suffix_suffix_suffix_suffix_suffix_suffix_suffix_suffix_suffix_suffix_suffix_suffix_suffix_suffix_suffix_suffix_suffix_suffix_suffix_suffix_suffix)","def save_aggregate_report_to_elasticsearch(aggregate_report, index_suffix=None, monthly_indexes=False): logger.debug(""Saving aggregate report to Elasticsearch"") aggregate_report = aggregate_report.copy() metadata = aggregate_report[""report_metadata""] org_name = metadata[""org_name""] report_id = metadata[""report_id""] domain = aggregate_report[""policy_published""][""domain""] begin_date = human_timestamp_to_datetime(metadata[""begin_date""]) end_date = human_timestamp_to_datetime(metadata[""end_date""]) begin_date_human = begin_date.strftime(""%Y-%m-%d %H:%M:%S"") end_date_human = end_date.strftime(""%Y-%m-%d %H:%M:%S"") if monthly_indexes: index_date = begin_date.strftime(""%Y-%m"") else: index_date = begin_date.strftime(""%Y-%m-%d"") aggregate_report[""begin_date""] = begin_date aggregate_report[""end_date""] = end_date date_range = [aggregate_report[""begin_date""], aggregate_report[""end_date""]] org_name_query = Q(dict(match=dict(org_name=org_name))) report_id_query = Q(dict(match=dict(report_id=report_id))) domain_query = Q(dict(match=""published_policy.domain"": domain)) begin_date_query = Q(dict(match=dict(date_range=begin_date))) end_date_query = Q(dict(match=dict(date_range=end_date))"
317,"def parse_git_log_stream(self, log_stream): if self.log_stream(log_stream): self.log_stream(log_stream) self.log_stream(log_stream) self.log_stream(log_stream) self.log_stream(log_stream) self.log_stream(log_stream) self.log_stream(log_stream) self.log_stream(log_stream) self.log_stream(log_stream) self.log_stream(log_stream) self.log_stream(log_stream) self.log_stream(log_stream) self.log_stream(log_stream) self.log_stream(log_stream) self.log_stream(log_stream) self.log_stream(log_stream) self.log_stream(log_stream) self.log_stream(log_stream) self.log_stream(log_stream) self.log_stream(log_stream) self.log_stream(log_stream) self.log_stream(log_stream) self.log_stream(log_stream) self.log_stream(log_stream) self.log_stream(log_stream) self.log_stream(log_stream) self.log_stream(log_stream) self.log_stream(log","def parse(self): for line in self.stream: line = line.rstrip('n') parsed = False self.nline += 1 while not parsed: parsed = self.handlers[self.state](line) if self.state == self.COMMIT and self.commit: commit = self._build_commit() logger.debug(""Commit %s parsed"", commit['commit']) yield commit if self.commit: commit = self._build_commit() logger.debug(""Commit %s parsed"", commit['commit']) yield commit"
318,"def _inferred_type(self, inferred_type): if self.inferred_type is None: self.inferred_type = self.inferred_type = self.inferred_type = self.inferred_type = self.inferred_type = self.inferred_type = self.inferred_type = self.inferred_type = self.inferred_type = self.inferred_type = self.inferred_type = self.inferred_type = self.inferred_type = self.inferred_type = self.inferred_type = self.inferred_type = self.inferred_type = self.inferred_type = self.inferred_type = self.inferred_type = self.inferred_type",def node_type(node: astroid.node_classes.NodeNG) -> Optional[type]: types = set() try: for var_type in node.infer(): if var_type == astroid.Uninferable or is_none(var_type): continue types.add(var_type) if len(types) > 1: return None except astroid.InferenceError: return None return types.pop() if types else None
319,"def synthesisedUnitOrIntf(self, modelCls, modelCls, modelCls, modelCls, modelCls, modelCls, modelCls, modelCls, modelCls, modelCls, modelCls, modelCls, modelCls, modelCls, modelCls, modelCls, modelCls, modelCls, modelCls, modelCls, modelCls, modelCls, modelCls)","def reconnectUnitSignalsToModel(synthesisedUnitOrIntf, modelCls): obj = synthesisedUnitOrIntf subInterfaces = obj._interfaces if subInterfaces: for intf in subInterfaces: reconnectUnitSignalsToModel(intf, modelCls) else: s = synthesisedUnitOrIntf s._sigInside = getattr(modelCls, s._sigInside.name)"
320,def x = y3 + py + q = 0 y1 y2 + y1 y3 + y2 y3 + y2 y3 + y2 y3 + y1 y3 + y2 y3 + y1 y3 + y2 y3 = 0 y1 y2 + y1 y3 + y2 y3 + y2 y3 = 0 y1 y2 + y3 = 0 y1 y2 + y3 + y3 + y3 + y3 + y3 + y3 + y3 + y3 + y3 + y3 + y3 + y3 + y3 + y3 + y3 + y3 + y3 + y3 + y3 + y3 + y3 + y3 = y3 + y3 + y3 + y3 + y3 + y3 + y3 = y3 + y3 + y3 + y3 + y3 + y3 + y3 + y3 + y3 + y3 + y3 + y3 +,"def cubic(a, b, c, d=None): if d: a, b, c = b / float(a), c / float(a), d / float(a) t = a / 3.0 p, q = b - 3 * t**2, c - b * t + 2 * t**3 u, v = quadratic(q, -(p/3.0)**3) if type(u) == type(0j): r, w = polar(u.real, u.imag) y1 = 2 * cbrt(r) * cos(w / 3.0) else: y1 = cbrt(u) + cbrt(v) y2, y3 = quadratic(y1, p + y1**2) return y1 - t, y2 - t, y3 - t"
321,"def handle_pOST_request(self, Boundary Url Action, Boundary Url Action, Boundary Url Action, Boundary Url Action, Boundary Url Action, Boundary Url Action, Boundary Url Action, Boundary Url Action, Boundary Url Action, Boundary Url Action, Boundary Url Action, Boundary Url Action, Boundary Url Action, Boundary Url Action, Boundary Url Action, Boundary Url Action, Boundary Url Action, Boundary Url Action)","def do_POST(self): self.send_response(urllib2.httplib.OK) self.end_headers() content_length = int(self.headers['Content-Length']) body = self.rfile.read(content_length) print(""Client: 0"".format(str(self.client_address))) print(""headers: 0"".format(self.headers)) print(""path: 0"".format(self.path)) print(""body: 0"".format(body))"
322,"def newnews_gen(self, pattern, timestamp): newnews_gen = self.newnews_gen(self.newnews_gen) if self.newnews_gen(self.newnews_gen)","def newnews(self, pattern, timestamp): return [x for x in self.newnews_gen(pattern, timestamp)]"
323,"def convert_timestamp(self, value, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError(self, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError","def datetimeobj_YmdHMS(value): i = int(value) S = i M = S//100 H = M//100 d = H//100 m = d//100 Y = m//100 return datetime.datetime( Y % 10000, m % 100, d % 100, H % 100, M % 100, S % 100, tzinfo=TZ_GMT )"
324,"def parse_e_alias(e_alias, e_alias, e_alias, e_alias, e_alias, e_alias, e_alias, e_alias, e_alias, e_alias, e_alias, e_alias, e_alias, e_alias, e_alias, e_alias, e_alias): e_alias, e_alias, e_alias, e_alias, e_alias, e_alias, e_alias, e_alias, e_alias, e_alias, e_alias, e_name, e_name, e_name, e_name, e_name, e_name, e_name, e_name, e_name, e_name, e_name, e_name, e_name, e_name, e_name, e_name, e_name, e_name, e_name, e_name, e_name, e_name, e_name, e_name,","def parse_rrset_alias(e_alias): alias_hosted_zone_id = e_alias.find('./*HostedZoneId').text alias_dns_name = e_alias.find('./*DNSName').text return alias_hosted_zone_id, alias_dns_name"
325,"def key-down_action(self, key-down_action): key-down_action(key-down_action): key-down_action(key-down_action): key-down_action(key-down_action): key-down_action(key-down_action): key-down_action(key-down_action): key-down_action(key-down_action): key-down_action(key-down_action): key-down_action(key-down_action): key-down_action): key-down_action(key-down_action): key-down_action(key-down_action): key-down_action): key-down_action(key-down_action): key-down_action): key-down_action(key-down_action): key-down_action): key-down_action(key-down_action): key-down_action(key-down_action): key-down_action(key-down_action): key-down_action(key-down_action): key-down_action): key-down_action(key-down_action): key-down_action(key-down_action): key-down_action(key-down_action):","def input_text_with_keyboard_emulation(self, text): ActionChains(self.driver).key_down(text).key_up(Keys.CONTROL).perform()"
326,"def create_input_file(self, option, jinja2): jinja2 = jinja2 = jinja2 jinja2 jinja2 jinja2 jinja2 jinja2 jinja2 jinja2 jinja2 jinja2 jinja2 jinja2 jinja2 jinja2 jinja2 jinja2 jinja2 jinja2 jinja2 jinja2 jinja2 jinja2 jinja2 jinja2 jinja2 jinja2.","def create_input(option, template_name, template_location=""template""): jinja2_input =  for item in option: try: jinja2_input.update(item) except ValueError: raise RuntimeError( (""inputs.py, create_input : format of item '0' is not "" ""supported. Expecting a dictionary."".format(str(item)))) import jinja2 try: template_loader = jinja2.FileSystemLoader(searchpath=template_location) template_env = jinja2.Environment(loader=template_loader) template = template_env.get_template(template_name) output_text = template.render(jinja2_input) except jinja2.TemplateNotFound: raise RuntimeError(""template '0' not found"".format(template_name)) return output_text"
327,"def compare_empty(h2o.h2oFrame, y, measure=false): compare_empty(h2o.h2oFrame, x, measure=false): compare_empty(h2o.h2oFrame, y, measure=false): compare_empty(h2o.h2oFrame, y, measure=false): compare_empty(h2oFrame, y, measure=false): compare_empty(h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2o.h2","def strdistance(self, y, measure=None, compare_empty=True): assert_is_type(y, H2OFrame) assert_is_type(measure, Enum('lv', 'lcs', 'qgram', 'jaccard', 'jw','soundex')) assert_is_type(compare_empty, bool) return H2OFrame._expr(expr=ExprNode(""strDistance"", self, y, measure, compare_empty))._frame()"
328,"def serialize_hdltype(self, hdltype, hdltype, hdltype, hdltype, hdltype, hdltype, hdltype, hdltype, hdltype, hdltype, hdltype, hdltype, hdltype, hdltype, hdltype, hdltype, hdltype, hdltype, hdltype, hdltype, hdltype, hdltype, hdltype, hdltype)","def HdlType(cls, typ: HdlType, ctx: SerializerCtx, declaration=False): if isinstance(typ, Bits): sFn = cls.HdlType_bits elif isinstance(typ, HEnum): sFn = cls.HdlType_enum elif isinstance(typ, HArray): sFn = cls.HdlType_array elif isinstance(typ, Integer): sFn = cls.HdlType_int elif isinstance(typ, HBool): sFn = cls.HdlType_bool else: raise NotImplementedError(""type declaration is not implemented"" "" for type %s"" % (typ.name)) return sFn(typ, ctx, declaration=declaration)"
329,"def convert_sky_coordinates(phi, theta, phirot, phirot, phirot, thetarot, phirot, phirot, phirot, phirot, phirot, phirot, phirot, phirot, phirot, phirot, phirot, phirot, phirot, phirot, phirot, phirot, phirot, phirot, phirot, phirot, phirot, phirot, phirot, phirot, phirot)","def transformSkyCoordinates(self, phi, theta): r=ones_like(phi) x, y, z = sphericalToCartesian(r, phi, theta) xrot, yrot, zrot = self.transformCartesianCoordinates(x, y, z) r, phirot, thetarot = cartesianToSpherical(xrot, yrot, zrot) return phirot, thetarot"
330,"def read_field_output_report(self, field_output_report, field_output_report, field_output_report, field_output_report, field_output_report, field_output_report, field_output_report)","def read_field_report(path, data_flag = ""*DATA"", meta_data_flag = ""*METADATA""): text = open(path).read() mdpos = text.find(meta_data_flag) dpos = text.find(data_flag) mdata = io.StringIO( ""n"".join(text[mdpos:dpos].split(""n"")[1:])) data = io.StringIO( ""n"".join(text[dpos:].split(""n"")[1:])) data = pd.read_csv(data, index_col = 0) data = data.groupby(data.index).mean() mdata = pd.read_csv(mdata, sep = ""="", header = None, index_col = 0)[1] mdata = mdata.to_dict() out =  out[""step_num""] = int(mdata[""step_num""]) out[""step_label""] = mdata[""step_label""] out[""frame""] = int(mdata[""frame""]) out[""frame_value""] = float(mdata[""frame_value""]) out[""part""] = mdata[""instance""] position_map = ""NODAL"": ""node"", ""ELEMENT_CENTROID"": ""element"", ""WHOLE_ELEMENT"": ""element"" out[""position""] = position_map[mdata[""position""]] out[""label""] = mdata[""label""] out[""data""] = data field_class = getattr(argiope.mesh, mdata[""argiope_class""]) return field_class(**out)"
331,"def flare_radius(self, flare_radius, end_height, end_height, flare_radius, end_height, flare_radius, end_height, flare_radius, end_height, flare_radius, end_height, flare_radius, end_height, flare_radius, end_height, flare_radius, end_height, flare_radius, end_height, flare_radius, end_height, flare_radius, flare_radius, end_height, flare_radius, end_height, flare_radius, flare_radius, flare_radius, flare_radius, flare_radius, flare_radius, flare_radius, flare_radius, flare_radius, flare_radius, flare_radius, flare_radius, flare_radius, flare_radius, flare_radius, flare_radius, flare_radius, flare_radius, flare_radius, flare_radius, flare_radius, flare_radius, flare_radius, flare_radius, flare_radius, flare_radius, flare_radius, flare_radius, flare_radius, flare_radius, flare_radius, flare_radius,","def radial_flare2(script, flare_radius=None, start_radius=None, end_radius=None, end_height=None): if (end_radius is not None) and (end_height is not None): #flare_radius = '-((start_radius-end_radius)2 + end_height2)/(2*(start_radius-end_radius))' if (end_radius - start_radius)  end_height: flare_radius = -((start_radius-end_radius)**2 + end_height**2)/(2*(start_radius-end_radius)) #print('flare_radius = %s' % flare_radius) else: print('Error, end_radius is too large for end_height; angle is > 90d') r_func = 'if(z>0, (r) + (flare_radius) - (flare_radius)*sqrt(1-z2/(flare_radius)2), (r))'.replace('flare_radius', str(flare_radius)) #r_func = 'if(z>0, (r) + (flare_radius) - (flare_radius)*sqrt(1-z2/(flare_radius)2), (r))'.replace('flare_radius', str(flare_radius)).replace('start_radius', str(start_radius)).replace('end_radius', str(end_radius)).replace('end_height', str(end_height)) function_cyl_co(script, r_func) return None"
332,"def compare_artist(self, artists): if self.artist(artist): self.artist(artist): self.artist(artist): self.artist(artist): self.artist(artist): self.artist(artist)","async def related_artists(self) -> List[Artist]: related = await self.__client.http.artist_related_artists(self.id) return list(Artist(self.__client, item) for item in related['artists'])"
333,def disconnect_instance(self): if self.instance(self.instance): self.instance(self.instance) self.instance(self.instance) self.instance(self.instance) self.instance(self.instance) self.instance(self.instance) self.instance(self.instance) self.instance(self.instance) self.instance(self.instance) self.instance(self.instance) self.instance,"def disconnect(self, instance, another_instance): if instance not in self: return False if another_instance not in self[instance]: return False self[instance].remove(another_instance) return True"
334,"def run_multiple_commmands(self, commands, commands, commands, commands, commands, commands, commands, commands, commands, commands, commands, commands, commands, commands, commands, commands, commands, commands, commands, commands, commands, commands)","def call_in_sequence(self, cmds, shell=True): for cmd in cmds: if subprocess.call(cmd, shell=shell) == 1: sys.exit(1)"
335,"def update_object(self, data): if self.get_data(data): self.get_data(data): self.get_data(data) self.get_data(data) self.get_data(data) self.get_data(data) self.get_data(data) self.get_data(data) self.get_data(data) self.get_data(data) self.get_data(data) self.get_data() self.get_data","def update(self, data): fields = [ 'id','status', 'type', 'persistence', 'date_start', 'date_finish', 'date_created', 'date_modified', 'checksum', 'processor_name', 'input', 'input_schema', 'output', 'output_schema','static','static_schema', 'var', 'var_template', ] self.annotation =  for f in fields: setattr(self, f, data[f]) self.name = data['static']['name'] if 'name' in data['static'] else '' self.annotation.update(self._flatten_field(data['input'], data['input_schema'], 'input')) self.annotation.update(self._flatten_field(data['output'], data['output_schema'], 'output')) self.annotation.update(self._flatten_field(data['static'], data['static_schema'],'static')) self.annotation.update(self._flatten_field(data['var'], data['var_template'], 'var'))"
336,"def close_file_descriptor(self, file_descriptor, file_descriptor, file_descriptor, file_descriptor, file_descriptor, file_descriptor, file_descriptor, file_descriptor, file_descriptor, file_descriptor, file_descriptor, file_descriptor, file_descriptor, file_descriptor, file_descriptor, file_descriptor)","def close_fds(keep_fds): keep_fds = set(keep_fds).union([1, 2]) try: open_fds = set(int(fd) for fd in os.listdir('/proc/self/fd')) except FileNotFoundError: import resource max_nfds = resource.getrlimit(resource.RLIMIT_NOFILE)[0] open_fds = set(fd for fd in range(3, max_nfds)) open_fds.add(0) for i in open_fds - keep_fds: try: os.close(i) except OSError: pass"
337,"def model(inputs, sample_size, length, sample_size, sample_size, sample_size, sample_size, sample_size, sample_size, sample_size, sample_size, sample_size, sample_size, sample_size, sample_size, sample_size, sample_size, sample_size, sample_size, sample_size, sample_size, sample_size, sample_size, sample_size, sample_size, sample_size, sample_size, sample_size, sample_size, sample_size, sample_size, sample_size)","def visualize_qualitative_analysis(inputs, model, samples=1, batch_size=3, length=8): average = lambda dist: tf.reduce_mean( input_tensor=dist.mean(), axis=0) with tf.compat.v1.name_scope(""val_reconstruction""): reconstruct = functools.partial(model.reconstruct, inputs=inputs, samples=samples) visualize_reconstruction(inputs, average(reconstruct())) visualize_reconstruction(inputs, average(reconstruct(sample_static=True)), name=""static_prior"") visualize_reconstruction(inputs, average(reconstruct(sample_dynamic=True)), name=""dynamic_prior"") visualize_reconstruction(inputs, average(reconstruct(swap_static=True)), name=""swap_static"") visualize_reconstruction(inputs, average(reconstruct(swap_dynamic=True)), name=""swap_dynamic"") with tf.compat.v1.name_scope(""generation""): generate = functools.partial(model.generate, batch_size=batch_size, length=length, samples=samples) image_summary(average(generate(fix_static=True)), ""fix_static"") image_summary(average(generate(fix_dynamic=True)), ""fix_dynamic"")"
338,"def get_sample_counts(self, sample_counts): self.get_sample_counts(sample_counts) self.get_sample_counts(sample_counts) self.get_sample_counts(sample_counts) self.get_sample_counts(sample_counts) self.get_sample_counts(sample_counts) self.get_sample_counts(sample_counts) self.get_sample_counts) self.get_sample_counts(sample_counts) self.get_sample_counts) self.get_sample_counts(sample_counts) self.get_sample_counts(sample_counts) self.get_sample_counts(sample_counts) self.get_sample_counts) self.get_sample_counts(sample_counts) self.get_sample_counts(sample_counts) self.get_sample_counts(sample_counts) self.get_sample_counts(sa","def rooted_samples_by_line(self, filename): rooted_leaf_samples, _ = self.live_data_copy() rooted_line_samples =  for root, counts in rooted_leaf_samples.items(): cur =  for key, count in counts.items(): code, lineno = key if code.co_filename!= filename: continue cur[lineno] = count rooted_line_samples[root] = cur return rooted_line_samples"
339,"def generate_text_chunks(self, el, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag, skip_tag","def flatten_el(el, include_hrefs, skip_tag=False): if not skip_tag: if el.tag == 'img': yield ('img', el.get('src'), start_tag(el)) else: yield start_tag(el) if el.tag in empty_tags and not el.text and not len(el) and not el.tail: return start_words = split_words(el.text) for word in start_words: yield html_escape(word) for child in el: for item in flatten_el(child, include_hrefs=include_hrefs): yield item if el.tag == 'a' and el.get('href') and include_hrefs: yield ('href', el.get('href')) if not skip_tag: yield end_tag(el) end_words = split_words(el.tail) for word in end_words: yield html_escape(word)"
340,"def nest_interceptable(self, double, kwargs): self.interceptable(self.interceptable): self.interceptable(self.interceptable) self.interceptable(self.interceptable) self.interceptable(self.interceptable) self.interceptable(self.interceptable) self.interceptable(self.interceptable) self.interceptable(self.interceptable) self.interceptable(self.interceptable) self.interceptable(self.interceptable) self.interceptable(self.interceptable) self.interceptable(self.interceptable) self.interceptable(self.interceptable) self.interceptable(self.interceptable) self.interceptable(self.interceptable) self.interceptable(self.interceptable) self.interceptable(self.interceptable) self.interceptable(self.interceptable) self.interceptable(self.interceptable) self.interceptable(self.interceptable) self.interceptable(self.interceptable) self.interceptable(self.interceptable) self.interceptable(self.interceptable) self.interceptable(self.interceptable) self.inter",def get_next_interceptor(): try: interceptor = _interceptor_stack.stack.pop() yield interceptor finally: _interceptor_stack.stack.append(interceptor)
341,"def set_statevector(self, statevector, statevector, statevector, statevector, statevector, statevector, statevector, statevector, statevector, statevector, statevector, statevector, statevector, statevector, statevector, statevector, statevector, statevector, statevector, statevector, statevector, statevector, statevector, statevector, statevector, statevector, statevector, statevector, statevector, statevector)","def _initialize_statevector(self): if self._initial_statevector is None: self._statevector = np.zeros(2 ** self._number_of_qubits, dtype=complex) self._statevector[0] = 1 else: self._statevector = self._initial_statevector.copy() self._statevector = np.reshape(self._statevector, self._number_of_qubits * [2])"
342,"def encoding(self, str, avoid, int): return self._encoding(self._encoding(self._encoding(self._encoding))","def get_rand_str(encoding='latin1', l=64, avoid=[]): ret = unicode('') while len(ret)  l: rndint = get_rand_int(encoding=encoding, avoid=avoid) ret += unichr(rndint) return ret"
343,"def compute_zero_crossings(y, threshold, dtype=boolean, zero_pos=boolean, zero_pos=boolean, zero_pos=boolean, zero_pos=boolean, zero_pos=boolean, zero_pos=boolean, zero_pos=boolean, zero_pos=boolean, zero_pos=boolean, zero_pos=boolean, zero_pos=boolean, zero_crossings, zero_crossings, zero_crossings, zero_crossings, zero_crossings, zero_crossings, zero_crossings, zero_crossings, zero_crossings, zero_crossings, zero_crossings, zero_crossings, zero_crossings, zero_crossings, zero_crossings, zero_crossings, zero_crossings, zero_crossings, y_crossings, y_crossings, y_crossings, y_crossings, y_crossings, y_crossings, y_crossings, y_crossings, y_crossings, y_crossings, y_crossings, y_","def zero_crossings(y, threshold=1e-10, ref_magnitude=None, pad=True, zero_pos=True, axis=-1): if threshold is None: threshold = 0.0 if six.callable(ref_magnitude): threshold = threshold * ref_magnitude(np.abs(y)) elif ref_magnitude is not None: threshold = threshold * ref_magnitude if threshold > 0: y = y.copy() y[np.abs(y) = threshold] = 0 if zero_pos: y_sign = np.signbit(y) else: y_sign = np.sign(y) slice_pre = [slice(None)] * y.ndim slice_pre[axis] = slice(1, None) slice_post = [slice(None)] * y.ndim slice_post[axis] = slice(-1) padding = [(0, 0)] * y.ndim padding[axis] = (1, 0) return np.pad((y_sign[tuple(slice_post)]!= y_sign[tuple(slice_pre)]), padding, mode='constant', constant_values=pad)"
344,"def args(self, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, chls, chls, chls, chls, chls, chls, chls, chls, chls, chls, chls, chls, chls, chls, chls, chls, chls, chls, chls, chls, chls, chls, chls, chls, chls, chls, chls, chls, chls, chls, chls, chls, chls, chls, chls, chls, chl","def line(self, *args): self.lines.append(','.join(['%.1f'%x for x in map(float,args)])) return self"
345,"def model_matrix(model_matrix, model_coefficients, offset, offset, offset, offset, offset, offset, offset, offset, offset, offset, offset, offset, offset, offset, offset, offset, offset, offset, offset, offset, offset, offset, offset, offset, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args","def prepare_args(model_matrix, response, model_coefficients, predicted_linear_response, offset, name=None): graph_deps = [model_matrix, response, model_coefficients, predicted_linear_response, offset] with tf.compat.v1.name_scope(name, 'prepare_args', graph_deps): dtype = dtype_util.common_dtype(graph_deps, np.float32) model_matrix = tf.convert_to_tensor( value=model_matrix, dtype=dtype, name='model_matrix') if offset is not None: offset = tf.convert_to_tensor(value=offset, dtype=dtype, name='offset') response = tf.convert_to_tensor( value=response, dtype=dtype, name='response') use_default_model_coefficients = model_coefficients is None if use_default_model_coefficients: batch_shape = tf.shape(input=model_matrix)[:-2] num_columns = tf.shape(input=model_matrix)[-1] model_coefficients = tf.zeros( shape=tf.concat([batch_shape, [num_columns]], axis=0), dtype=dtype, name='model_coefficients') else: model_coefficients = tf.convert_to_tensor( value=model_coefficients, dtype=dtype, name='model_coefficients') if predicted_linear_response is None: if use_default_model_coefficients: if offset is None: predicted_linear_response = tf.zeros_like( response, dtype,"
346,"def publish_weather_data(self, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args): args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args,","def set( self, pressure='NA', dewpoint='NA', humidity='NA', tempf='NA', rainin='NA', rainday='NA', dateutc='NA', windgust='NA', windgustdir='NA', windspeed='NA', winddir='NA', clouds='NA', weather='NA', *args, **kw): self.args.update(  'baromin':pressure, 'clouds':clouds, 'dailyrainin':rainday, 'dateutc':dateutc, 'dewptf':dewpoint, 'humidity':humidity, 'rainin':rainin, 'tempf':tempf, 'weather':weather, 'winddir':winddir, 'windgustdir':windgustdir, 'windgustmph':windgust, 'windspeedmph':windspeed,  ) log.debug( self.args )"
347,"def edit_vcard(self, selected_vcard, editor): editor = self.edit_vcard(edit_vcard, editor)","def source_subcommand(selected_vcard, editor): child = subprocess.Popen([editor, selected_vcard.filename]) child.communicate()"
348,"def add_result(self, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj): traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj","def multiply(traj, result_list): z=traj.x*traj.y result_list[traj.v_idx] = z"
349,"def x_ref(x_ref_min, x_ref_max, x_ref_max, x_ref_max, x_ref_max, x_ref_max, x_ref_max, x_ref_max, x_ref_max, x_ref_max, x_ref_max, x_ref_max, x_ref_max, x_ref_min, x_ref_min, x_ref_max, x_ref_max, x_ref_max, x_ref_max, x_ref_max, x_ref_max, x_ref_max, x_ref_max, x_ref_max, x_ref_max, x_ref_max, x_ref_max, x_ref_max, x_ref_max, x_ref_max, x_ref_max, x_ref_max, x_ref_max, x_ref_max, x_ref_max, x_ref_max, x_re","def batch_interp_regular_1d_grid(x, x_ref_min, x_ref_max, y_ref, axis=-1, fill_value='constant_extension', fill_value_below=None, fill_value_above=None, grid_regularizing_transform=None, name=None): return _interp_regular_1d_grid_impl( x, x_ref_min, x_ref_max, y_ref, axis=axis, batch_y_ref=True, fill_value=fill_value, fill_value_below=fill_value_below, fill_value_above=fill_value_above, grid_regularizing_transform=grid_regularizing_transform, name=name or 'batch_interp_regular_1d_grid')"
350,"def approve(address _spender, uint256 _value): append(address _spender, uint256 _value) append(address _spender, uint256 _value) append(address _spender, uint256 _value) append(address _spender, uint256 _value) append(address _spender) append(address _spender, uint256 _value) append(address _value) append(address _value) append(address _value) append(address _value) append(address _value) append(address _value) append(address _value) append(address _value) append(address _value) append(address _value) append(address _value) append(address _value) append(address _value) append(address _value) append(address _value) append(address _value) append(address","def approve(ctx, _spender='address', _value='uint256', returns=STATUS): ctx.allowances[ctx.msg_sender][_spender] += _value ctx.Approval(ctx.msg_sender, _spender, _value) return OK"
351,"def validate_accept_haven(self, accept_none, accept_none, accept_none, accept_none, accept_none, accept_none, accept_none, accept_none, accept_none, accept_none, accept_none, accept_none, accept_none, accept_none, accept_none, accept_none)","def validate_accept(form, field): if field.data and form.reject.data: raise validators.ValidationError( _(""Both reject and accept cannot be set at the same time."") )"
352,"def curses_horizontal_padding(self, cursor): if curses_horizontal_padding(self, cursor): curses_horizontal_padding(self, cursor): curses_horizontal_padding(self, cursor): curses_horizontal_padding(self, cursor): curses_horizontal_padding(self, cursor): curses_horizontal_padding(self, cursor): curses_horizontal_padding(self, cursor): curses_horizontal_padding(self, cursor): curses_horizontal_padding(self, cursor)): curses_horizontal_padding(self, cursor): curses_horizontal_padding(self, cursor): curses_horizontal_padding(self, cursor): curses_horizontal_padding(self, curses_horizontal_padding): curses_horizontal_padding(self, curses_horizontal_padding): curses_horizontal_padding(self, curses_horizontal_padding): curses_horizontal_padding(self, curses_horizontal","def _rightleft(self, direction): if direction == ""left"" and self.padding!= 0: self.padding -= 1 if direction == ""right"" and  self.screen.getmaxyx()[1] + self.padding  self.max_width: self.padding += 1"
353,"def run_command_inside_sandbox(self, args, timeout, stdin, check, timeout, check): args = self.run_command_inside_sandbox_inside_sandbox_inside_sandbox_inside_sandbox_inside_sandbox_inside_sandbox_inside_sandbox_inside_sandbox_inside_sandbox_inside_sandbox_inside_inside_sandbox_inside_outside_inside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_outside_","def run_command(self, args: List[str], max_num_processes: int=None, max_stack_size: int=None, max_virtual_memory: int=None, as_root: bool=False, stdin: FileIO=None, timeout: int=None, check: bool=False, truncate_stdout: int=None, truncate_stderr: int=None) -> 'CompletedCommand': cmd = ['docker', 'exec', '-i', self.name, 'cmd_runner.py'] if stdin is None: cmd.append('--stdin_devnull') if max_num_processes is not None: cmd += ['--max_num_processes', str(max_num_processes)] if max_stack_size is not None: cmd += ['--max_stack_size', str(max_stack_size)] if max_virtual_memory is not None: cmd += ['--max_virtual_memory', str(max_virtual_memory)] if timeout is not None: cmd += ['--timeout', str(timeout)] if truncate_stdout is not None: cmd += ['--truncate_stdout', str(truncate_stdout)] if truncate_stderr is not None: cmd += ['--truncate_stderr', str(truncate_stderr)] if not as_root: cmd += ['--linux_user_id', str(self._linux_uid)] cmd += args if self.debug: print('running:"
354,def ________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________,"def process(filename=None, url=None, key=None): if filename is not None: yield from process_json(filename) elif url and key: yield from process_url(url, key)"
355,"def check_certificate_expired(self, certificate_expired): if certificate_expired(certificate_expired): if certificate_expired(certificate_expired): if certificate_expired(certificate_expired): if certificate_expired(certificate_expired): if certificate_expired(certificate_expired(certificate_expired)): if certificate_expired(certificate_expired): if certificate_expired(certificate_expired): if certificate_expired(certificate_expired): if certificate_expired(certificate_expired): if certificate_expired(certificate_expired_expired_expired_expired_expired_expired_expired_expired_expired_expired_expired_expired_expired_expired_","def has_expired(self): time_string = _native(self.get_notAfter()) not_after = datetime.datetime.strptime(time_string, ""%Y%m%d%H%M%SZ"") return not_after  datetime.datetime.utcnow()"
356,"def cache_token(self, token): if self._token(token): self._token(token) self._token(token) self._token(token) self._token(token) self._token(token) self._token(token) self._token(token) self._token(token) self._token(token) self._token(token) self._token(token) self._token(token) self._token(token) self._token(token) self._token(token) self._token(token) self._token(token) self._token(token) self._token(token) self._token(token) self._token(token) self._token(token) self._token(token) self._token(token) self._token(token) self._token(token) self._token(token) self._token(token) self._token(token) self._token(token) self._token","def tenant_token(self): rv = getattr(self, '_tenant_token', None) if rv is None: rv = self._tenant_token = self.tenant.get_token() return rv"
357,def ________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________,"def galleries(self): api_version = self._get_api_version('galleries') if api_version == '2018-06-01': from.v2018_06_01.operations import GalleriesOperations as OperationClass elif api_version == '2019-03-01': from.v2019_03_01.operations import GalleriesOperations as OperationClass else: raise NotImplementedError(""APIVersion  is not available"".format(api_version)) return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))"
358,"def delta_function_distribution(self, delta_function_distribution, delta_function_distribution, delta_function_distribution, delta_function_distribution, delta_function_distribution, delta_function_distribution, delta_function_distribution)","def delta_function(cls, x_pos, weight=1., min_width=MIN_INTEGRATION_PEAK): distribution = cls(x_pos,0.,is_log=True, min_width=min_width) distribution.weight = weight return distribution"
359,"def extract_file_information(self, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs)","def _srvc_extract_file_information(self, kwargs): if 'filename' in kwargs: self._filename = kwargs.pop('filename') if 'file_title' in kwargs: self._file_title = kwargs.pop('file_title') if 'trajectory_name' in kwargs: self._trajectory_name = kwargs.pop('trajectory_name') if 'trajectory_index' in kwargs: self._trajectory_index = kwargs.pop('trajectory_index')"
360,"def fetch_item(self, queue): if queue is not empty, if queue is empty, if queue is empty, if queue is empty, if queue is empty, if queue is empty, if queue is empty, if queue is empty, if queue is empty, if queue is empty, if queue is empty, if queue is empty, if queue is empty, if queue is empty, if queue is empty, if queue is empty, if queue is empty, if queue is empty, if queue is empty, if queue is empty, if queue is empty, if queue is empty, if queue is empty, if queue is empty, if queue is empty, if queue is empty, if queue is empty, if queue is empty, if queue is empty, if queue is empty, if queue is empty, if queue = if queue = if queue = if queue = if queue = if queue = if queue = if queue = if queue = queue = queue = queue = queue = queue = queue = queue = queue = queue = queue = queue = queue = queue = queue = queue = queue = queue = queue = queue = queue = queue = queue = queue = queue = queue = queue = queue = queue = queue = queue = queue = queue =",def get(self): with self.cvar: while True: if self.size.value > 0: rval = self.vals[self.tail.value] self.tail.value = (self.tail.value + 1) % len(self.vals) self.size.value -= 1 if rval == -2: return QueueClosed assert(rval >= 0) return rval self.cvar.wait()
361,"def consume_text_stream(iterable, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen) maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen, maxlen","def spool(iterable, maxlen=1250): words = int() text = str() try: while True: while words  maxlen: stream = yield text = reduce(accumulator, stream, text) words = reduce(accumulator, stream, words) iterable.send(text) words = int() text = str() except GeneratorExit: iterable.send(text) iterable.close()"
362,"def create_api_wrapper(self, api_key): api_wrapper = self.get_api_key(api_key) self.get_api_key(api_key) self.get_api_key(api_key) self.get_api_key(api_key) self.get_api_key(api_key) self.get_api_key(api_key) self.get_api_key(api_key) self.get_api_key(api_key) self.get_api_key) self.get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_get_","def search(term=None, phrase=None, limit=DEFAULT_SEARCH_LIMIT, api_key=GIPHY_PUBLIC_KEY, strict=False, rating=None): return Giphy(api_key=api_key, strict=strict).search( term=term, phrase=phrase, limit=limit, rating=rating)"
363,"def _consolidate_flac_padding_metadata_blocks(self, padding_metadata_blocks): if _consolidate_flac_padding_metadata_blocks(padding_metadata_blocks): if _consolidate_flac_padding_metadata_blocks(padding_metadata_blocks): if _consolidate_flac_metadata_blocks(padding_metadata_blocks): padding_metadata_blocks(padding_metadata_blocks): padding_metadata_metadata_metadata_metadata_metadata_metadata_metadata_metadata_metadata_metadata_metadata_metadata_metadata_metadata_metadata_metadata_metadata_metadata_metadata_metadata_metadata_metadata_metadata_metadata_metadata_metadata_metadata_metadata_metadata_metadata_metadata_metadata_meta","def group_padding(blocks): paddings = [b for b in blocks if isinstance(b, Padding)] for p in paddings: blocks.remove(p) size = sum(padding.length for padding in paddings) padding = Padding() padding.length = size + 4 * (len(paddings) - 1) blocks.append(padding)"
364,"def q(z_1:T, x_1:T, f): q(z_1:T, x_1:T, f): q(z_1:T, f): q(z_1:T, x_1:T, f): q(z_1:T, f): q(z_1:T, f): q(z_1:T, f): q(z_1:T, f): q(z_1:T, f): q(z_1:T, x_1:T, f): q(z_1:T, x_1:T, x_1:T, f): q(z_1:T, f): q(z_1:T, f): q(z_1:T, x_1:T, x_1:T, f): f): f): f): f)","def call(self, inputs): features, static_sample = inputs length = tf.shape(input=features)[-2] static_sample = static_sample[..., tf.newaxis, :] + tf.zeros([length, 1]) sample_shape_static = tf.shape(input=static_sample)[:-3] sample_shape_inputs = tf.shape(input=features)[:-3] broadcast_shape_inputs = tf.concat((sample_shape_static, [1, 1, 1]), 0) broadcast_shape_static = tf.concat((sample_shape_inputs, [1, 1, 1]), 0) features = features + tf.zeros(broadcast_shape_inputs) static_sample = static_sample + tf.zeros(broadcast_shape_static) combined = tf.concat((features, static_sample), axis=-1) collapsed_shape = tf.concat(([-1], tf.shape(input=combined)[-2:]), axis=0) out = tf.reshape(combined, collapsed_shape) out = self.bilstm(out) out = self.rnn(out) expanded_shape = tf.concat( (tf.shape(input=combined)[:-2], tf.shape(input=out)[1:]), axis=0) out = tf.reshape(out, expanded_shape) out = self.output_layer(out) loc = out[..., :self.latent_size] scale_diag = tf.nn.softplus(out[..., self.latent_size:]) + 1e-5 return tfd."
365,"def constructor(self, tags): if self.get_node(self.get_node(self.get_node)): self.get_node(self.get_node): self.get_node(self.get_node)","def for_tag(cls, parser, token, preposition, error_hint): tokens = token.split_contents() if len(tokens) >= 3 and tokens[1] == preposition: as_var = cls.get_as_var(tokens) tree_alias = parser.compile_filter(tokens[2]) return cls(tree_alias, as_var) raise template.TemplateSyntaxError( '%r tag requires at least two arguments. E.g. %% %s %%.' % (tokens[0], error_hint))"
366,"def get_file(self, bucket_name, object_name, filename): self.get_file(self.get_file(self.get_file(self.get_file(self.get_file(self.get_file_name))) self.get_file(self.get_file_name) self.get_file(self.get_file_name) self.get_file(self.get_file_name) self.get_file(self.get_filename) self.get_filename) self.get_filename) self.get_filename(self.get_filename) self.get_filename) self.get_filename) self.get_filename(self.get_filename) self.get_filename(self.get_filename) self.get_filename) self.get_filename(self.get_filename) self.get_filename) self.get_filename(self.get_filename) self.get_filename(self.get_filename) self.get_filename(self.get_filename) self.get_filename(self.get_filename) self.get_filename(self.get_filename) self.get_filename(self.get_filename) self.","def download(self, bucket_name, object_name, filename=None): client = self.get_conn() bucket = client.get_bucket(bucket_name) blob = bucket.blob(blob_name=object_name) if filename: blob.download_to_filename(filename) self.log.info('File downloaded to %s', filename) return blob.download_as_string()"
367,"def disease_obj(self, disease_obj, disease_obj, disease_obj, disease_obj, disease_obj, disease_obj, disease_obj, disease_obj, disease_obj, disease_obj, disease_obj, disease_obj, disease_obj, disease_obj, disease_obj, disease_obj, disease_obj, disease_obj, disease_obj)","def load_disease_term(self, disease_obj): LOG.debug(""Loading disease term %s into database"", disease_obj['_id']) try: self.disease_term_collection.insert_one(disease_obj) except DuplicateKeyError as err: raise IntegrityError(""Disease term %s already exists in database"".format(disease_obj['_id'])) LOG.debug(""Disease term saved"")"
368,"def log(x, cdf(x)): cdf(x, cdf(x)): cdf(x): cdf(x): cdf(x): cdf(x): cdf(x): cdf(x): cdf(x): cdf(x): cdf(x): cdf(x): cdf(x): cdf(x): cdf(x): cdf(x): cdf(x): cdf(x): cdf(x): cdf(x): cdf(x): cdf(x): cdf(x): cdf(x): cdf(x): cdf(x): cdf(x): cdf(x): cdf(x): cdf(x): cdf(x): cdf(x): cdf(x): cdf(x): cdf(x): cdf","def _log_ndtr_lower(x, series_order): x_2 = tf.square(x) log_scale = -0.5 * x_2 - tf.math.log(-x) - 0.5 * np.log(2. * np.pi) return log_scale + tf.math.log(_log_ndtr_asymptotic_series(x, series_order))"
369,"def start_virtual_machine(self, service_name, deployment_name, role_name, role_name, role_name, service_name, deployment_name, role_name, role_name, role_name, service_name, deployment_name, role_name, role_name, role_name, service_name, deployment_name, role_name, role_name, service_name, deployment_name, role_name, role_name, role_name, service_name, service_name, deployment_name, role_name, role_name, role_name, role_name, role_name, role_name, service_name, role_name, role_name, role_name, role_name, service_name, role_name, role_name, role_name, role_name, service_name, role_name, role_name, role_name, role_name, role_name, role_name, role_name, service_name, service_name, role_name, role_name, role_name, role_name, role_name, role_name, service_name, role_name, service_name, role_name, role_name, service_name, role_name, service_name, role_name, role_name, service_name, role_name, role_","def restart_role(self, service_name, deployment_name, role_name): _validate_not_none('service_name', service_name) _validate_not_none('deployment_name', deployment_name) _validate_not_none('role_name', role_name) return self._perform_post( self._get_role_instance_operations_path( service_name, deployment_name, role_name), _XmlSerializer.restart_role_operation_to_xml( ), as_async=True)"
370,"def max_value(x, y, z): def max_value(x, y, z): def max_value(x, y, z): def max_value(x, y, z): def max_value(x, y, z): def max_value(x, y, z): def max_value(x, y, z): def max_value(x, y)","def maxs(self): return np.array([self.x_max, self.y_max, self.z_max])"
371,"def r""== r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""= r""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""","def t_EQUAL(self, t): r""="" t.endlexpos = t.lexpos + len(t.value) return t"
372,"def delete_thing(self, thingTypeId, thingId, thingId, thingId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId, thingTypeId","def removeThing(self, thingTypeId, thingId): thingUrl = ApiClient.thingUrl % (self.host, thingTypeId, thingId) r = requests.delete(thingUrl, auth=self.credentials, verify=self.verify) status = r.status_code if status == 204: self.logger.debug(""Thing was successfully removed"") return True elif status == 401: raise ibmiotf.APIException(401, ""The authentication token is empty or invalid"", None) elif status == 403: raise ibmiotf.APIException(403, ""The authentication method is invalid or the api key used does not exist"", None) elif status == 404: raise ibmiotf.APIException(404, ""A thing type or thing instance with the specified id does not exist."", None) elif status == 409: raise ibmiotf.APIException(409, ""The thing instance is aggregated into another thing instance."", None) elif status == 500: raise ibmiotf.APIException(500, ""Unexpected error"", None) else: raise ibmiotf.APIException(None, ""Unexpected error"", None)"
373,"def send_SIGINT(self, child, SIGINT): if child is not a SIGINT, if child is not a SIGINT, if child is not a SIGINT, if child is not a SIGINT, if child is not a SIGINT, if child is not a SIGINT, if child is not a SIGINT, if child is not a SIGINT, if child is not a SIGINT =","def sendintr(self): if hasattr(termios, 'VINTR'): char = termios.tcgetattr(self.child_fd)[6][termios.VINTR] else: char = chr(3) self.send (char)"
374,"def collect_block_level_options_line_numbers(self, block_level_options_line_numbers, block_level_options_line_numbers, block_level_options_line_numbers, block_level_options_numbers, block_level_options_numbers, block_level_options_numbers, block_level_options_numbers, block_level_options_numbers, block_level_options_numbers, block_level_options_numbers, block_level_options_numbers)","def collect_block_lines(self, msgs_store, module_node): for msg, lines in self._module_msgs_state.items(): self._raw_module_msgs_state[msg] = lines.copy() orig_state = self._module_msgs_state.copy() self._module_msgs_state =  self._suppression_mapping =  self._effective_max_line_number = module_node.tolineno self._collect_block_lines(msgs_store, module_node, orig_state)"
375,"def embed_IPython(self, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config,","def embed(**kwargs): config = kwargs.get('config') header = kwargs.pop('header', u'') if config is None: config = load_default_config() config.InteractiveShellEmbed = config.TerminalInteractiveShell kwargs['config'] = config global _embedded_shell if _embedded_shell is None: _embedded_shell = InteractiveShellEmbed(**kwargs) _embedded_shell(header=header, stack_depth=2)"
376,"def update_object(self, obj): obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj = obj =","def from_object(self, obj): if isinstance(obj, string_types): obj = import_string(obj) for key in dir(obj): if key.isupper(): self[key] = getattr(obj, key)"
377,def collect_method(self): if self.get_method(self.get_method(self.get_method(self.get_method(self.get_method(self.get_method(self.get_method))))),"def rpc_spec_table(app): table =  for attr, value in inspect.getmembers(app): rpc_spec = get_rpc_spec(value, default=None) if rpc_spec is None: continue table[rpc_spec.name] = (value, rpc_spec) return table"
378,"def parse_qs(self, parse_qsl, parse_qsl, parse_qsl, parse_qsl, parse_qsl, parse_qsl, parse_qsl, parse_qsl, parse_qsl, parse_qsl, parse_qsl, parse_qsl, parse_qsl, parse_qsl, parse_qsl, parse_qsl)","def parse_qsd(data, name=""query string"", exception=PluginError, schema=None, **params): value = dict(parse_qsl(data, **params)) if schema: value = schema.validate(value, name=name, exception=exception) return value"
379,"def extract_xml(input_path, input_path, input_path, input_path, input_path, input_path, input_path, input_path, input_path, input_path, input_path, input_path, input_path, input_path, input_path, input_path, input_path, input_path, input_path, input_path, input_path, input_path, input_path, str, input_path, input_path, input_path, str, input_path, input_path, input_path, str, input_path, input_path, str, input_path, input_path, input_path, input_path, input_path, input_path, input_path, input_path, input_path, input_path, input_path, input_path, input_path, input_path, input_path, input_path, str, input_path, str, input_path, str, input_path, str, input_path, input_path, input_path, input_path, input_path, input_path, input_path, input_path, input_path, input_path, output_path, output_path, output_path, output_path, output_path, output_path, output_path, output_path, output_","def extract_xml(input_): if type(input_) == str: file_object = open(input_, ""rb"") elif type(input_) == bytes: file_object = BytesIO(input_) else: file_object = input_ try: header = file_object.read(6) file_object.seek(0) if header.startswith(MAGIC_ZIP): _zip = zipfile.ZipFile(file_object) xml = _zip.open(_zip.namelist()[0]).read().decode() elif header.startswith(MAGIC_GZIP): xml = GzipFile(fileobj=file_object).read().decode() elif header.startswith(MAGIC_XML): xml = file_object.read().decode() else: file_object.close() raise InvalidAggregateReport(""Not a valid zip, gzip, or xml file"") file_object.close() except UnicodeDecodeError: raise InvalidAggregateReport(""File objects must be opened in binary "" ""(rb) mode"") except Exception as error: raise InvalidAggregateReport( ""Invalid archive file: 0"".format(error.__str__())) return xml"
380,"def get_size(self, get_size, add_padding, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, return_mb, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up, round_up,","def get_size(self, add_padding=True, round_up=True, return_mb=True): if not hasattr(self,'manifests'): bot.error('Please retrieve manifests for an image first.') sys.exit(1) size = 768 for schemaVersion, manifest in self.manifests.items(): if ""layers"" in manifest: size = 0 for layer in manifest[""layers""]: if ""size"" in layer: size += layer['size'] if add_padding is True: size = size * 5 if return_mb is True: size = size / (1024 * 1024) if round_up is True: size = math.ceil(size) size = int(size) return size"
381,def scapy_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_packet_,def pkt_text(pkt): if pkt.src.upper() in BANNED_DEVICES: body = '' elif pkt.src.upper()[:8] in AMAZON_DEVICES: body ='(Amazon Device)'.format(pkt.src) else: body = pkt.src return body
382,"def set_color_map(self, QColor, QColor, QColor, QColor, QColor, QColor, QColor, QColor, QColor, QColor, QColor, QColor, QColor, QColor, QColor, QColor, QColor, QColor, QColor, QColor, QColor, QColor, QColor, QColor, QColor, QColor, QColor, QColor, QColor)","def set_background_color(self, color): self.default_color_map = self.darkbg_color_map.copy() if color.value() >= 127: for i in xrange(8): self.default_color_map[i + 8] = self.default_color_map[i] self.default_color_map[7] = self.default_color_map[15] = 'black' self.color_map.update(self.default_color_map)"
383,"def load_all_modules(module, package, module): if module, package, module, pylint_checker = pylint_checker = pylint_checker = pylint_checker = pylint_checker = pylint_checker = pylint_checker = pylint_checker = pylint_checker = pylint_checker = pylint_checker = pylint_checker = pylint_checker = pylint_checker = pylint_checker = pylint_checker = pylint_checker = pylint_checker = pylint_checker = pylint_checker = pylint_checker = pylint_checker = pylint_checker = pylint_checker = pylint_checker = pylint_checker = pylint_checker = py","def register_plugins(linter, directory): imported =  for filename in listdir(directory): base, extension = splitext(filename) if base in imported or base == ""__pycache__"": continue if ( extension in PY_EXTS and base!= ""__init__"" or (not extension and isdir(join(directory, base))) ): try: module = modutils.load_module_from_file(join(directory, filename)) except ValueError: continue except ImportError as exc: print( ""Problem importing module %s: %s"" % (filename, exc), file=sys.stderr ) else: if hasattr(module, ""register""): module.register(linter) imported[base] = 1"
384,"def getPosition(self, position): self.getPosition(position) self.getPosition(position) self.getPosition(position) self.getPosition(position) self.getPosition(position) self.getPosition(position) self.getPosition(position) self.getPosition(position) self.getPosition(position) self.getPosition(position)","def position(self): pos = self._position if pos is None and self.children: ch1 = self.children[0] if isinstance(ch1, ParseNode): pos = ch1.position return pos"
385,"def sign_input(self, sign_input, sign_input, sign_input, sign_input, sign_input, sign_input, sign_input, sign_input, sign_input, sign_input, sign_input, sign_input, sign_input, sign_input, sign_input, sign_input, sign_input, sign_input, sign_input, sign_input, sign_input, sign_input, sign_input, sign_input, sign_input, sign_input)","def sign_inputs(self, key_generator): if not self.hash: raise RuntimeError('Cannot sign inputs until bundle is finalized.') i = 0 while i  len(self): txn = self[i] if txn.value  0: if txn.address.key_index is None: raise with_context( exc=ValueError( 'Unable to sign input input;''key_index is None''(exc.context has more info).'.format( input=txn.address, ), ), context= 'transaction': txn,, ) if txn.address.security_level is None: raise with_context( exc=ValueError( 'Unable to sign input input;''security_level is None''(exc.context has more info).'.format( input=txn.address, ), ), context= 'transaction': txn,, ) self.sign_input_at(i, key_generator.get_key_for(txn.address)) i += txn.address.security_level else: i += 1"
386,"def accessed_attribute(self, function/method, super call, metaclasses): if self.get_attribute(self.get_attribute): self.get_attribute(self.get_attribute) self.get_attribute(self.get_attribute) self.get_attribute(self.get_attribute) self.get_attribute(self.get_attribute) self.get_attribute) self.get_attribute) self.get_attribute(self.get_attribute) self.get_attribute(self.get_attribute) self.get_attribute(self.get_attribute) self.get_attribute) self.get_attribute(self.get_attribute) self.get_attribute(self.get_attribute) self.get_attribute(self.get_attribute) self.get_attribute(self.get_attribute) self.get_attribute(self.get_attribute) self.get_attribute(self","def visit_attribute(self, node): for pattern in self.config.generated_members: if re.match(pattern, node.attrname): return if re.match(pattern, node.as_string()): return try: inferred = list(node.expr.infer()) except exceptions.InferenceError: return missingattr = set() non_opaque_inference_results = [ owner for owner in inferred if owner is not astroid.Uninferable and not isinstance(owner, astroid.nodes.Unknown) ] if ( len(non_opaque_inference_results)!= len(inferred) and self.config.ignore_on_opaque_inference ): return for owner in non_opaque_inference_results: name = getattr(owner, ""name"", None) if _is_owner_ignored( owner, name, self.config.ignored_classes, self.config.ignored_modules ): continue try: if not [ n for n in owner.getattr(node.attrname) if not isinstance(n.statement(), astroid.AugAssign) ]: missingattr.add((owner, name)) continue except AttributeError: continue except exceptions.NotFoundError: if not _emit_no_member( node, owner, name, ignored_mixins=self.config.ignore_mixin_members, ignored_none=self.config.ignore_none, ): continue missingattr.add((owner, name)) continue break else: done = set() for owner, name in missingattr: if isinstance(owner, astroid.Instance): actual = owner._proxied else: actual = owner if actual in done:"
387,"def create_string(self, ', ', ', ', ', ', ', ', ', ', ', ', ', ', ', ', ', ', ', ', ', ', ', ', ', ', ', ', ', ', ', ', ', ', ')",def domain_name_left_cuts(domain): cuts = [] if domain: parts = domain.split('.') for i in range(len(parts)): cuts.append( '.'.join(parts[i:])) return cuts
388,"def getdict(self,'response', 'ChanVariable','response','response','response','response','response','response','response','response','response','response','response','response','response','response','response','response','response','response','response','response','response','response','response','response','response','response','response','response','response','response','res","def getdict(self, key): values = self.get(key, None) if not isinstance(values, list): raise TypeError(""0 must be a list. got 1"".format(key, values)) result = utils.CaseInsensitiveDict() for item in values: k, v = item.split('=', 1) result[k] = v return result"
389,"def execute_network_run(experimental_run, pre-run): if pre-run is not pre-run: if pre-run is not pre-run: if pre-run is not pre-run: if pre-run is not pre-run: if pre-run is not pre-run: if pre-run is not pre-run: if pre-run is not pre-run: if pre-run is not pre-run: if pre-run is not pre-run: if pre-run = if pre-run = if pre-run = if pre-run = if pre-run = if pre-run = if pre-run = if pre-run = if pre-run = if pre-run = if pre-run = if pre-run = if pre-run = if pre-run = if pre-run = if pre-run = if pre-run = if pre-run = if pre-run = if pre-run = if pre-run = if pre-run = if pre-run = if pre-run = if pre-run = if pre-run = if pre-run = if pre-run = if pre-run = if pre-run =","def _execute_network_run(self, traj, network, network_dict, component_list, analyser_list, pre_run=False): subrun_list = self._extract_subruns(traj, pre_run=pre_run) subrun_number = 0 while len(subrun_list) > 0: current_subrun = subrun_list.pop(0) for component in component_list: component.add_to_network(traj, network, current_subrun, subrun_list, network_dict) for analyser in analyser_list: analyser.add_to_network(traj, network, current_subrun, subrun_list, network_dict) self.add_to_network(traj, network, current_subrun, subrun_list, network_dict) self._logger.info('STARTING subrun %s (#%d) lasting %s.' % (current_subrun.v_name, subrun_number, str(current_subrun.f_get()))) network.run(duration=current_subrun.f_get(), report=self._report, report_period=self._report_period) for analyser in analyser_list: analyser.analyse(traj, network, current_subrun, subrun_list, network_dict) self.remove_from_network(traj, network, current_subrun, subrun_list, network_dict) for analyser in analyser_list: analyser.remove_from_network(traj, network, current_subrun, subrun_list, network_dict) for component in component_list: component.remove_from_network(traj, network, current_subrun, subrun_list, network_dict) subrun_number += 1"
390,"def get_token(self, chunks, tokens): self.get_token(tokens) self.get_token(tokens) self.get_token(tokens) self.get_token(tokens) self.get_token(tokens) self.get_token(tokens) self.get_token(tokens) self.get_token(tokens) self.get_token(tokens) self.get_token(tokens) self.get_token(tokens) self.get_token(tokens) self.get_token(tokens) self.get_token(tokens) self.get_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token","def fixup_chunks(chunks): tag_accum = [] cur_word = None result = [] for chunk in chunks: if isinstance(chunk, tuple): if chunk[0] == 'img': src = chunk[1] tag, trailing_whitespace = split_trailing_whitespace(chunk[2]) cur_word = tag_token('img', src, html_repr=tag, pre_tags=tag_accum, trailing_whitespace=trailing_whitespace) tag_accum = [] result.append(cur_word) elif chunk[0] == 'href': href = chunk[1] cur_word = href_token(href, pre_tags=tag_accum, trailing_whitespace="" "") tag_accum = [] result.append(cur_word) continue if is_word(chunk): chunk, trailing_whitespace = split_trailing_whitespace(chunk) cur_word = token(chunk, pre_tags=tag_accum, trailing_whitespace=trailing_whitespace) tag_accum = [] result.append(cur_word) elif is_start_tag(chunk): tag_accum.append(chunk) elif is_end_tag(chunk): if tag_accum: tag_accum.append(chunk) else: assert cur_word, ( ""Weird state, cur_word=%r, result=%r, chunks=%r of %r"" % (cur_word, result, chunk, chunks)) cur_word.post_tags.append(chunk) else: assert(0) if not result: return [token('', pre_tags=tag_accum)] else: result[-1].post_tags.extend(tag_accum) return result"
391,"def create(self, OMIM-AUTO): self.create(self.create(self.create, OMIM-AUTO)) self.create(OMIM-AUTO) self.create(OMIM-AUTO) self.create(OMIM-AUTO) self.create(OMIM-AUTO) self.create(OMIM-AUTO) self.create(OMIM-AUTO) self.create(OMIM-AUTO) self.create(OMIM-AUTO) self.create(OMIM-AUTO) self.create(OMIM-AUTO) self.create(OMIM-AUTO) self.create(OMIM-AUTO) self.create(OMIM-AUTO) self.create(OMIM-AUTO) self.create(OMIM-AUTO) self.create(OMIM-AUTO) self.create(OMIM-AUTO) self.create(OMIM-AUTO) self.create(OMIM-AUTO) self.create(OMIM-AUTO) self.create(OMIM-AUTO) self.create(OMIM-AUTO) self.create(OMIM-AUTO) self.create(OMIM-AUTO) self.create(OMIM-AUTO) self.create(OMIM-AUTO) self.create(OM","def load_omim_panel(self, api_key, institute=None): existing_panel = self.gene_panel(panel_id='OMIM-AUTO') if not existing_panel: LOG.warning(""OMIM-AUTO does not exists in database"") LOG.info('Creating a first version') version = 1.0 if existing_panel: version = float(math.floor(existing_panel['version']) + 1) LOG.info(""Setting version to %s"", version) try: mim_files = fetch_mim_files(api_key=api_key, genemap2=True, mim2genes=True) except Exception as err: raise err date_string = None for line in mim_files['genemap2']: if 'Generated' in line: date_string = line.split(':')[-1].lstrip().rstrip() date_obj = get_date(date_string) if existing_panel: if existing_panel['date'] == date_obj: LOG.warning(""There is no new version of OMIM"") return panel_data =  panel_data['path'] = None panel_data['type'] = 'clinical' panel_data['date'] = date_obj panel_data['panel_id'] = 'OMIM-AUTO' panel_data['institute'] = institute or 'cust002' panel_data['version'] = version panel_data['display_name'] = 'OMIM-AUTO' panel_data['genes'] = [] alias_genes = self.genes_by_alias() genes = get_omim_panel_genes( genemap2_lines = mim_files['genemap2'], mim2gene_lines = mim_files['mim2genes'], alias_gene"
392,"def streaming_log(ti, try_number, metadata): ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = ti = t","def _read(self, ti, try_number, metadata=None): if not metadata: metadata = 'offset': 0 if 'offset' not in metadata: metadata['offset'] = 0 offset = metadata['offset'] log_id = self._render_log_id(ti, try_number) logs = self.es_read(log_id, offset) next_offset = offset if not logs else logs[-1].offset metadata['offset'] = next_offset metadata['end_of_log'] = False if not logs  else logs[-1].message == self.end_of_log_mark.strip() cur_ts = pendulum.now() if 'last_log_timestamp' in metadata: last_log_ts = timezone.parse(metadata['last_log_timestamp']) if cur_ts.diff(last_log_ts).in_minutes() >= 5: metadata['end_of_log'] = True if offset!= next_offset or 'last_log_timestamp' not in metadata: metadata['last_log_timestamp'] = str(cur_ts) message = 'n'.join([log.message for log in logs]) return message, metadata"
393,"def thrift_span(self, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span, thrift_span,",def span_to_bytes(thrift_span): transport = TMemoryBuffer() protocol = TBinaryProtocol(transport) thrift_span.write(protocol) return bytes(transport.getvalue())
394,"def get_event(self, month, year): if self.get_event(self.get_event(month, year)) return self.get_event(month, year)","def all_month_events(self, year, month, category=None, tag=None, loc=False, cncl=False): kwargs = self._get_kwargs(category, tag) ym_first, ym_last = self.get_first_and_last(year, month) pref = [] if loc: pref.append(""location"") if cncl: pref.append(""cancellations"") r = Q(repeat=""YEARLY"") dstart_mo = Q(start_date__month=month) dend_mo = Q(end_date__month=month) dstart_yr = Q(start_date__year=year) dend_yr = Q(end_date__year=year) return self.model.objects.filter( r & (dstart_mo | dend_mo) | (Q(repeat=""NEVER"")) | ((dstart_yr | dend_yr) & (dstart_mo | dend_yr)), Q(end_repeat=None) | Q(end_repeat__gte=ym_first), start_date__lte=ym_last ).filter(**kwargs).prefetch_related(*pref).order_by('start_date').distinct()"
395,"def_buf(def_buf, def_buf, def_buf, raw_a, raw_b, raw_a, raw_b, raw_b, raw_a, raw_b, raw_b, raw_a, raw_b, raw_b, raw_b, raw_a, raw_b, raw_b, raw_a, raw_b, raw_b, raw_b, raw_a, raw_b, raw_b, raw_a, raw_b, raw_a, raw_b, raw_a, raw_b, raw_a, raw_b, raw_b, raw_b, raw_b, raw_a, raw_b, raw_b, raw_b, raw_b, raw_b, raw_a, raw_b, raw_b, raw_b, raw_b, raw_a, raw_b, raw_b, raw_b, raw_b, raw_a, raw_b, raw_b, raw_a, raw_b, raw_b, raw_b, raw_b, raw_b, raw_b, raw_b, raw_a, raw_b, raw_b, raw_b, raw_b, raw_b, raw_b, raw_b, raw_b","def dbInsert(self, def_buf, raw_a, raw_b): self.dbExec(self.sqlInsert(def_buf, raw_a, raw_b))"
396,"def heartbeating(self, heartbeating, heartbeating, heartbeating, heartbeating, heartbeating, heartbeating, heartbeating, heartbeating, heartbeating, heartbeating, heartbeating, heartbeating, heartbeating, heartbeating, heartbeating, heartbeating, heartbeating, heartbeating, heartbeating, heartbeating, heartbeating, heartbeating, heartbeating, heartbeating, heartbeating, heartbeating, heartbeating, heartbeating)",def stop_hb(self): if self._beating: self._beating = False self._hb_periodic_callback.stop() if not self.hb_stream.closed(): self.hb_stream.on_recv(None)
397,"def generate_subspaces(self, expressions_list, dimensions): if self.get_subspaces(self.get_subspaces): self.get_subspaces(self.get_subspaces) self.get_subspaces(self.get_subspaces) self.get_subspaces(self.get_subspaces) self.get_subspaces(self.get_subspaces) self.get_subspaces(self.get_subspaces) self.get_subspaces(self.get_subspaces) self.get_subspaces(self.get_subspaces) self.get_subspaces_subspaces_subspaces_subspaces_subspaces_subspaces_subspaces_subspaces_subspaces_subspaces_subspaces_subspaces_subspaces_subspaces_subspaces_subspaces_subspaces_subspaces_subspaces_subspaces_subspaces_subspaces_subspaces_subspaces_subspaces_subspaces_subspaces_subspaces_subspaces_subspaces_subspaces_subspaces_subspaces","def subspaces(self, expressions_list=None, dimensions=None, exclude=None, **kwargs): if dimensions is not None: expressions_list = list(itertools.combinations(self.get_column_names(), dimensions)) if exclude is not None: import six def excluded(expressions): if callable(exclude): return exclude(expressions) elif isinstance(exclude, six.string_types): return exclude in expressions elif isinstance(exclude, (list, tuple)): for e in exclude: if isinstance(e, six.string_types): if e in expressions: return True elif isinstance(e, (list, tuple)): if set(e).issubset(expressions): return True else: raise ValueError(""elements of exclude should contain a string or a sequence of strings"") else: raise ValueError(""exclude should contain a string, a sequence of strings, or should be a callable"") return False expressions_list = [expr for expr in expressions_list if not excluded(expr)] logger.debug(""expression list generated: %r"", expressions_list) import vaex.legacy return vaex.legacy.Subspaces([self(*expressions, **kwargs) for expressions in expressions_list])"
398,def ________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________,def get_identifier(self): if self._path: return os.path.basename(self._path) else: return hashlib.sha256(hashlib.sha256(repr(self._data).encode())).hexdigest()
399,"def synchronize_local_po_file(self, msgids, msgids, msgids, msgids, msgids, msgids, msgids, msgids, msgids, msgids, msgids, msgids, msgids, msgids, msgids, msgids, msgids)","def synchronize(self): gdocs_trans_csv = os.path.join(self.temp_path, GDOCS_TRANS_CSV) gdocs_meta_csv = os.path.join(self.temp_path, GDOCS_META_CSV) local_trans_csv = os.path.join(self.temp_path, LOCAL_TRANS_CSV) local_meta_csv = os.path.join(self.temp_path, LOCAL_META_CSV) try: entry = self._download_csv_from_gdocs(gdocs_trans_csv, gdocs_meta_csv) except PODocsError as e: if 'Sheet 1 not found' in str(e)  or 'Conversion failed unexpectedly' in str(e): self.upload() else: raise PODocsError(e) else: self._merge_local_and_gdoc(entry, local_trans_csv, local_meta_csv, gdocs_trans_csv, gdocs_meta_csv) try: csv_to_po(local_trans_csv, local_meta_csv, self.locale_root, self.po_files_path, self.header) except IOError as e: raise PODocsError(e) self._clear_temp()"
400,"def delete_stellar_tables(self, table_name): table_name = self.stellar_tables(table_name, table_name, table_name, table_name)","def gc(): def after_delete(database): click.echo(""Deleted table %s"" % database) app = get_app() upgrade_from_old_version(app) app.delete_orphan_snapshots(after_delete)"
401,"def normalize_host(self, str host, encode_with_idna): if self.host(self.host): self.encode_with_idna: self.encode_with_idna: self.encode_with_idna: self.encode_with_idna: self.encode_with_idna: self.encode_with_idna: self.encode_with_idna: self.encode_with_idna: self.encode_with_idna: self.encode_with_idna: self.encode_with_idna: self.encode_with_idna: self.encode_with_idna: self.encode_with_idna: self.encode_with_idna: self.encode_with_idna: self.encode_with_idna: self.encode_with_idna: self.encode_with_idna: self.encode_with_idna: self.encode_with_idna: self.encode_with_idna: self.","def _normalize_host(host, enable_long_host=False, encode_with_idna=None, scheme=None): if encode_with_idna is not None: enable_idna = encode_with_idna else: enable_idna = scheme.lower() in IDNA_SCHEMES if scheme else False if enable_idna: try: host = '.'.join(segment.encode('idna').decode() for segment in host.split('.')) except UnicodeError as exc: raise ValueError('host is invalid - 0'.format(exc)) else: host = parse.quote(host.encode('utf-8'), safe=HOST_SAFE_CHARS) if len(host) > 255 and not enable_long_host: raise ValueError('host too long') return host"
402,"def open_port(self, callback): self.open_port(self.open_port) self.open_port(self.open_port) self.open_port(self.open_port) self.open_port(self.open_port) self.open_port(self.open_port) self.open_port(self.open_port) self.open_port(self.open_port) self.open_port(self.open_port) self.open_port(self.open_port) self.open_port(self.open_port) self.open_port(self.open_port) self.open_port(self.open_port) self.open_port(self.open_port) self.open_port(self.open_port) self.open_port) self.open_port) self.open_port(self.open_port) self.open_port(self.open_port) self.open_port(self.open_port) self.open_port(self.open_port) self.open_port(self.open_port) self.open_port(self.open_port) self.open_port(self.open_port) self.open_port(self.open_port) self.open_port(self.open_port","def open(self): logging.debug(""Opening rpc system"") d = self._connectionpool.open(self._packet_received) def opened(_): logging.debug(""RPC system is open"") self._opened = True logging.debug(""Starting ping loop"") self._ping_loop.start(self._ping_interval, now=False) d.addCallback(opened) return d"
403,"def load_bytecode(self, bytecode): self.load_bytecode(bytecode) self.load_bytecode(bytecode) self.load_bytecode(bytecode) self.load_bytecode(bytecode) self.load_bytecode(bytecode) self.load_bytecode(bytecode) self.load_bytecode(bytecode) self.load_bytecode(bytecode) self.load_bytecode(bytecode) self.load_bytecode(bytecode) self.load_bytecode) self.load_bytecode(bytecode) self.load_bytecode) self.load_bytecode(bytecode) self.load_bytecode) self.load_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode_bytecode","def load_bytecode(self, f): magic = f.read(len(bc_magic)) if magic!= bc_magic: self.reset() return checksum = pickle.load(f) if self.checksum!= checksum: self.reset() return self.code = marshal_load(f)"
404,"def terminal_ipythonapp(self, method): if self.get_ipythonapp(method): self.get_ipythonapp(method): self.get_ipythonapp(method): self.get_ipythonapp(method)","def _classes_default(self): return [ InteractiveShellApp, self.__class__, TerminalInteractiveShell, PromptManager, HistoryManager, ProfileDir, PlainTextFormatter, IPCompleter, ScriptMagics, ]"
405,"def get_attr(self, object_key, argument_key, default_result, default_result, default_result, default_result, default_result, default_result, default_result, default_result, default_result, default_result, default_result, default_result, default_result, default_result, coerce_result, coerce_result, coerce_result, coerce_result, coerce_result_result_result_result_result_result_result_result_result_result_result_result_result_result_result_result_result_result_result_result_result_result_result_result_result)","def multiget_cached(object_key, argument_key=None, default_result=None, result_fields=None, join_table_name=None, coerce_args_to_strings=False): def create_wrapper(inner_f): return MultigetCacheWrapper( inner_f, object_key, argument_key, default_result, result_fields, join_table_name, coerce_args_to_strings=coerce_args_to_strings ) return create_wrapper"
406,"def make_json_request(self, url, str, data, dict): url = self.json_request(self.json_request(self.json_request(self.json_request)) url = self.json_request(self.json_request) self.json_request(self.json_request) self.json_request(self.json_request) self.json_request(self.json_request) self.json_request(self.json_request) self.json_request(self.json_request) self.json_request(self.json_request) self.json_request(self.json_request) self.json_request(self.json_request) self.json_request) self.json_request(self.json_request) self.json_request(self.json_request) self.json_request(self.json_request) self.json_request(self.json_request) self.json_request(self.json_request) self.json_","def jsonex_request(url, data, headers=None): url, headers = _parse_authentication(url) headers['Content-Type'] = 'application/json' try: req = Request(url, headers=headers) response = urlopen(req, jsonex_dumps(data)) res_str = response.read() res = jsonex_loads(res_str) except HTTPError as e: if 'Content-Type' in e.headers and e.headers['Content-Type'] == 'application/json': res = jsonex_loads(e.read()) else: raise exc.ServerError('Server at """" failed: '.format(url, e)) except URLError as e: raise exc.ConnectionError('Connection to """" failed: '.format(url, e)) if 'error' in res: raise res['error'] return res"
407,"def rss_feed(self, rss_feed, rss_feed, rss_feed, rss_feed, rss_feed, rss_feed, rss_feed, rss_feed, rss_feed, rss_feed, rss_feed, rss_feed, rss_feed, rss_feed, rss_feed): rss_feed, rss_feed, rss_feed, rss_feed, rss_feed, rss_feed, rss_feed, rss_feed, rss_feed, rss_feed, rss_feed, rss_feed, rss_feed, rss_feed, rss_feed, rss_feed, rss_feed, rss_feed, rss_feed, rss_feed, rss_feed, rss_feed, rss_feed, rss_feed, rss_feed, rss_feed,","def scrape(ctx, url): data = load_feed(url) feed = data['feed'] entries = data['entries'] _type = 'community' country = 'Czech Republic' for entry in entries: _id = sluggify(entry['id']) city = entry['tags'][0]['term'] landing = entry['link'] start_time = dt_normalize(entry['published_parsed'], local_tz=True) title = entry['title'] summary = entry['summary'] link = entry['link'] ipdb.set_trace()"
408,"def convert_channel_to_unitary_representation(self, channel, unitary_representation, unitary_representation, unitary_representation, unitary_representation, unitary_representation, unitary_representation, unitary_representation, unitary_representation, unitary_representation)","def to_operator(self): mat = _to_operator(self.rep, self._data, *self.dim) return Operator(mat, self.input_dims(), self.output_dims())"
409,"def _mock(self, interceptors): if self.mock(self.mock): self.mock(mock): self.mock(mock): self.mock(mock): self.mock(mock): self.mock(mock): self.mock(mock): self.mock(mock): self.mock(mock): self.mock(mock): self.mock(mock)",def activate(self): if self.active: return None self.mock_engine.activate() self.active = True
410,"def run_module(self, module): if self.run_module(module): self.run_module(module): self.run_module(module): self.run_module(module): self.run_module(module): self.run_module(module): self.run_module(module): self.run_module(module)","def is_single_module(): ret = False counts = get_counts() if counts[""modules""] == 1: ret = True elif counts[""modules""]  1: ret = is_single_class() return ret"
411,"def add_permission_view_role(permission_name, view_menu_name, view_menu_name, view_menu_name, view_menu_name, view_menu_name, view_menu_name, view_menu_name, view_menu_name, view_menu_name, view_menu_name, view_menu_name, view_menu_name, view_menu_name, view_menu_name, view_menu_name, view_menu_name, view_menu_name)","def _merge_perm(self, permission_name, view_menu_name): permission = self.find_permission(permission_name) view_menu = self.find_view_menu(view_menu_name) pv = None if permission and view_menu: pv = self.get_session.query(self.permissionview_model).filter_by( permission=permission, view_menu=view_menu).first() if not pv and permission_name and view_menu_name: self.add_permission_view_menu(permission_name, view_menu_name)"
412,"def array_like_gaussian_function(x, mu, sigma, amplitude, centre, width, amplitude, mu, sigma, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x,","def gauss(x, *p): A, mu, sigma = p return A * np.exp(-0.5 * (-mu + x)**2 / sigma**2)"
413,"def create(self, join, join): self.create(self.create(self.create(self.create))) self.create(self.create) self.create(self.create) self.create(self.create) self.create(self.create) self.create(self.create) self.create(self.create) self.create(self.create) self.create(self.create) self.create(self.create) self.create(self.create) self.create(self.create","def from_join(cls, join: Join) -> 'ConditionalJoin': return cls( join.table_name, join.parent_alias, join.table_alias, join.join_type, join.join_field, join.nullable )"
414,"def smiles(self, smiles, smiles): smiles = self.smiles(self.smiles)","def enumerate_tautomers_smiles(smiles): mol = Chem.MolFromSmiles(smiles, sanitize=False) mol = Standardizer().standardize(mol) tautomers = TautomerEnumerator().enumerate(mol) return Chem.MolToSmiles(m, isomericSmiles=True) for m in tautomers"
415,"def serialize_dataframe(name, description, data_type_id): data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id = data_type_id","def add_from_dataframe(self, dataframe, data_type_id, name, description): _not_none('dataframe', dataframe) _not_none_or_empty('data_type_id', data_type_id) _not_none_or_empty('name', name) _not_none_or_empty('description', description) try: output = BytesIO() serialize_dataframe(output, data_type_id, dataframe) raw_data = output.getvalue() finally: output.close() return self._upload(raw_data, data_type_id, name, description)"
416,"def get_invalid_callback_result(self, item): return self.get_invalid_callback(self.get_invalid_callback)","async def process_callback_result(self, callback_result): callback_result_name = type(callback_result).__name__ process_func_name = self.callback_result_map.get( callback_result_name, '') process_func = getattr(self, process_func_name, None) if process_func is not None: await process_func(callback_result) else: raise InvalidCallbackResult( f'Parse invalid callback result type: callback_result_name>' )"
417,"def header_separator(self, header_separator, colomn_separator, colomn_separator, colomn_separator, colomn_separator, colomn_separator, colomn_separator, colomn_separator, colomn_separator, colomn_separator, data_to_print, data_to_print, data_to_print, data_to_print, data_to_print, data_to_print, data_to_print, data_to_print, data_to_print, data_to_print, data_to_print, data_to_print, data_to_print, data_to_print, data_to_print, data_to_print, data_to_print, data_to_print, data_to_print, data_to_print, data_to_print, data_to_print, data_to_print, data_to_print, data_to_print, data_to_print, data_to_print, data_to_print, data_to_print, data_to_print, data_to_print, data_to_print","def _header_constructor( cls, data_to_print, header_separator=""-"", column_separator="" "" ): header_data = [] header_size = """" before_size = ""%-"" after_size = ""s"" if header_separator: header_separator_data = [] length_data_to_print = len(data_to_print) - 1 i = 0 for data in data_to_print: size = data_to_print[data] header_data.append(data) header_size += before_size + str(size) + after_size if i  length_data_to_print: header_size += column_separator if header_separator: header_separator_data.append(header_separator * size) i += 1 if header_separator: return [ header_size % tuple(header_data), header_size % tuple(header_separator_data), ] return [header_size % tuple(header_data)]"
418,def remove_default(self): if self.config(self.config): self.config(self.config) self.config(self.config) self.config(self.config) self.config(self.config) self.config(self.config) self.config(self.config) self.config(self.config) self.config(self.config) self.config(self.config) self.config(self.config) self.config(self.config) self.config(self.config,"def remove_option(self, section, option, remove_default=True): if super().has_option(section, option): super().remove_option(section, option) if self.airflow_defaults.has_option(section, option) and remove_default: self.airflow_defaults.remove_option(section, option)"
419,"def docwriter_get_object_name(self, func): self.get_object_name(self.get_object_name) self.get_object_name(self.get_object_name) self.get_object_name(self.get_object_name) self.get_object_name(self.get_object_name) self.get_object_name(self.get_object_name) self.get_object_name(self.get_object_name) self.get_object_name(self.get_object_name) self.get_object_name(self.get_object_name) self.get_object_name) self.get_object_name) self.get_object_name(self.get_object_name) self.get_object_name(self.get_object_name) self.get_object_name) self.get_object_name(self.get_object_name) self.get_object_name) self.get_object_name(self.get_object_name) self.get_object_name(self.get_object_name) self.get_object_name(self.get_object_name) self.get_object_name(self.get_object_name) self.get_object_name(self","def _get_object_name(self, line): name = line.split()[1].split('(')[0].strip() return name.rstrip(':')"
420,"def evolve_quantumstate(self, state, qargs, qargs, QiskitError, QiskitError, QiskitError, QiskitError, QiskitError, QiskitError, QiskitError, QiskitError, QiskitError, QiskitError, QiskitError, QiskitError, QiskitError, QiskitError, QiskitError, QiskitError, QiskitError)","def _evolve(self, state, qargs=None): state = self._format_state(state) if qargs is None: if state.shape[0]!= self._input_dim: raise QiskitError( ""Operator input dimension is not equal to state dimension."" ) if state.ndim == 1: return np.dot(self.data, state) return np.dot( np.dot(self.data, state), np.transpose(np.conj(self.data))) return self._evolve_subsystem(state, qargs)"
421,"def upload_file(LOCAL_PATH, LOCAL_PATH, LOCAL_PATH, LOCAL_PATH, LOCAL_PATH, LOCAL_PATH, LOCAL_PATH, LOCAL_PATH, LOCAL_PATH, LOCAL_PATH, LOCAL_PATH, LOCAL_PATH, LOCAL_PATH, LOCAL_PATH, LOCAL_PATH, LOCAL_PATH, LOCAL_PATH, LOCAL_PATH, LOCAL_PATH)","def upload(remote_path, local_path): storage = STORAGES['s3']() conf = s3conf.S3Conf(storage=storage) conf.upload(local_path, remote_path)"
422,"def adjust_hue(hue_factor, hue_factor, hue_factor, hue_factor, hue_factor, hue_factor, hue_factor, hue_factor, hue_factor, hue_factor, hue_factor, hue_factor, hue_factor, hue_factor, hue_factor, hue_factor, hue_factor, hue_factor, hue_factor, hue_factor, hue_factor, hue_factor, hue_factor, hue_factor, hue_factor, hue_factor, hue_factor, hue_factor, hue_factor, hue_factor)","def adjust_hue(img, hue_factor): if not(-0.5 = hue_factor = 0.5): raise ValueError('hue_factor is not in [-0.5, 0.5].'.format(hue_factor)) if not _is_pil_image(img): raise TypeError('img should be PIL Image. Got '.format(type(img))) input_mode = img.mode if input_mode in 'L', '1', 'I', 'F': return img h, s, v = img.convert('HSV').split() np_h = np.array(h, dtype=np.uint8) with np.errstate(over='ignore'): np_h += np.uint8(hue_factor * 255) h = Image.fromarray(np_h, 'L') img = Image.merge('HSV', (h, s, v)).convert(input_mode) return img"
423,def find_file(device_connection): if device(device_connection): if device(device_connection): if device(device_connection): if device(device_connection): if device(device_connection): if device(device_connection): if device(device_connection)): if device(device_connection): if device(device_connection): if device(device_connection): if device(device_connection): if device(device_connection): if device(device_connection): if device(device_connection): if device(device_connection): if device(device_connection): if device(device_connection): if device(device_connection): if device(device_connection): device(device_connection): device(device_connection): device(device_connection): device(device_connection): device(device_connection): device(device_connection): device(device_connection): device(device_connection): device(device_connection): device(device_connection): device(device_connection): device(device_connection):,"def find_yang_file(profile, filename, path): module_dir = os.path.dirname(__file__) full_path = os.path.join(module_dir, ""mappings"", profile, path, filename) if os.path.exists(full_path): return full_path else: msg = ""Couldn't find parsing file: "".format(full_path) logger.error(msg) raise IOError(msg)"
424,"def test_funny(self, INDENT, pass, END): def test_funny(self, END)): def test_funny(self, END): def test_funny(self, END)): def test_funny(self, END): def test_funny(self, END)): def test_funny(self, END): def test_funny(self, END): def test_funny(self, END): def test_funny(self, INDENT)): def test_funny(self, INDENT): def test_funny(self, INDENT): def test_funny(self, INDENT): def test_funny): def test_funny: def test_funny: def test_funny: def test_funny: def test_funny: def test_funny: def test_funny: def test_funny: def test_funny","def _tokenize(readline): assert callable(readline), ""readline should be a function"" tokens = [Token(tok) for tok in tokenize.generate_tokens(readline)] indents_stack = [0] for tok in tokens: if tok.op == INDENT: tok.pre_indent = indents_stack[-1] indents_stack.append(tok.end_col) tok.post_indent = tok.end_col elif tok.op == DEDENT: tok.pre_indent = indents_stack.pop() tok.post_indent = indents_stack[-1] elif tok.op == COMMENT: tok.pre_indent = tok.post_indent = indents_stack[-1] i = len(tokens) - 1 while i >= 2: pptok, ptok, tok = tokens[i - 2:i + 1] if tok.op == INDENT: if ptok.op == NL and pptok.op == COMMENT: indent, nl, comment = tok, ptok, pptok assert nl.start_col == comment.end_col underindent = indent.post_indent - comment.start_col if underindent > 0: _warn(""Comment '%s' is under-indented. Fixing..."" % comment.str) comment.move(0, underindent) nl.move(0, underindent) indent.move(-1, 0) tokens[i - 2:i + 1] = indent, comment, nl comment.pre_indent = comment.post_indent = indent.post_indent assert indent.end_row =="
425,"def sample_outputs(self, LM): if LM is not a LM_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTPUT_OUTP","def sample(self, input, steps): inputs = [[onehot(self.input_dim, x) for x in input]] for _ in range(steps): target = self.compute(inputs)[0,-1].argmax() input.append(target) inputs[0].append(onehot(self.input_dim, target)) return input"
426,"def over_under(self, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan)",def over_under(self): doc = self.get_doc() table = doc('table#game_info') giTable = sportsref.utils.parse_info_table(table) if 'over_under' in giTable: ou = giTable['over_under'] return float(ou.split()[0]) else: return None
427,"def disconnect_internal_signals(self, unit): if self.unit(unit): self.unit(unit): self.unit(unit): self.unit(unit): self.unit(unit): self.unit(unit)","def _cleanAsSubunit(self): for pi in self._entity.ports: pi.connectInternSig() for i in chain(self._interfaces, self._private_interfaces): i._clean()"
428,"def get(self, list(Member)): if self.get(Member): self.get(Member): self.get(Member): self.get(Member): self.get(Member)","def get_members(self, **query_params): members = self.get_members_json(self.base_uri, query_params=query_params) members_list = [] for member_json in members: members_list.append(self.create_member(member_json)) return members_list"
429,def register_resources_with_resources(self): self.resources(self.resources) self.resources(self.resources) self.resources(self.resources) self.resources(self.resources) self.resources(self.resources) self.resources(self.resources) self.resources(self.resources) self.resources(self.resources) self.resources(self.resources) self.resources) self.resources) self.resources_resources_resources) self.resources_resources_resources_resources_resources_resources_resources_resources_resources_resources_resources_resources_resources_resources_resources_resources_resources_resources_resources_resources_resources_resources_resources_resources_resources_resources_resources_resources_resources_resources_resources_resources_resources_resources_resources_resources_resource,"def register_resources(self, **resources): for key, resource in resources.items(): if key in self._resources: raise AlreadyExistsException('A Service for  is already registered.'.format(key)) self._init_resource(key, resource)"
430,"def _get_rows(self, rows): if self.get_rows(rows): self.get_rows(rows) self.get_rows(rows) self.get_rows(rows) self.get_rows(rows) self.get_rows(rows) self.get_rows(rows) self.get_rows(rows) self.get_rows(rows) self.get_rows(rows) self.get_rows) self.get_rows(rows) self.get_rows(rows) self.get_rows(rows) self.get_rows(rows) self.get_rows(rows) self.get_rows(rows) self.get_rows) self.get_rows(rows) self.get_rows(rows) self.get_rows(rows) self.get_rows(rows) self.get_rows(rows) self.get_rows(rows) self.get_rows() self.get_rows() self.get_rows() self.get_rows() self.get_rows() self.get_rows() self.get_rows","def _count_table_rows(self, table_name): cursor = self._db.cursor() select_stmt = ""SELECT COUNT(*) FROM "" + table_name try: cursor.execute(select_stmt) row = cursor.fetchone() except sqlite3.DatabaseError as e: msg = ""invalid archive file; cause: %s"" % str(e) raise ArchiveError(cause=msg) finally: cursor.close() return row[0]"
431,"def check_job_id(self, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id, job_id)","def get_jobs_url(self, job_id): return compat.urllib_parse.urlunsplit(( self.uri.scheme, self.uri.netloc, self.uri.path.rstrip('/') + '/jobs/' + job_id, self.uri.query, self.uri.fragment, ))"
432,"def draw_bland_altman_plot(x, y, interval, indep_conf, kwargs): if kwargs = kwargs, ax = kwargs if kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs =","def bland_altman(x, y, interval=None, indep_conf=None, ax=None, c=None, **kwargs): ret = False if ax is None: fig, ax = plt.subplots(1, 1) ret = True ind = (np.isnan(x) | np.isnan(y)) x = x[ind] y = y[ind] xy_mean = (x + y) / 2 xy_resid = (y - x) ax.scatter(xy_mean, xy_resid, lw=0.5, edgecolor='k', alpha=0.6, c=c, s=15, **kwargs) ax.axhline(0, ls='dashed', c='k', alpha=0.6, zorder=-1) ax.axhline(np.median(xy_resid), ls='dashed', c=c, alpha=0.8) if interval is not None: perc = 100 - interval * 100 ints = [perc / 2, 100 - perc / 2] lims = np.percentile(xy_resid, ints) ax.axhspan(*lims, color=c, alpha=0.1, zorder=-3) if indep_conf is not None: ax.axhspan(-indep_conf, indep_conf, color=(0,0,0,0.1), zorder=-2) ax.set_ylabel('y - x') ax.set_xlabel('mean (x, y)') if ret: return fig, ax"
433,def reduced index: modulo of extensible has been applied: modulo of extensible has been applied: modulo of extensible has been applied: modulo of extensible has been applied: modulo of extensible has been applied: modulo of extensible has been applied: modulo of extensible has been applied: modulo of extensible has been applied: modulo of extensible has been applied: modulo of extensible has been applied: modulo of extensible has been applied: modulo of extensible has been applied:,"def get_field_reduced_index(self, index): if self.extensible_info is None: return index cycle_start, cycle_len, _ = self.extensible_info if index  cycle_start: return index return cycle_start + ((index - cycle_start) % cycle_len)"
434,"def secant_square_step(self, secant_square_step, secant_square_step, secant_square_step, secant_square_step, secant_square_step, secant_square_step, secant_square_step, secant_square_step, secant_square_step, secant_square_step, secant_square_step, secant_square_step, secant_square_step, secant_square_step, secant_square_step, secant_square_step, secant_square_step, secant_square_step, secant_square_step, secant_square_step, secant_square_step, secant_square_step)","def _secant2_inner_update(value_and_gradients_function, initial_args, val_0, val_c, f_lim, sufficient_decrease_param, curvature_param): new_failed = initial_args.active & is_finite(val_c) active = initial_args.active & new_failed failed = initial_args.failed | new_failed found_wolfe = active & _satisfies_wolfe( val_0, val_c, f_lim, sufficient_decrease_param, curvature_param) val_left = val_where(found_wolfe, val_c, initial_args.left) val_right = val_where(found_wolfe, val_c, initial_args.right) converged = initial_args.converged | found_wolfe active = active & found_wolfe def _apply_update(): update_result = update( value_and_gradients_function, val_left, val_right, val_c, f_lim, active=active) return _Secant2Result( active=tf.zeros_like(active), converged=converged, failed=failed | update_result.failed, num_evals=initial_args.num_evals + update_result.num_evals, left=update_result.left, right=update_result.right) def _default(): return _Secant2Result( active=active, converged=converged, failed=failed, num_evals=initial_args.num_evals, left=val_left, right=val_right) return prefer_static.cond( tf.reduce_any(input_tensor=active), _apply_update, _default)"
435,"def close_all_sub-spans(self, start, stop): self.close_all_sub-spans(self.close_all_sub-spans) self.close_all_sub-spans(self.close_all_sub-spans) self.close_all_sub-spans(self.close_all_sub-spans) self.close_all_sub-spans(self.close_all_sub-spans) self.close_all_sub-spans(self.close_all_sub-spans","def _close_subspans(self, start: int, stop: int) -> None: ss, se = self._span for spans in self._type_to_spans.values(): b = bisect(spans, [start]) for i, (s, e) in enumerate(spans[b:bisect(spans, [stop], b)]): if e = stop: if ss!= s or se!= e: spans.pop(i + b)[:] = -1, -1 b -= 1"
436,"def upload_file(self, file_obj, store, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file_obj, file","def upload(cls, file_obj, store=None): if store is None: store = 'auto' elif store: store = '1' else: store = '0' data =  'UPLOADCARE_STORE': store,  files = uploading_request('POST', 'base/', data=data, files='file': file_obj) file_ = cls(files['file']) return file_"
437,"def find_external_links(self, template, url): if url = self.external_links(self.external_links(self.external_links(self.external_links))) self.external_links(self.external_links) self.external_links(self.external_links) self.external_links(self.external_links) self.external_links(self.external_links) self.external_links(self.external_links) self.external_links) self.external_links(self.external_links) self.external_links) self.external_links(self.external_links) self.external_links) self.external_links(self.external_links) self.external_links(self.external_links) self.external_links(self.external_links) self.external_links) self.external_links(self.external_links) self.ex","def external_links(self) -> List['ExternalLink']: external_links = [] external_links_append = external_links.append type_to_spans = self._type_to_spans lststr = self._lststr ss, se = self._span spans = type_to_spans.setdefault('ExternalLink', []) if not spans: spans_append = spans.append for m in EXTERNAL_LINK_FINDITER(self._ext_link_shadow): s, e = m.span() span = [ss + s, ss + e] spans_append(span) external_links_append( ExternalLink(lststr, type_to_spans, span, 'ExternalLink')) return external_links span_tuple_to_span_get = (s[0], s[1]): s for s in spans.get for m in EXTERNAL_LINK_FINDITER(self._ext_link_shadow): s, e = m.span() span = s, e = [s + ss, e + ss] old_span = span_tuple_to_span_get((s, e)) if old_span is None: insort(spans, span) else: span = old_span external_links_append( ExternalLink(lststr, type_to_spans, span, 'ExternalLink')) return external_links"
438,"def self._input_string(self._input_string, self._input_string, self._input_string, self._input_string, self._input_string, self._input_string, self._input_string, self._input_string, self._input_string, self._input_string, self._input_string, self._input_string, self._input_string)","def _make_token(self, start, end): # tok_string = self._input_string[start:end].encode('utf-8') if only_whitespace.match(tok_string): # return None tok = Token() tok.token = tok_string tok.token_num = self.tok_num if 'BYTES' in self.config['offset_types']: tok.offsets[OffsetType.BYTES] = Offset( type = OffsetType.BYTES, first=self.byte_idx + len(self._input_string[:start].encode('utf-8')), length=len(tok_string), value=self.config['offset_debugging'] and tok_string or None, ) if 'LINES' in self.config['offset_types']: tok.offsets[OffsetType.LINES] = Offset( type = OffsetType.LINES, first=self.line_idx, length=1, value=self.config['offset_debugging'] and tok_string or None, ) self.tok_num += 1 # tok.sentence_pos = self.sent_pos self.sent_pos += 1 return tok"
439,"def trefoil_knot_parametric_curve(p,q): trefoil_knot_parametric_curve(p,q): trefoil_knot_parametric_curve(p,q): trefoil_knot_parametric_curve(p,q): trefoil_knot_parametric_curve(p,q): trefoil_knot_parametric_curve(p,q)) trefoil_knot_parametric_curve(p,q) trefoil_knot_parametric_curve(p,q)) trefoil_knot_parametric_curve(p,q) trefoil_knot_parametric_curve(p,q) trefoil_knot_parametric_curve(p,q) trefoil_knot(p,q) trefoil_knot(p,q) trefoil_knot(p,q) trefoil_knot(p,q) trefoil_knot(p,q) trefoil_knot(p,q) tref","def torus_knot(t, p=3, q=4, scale=1.0, radius=2.0): return ['scale*(sin(t) + (radius)*sin(p*(t)))'.format(t=t, p=p, scale=scale, radius=radius),'scale*(cos(t) - (radius)*cos(p*(t)))'.format(t=t, p=p, scale=scale, radius=radius),'scale*(-sin(q*(t)))'.format(t=t, q=q, scale=scale)]"
440,"def create_rst_file(self, notebook, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst) rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst, rst,","def create_rst(self, nb, in_dir, odir): raw_rst, resources = nbconvert.export_by_name('rst', nb) rst_content = '' i0 = 0 m = None bokeh_str = '' if 'bokeh' in raw_rst and self.insert_bokeh: bokeh_str += self.BOKEH_TEMPLATE.format( version=self.insert_bokeh) if 'bokeh' in raw_rst and self.insert_bokeh_widgets: bokeh_str += self.BOKEH_WIDGETS_TEMPLATE.format( version=self.insert_bokeh_widgets) for m in code_blocks.finditer(raw_rst): lines = m.group().splitlines(True) header, content = lines[0], ''.join(lines[1:]) no_magics = magic_patt.sub('g1>', content) if no_magics.strip(): rst_content += ( raw_rst[i0:m.start()] + bokeh_str + header + no_magics) bokeh_str = '' i0 = m.end() else: rst_content += raw_rst[i0:m.start()] i0 = m.end() if m is not None: rst_content += bokeh_str + raw_rst[m.end():] else: rst_content = raw_rst rst_content = '.. _%s:nn' % self.reference +  rst_content url = self.url if url is not None: r"
441,"def transplant_class(self, __name__, __module__, __module__, __name__, __module__, __name__, __module__, __name__, __module__, __name__, __module__, __name__, __module__, __name__, __module__, __name__, __name__, __name__, __name__, __name__, __name__, __name__, __name__, __name__, __name__, __name__, __name__, __name__, __name__, __name__, __name__, __name__, __name__, __name__, __name__, __name__, __name__, __name__, __name__, __name__, __","def transplant_class(cls, module): class C(cls): pass C.__module__ = module C.__name__ = cls.__name__ return C"
442,"def send_termination_string(self, meter): self.get_termination_string(self.get_termination_string) self.get_termination_string(self.get_termination_string) self.get_termination_string(self.get_termination_string) self.get_termination_string(self.get_termination_string) self.get_termination_string(self.get_termination_string) self.get_termination_string(self.get_termination_string) self.get_termination_string) self.get_termination_string(self.get_termination_string) self.get_termination_string) self.get_termination_string(self.get_termination_string) self.get_termination_string) self.get_termination_string(self.get_termination_string) self.get_termination_string) self.get_termination_string(self.get_termination_string) self.get_termination_string(self.get_termination_string) self.get_termination_string(self.get_status_string) self.get_status_string) self.get_status_status_status_status_status_status_stat","def serialPostEnd(self): ekm_log(""Termination string sent ("" + self.m_context + "")"") try: self.m_serial_port.write(""0142300375"".decode(""hex"")) except: ekm_log(traceback.format_exc(sys.exc_info())) pass"
443,"def type('type', 'key', 'object-list', 'external-list','reference', 'node', 'node', 'node', 'node', 'node', 'node', 'node', 'node', 'node', 'node', 'node', 'node', 'node', 'node', 'node', 'node', 'node', 'node', 'node', 'node', 'node', 'node', 'node', 'node', 'node', 'node', 'node', 'node', 'node', 'node', 'node', 'node', 'node', 'node', 'node', 'node', 'node', 'node","def detailed_type(self): if self._detailed_type is None: if (""reference"" in self.tags) or (""reference-class-name"" in self.tags): self._detailed_type = ""reference"" elif ""type"" in self.tags: self._detailed_type = self.tags[""type""][0].lower() elif ""key"" in self.tags: self._detailed_type = ""choice"" elif ""object-list"" in self.tags: self._detailed_type = ""object-list"" elif ""external-list"" in self.tags: self._detailed_type = ""external-list"" elif self.basic_type == ""A"": self._detailed_type = ""alpha"" elif self.basic_type == ""N"": self._detailed_type = ""real"" else: raise ValueError(""Can't find detailed type."") return self._detailed_type"
444,"def self_partition(self, self.string): self.partition(self.string): self.partition(self.partition): self.partition = self.partition(self.partition)","def _atomic_partition(self, char: int) -> Tuple[str, str, str]: s, e = self._span index = self._shadow.find(char) if index == -1: return self._lststr[0][s:e], '', '' lststr0 = self._lststr[0] return lststr0[s:s + index], chr(char), lststr0[s + index + 1:e]"
445,"def likelypathogenic(self, ps_terms, pm_terms, pp_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms, ps_terms","def is_likely_pathogenic(pvs, ps_terms, pm_terms, pp_terms): if pvs: if pm_terms: return True if ps_terms: if pm_terms: return True if len(pp_terms) >= 2: return True if pm_terms: if len(pm_terms) >= 3: return True elif len(pm_terms) >= 2: if len(pp_terms) >= 2: return True elif len(pp_terms) >= 4: return True return False"
446,"def beatsync_feats(beat_frames, beat_times, beat_frames, beat_times, beat_frames, beat_frames, beat_times, beat_frames, beat_frames, beat_frames, beat_times, beat_frames, beat_frames, beat_frames, beat_frames, beat_frames, beat_times, beat_frames, beat_frames, beat_frames, beat_frames, beat_frames, beat_frames, beat_frames, beat_frames, beat_frames, beat_frames, beat_frames, beat_frames, beat_frames, beat_frames, beat_frames, beat_frames, beat_frames, beat_frames, beat_frames, beat_frames, beat_times, beat_frames, beat_frames, beat_frames, beat_frames, beat_frames, beat_frames, beat_frames, beat_frames, beat_frames, beat_frames, beat_frames, beat_times, beat_frames, beat_frames, beat_frames, beat_frames, beat_frames, beat_frames, beat_frames, beat_","def compute_beat_sync_features(self, beat_frames, beat_times, pad): if beat_frames is None: return None, None beatsync_feats = librosa.util.utils.sync(self._framesync_features.T, beat_frames, pad=pad).T beatsync_times = np.copy(beat_times) if beatsync_times.shape[0]!= beatsync_feats.shape[0]: beatsync_times = np.concatenate((beatsync_times, [self._framesync_times[-1]])) return beatsync_feats, beatsync_times"
447,"def tmdbsimple(tmdbsimple, tmdbsimple, tmdbsimple, tmdbsimple, tmdbsimple, tmdbsimple, tmdbsimple, tmdbsimple, tmdbsimple, tmdbsimple, tmdbsimple, tmdbsimple, tmdbsimple, tmdbsimple, tmdbsimple, tmdbsimple, tmdbsimple, tmdbsimple, tmdbsimple, tmdbsimple, tmdbsimple, tmdbsimple, tmdbsimple, tmdbsimple, tmdbsimple, tmdbsimple, tmdbsimple, tmdbsimple, tmdbsimple, tmdbsimple, tmdbsimple, tmdbsimple,","def _set_attrs_to_values(self, response=): if isinstance(response, dict): for key in response.keys(): if not hasattr(self, key) or not callable(getattr(self, key)): setattr(self, key, response[key])"
448,"def utf-8(self, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8), utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8, utf-8,","def _utf8_params(params): assert isinstance(params, dict) encoded_params = [] for k, v in params.items(): if v is None: continue if isinstance(v, integer_types + (float,)): v = str(v) if isinstance(v, (list, tuple)): v = [to_bytes(x) for x in v] else: v = to_bytes(v) encoded_params.append((k, v)) return dict(encoded_params)"
449,def compute_newness_curve(self): if self._curve(self._curve): self._curve(self._curve) if self._curve(self._curve),"def compute_nc(X): N = X.shape[0] nc = np.zeros(N) for i in range(N - 1): nc[i] = distance.euclidean(X[i, :], X[i + 1, :]) nc += np.abs(nc.min()) nc /= float(nc.max()) return nc"
450,"def make_service(self, opt, config,'messages', 'freq', 'config', 'freq', 'config','messages', 'freq', 'config', 'freq', 'config', 'freq', 'config','messages', 'freq', 'config', 'freq', 'freq', 'config','messages', 'freq','messages', 'freq', 'freq', 'freq', 'freq', 'freq', 'freq', 'freq', 'freq', 'freq', 'freq', 'freq', 'freq', 'freq', 'freq', 'freq', 'freq', 'freq', 'freq', 'freq', 'freq', 'freq', 'freq'","def makeService(opt): restarter, path = parseConfig(opt) now = time.time() checker = functools.partial(check, path, now) beatcheck = tainternet.TimerService(opt['freq'], run, restarter, checker, time.time) beatcheck.setName('beatcheck') return heart.wrapHeart(beatcheck)"
451,"def status_report(self, status_report, status_report, status_report, status_report, status_report, status_report, status_report, status_report, status_report, status_report, status_report, status_report, status_report, status_report, status_report)","def get(self, worker_id): code = 200 if worker_id == 'all': report = 'workers': [ 'id': job,'report': self._inspect_worker(job) for job in self.jobs]  elif worker_id in self.jobs: report =  'id': worker_id,'report': self._inspect_worker(worker_id)  else: report = 'error': 'job  unknown'.format(worker_id) code = 404 return flask.jsonify(report), code"
452,"def set_style(self, style): self.set_style(style): self.set_style(style) self.set_style(style) self.set_style(style) self.set_style(style) self.set_style(style) self.set_style(style) self.set_style(style) self.set_style(style) self.set_style(style) self.set_style(style) self.set_style(style) self.set_style(style) self.set_style) self.set_style(style) self.set_style(style) self.set_style(style) self.set_style(style) self.set_style(style) self.set_style(style) self.set_style(style) self.set_style) self.set_style(style) self.set_style(style) self.set_style(style) self.set_style) self.set_style(style) self.set_style(style) self.set_style(style) self.set_style(style) self.set_style(style) self.set_style(style) self.set_style(style) self.set_style(style) self.set_style(style) self.set_style(style) self.set_style",def _syntax_style_changed(self): if self._highlighter is None: return if self.syntax_style: self._highlighter.set_style(self.syntax_style) else: self._highlighter.set_style_sheet(self.style_sheet)
453,"def match_type(self, variable): if self.match(variable): if self.match(variable): return self.match(variable)","def check(self, var): return isinstance(var, tuple) and all(_check_type(t, self._element_type) for t in var)"
454,"def get_streamcorpus(self, client, abs_url, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id) doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id, doc_id,","def get_kvlayer_stream_item_by_doc_id(client, doc_id): if client is None: client = kvlayer.client() client.setup_namespace(STREAM_ITEM_TABLE_DEFS, STREAM_ITEM_VALUE_DEFS) doc_id_range = make_doc_id_range(doc_id) for k, v in client.scan(STREAM_ITEMS_TABLE, doc_id_range): if v is not None: errors, bytestr = streamcorpus.decrypt_and_uncompress(v) yield streamcorpus.deserialize(bytestr)"
455,"def add_file(self, xml, xml, xml, xml, xml, xml, xml, xml, xml, xml, xml)","def xml_file(self, cu, analysis): package_name = rpartition(cu.name, ""."")[0] className = cu.name package = self.packages.setdefault(package_name, [, 0, 0, 0, 0]) xclass = self.xml_out.createElement(""class"") xclass.appendChild(self.xml_out.createElement(""methods"")) xlines = self.xml_out.createElement(""lines"") xclass.appendChild(xlines) xclass.setAttribute(""name"", className) filename = cu.file_locator.relative_filename(cu.filename) xclass.setAttribute(""filename"", filename.replace("""", ""/"")) xclass.setAttribute(""complexity"", ""0"") branch_stats = analysis.branch_stats() for line in sorted(analysis.statements): xline = self.xml_out.createElement(""line"") xline.setAttribute(""number"", str(line)) xline.setAttribute(""hits"", str(int(line not in analysis.missing))) if self.arcs: if line in branch_stats: total, taken = branch_stats[line] xline.setAttribute(""branch"", ""true"") xline.setAttribute(""condition-coverage"", ""%d%% (%d/%d)"" % (100*taken/total, taken, total) ) xlines.appendChild(xline) class_lines = len(analysis.statements) class_hits = class_lines - len(analysis.missing) if self.arcs: class_branches = sum([t for t,k in branch_stats.values()]) missing_branches = sum([t-k for t,k in branch_"
456,"def compute_matrix_power(self, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, qiskitErrorErrorErrorErrorErrorErrorErrorErrorErrorErrorErrorErrorErrorErrorErrorErrorErrorErrorErrorErrorErrorErrorErrorErrorErrorErrorErrorErrorErrorErrorError)","def power(self, n): if n > 0: return super().power(n) return Stinespring(SuperOp(self).power(n))"
457,def logging_start(self): self.log_start(self.log_start) self.log_start(self.log_start) self.log_start(self.log_start) self.log_start(self.log_start) self.log_start(self.log_start) self.log_start(self.log_start) self.log_start(self.log_start) self.log_start(self.log_start) self.log_start(self.log_start) self.log_start,"def log_init(outlog, log_level=logging.INFO): logging.getLogger(""gseapy"").handlers = [] logging.basicConfig(level = logging.DEBUG, format = 'LINE %(lineno)-4d: %(asctime)s [%(levelname)-8s] %(message)s', filename = outlog, filemode = 'w') console = logging.StreamHandler() console.setLevel(log_level) formatter = logging.Formatter('%(asctime)s %(message)s') console.setFormatter(formatter) logging.getLogger(""gseapy"").addHandler(console) logger = logging.getLogger(""gseapy"") return logger"
458,def ________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________,"def outputs_matching(outputs, patterns): if isinstance(patterns, basestring): patterns = (patterns, ) if isinstance(outputs, dict): outputs = outputs.items() for name, expr in outputs: for pattern in patterns: if fnmatch.fnmatch(name, pattern): yield name, expr break"
459,def write_files(int): write_files(int): write_files(int): write_files(int): write_files(int): write_files(int): write_files(int): write_files(int): write_files(int): write_files(int): write_files(int): write_files(int): write_files(int): write_files(int): write_files(int): write_files(int),"def verified(context, collaborator, test, outpath=None): written_files = 0 collaborator = collaborator or 'cust000' LOG.info('Exporting verified variants for cust '.format(collaborator)) adapter = context.obj['adapter'] verified_vars = adapter.verified(institute_id=collaborator) LOG.info('FOUND  verified variants for institute '.format(len(verified_vars), collaborator)) if not verified_vars: LOG.warning('There are no verified variants for institute  in database!'.format(collaborator)) return None document_lines = export_verified_variants(verified_vars) today = datetime.datetime.now().strftime('%Y-%m-%d') document_name = '.'.join(['verified_variants', collaborator, today]) + '.xlsx' if test and document_lines: written_files +=1 LOG.info('Success. Verified variants file contains  lines'.format(len(document_lines))) return written_files if not outpath: outpath = str(os.getcwd()) workbook = Workbook(os.path.join(outpath,document_name)) Report_Sheet = workbook.add_worksheet() row = 0 for col,field in enumerate(VERIFIED_VARIANTS_HEADER): Report_Sheet.write(row,col,field) for row, line in enumerate(document_lines,1): for col, field in enumerate(line): Report_Sheet.write(row,col,field) workbook.close() if os.path.exists(os.path.join(outpath,document_name)): LOG.info('Success. Verified variants file of  lines was written to disk'."
460,"def alias_genes(self, genes, ids, hgnc_ids, alias_genes, ids, hgnc_ids, ids, hgnc_ids, hgnc_ids, hgnc_ids, hgnc_ids, hgnc_ids, hgnc_ids, hgnc_ids, hgnc_ids, hgnc_ids, hgnc_ids, hgnc_ids, hgnc_ids, hgnc_ids, hgnc_ids, hgnc_ids, hgnc_ids) hgnc_ids = hgnc_ids = hgnc_ids = hgnc_ids = hgnc_ids = hgnc_ids = hgnc_ids = hgnc_ids = h","def genes_by_alias(self, build='37', genes=None): LOG.info(""Fetching all genes by alias"") alias_genes =  if not genes: genes = self.hgnc_collection.find('build':build) for gene in genes: hgnc_id = gene['hgnc_id'] hgnc_symbol = gene['hgnc_symbol'] for alias in gene['aliases']: true_id = None if alias == hgnc_symbol: true_id = hgnc_id if alias in alias_genes: alias_genes[alias]['ids'].add(hgnc_id) if true_id: alias_genes[alias]['true'] = hgnc_id else: alias_genes[alias] =  'true': hgnc_id, 'ids': set([hgnc_id])  return alias_genes"
461,"def delete_links(groupA.groupB.linkA, groupWithLink, groupWithLink, groupWithLink, groupWithLink, groupWithLink, groupWithLink, groupWithLink, groupWithLink, groupWithLink, groupWithLink, groupWithLink, groupWithLink, groupWithLink, groupWithLink, groupWithLink, groupWithLink, groupWithLink, groupWithLink, groupWithLink, groupWithLink, groupWithLink, groupWithLink, groupWithLink, groupWithLink, groupWithLink, groupWithLink)","def f_delete_links(self, iterator_of_links, remove_from_trajectory=False): to_delete_links = [] group_link_pairs = [] for elem in iterator_of_links: if isinstance(elem, str): split_names = elem.split('.') parent_name = '.'.join(split_names[:-1]) link = split_names[-1] parent_node = self.f_get(parent_name) if parent_name!= '' else self link_name = parent_node.v_full_name + '.' + link if parent_name!= '' else link to_delete_links.append((pypetconstants.DELETE_LINK, link_name)) group_link_pairs.append((parent_node, link)) else: link_name = elem[0].v_full_name + '.' + elem[1] to_delete_links.append((pypetconstants.DELETE_LINK, link_name)) group_link_pairs.append(elem) try: self._storage_service.store(pypetconstants.LIST, to_delete_links, trajectory_name=self.v_name) except: self._logger.error('Could not remove %s from the trajectory. Maybe the''item(s) was/were never stored to disk.' % str(to_delete_links)) raise if remove_from_trajectory: for group, link in group_link_pairs: group.f_remove_link(link)"
462,"def replace_pattern(self, pattern, replacement, bool): if bool is None: if bool is None: if bool is None: bool is None: bool is None: bool is None: bool is None: bool is None: bool is None: bool is None: bool is None: bool is None: bool is None: bool is None: bool is None: bool is None: bool is None: bool is None: bool is None: bool is None: bool is None: bool is None: bool is None: bool is None: bool is None: bool is None: bool is None: bool is None: bool is None: bool is None: bool is None: bool is None: bools = bools = bools = ols = ols = ols = ols = ols = ols = ols = ols = ols = ols = ols = ols = ols = ol","def sub(self, pattern, replacement, ignore_case=False): return H2OFrame._expr(expr=ExprNode(""replacefirst"", self, pattern, replacement, ignore_case))"
463,"def profile_statistics_on_a_function(self, function): self.statistics_on_a_function(function): self.statistics_on_a_function(function): self.statistics_on_a_function(function): self.statistics_on_a_function(function)","def profile_function(self): with _StatProfiler() as prof: result = self._run_object(*self._run_args, **self._run_kwargs) call_tree = prof.call_tree return  'objectName': self._object_name,'sampleInterval': _SAMPLE_INTERVAL, 'runTime': prof.run_time, 'callStats': call_tree, 'totalSamples': call_tree.get('sampleCount', 0),'result': result, 'timestamp': int(time.time())"
464,"def calculate_capacity(self, sl, fd, vertical_load, slope, verbose, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity)","def capacity_nzs_vm4_2011(sl, fd, h_l=0, h_b=0, vertical_load=1, slope=0, verbose=0, **kwargs): if not kwargs.get(""disable_requires"", False): models.check_required(sl, [""phi_r"", ""cohesion"", ""unit_dry_weight""]) models.check_required(fd, [""length"", ""width"", ""depth""]) horizontal_load = np.sqrt(h_l ** 2 + h_b ** 2) h_eff_b = kwargs.get(""h_eff_b"", 0) h_eff_l = kwargs.get(""h_eff_l"", 0) loc_v_l = kwargs.get(""loc_v_l"", fd.length / 2) loc_v_b = kwargs.get(""loc_v_b"", fd.width / 2) ecc_b = h_b * h_eff_b / vertical_load ecc_l = h_l * h_eff_l / vertical_load width_eff = min(fd.width, 2 * (loc_v_b + ecc_b), 2 * (fd.width - loc_v_b - ecc_b)) length_eff = min(fd.length, 2 * (loc_v_l + ecc_l), 2 * (fd.length - loc_v_l - ecc_l)) area_foundation = length_eff * width_eff if width_eff / 2  fd.width / 6: raise DesignError(""failed on eccentricity"") fd.nq_factor = ((np.tan(np.p"
465,def get_list(list): return list(list): return list(list): return list(list): return list(list),"def get_lists(self, **query_params): lists = self.get_lists_json(self.base_uri, query_params=query_params) lists_list = [] for list_json in lists: lists_list.append(self.create_list(list_json)) return lists_list"
466,"def load_metrics_file(self, metrics): self.get_metrics(metrics): self.get_metrics(metrics) self.get_metrics(metrics) self.get_metrics(metrics) self.get_metrics(metrics) self.get_metrics(metrics) self.get_metrics(metrics) self.get_metrics(metrics) self.get_metrics(metrics) self.get_metrics) self.get_metrics) self.get_metrics(metrics) self.get_metrics(metrics) self.get_metrics) self.get_metrics(metrics) self.get_metrics(metrics) self.get_metrics) self.get_metrics(metrics) self.get_metrics(metrics) self.get_metrics(metrics) self.get_metrics(metrics) self.get_metrics) self.get_metrics(metrics) self.get_metrics(metrics) self.get_metrics(metrics) self.get","def read(self): f = open(self.path, ""r"") self.manifest_json = f.read()"
467,"def flag_user(self, admin): if self.admin(admin): self.admin(admin): self.admin(admin): self.admin(admin)","def create_admin(self, account_id, user_id, role): url = ADMINS_API.format(account_id) body = ""user_id"": unquote(str(user_id)), ""role"": role, ""send_confirmation"": False return CanvasAdmin(data=self._post_resource(url, body))"
468,def get_revoked(self): if self.get_revoked(self.get_revoked): self.get_revoked(self.get_revoked) self.get_revoked(self.get_revoked) self.get_revoked(self.get_revoked) self.get_revoked(self.get_revoked) self.get_revoked(self.get_revoked) self.get_revoked(self.get_revoked) self.get_revoked(self.get_revoked) self.get_revoked_self.get_revoked_self.get_revoked_self.get_revoked_self.get_revoked_self.get_revoked_self.get_revoked_self.get_revoked_self.get_revoked_self.get_revoked_self.get_revoked_self.get_revoked_self.get_revoked_self.get_revoked_self.get_revoked_self.get_revoked_self.get_revoked_self.get_revoked_self.get_revoked_self.get_r,def check_get_revoked(self): crl = CRL() for i in xrange(100): crl.add_revoked(Revoked()) for i in xrange(self.iterations): crl.get_revoked()
469,"def send_shutdown(self, recv): if self.shutdown(self.shutdown): self.shutdown(self.shutdown): self.shutdown(self.shutdown): self.shutdown(self.shutdown): self.shutdown(self.shutdown)","def shutdown(self): result = _lib.SSL_shutdown(self._ssl) if result  0: self._raise_ssl_error(self._ssl, result) elif result > 0: return True else: return False"
470,"def store_ipython_references(self, __builtin__, __builtin__, __builtin__, __builtin__, __builtin__, __builtin__, __builtin__, __builtin__, __builtin__, __builtin__, __builtin__, __builtin__, __builtin__, __builtin__, __builtin__, __builtin__, __builtin__)","def activate(self): add_builtin = self.add_builtin for name, func in self.auto_builtins.iteritems(): add_builtin(name, func)"
471,"def extraBits('D', NPOSTFIX=2, NDIRECT=10): d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d = d =","def extraBits(self, index): if index16+self.NDIRECT: return 0 return 1 + ((index - self.NDIRECT - 16) >> (self.NPOSTFIX + 1))"
472,def execute_context_processor(self): if self.request(self.request): self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request) self.request(self.request) self.request) self.request(self.request) self.request) self.request(self.request) self.request) self.request(self.request) self.request(self.request) self.request) self.request(self.request) self.request(self.request) self.request) self.request(self.request) self.request(self.request) self.re,"def app_context_processor(self, f): self.record_once(lambda s: s.app.template_context_processors.setdefault(None, []).append(f)) return f"
473,"def _encode_data(self, data, data, data, data, data, data, data, data, data, data, data, data, data, data, data, data, data, data, data, data, data, data, data, data)","def to_xml(self): if self.data: self.document = self._update_document(self.document, self.data) return self.document"
474,"def copy_leaves(node=node, overwrite=node, with_links=node, with_links=node, with_links=node, with_links=node, with_links=node, with_links=node, with_links=node, with_links=node, with_links=node, overwrite=node, overwrite=node, overwrite=node) overwrite=node, overwrite=node, overwrite=node, overwrite=node, overwrite=node, overwrite=node, overwrite=node, overwrite=node, overwrite=node, overwrite=node, overwrite=node, overwrite=node=node=node=node=node=node=node=node=node=node=node=node=node=node=node=node=node=node=node=node=node=node=node=node","def _copy_from(self, node, copy_leaves=True, overwrite=False, with_links=True): group_full_name = group.v_full_name try: found_group = self.f_get(group_full_name, with_links=False, shortcuts=False, auto_load=False) if overwrite: _copy_skeleton(found_group, group) return found_group except AttributeError: pass new_group = self.f_add_group(group_full_name) _copy_skeleton(new_group, group) return new_group is_run = self._is_run self._is_run = False try: if node.v_is_leaf: return _add_leaf(node) elif node.v_is_group: other_root = node.v_root if other_root is self: raise RuntimeError('You cannot copy a given tree to itself!') result = _add_group(node) nodes_iterator = node.f_iter_nodes(recursive=True, with_links=with_links) has_links = [] if node._links: has_links.append(node) for child in nodes_iterator: if child.v_is_leaf: _add_leaf(child) else: _add_group(child) if child._links: has_links.append(child) if with_links: for current in has_links: mine = self.f_get(current.v_full_name, with_links=False, shortcuts=False, auto_load=False) my_link_set = set(mine._links.keys()) other_link_set = set(current._links.keys()) new_links = other_link_set - my_link_set for link in new"
475,def modify_user_permissions(user_permissions): if user_permissions = user_permissions = user_permissions = user_permissions = user_permissions = user_permissions = user_permissions = user_permissions = user_permissions = user_permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions = permissions =,"def modify_permissions(self, permissions): group = Group.objects.get(name='Admin') if permissions == 'admin': self.groups.add(group) else: self.groups.remove(group)"
476,"def build_theano_expression(outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, acc, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs, outputs,","def accuracy(self, outputs): output = outputs[self.output_name] predict = TT.argmax(output, axis=-1) correct = TT.eq(predict, self._target) acc = correct.mean() if self._weights is not None: acc = (self._weights * correct).sum() / self._weights.sum() return acc"
477,"def x, y, z, sx, sy, sz, sx, sy, sz, sx, sy, sz, sx, sy, sz, sx, sy, sz, sx, sy, sz, sx, sy, sz, sx, sy, sz, sx, stokes, stokes, stokes, stokes, stokes, stokes, stokes, stokes, stokes, stokes, stokes, stokes, stokes, stokes, stokes, stokes, stokes, stokes, stokes, stokes, stokes, stokes, stokes, stokes, stokes, stokes, stokes, stokes, stokes, stokes, stokes, stokes, stokes, stokes, stokes, stokes, stokes, sto","def set_label_convention(self, convention): ketex = ""$left.|%srightrangle$"" if convention == ""original"": self.xlabel = ['$x$', ''] self.ylabel = ['$y$', ''] self.zlabel = ['$left|0right>$', '$left|1right>$'] elif convention == ""xyz"": self.xlabel = ['$x$', ''] self.ylabel = ['$y$', ''] self.zlabel = ['$z$', ''] elif convention == ""sx sy sz"": self.xlabel = ['$s_x$', ''] self.ylabel = ['$s_y$', ''] self.zlabel = ['$s_z$', ''] elif convention == ""01"": self.xlabel = ['', ''] self.ylabel = ['', ''] self.zlabel = ['$left|0right>$', '$left|1right>$'] elif convention == ""polarization jones"": self.xlabel = [ketex % ""nearrowhspace-1.46swarrow"", ketex % ""nwarrowhspace-1.46searrow""] self.ylabel = [ketex % ""circlearrowleft"", ketex % ""circlearrowright""] self.zlabel = [ketex % ""leftrightarrow"", ketex % ""updownarrow""] elif convention == ""polarization jones letters"": self.xlabel = [ketex % ""D"", ketex % ""A""] self.ylabel = [ketex % ""L"", ketex % ""R""] self.zl"
478,"def run_in_transform_mode(self, transform_mode): run in transform_mode(self, transform_mode) run in transform_mode(self, transform_mode)","def transform(config): if transform_possible: ExampleLoader.register() args, sys.argv[1:] = sys.argv[1:], config.args try: return runpy.run_path(config.runner, run_name=""__main__"") finally: sys.argv[1:] = args"
479,"def return_response_body(self, bytes): if self.response_body(self.response_body): self.response_body(self.response_body)","def response(self): if isinstance(self.body, bytes): out = self.body elif isinstance(self.body, str): out = self.body.encode(self.charset) else: out = b'' self.add_header('Content-Type', self.content_type) self.add_header('Content-Length', str(len(out))) self.start(self.status_line, self._headers) return [out]"
480,"def load_locales(locales, region, languages, region): if locales are not given, languages and region are used to construct locales to load.___________________________________________________________________________________________________________________________________________","def get_locales(self, languages=None, locales=None, region=None, use_given_order=False, allow_conflicting_locales=False): for _, locale in self._load_data( languages=languages, locales=locales, region=region, use_given_order=use_given_order, allow_conflicting_locales=allow_conflicting_locales): yield locale"
481,"def prepare_filename(self, filename, filename): self.prepare(filename, filename) self.prepare(filename) self.prepare(filename) self.prepare(filename) self.prepare(filename) self.prepare(filename) self.prepare(filename) self.prepare(filename) self.prepare(filename) self.prepare(filename) self.prepare(filename) self.prepare(filename) self.prepare(filename) self.prepare(filename) self.prepare(filename) self.prepare(filename) self.prepare(filename) self.prepare(filename) self.prepare(filename) self.prepare(filename) self.prepare(filename) self.prepare(filename) self.prepare(filename) self.prepare(filename) self.prepare(filename) self.prepare(filename) self.prepare(filename) self.prepare(filename) self.prepare(filename) self.prepare(filename) self.prepare(filename) self","def _prepare_file_payload(filename): if not filename: return None absfilename = os.path.abspath(filename) if not os.path.exists(absfilename): raise H2OValueError(""File %s does not exist"" % filename, skip_frames=1) return os.path.basename(absfilename): open(absfilename, ""rb"")"
482,"def load_data_file(data_file, name_mode, name_mode, name_mode, name_mode, name_mode, name_mode, name_mode, name_mode, name_mode, name_mode, name_mode, name_mode, name_mode, name_mode, name_mode, name_mode, name_mode, name_mode, name_mode, name_mode, name_mode, name_mode, name_mode, name_mode, name_mode, name_mode, name_mode, name_mode, name_mode, name_mode, name_mode, name_mode, name_mode, name_mode)","def read_data(data_file, dataformat, name_mode): with open(data_file) as f: lines = f.readlines() if'meta_regex' in dataformat.keys(): meta = Bunch() for k, v in dataformat['meta_regex'].items(): try: out = re.search(v[-1], lines[int(k)]).groups() except: raise ValueError('Failed reading metadata when applying:n regex: nton line: '.format(v[-1], lines[int(k)])) for i in np.arange(len(v[0])): meta[v[0][i]] = out[i] else: meta =  if name_mode == 'file_names': sample = os.path.basename(data_file).split('.')[0] elif name_mode =='metadata_names': sample = meta['name'] else: sample = name_mode columns = np.array(lines[dataformat['column_id']['name_row']].strip().split( dataformat['column_id']['delimiter'])) if 'pattern' in dataformat['column_id'].keys(): pr = re.compile(dataformat['column_id']['pattern']) analytes = [pr.match(c).groups()[0] for c in columns if pr.match(c)] if 'preformat_replace' in dataformat.keys(): with open(data_file) as f: fbuffer = f.read() for k, v in dataformat['preformat_replace'].items(): fbuffer = re.sub(k, v, fbuffer) read_data = np.gen"
483,"def _resolve_name(self, context, varname, str|unicode, varname, str|unicode, varname, str|unicode, varname, str|unicode, varname, str|unicode, varname, str|unicode, varname, str|unicode, str|unicode, str|unicode, str|unicode, str|unicode, str|unicode, varname, str|unicode, varname, varname, varname, varname, varname, varname, varname, varname, varname, varname, varname, varname, varname, varname) varname) varname) varname) varname) varname) varname) varname) varname) varname) varname) varname) varname) varname) varname) varname) varname) varname) varname) varname) varname) varname) varname) varname) varname) varname) varname) varname) varname) varname) varname","def resolve_var(self, varname, context=None): context = context or self.current_page_context if isinstance(varname, FilterExpression): varname = varname.resolve(context) else: varname = varname.strip() try: varname = Variable(varname).resolve(context) except VariableDoesNotExist: varname = varname return varname"
484,"def fire_traits(self, config): if self.traits(self.traits): self.traits(self.traits): self.traits(self.traits): self.traits(self.traits)","def update_config(self, config): newconfig = deepcopy(self.config) newconfig._merge(config) self.config = newconfig"
485,def ________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________,def available_drivers(): drivers =  for i in range(gdal.GetDriverCount()): d = gdal.GetDriver(i) drivers[d.ShortName] = d.GetMetadata() return drivers
486,"def config_path(self, config_path, dir>): if config_path is None: if config_path is None: if config_path is None: if config_path is None: if config_path is None: if config_path is None: config_path is None: config_path is None: config_path is None: config_path is None: config_path is None: config_path = config_path = config_path = config_path = config_path = config_path = config_path = config_path = config_path = config_path = config_path = config_path = config_path = config_path = config_path = config_path = config_path = config_path = config_path = config_path = config_path = config_path = config_path = config_path = config_path = config_path = config_path = config_path = config_path = config_path = config_path = config_path = config_path = config_path = config_path = config_path = config_path = config_path =","def get_config_path(): if os.environ.get(BUGWARRIORRC): return os.environ[BUGWARRIORRC] xdg_config_home = ( os.environ.get('XDG_CONFIG_HOME') or os.path.expanduser('/.config')) xdg_config_dirs = ( (os.environ.get('XDG_CONFIG_DIRS') or '/etc/xdg').split(':')) paths = [ os.path.join(xdg_config_home, 'bugwarrior', 'bugwarriorrc'), os.path.expanduser(""/.bugwarriorrc"")] paths += [ os.path.join(d, 'bugwarrior', 'bugwarriorrc') for d in xdg_config_dirs] for path in paths: if os.path.exists(path): return path return paths[0]"
487,"def get_virtual_machine(self, service_name, deployment_name, role_name, role_name, role_name, role_name, service_name, deployment_name, role_name, role_name, role_name, service_name, deployment_name, role_name, role_name, role_name, service_name, deployment_name, role_name, role_name, role_name, service_name, deployment_name, role_name, role_name)","def get_role(self, service_name, deployment_name, role_name): _validate_not_none('service_name', service_name) _validate_not_none('deployment_name', deployment_name) _validate_not_none('role_name', role_name) return self._perform_get( self._get_role_path(service_name, deployment_name, role_name), PersistentVMRole)"
488,"def get_password(self, password): self.get_password(self.get_password) self.get_password(self.get_password) self.get_password(self.get_password) self.get_password(self.get_password) self.get_password(self.get_password) self.get_password(self.get_password) self.get_password(self.get_password) self.get_password(self.get_password) self.get_password) self.get_password) self.get_password(self.get_password) self.get_password(self.get_password) self.get_password) self.get_password) self.get_password(self.get_password) self.get_password) self.get_password(self.get_password) self.get_password) self.get_password_password_password_password_password_password_password_password_password_password_password_password_password_password_password_password_password_password_password_password_password_password_password_password_pass","def oracle_eval(command): p = subprocess.Popen( command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE) p.wait() if p.returncode == 0: return p.stdout.readline().strip().decode('utf-8') else: die( ""Error retrieving password: command returned 'error'"".format( command=command, error=p.stderr.read().strip()))"
489,"def _process(self, filename, size): if self.process(filename, size): self.process(filename, size) self.process(filename, size) self.process(filename) self.process(filename) self.process(filename) self.process(filename) self.process(filename) self.process(filename) self.process(filename) self.process(filename) self.process(filename) self.process(filename) self.process(filename) self.process(filename) self.process(filename) self.process(filename) self.process(filename) self.process(filename) self.process(filename) self.process(filename) self.process(filename) self.process(filename) self.process(filename) self.process(filename) self.process(filename) self.process(filename) self.process(filename) self.process(filename) self.process(filename) self.process(filename) self.process(filename) self.process(filename) self.process(filename) self.process(filename) self.process(filename) self.process(filename) self.process(filename) self.process(filename) self.process","def process(file, size): from. import conf raw_image = images.from_file(file) size_dict = conf.SIZES[size] for processor in size_dict['PROCESSORS']: raw_image = processor['processor'](raw_image, **processor['kwargs']) image_io = io.BytesIO() raw_image.save(file=image_io) image_file = ContentFile(image_io.getvalue()) #print dir(image_file) return image_file"
490,"def containerize_calls(self, generator_function(func)): self.get_function(func)) self.get_function(func) self.get_function(func) self.get_function(func) self.get_function(func) self.get_function(func) self.get_function(func) self.get_function(func) self.get_function(func) self.get_function(func) self.get_function(func) self.get_function(func) self.get_function(func) self.get_function(func) self.get_function(func) self.get_function(func)","def generator_container(generator_function): return GeneratorContainer(generator_function, *args, **kwargs) return generator_container_wrapper"
491,"def get_item(self, alias): if self.get_item(alias): self.get_item(alias): self.get_item(alias)","def alias_item(self, alias): ident = self.alias[alias] return self.items[ident]"
492,"def check_task_arguments(self, task_arguments, task_arguments, task_arguments, task_arguments, task_arguments, task_arguments, task_arguments, task_arguments, task_arguments, task_arguments, task_arguments, task_arguments, task_arguments, task_arguments, task_arguments, task_arguments, task_arguments, task_arguments, task_arguments) task_arguments) task_arguments) task_arguments) task_arguments) task_arguments) task_arguments) task_arguments) task_arguments) task_arguments)","def __validate_args(task_id, backend, category, backend_args): if not task_id or task_id.strip() == """": msg = ""Missing task_id for task"" raise ValueError(msg) if not backend or backend.strip() == """": msg = ""Missing backend for task '%s'"" % task_id raise ValueError(msg) if backend_args and not isinstance(backend_args, dict): msg = ""Backend_args is not a dict, task '%s'"" % task_id raise ValueError(msg) if not category or category.strip() == """": msg = ""Missing category for task '%s'"" % task_id raise ValueError(msg)"
493,"def tile_tuples(x, y, z): bbox = bbox, zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = bbox = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = bbox = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs = zlevs =","def from_bbox(bbox, zlevs): env = Envelope(bbox) for z in zlevs: corners = [to_tile(*coord + (z,)) for coord in (env.ul, env.lr)] xs, ys = [range(p1, p2 + 1) for p1, p2 in zip(*corners)] for coord in itertools.product(xs, ys, (z,)): yield coord"
494,"def get_window(self, window, np.ndarray, sr, sr, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.narray, np.narray, np.narray, np.narray, np.narray, np.narray, np.narray, np.narray, np.narray, np.narray, np.narray, np.narray, np.narray, np.narray, np.narray, np.narray, np.narray, np.narray, np.narray, np.narray, np.narray, np.narray,","def melspectrogram(y=None, sr=22050, S=None, n_fft=2048, hop_length=512, win_length=None, window='hann', center=True, pad_mode='reflect', power=2.0, **kwargs): S, n_fft = _spectrogram(y=y, S=S, n_fft=n_fft, hop_length=hop_length, power=power, win_length=win_length, window=window, center=center, pad_mode=pad_mode) mel_basis = filters.mel(sr, n_fft, **kwargs) return np.dot(mel_basis, S)"
495,"def fetch_resource(self, entrypoint, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params","def _fetch(self, entry_point, params): url = self.API_URL % 'base_url': self.base_url, 'entrypoint': entry_point logger.debug(""Mattermost client requests: %s params: %s"", entry_point, str(params)) r = self.fetch(url, payload=params) return r.text"
496,"def get_action(action_name, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_name, action_name, action_id, action_id, action_id, action_name, action_id, action_name, action_id, action_id, action_name, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_id, action_id","def get_action(self, action_name, action_id): if action_name not in self.actions: return None for action in self.actions[action_name]: if action.id == action_id: return action return None"
497,def distribution_editable_install(self): if self.editable_install(self.editable_install): self.editable_install(self.editable_install) self.editable_install(self.editable_install) self.editable_install(self.editable_install) self.editable_install(self.editable_install) self.editable_install(self.editable_install) self.editable_install(self.editable_install) self.editable_install(self.editable_install) self.editable_install,"def dist_is_editable(dist): from pip import FrozenRequirement req = FrozenRequirement.from_dist(dist, []) return req.editable"
498,"def callback(self, mqttc, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj,","def _onLog(self, mqttc, obj, level, string): self.logger.debug(""%d %s"" % (level, string))"
499,"def reconstruct(inputs, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct, reconstruct)","def visualize_reconstruction(inputs, reconstruct, num=3, name=""reconstruction""): reconstruct = tf.clip_by_value(reconstruct, 0., 1.) inputs_and_reconstruct = tf.concat((inputs[:num], reconstruct[:num]), axis=0) image_summary(inputs_and_reconstruct, name)"
500,"def remove_nodes(self, nodes): if self.nodes(nodes): self.nodes(nodes): self.nodes(nodes): self.nodes(nodes)","def remove_nondescendants_of(self, node): if isinstance(node, int): warnings.warn('Calling remove_nondescendants_of() with a node id is deprecated,''use a DAGNode instead', DeprecationWarning, 2) node = self._id_to_node[node] dec = nx.descendants(self._multi_graph, node) comp = list(set(self._multi_graph.nodes()) - set(dec)) for n in comp: if n.type == ""op"": self.remove_op_node(n)"
501,"def corr(x, y, sample_axis=0, event_axis=None, corr_matrix=None, corr_matrix=None, corr_matrix=None, corr_matrix=None, corr_matrix=None, corr_matrix=None, corr_matrix=None, corr_matrix=None, corr_matrix=None, corr_matrix=None, corr_matrix=None, corr_matrix=None, corr_matrix=None, corr_matrix=None, corr_matrix=None, corr_matrix=None, corr_matrix=None, corr_matrix=None, corr_matrix=None, corr_matrix=None, corr_matrix=None, corr_matrix=None, corr_matrix=None, corr_matrix=None, corr_matrix=None, corr_matrix=None, corr_matrix=None, corr_mat","def correlation(x, y=None, sample_axis=0, event_axis=-1, keepdims=False, name=None): with tf.compat.v1.name_scope( name, 'correlation', values=[x, y, event_axis, sample_axis]): x /= stddev(x, sample_axis=sample_axis, keepdims=True) if y is not None: y /= stddev(y, sample_axis=sample_axis, keepdims=True) return covariance( x=x, y=y, event_axis=event_axis, sample_axis=sample_axis, keepdims=keepdims)"
502,"def send_arming_command(self, alarm): self.arming_command(arming_command): self.arming_command(arming_command) self.arming_command(arming_command) self.arming_command(arming_command) self.arming_command(arming_command) self.arming_command(arming_command) self.arming_command(arming_command) self.arming_command(arming_command) self.arming_command(arming_command) self.arming_command(arming_command) self.arming_command(arming_command) self.arming_command) self.arming_command(arming_command) self.arming_command) self.arming_command(arming_command) self.arming_command(arming_command) self.arming_command) self.arming_command(arming_command) self.arming_command) self.arming_command(arming_command) self.arming_command(arming_command) self.arming_command(arming_command) self.arming_command(arming_command) self.arming_command(arming_command) self.arming_command(arming_command) self.arming","async def arm(self, code, partition_list, mode=0): _LOGGER.debug(""Sending arm command, mode: %s!"", mode) while len(code)  16: code += 'F' code_bytes = bytearray.fromhex(code) mode_command = 0x80 + mode data = generate_query(mode_command.to_bytes(1, 'big') + code_bytes + partition_bytes(partition_list)) await self._send_data(data)"
503,"def build(self, O_OBJ, O_ATTR, O_ATTR, O_ATTR, O_ATTR, O_ATTR, O_ATTR, O_ATTR, O_ATTR, O_ATTR, O_ATTR, O_ATTR, O_ATTR, O_ATTR, O_ATTR, O_ATTR, O_ATTR, O_ATTR, O_ATTR, O_ATTR, O_ATTR, O_ATTR, O_ATTR)","def build_class(o_obj): cls = ET.Element('xs:element', name=o_obj.key_lett, minOccurs='0', maxOccurs='unbounded') attributes = ET.SubElement(cls, 'xs:complexType') for o_attr in nav_many(o_obj).O_ATTR[102](): o_attr_ref = get_refered_attribute(o_attr) s_dt = nav_one(o_attr_ref).S_DT[114]() while nav_one(s_dt).S_UDT[17](): s_dt = nav_one(s_dt).S_UDT[17].S_DT[18]() type_name = get_type_name(s_dt) if type_name and not nav_one(o_attr).O_BATTR[106].O_DBATTR[107](): ET.SubElement(attributes, 'xs:attribute', name=o_attr.name, type=type_name) else: logger.warning('Omitting %s.%s' % (o_obj.key_lett, o_attr.Name)) return cls"
504,"def update_agent_state(self, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state)",def update_agent_state(): configure_service('capture.admin') status = 'idle' if get_service_status(db.Service.SCHEDULE) == db.ServiceStatus.STOPPED: status = 'offline' elif get_service_status(db.Service.CAPTURE) == db.ServiceStatus.BUSY: status = 'capturing' elif get_service_status(db.Service.INGEST) == db.ServiceStatus.BUSY: status = 'uploading' register_ca(status=status)
505,"def last_exception(self, Descriptor): if self.descriptor(self.descriptor): self.descriptor(self.descriptor): self.descriptor(self.descriptor): self.descriptor(self.descriptor)","def exc_thrown_by_descriptor(): traceback = sys.exc_info()[2] tb_locals = traceback.tb_frame.f_locals if ""self"" in tb_locals: if not isinstance(tb_locals[""self""], Descriptor): return False return True return False"
506,"def translator(self, translator, argument parser, translator, parser, parser, parser, translator, parser, parser, translator, parser, parser, translator, parser, parser, parser, translator, parser, parser, translator, parser, parser, parser, translator, parser, parser, parser, parser, translator, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, pars","def main(): args = command_line() translate = partial(translator, args.source, args.dest, version=' '.join([__version__, __build__])) return source(spool(set_task(translate, translit=args.translit)), args.text)"
507,"def map(self, pred_map, succ_map, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple, tuple","def _full_pred_succ_maps(self, pred_map, succ_map, input_circuit, wire_map): full_pred_map =  full_succ_map =  for w in input_circuit.input_map: if w in wire_map: full_pred_map[wire_map[w]] = pred_map[wire_map[w]] full_succ_map[wire_map[w]] = succ_map[wire_map[w]] else: full_succ_map[w] = self.output_map[w] full_pred_map[w] = self._multi_graph.predecessors( self.output_map[w])[0] if len(list(self._multi_graph.predecessors(self.output_map[w])))!= 1: raise DAGCircuitError(""too many predecessors for %s[%d] "" ""output node"" % (w[0], w[1])) return full_pred_map, full_succ_map"
508,"def get_TV_series_id(page, language, append_to_response): append_to_response(page, language, append_to_response): append_to_response(language, append_to_response): append_to_response(language, append_to_response): append_to_response(language, append_to_response): append_to_response): append_to_response)","def similar(self, **kwargs): path = self._get_id_path('similar') response = self._GET(path, kwargs) self._set_attrs_to_values(response) return response"
509,"def set_value(self, bounds,free,errors): self.get_value(bounds,free,errors) self.get_value(bounds,free,errors) self.get_value(bounds,free,errors) self.get_value(bounds,free,errors) self.get_value(bounds,free,errors) self.get_value(bounds,free,errors) self.get_value(bounds,free,errors) self.get_value(bounds,free,errors) self.get_value(bounds,free,errors) self.get_value(bounds,free,errors) self.get_value(bounds,free,errors) self.get_value(bounds,free,errors) self.get_value(bounds,free,errors) self.get_value(bounds,free,errors) self.get_value(bounds,free,errors) self.get_value(free,errors) self.get_value(free,errors) self.get_value(free,errors) self.get_value(free,errors) self.get_value(free,errors) self.get_value(free,errors) self.get_value(","def set(self, **kwargs): if 'bounds' in kwargs: self.set_bounds(kwargs.pop('bounds')) if 'free' in kwargs: self.set_free(kwargs.pop('free')) if 'errors' in kwargs: self.set_errors(kwargs.pop('errors')) if 'value' in kwargs: self.set_value(kwargs.pop('value'))"
510,"def link_reject(self, AMQP1.0, AMQP1.0, AMQP1.0, AMQP1.0, AMQP1.0, AMQP1.0, AMQP1.0, AMQP1.0, AMQP1.0, AMQP1.0, AMQP1.0, AMQP1.0, AMQP1.0, AMQP1.0, AMQP1.0, AMQP1.0, AMQP1.0, AMQP1.0, AMQP1.0, AMQP1.0, AMQP1.0, AMQP1.0, AMQP1.0)","def reject(self, pn_condition=None): self._pn_link.source.type = proton.Terminus.UNSPECIFIED super(SenderLink, self).reject(pn_condition)"
511,"def create_hosted_zone(self, caller_reference, comment): self.create_hosted_zone(self.create_hosted_zone) self.create_hosted_zone(self.create_hosted_zone) self.create_hosted_zone(self.create_hosted_zone) self.create_hosted_zone(self.create_hosted_zone) self.create_hosted_zone) self.create_hosted_zone(self.create_hosted_zone) self.create_hosted_zone) self.create_hosted_zone(self.create_zone) self.create_zone(self.create_zone) self.create_zone) self.create_zone(self.create_zone) self.create_zone) self.create_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_zone_","def create_hosted_zone(self, name, caller_reference=None, comment=None): body = xml_generators.create_hosted_zone_writer( connection=self, name=name, caller_reference=caller_reference, comment=comment ) root = self._send_request( path='hostedzone', data=body, method='POST', ) return xml_parsers.created_hosted_zone_parser( root=root, connection=self )"
512,"def send_response(self, queue): self.response(self.response) self.response(self.response) self.response(self.response) self.response(self.response) self.response(self.response) self.response(self.response) self.response(self.response) self.response_response_response_response_response_response_response_response_response_response_response_response_response_response_response_response_response_response_response_response_response_response_response_response_response_response_response_response_res","def dispatch_reply(self, reply, value): method = reply.method call_id = reply.call_id task_id = reply.task_id if method & ACK: try: result_queue = self.result_queues[call_id] except KeyError: raise KeyError('already established or unprepared call') if method == ACCEPT: worker_info = value result = RemoteResult(self, call_id, task_id, worker_info) self.results[call_id][task_id] = result result_queue.put_nowait(result) elif method == REJECT: result_queue.put_nowait(None) else: result = self.results[call_id][task_id] result.set_reply(reply.method, value)"
513,"def convert_sequence(self, code): self.sequence(code): self.sequence(code): self.sequence(code): self.sequence(code)","def make_html_code( self, lines ): line = code_header + 'n' for l in lines: line = line + html_quote( l ) + 'n' return line + code_footer"
514,"def aggregation_levels(self, var_name, value_name, var_name, var_name, var_name, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels) drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_levels, drop_level","def roll_up( df, levels: List[str], groupby_vars: List[str], extra_groupby_cols: List[str] = None, var_name: str = 'type', value_name: str = 'value', agg_func: str ='sum', drop_levels: List[str] = None ): dfs = list() groupby_cols_cpy = list(levels) levels_cpy = list(levels) levels_cpy.reverse() extra_groupby_cols = extra_groupby_cols or [] drop_levels = drop_levels or [] previous_level = None for top_level in levels_cpy: gb_df = getattr( df.groupby(groupby_cols_cpy + extra_groupby_cols)[groupby_vars], agg_func)().reset_index() gb_df[var_name] = top_level gb_df[value_name] = gb_df[top_level] dfs.append(gb_df) if previous_level in drop_levels: del dfs[-2] previous_level = top_level groupby_cols_cpy.pop() return pd.concat(dfs, sort=False).reset_index()"
515,"def get_manifest(self, v2): if self.get_manifest(self.get_manifest): if self.get_manifest(self.get_manifest): self.get_manifest(self.get_manifest) self.get_manifest(self.get_manifest) self.get_manifest(self.get_manifest) self.get_manifest(self.get_manifest) self.get_manifest(self.get_manifest) self.get_manifest(self.get_manifest) self.get_manifest(self.get_manifest) self.get_manifest(self.get_manifest) self.get_manifest(self.get_manifest) self.get_manifest(self.get_manifest) self.get_manifest(self.get_manifest) self.get_manifest(self.get_manifest) self.get_manifest(self.get_manifest) self.get_manifest(self.get_manifest) self.get_manifest(self.get_manifest) self.get_manifest","def get_manifests(self, repo_name, digest=None): if not hasattr(self,'manifests'): self.manifests =  schemaVersions = ['v1', 'v2', 'config'] for schemaVersion in schemaVersions: manifest = self._get_manifest(repo_name, digest, schemaVersion) if manifest is not None: if schemaVersion == ""v2"" and ""config"" in manifest: bot.debug('Attempting to get config as blob in verison 2 manifest') url = self._get_layerLink(repo_name, manifest['config']['digest']) headers = 'Accept': manifest['config']['mediaType'] self.manifests['config'] = self._get(url, headers=headers) self.manifests[schemaVersion] = manifest return self.manifests"
516,"def create_port_range(self, port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range): port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range, port_range)","def create_tcp_rpc_system(hostname=None, port_range=(0,), ping_interval=1, ping_timeout=0.5): def ownid_factory(listeningport): port = listeningport.getHost().port return ""%s:%s"" %(hostname, port) def make_client_endpoint(peer): host, port = peer.split("":"") if host == socket.getfqdn(): host = ""localhost"" return endpoints.TCP4ClientEndpoint(reactor, host, int(port), timeout=5) if hostname is None: hostname = socket.getfqdn() server_endpointA = TCP4ServerRangeEndpoint(reactor, port_range) pool = connectionpool.ConnectionPool(server_endpointA, make_client_endpoint, ownid_factory) return RPCSystem(pool, ping_interval=ping_interval, ping_timeout=ping_timeout)"
517,"def remove_element(self, tail): if self.get_element(self.get_element(self.get_element(self.get_element(self.get_element(self.get_element(self.get_element(self.get_element(self.get_element(self.get_element(self.get_element(self.get_element(self.get_element(self.get_element(self.get_element(self.get_element(self.get_element(self.get_element(self.get_element(self.get_element(self.get_element(self.get_element)))))))))",def drop_tree(self): parent = self.getparent() assert parent is not None if self.tail: previous = self.getprevious() if previous is None: parent.text = (parent.text or '') + self.tail else: previous.tail = (previous.tail or '') + self.tail parent.remove(self)
518,def save_features(self): self.save_features(self.save_features): self.save_features(self.save_features) self.save_features(self.save_features) self.save_features(self.save_features) self.save_features(self.save_features) self.save_features(self.save_features) self.save_features) self.save_features) self.save_features(self.save_features) self.save_features) self.save_features(self.save_features) self.save_features) self.save_features) self.save_features,"def write_features(self): out_json = collections.OrderedDict() try: self.read_features() except (WrongFeaturesFormatError, FeaturesNotFound, NoFeaturesFileError): out_json = collections.OrderedDict(""metadata"":  ""versions"": ""librosa"": librosa.__version__, ""msaf"": msaf.__version__, ""numpy"": np.__version__, ""timestamp"": datetime.datetime.today().strftime( ""%Y/%m/%d %H:%M:%S"")) out_json[""globals""] =  ""dur"": self.dur, ""sample_rate"": self.sr, ""hop_length"": self.hop_length, ""audio_file"": self.file_struct.audio_file  out_json[""est_beats""] = self._est_beats_times.tolist() out_json[""est_beatsync_times""] = self._est_beatsync_times.tolist() if self._ann_beats_times is not None: out_json[""ann_beats""] = self._ann_beats_times.tolist() out_json[""ann_beatsync_times""] = self._ann_beatsync_times.tolist() except FeatureParamsError: with open(self.file_struct.features_file) as f: out_json = json.load(f) finally: out_json[self.get_id()] =  out_json[self.get_id()][""params""] =  for param_name in self.get_param_names(): value = getattr(self, param_name"
519,"def first_line_in_a_pdf(self, first_line_in_a_pdf, first_line_in_a_pdf, first_line_in_a_pdf, first_line_in_a_pdf, first_line_in_a_pdf, first_line_in_a_pdf, first_line_in_a_pdf, first_line_in_a_pdf, first_line_in_a_pdf, first_line_in_a_pdf, first_line_in_a_page_in_a_page_in_a_page_in_a_page_in_a_page_in_a_page_in_a_page_in_a_page_in_a_page_in_a_page_in_a_page_in_a_page_in_a_page_in_a_page_in_a______________________________________________________________________________________________","def _put_header(self): self.session._out('%%PDF-%s' % self.pdf_version) if self.session.compression: self.session.buffer += '%' + chr(235) + chr(236) + chr(237) + chr(238) + ""n"""
520,"def perform_eigendecompositon(self, left- and right- matrices): self.decompositon(left- and right- matrices) self.decompositon(left- and right- matrices) self.decompositon(left- and right- matrices) self.decompositon(left- and right- matrices) self.decompositon(left- and right- matrices) self.decompositon(left- matrices) self.decompositon(left- matrices) self.decompositon(left- matrices) self.decompositon(left- matrices) self.decompositon(right- matrices) self.decompositon(left- matrices) self.decompositon(right- matrices) self.decompositon(left- matrices) self.decompositon(left- matrices) self.decompositon(left- matrices) self.decompositon(left- matrices) self.decompositon(left- matrices) self.decom","def _eig_sym(self): tmpp = np.sqrt(self.Pi) symQ = self.W*np.outer(tmpp, tmpp) eigvals, eigvecs = np.linalg.eigh(symQ) tmp_v = eigvecs.T*tmpp one_norm = np.sum(np.abs(tmp_v), axis=1) self.v = tmp_v.T/one_norm self.v_inv = (eigvecs*one_norm).T/tmpp self.eigenvals = eigvals"
521,"def parse_config(self, config): if self.config(config): self.config(config): self.config(config): self.config(config): self.config(config): self.config(config): self.config(config): self.config(config): self.config(config): self.config(config): self.config(config): self.config(config)","def parse(self, config): if ""type"" not in config: raise InvalidConfigurationException(""The dashboard configuration has not defined a 'type'. %s"" % config) component_type = config[""type""] component_config = self._get_config_by_id(self._component_configs, component_type) if not component_config: raise ComponentNotFoundForType(component_type) options = config.get(""options"", ) component = self._parse_item(component_config, options, config) component.name = config.get(""name"", """") return component"
522,"def get_text_value(self, text): self.get_text_value(self.get_text_value) self.get_text_value(self.get_text_value) self.get_text_value(self.get_text_value) self.get_text_value(self.get_text_value) self.get_text_value(self.get_text_value) self.get_text_value(self.get_text_value) self.get_text_value(self.get_text_value) self.get_text_value(self.get_text_value) self.get_text_value(self.get_text_value) self.get_text_value(self.get_text_value) self.get_text_value) self.get_text_value(self.get_text_value) self.get_text_value) self.get_text_value(self.get_text_value) self.get_text_value(self.get_text_value) self.get_text_value(self.get_text_value) self.get_text_value(self.get_text_value) self.get_text_value(self.get_text_value) self.get_text_value(self.get_text_","def value(self, n: Node) -> str: id_n = id(n) idcache = self.id_cache if id_n not in idcache: return """" name = idcache[id_n] tag_cache = self.tag_cache if name not in tag_cache: raise Exception(""Incoherent tag cache"") tag = tag_cache[name] k = ""%d:%d"" % (tag._begin, tag._end) valcache = self._streams[-1].value_cache if k not in valcache: valcache[k] = str(tag) return valcache[k]"
523,"def ScaleH2OFrame(x, y, params, y): if x is None: if x is None: scaleH2OFrame(x, y, params, y, params): scaleH2OFrame(x, y, params): scaleH2OFrame(x, y, params): scaleH2OFrame(x, y, params) scaleH2OFrame(x) scaleH2OFrame(x) scaleH2OFrame(x) scaleH2OFrame(x) scaleH2OFrame(x) scaleH2OFrame(x) scaleH2OFrame(x) scaleH2OFrame(x) scaleH2OFrame(x) scaleH2OFrame(x) scaleH2OFrame(x) scaleH2OFrame(x) scaleH2OFrame(x) scaleH2OFrame(x) scaleH2OFrame(x) scaleH2OFrame(x) scaleH2OFrame(x) scaleH2OFrame(x) scaleH2","def transform(self, X, y=None, **params): return X.scale(self.means, self.stds)"
524,"def _attachment(self, card): if self.card(self.card): self.attachment(self.attachment): self.attachment(self.attachment) self.attachment(self.attachment) self.attachment(self.attachment) self.attachment(self.attachment) self.attachment(self.attachment) self.attachment(self.attachment) self.attachment(self.attachment) self.attachment(self.attachment) self.attachment(self.attachment) self.attachment(self.attachment) self.attachment(self.attachment) self.attachment(self.attachment) self.attachment) self.attachment(self.attachment) self.attachment) self.attachment(self.attachment) self.attachment(self.attachment) self.attachment(self.attachment) self.attachment) self.attachment(self.attachment) self.attachment(self.attachment) self.attachment(self.attachment) self.attachment(self.attachment) self.attachment(self.attachment) self.attachment(self.attachment) self.attachment(self.attachment) self.attachment(self.attachment) self.attachment(self.attachment)","def oauth_card(card: OAuthCard) -> Attachment: if not isinstance(card, OAuthCard): raise TypeError('CardFactory.oauth_card(): card argument is not an instance of an OAuthCard,''unable to prepare attachment.') return Attachment(content_type=CardFactory.content_types.oauth_card, content=card)"
525,"def parse_vcfRecord(self, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfReader, vcfRead","def vcf_records(self, qualified=False): if qualified: sample_names = self.qualified_sample_names else: sample_names = self.sample_names for line in self._file_reader.read_lines(): if line.startswith(""#""): continue yield VcfRecord.parse_record(line, sample_names)"
526,"def plot_results(num_line, data_wanted, data_wanted, information): if num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line = num_line =","def display_the_graphic(self, num_line, wavelength, data_wanted, information): self.nb_case = num_line - 1 self.graphic_slider(self.nb_case) self.mpl_canvas.update_fields(wavelength, data_wanted, self.slider_value) if self.ui.show_grid.checkState() == 2: grid = True else: grid = False if self.ui.show_all_curves.checkState() == 2: self.flag_curves = True self.mpl_canvas.display_graphic(self.flag_curves, self.ui, grid) self.print_graphic_information(self.slider_value, information) else: self.flag_curves = False self.mpl_canvas.display_graphic(self.flag_curves, self.ui, grid) self.print_graphic_information(self.slider_value, information)"
527,"def create_transfer_job(self, body, body): body = self.transferJob(self.transferJob) self.transferJob(self.transferJob) self.transferJob(self.transferJob) self.transferJob(self.transferJob) self.transferJob(self.transferJob) self.transferJob(self.transferJob) self.transferJob(self.transferJob) self.transferJob(self.transferJob) self.transferJob) self.transferJob) self.transferJob(self.transferJob) self.transferJob) self.transferJob(self.transferJob) self.transferJob) self.transferJob(self.transferJob) self.transferJob) self.transferJob(self.transferJob) self.transferJob(self.transferJob) self.transferJob) self.transferJob(self.transferJob) self.transferJob) self","def create_transfer_job(self, body): body = self._inject_project_id(body, BODY, PROJECT_ID) return self.get_conn().transferJobs().create(body=body).execute(num_retries=self.num_retries)"
528,"def download(self, str, str->True, str->True, str->True, str->True, str->True, str->True, str->True, str->True, str->True, str->True, str->True, str->True, str->True, str->True, str->True, str->True, str->True, str->True, str->True, str->True, str->True)","def ffmpeg_download_stream(files, title, ext, params=, output_dir='.', stream=True): output = title + '.' + ext if not (output_dir == '.'): output = output_dir + '/' + output print('Downloading streaming content with FFmpeg, press q to stop recording...') if stream: ffmpeg_params = [FFMPEG] + ['-y', '-re', '-i'] else: ffmpeg_params = [FFMPEG] + ['-y', '-i'] ffmpeg_params.append(files) #not the same here!!!! if FFMPEG == 'avconv': #who cares? ffmpeg_params += ['-c', 'copy', output] else: ffmpeg_params += ['-c', 'copy', '-bsf:a', 'aac_adtstoasc'] if params is not None: if len(params) > 0: for k, v in params: ffmpeg_params.append(k) ffmpeg_params.append(v) ffmpeg_params.append(output) print(' '.join(ffmpeg_params)) try: a = subprocess.Popen(ffmpeg_params, stdin= subprocess.PIPE) a.communicate() except KeyboardInterrupt: try: a.stdin.write('q'.encode('utf-8')) except: pass return True"
529,"def float_point(self, float_point): return self.float_point(float_point) return self.float_point(float_point)","def is_complex(dtype): dtype = tf.as_dtype(dtype) if hasattr(dtype, 'is_complex'): return dtype.is_complex return np.issubdtype(np.dtype(dtype), np.complex)"
530,"def handle_exception(self, APIException, Http404, PermissionDenied, Http404, Http404, Http404, Http404, Http404, Http404, Http404, Http404, Http404, Http404, Http404, Http404, Http404, Http404)","def simple_error_handler(exc, *args): if isinstance(exc, exceptions.APIException): headers =  if getattr(exc, 'auth_header', None): headers['WWW-Authenticate'] = exc.auth_header if getattr(exc, 'wait', None): headers['X-Throttle-Wait-Seconds'] = '%d' % exc.wait return Response('error': exc.detail, status=exc.status_code, headers=headers) elif isinstance(exc, Http404): return Response('error': 'Not found', status=status.HTTP_404_NOT_FOUND) elif isinstance(exc, PermissionDenied): return Response('error': 'Permission denied', status=status.HTTP_403_FORBIDDEN) return None"
531,def ________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________,"def expand_modules(files_or_modules, black_list, black_list_re): result = [] errors = [] for something in files_or_modules: if basename(something) in black_list: continue if _basename_in_blacklist_re(basename(something), black_list_re): continue if exists(something): try: modname = ""."".join(modutils.modpath_from_file(something)) except ImportError: modname = splitext(basename(something))[0] if isdir(something): filepath = join(something, ""__init__.py"") else: filepath = something else: modname = something try: filepath = modutils.file_from_modpath(modname.split(""."")) if filepath is None: continue except (ImportError, SyntaxError) as ex: errors.append(""key"": ""fatal"", ""mod"": modname, ""ex"": ex) continue filepath = normpath(filepath) modparts = (modname or something).split(""."") try: spec = modutils.file_info_from_modpath(modparts, path=sys.path) except ImportError: is_namespace = False is_directory = isdir(something) else: is_namespace = modutils.is_namespace(spec) is_directory = modutils.is_directory(spec) if not is_namespace: result.append(  ""path"": filepath, ""name"": modname, ""isarg"": True, ""basepath"": filepath, ""basename"": modname,  ) has_init = ( not (modname.endswith("".__init__"") or modname == ""__init__"") and basename(filepath) == ""__init__.py"" ) if has_init or is_namespace or is_directory: for subfilepath in mod"
532,"def config_file(self, config_file): config_file = self.config_file(config_file) self.config_file(config_file) self.config_file(config_file) self.config_file(config_file) self.config_file(config_file) self.config_file(config_file) self.config_file(config_file) self.config_file(config_file) self.config_file(config_file) self.config_file(config_file) self.config_file) self.config_file(config_file) self.config_file) self.config_file(config_file) self.config_file(config_file) self.config_file) self.config_file(config_file) self.config_file(config_file) self.config_file) self.config_file(config_file) self.config_file) self.config_file(config_file) self.config_file(config_file) self.config_file(config_file) self.config_file(config_file) self.config_file(config_file) self.config_file) self.config_file(config_file) self.config_file(config_file) self.config_file(config_file) self.config","def copy_config_file(self, config_file, path=None, overwrite=False): dst = os.path.join(self.location, config_file) if os.path.isfile(dst) and not overwrite: return False if path is None: path = os.path.join(get_ipython_package_dir(), u'config', u'profile', u'default') src = os.path.join(path, config_file) shutil.copy(src, dst) return True"
533,"def astroid.ImportFrom(self, astroid.ImportFrom, astroid.ImportFrom, astroid.ImportFrom, astroid.ImportFrom, astroid.ImportFrom, astroid.ImportFrom, astroid.ImportFrom, astroid.ImportFrom, astroid.ImportFrom, astroid.ImportFrom, astroid.ImportFrom, astroid.ImportFrom","def visit_importfrom(self, node): if self.pkgdiagram: self.pkgdiagram.add_from_depend(node, node.modname)"
534,"def trainable_initial_states(self, h_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last, c_last,","def cudnn_bi_lstm(units, n_hidden, seq_lengths=None, n_layers=1, trainable_initial_states=False, name='cudnn_bi_gru', reuse=False): with tf.variable_scope(name, reuse=reuse): if seq_lengths is None: seq_lengths = tf.ones([tf.shape(units)[0]], dtype=tf.int32) * tf.shape(units)[1] with tf.variable_scope('Forward'): h_fw, (h_fw_last, c_fw_last) = cudnn_lstm_wrapper(units, n_hidden, n_layers=n_layers, trainable_initial_states=trainable_initial_states, seq_lengths=seq_lengths) with tf.variable_scope('Backward'): reversed_units = tf.reverse_sequence(units, seq_lengths=seq_lengths, seq_dim=1, batch_dim=0) h_bw, (h_bw_last, c_bw_last) = cudnn_lstm_wrapper(reversed_units, n_hidden, n_layers=n_layers, trainable_initial_states=trainable_initial_states, seq_lengths=seq_lengths) h_bw = tf.reverse_sequence(h_bw, seq_lengths=seq_lengths, seq_dim=1, batch_dim=0) return (h_fw, h_bw), ((h_fw_last, c_fw_last), (h_bw_last, c"
535,"def create_label(self, box): self.create_label(box) self.create_label(box) self.create_label(box) self.create_label(box) self.create_label(box) self.create_label(box) self.create_label(box) self.create_label(box) self.create_label(box) self.create_label(box) self.create_label(box) self.create_label(box) self.create_label(box) self.create_label(box) self.create_label(box) self.create_label(box) self.create_label(box) self.create_label(box) self.create_label(box) self.create_label(box) self.create_label(box) self.create_label(box) self.create_label(box) self.create_label(box) self.create_label(box) self.create_label(box) self.create_label(box) self.create_label(box) self.create_label(box) self.create_label(box) self.create_label(box) self.create_label(box","def label_for_box(instruction): label = instruction.name.capitalize() params = TextDrawing.params_for_label(instruction) if params: label += ""(%s)"" % ','.join(params) return label"
536,"def get_image_language(self, language, language): self.get_image_language(language, language) self.get_image_language(language) self.get_image_language(language) self.get_image_language(language) self.get_image_language(language) self.get_image_language(language) self.get_image_language(language) self.get_image_language(language) self.get_image_language(language) self.get_image_language(language) self.get_image_language) self.get_image_language(language) self.get_image_language(language) self.get_image_language(language) self.get_image_language(language) self.get_image_language(language) self.get_image_language(language) self.get_image_language(language) self.get_image_language) self.get_image_language(language) self.get_image_language(language) self.get_image_language(language) self.get_image_language(language) self.get_image_language(language) self.get_image_language(language) self.get_image_language(language) self.get_image_language(language) self.get_image_language(language) self.get_image_","def images(self, **kwargs): path = self._get_series_id_season_number_path('images') response = self._GET(path, kwargs) self._set_attrs_to_values(response) return response"
537,def transform_command_line_properties(self): if self.get_command_line_properties(self.get_command_line_properties): self.get_command_line_properties(self.get_command_line_properties) self.get_command_line_properties(self.get_command_line_properties) self.get_command_line_properties(self.get_command_line_properties) self.get_command_line_properties(self.get_command_line_properties) self.get_command_line_properties) self.get_command_properties(self.get_command_properties) self.get_properties) self.get_properties(self.get_properties) self.get_properties) self.get_properties(self.get_properties) self.get_properties) self.get_properties(self.get_properties) self.get_properties(self.get_properties) self.get_properties) self.get_properties(self.get_properties) self.get_properties(self.get_properties) self,"def _process_properties(self, properties): if properties is not None: self._properties =  for p in properties: d = p.split('=') self._properties[d[0]] = d[1]"
538,"def get_all_restrictions(self, users, fireroles): self.get_all_restrictions(users, fireroles) self.get_all_restrictions(users, fireroles) self.get_all_restrictions(users, fireroles) self.get_all_restrictions(users, fireroles) self.get_all_restrictions(users, fireroles) self.get_all_all_restrictions(users, fireroles) self.get_all_all_restrictions(users, fireroles) self.get_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all","def _get_collection_restrictions(collection): try: from invenio.dbquery import run_sql from invenio.access_control_firerole import compile_role_definition except ImportError: from invenio.modules.access.firerole import compile_role_definition from invenio.legacy.dbquery import run_sql res = run_sql( 'SELECT r.firerole_def_src, email''FROM accROLE as r''JOIN accROLE_accACTION_accARGUMENT ON r.id=id_accROLE''JOIN accARGUMENT AS a ON a.id=id_accARGUMENT''JOIN user_accROLE AS u ON r.id=u.id_accROLE''JOIN user ON user.id=u.id_user''WHERE a.keyword=""collection"" AND''a.value=%s AND''id_accACTION=(select id from accACTION where name=""viewrestrcoll"")', (collection, ), run_on_slave=True ) fireroles = set() users = set() for f, u in res: fireroles.add(compile_role_definition(f)) users.add(u) return 'fireroles': list(fireroles), 'users': users"
539,def find_magic(self): if self.magic(self.magic): self.magic(self.magic) return self.magic(self.magic),"def find_magic(self, magic_name, magic_kind='line'): return self.magics_manager.magics[magic_kind].get(magic_name)"
540,"def kl(a,b): kl = kl(a,b) if kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = kl = k","def _make_kl_divergence_fn( distribution_b, use_exact_kl=False, test_points_reduce_axis=(), test_points_fn=tf.convert_to_tensor, weight=None): with tf.compat.v1.name_scope('kldivergence_loss'): distribution_b_ = (distribution_b() if callable(distribution_b) else distribution_b) kl = kl_divergence_fn(distribution_a, distribution_b_) if weight is not None: kl = tf.cast(weight, dtype=kl.dtype) * kl return tf.reduce_sum(input_tensor=kl, name='batch_total_kl_divergence') return _fn"
541,"r""[r""[r""[r""[r""[r""[r]][r""[r""[r]][r""[r""[r]][r""[r""[r]][r""[r""[r]][r""[r][r][r][r][r][r][r][r][r][r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""[r""","def t_LSQBR(self, t): r""["" t.endlexpos = t.lexpos + len(t.value) return t"
542,def show_all_gene_panels_in_database_database(self): self.database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_database_data,"def panels(context, institute): LOG.info(""Running scout view panels"") adapter = context.obj['adapter'] panel_objs = adapter.gene_panels(institute_id=institute) if panel_objs.count() == 0: LOG.info(""No panels found"") context.abort() click.echo(""#panel_nametversiontnr_genestdate"") for panel_obj in panel_objs: click.echo(""0t1t2t3"".format( panel_obj['panel_name'], str(panel_obj['version']), len(panel_obj['genes']), str(panel_obj['date'].strftime('%Y-%m-%d')) ))"
543,"def load_pretrained_bigdl_model(path, path): if path is not path, return path.get_pretrained_bigdl_model(path, path)","def loadModel(modelPath, weightPath =None, bigdl_type=""float""): jmodel = callBigDlFunc(bigdl_type, ""loadBigDLModule"", modelPath, weightPath) return Layer.of(jmodel)"
544,"def create(self, notebook_id, notebook_id, notebook_id, notebook_id, notebook_id, notebook_id, notebook_id, notebook_id, notebook_id, notebook_id, notebook_id, notebook_id, notebook_id, notebook_id, notebook_id, notebook_id, notebook_id, notebook_id, notebook_id, notebook_id, notebook_id, notebook_id, notebook_id, notebook_id, notebook_id)","def new_notebook(self): path, name = self.increment_filename('Untitled') notebook_id = self.new_notebook_id(name) metadata = current.new_metadata(name=name) nb = current.new_notebook(metadata=metadata) with open(path,'w') as f: current.write(nb, f, u'json') return notebook_id"
545,"def self.app(self.app, self.app, self.app, self.app, self.app, self.app, self.app, self.app, self.app, self.app, self.app, self.app, self.app, self.app, self.app, self.app, self.app, self.app, self.app, self.app, self.app, self.app, self.app, self.app, self.app, self.app, self.app, self.app, self.app, self.app)","def needs_ext(self): if settings.SYSTEMJS_DEFAULT_JS_EXTENSIONS: name, ext = posixpath.splitext(self.app) if not ext: return True return False"
546,"def _accumulate(self, max, max): if self._accumulate(self._accumulate): self._accumulate(self._accumulate) self._accumulate(self._accumulate) self._accumulate(self._accumulate) self._accumulate(self._accumulate) self._accumulate(self._accumulate) self._accumulate(self._accumulate) self._accumulate(self._accumulate) self._accumulate(self._accumulate) self._accumulate(self._accumulate) self._accumulate(self._accumulate) self._accumulate(self._accumulate) self._accumulate) self._accumulate(self._accumulate) self._accumulate) self._accumulate(self._accumulate) self._accumulate(self._accumulate) self._accumulate) self._accumulate(self._accumulate) self._accumulate) self._accumulate(self._accumulate) self._accumulate(self._accumulate) self._accumulate(self._accumulate) self._accumulate(self._accumulate) self._accumulate(self._accumulate) self._accumulate_","def handle(self, integers, **options): print integers, options return super(Command, self).handle(integers, **options)"
547,def test(pySpark.DataFrame): pySpark.DataFrame(pySpark.DataFrame): pySpark.DataFrame(pySpark.DataFrame): pySpark.DataFrame): pySpark.DataFrame(pySpark.DataFrame): pySpark.DataFrame(pySpark.DataFrame): pySpark.DataFrame): pySpark.DataFrame(pySpark.DataFrame): pySpark.DataFrame): pySpark.DataFrame(pySpark.DataFrame): pySpark.DataFrame(pySpark.DataFrame): pySpark.DataFrame): pySpark.DataFrame = pySpark.DataFrame = pySpark.DataFrame = pySpark.DataFrame = pySpark.DataFrame = pySpark.DataFrame = pySpark.DataFrame = pySpark,"def get_user_affinity(self, test): ) return self.spark.sql(query)"
548,"def get_instance_url(self, instance_url): self.get_instance_url(self.get_instance_url) self.get_instance_url(self.get_instance_url) self.get_instance_url(self.get_instance_url) self.get_instance_url(self.get_instance_url) self.get_instance_url(self.get_instance_url) self.get_instance_url) self.get_instance_url) self.get_instance_url(self.get_instance_url) self.get_instance_url) self.get_instance_url(self.get_instance_url) self.get_instance_url) self.get_instance_url(self.get_instance_url) self.get_instance_url) self.get_instance_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url","def instance_url(self): id_ = self.get(self.ID_ATTR) base = self.class_url() if id_: return '/'.join([base, six.text_type(id_)]) else: raise Exception( 'Could not determine which URL to request: %s instance''has invalid ID: %r' % (type(self).__name__, id_), self.ID_ATTR)"
549,"def draw_arc(xc, yc): draw_arc(xc, yc): draw_arc(xc, yc): draw_arc(xc, yc): draw_arc(xc, yc): draw_arc(xc, yc)","def draw_arc(self, x1y1, x2y2, xcyc): self._buf_image.DrawArcPoint(x1y1, x2y2, xcyc) if self.auto_refresh: dc = wx.ClientDC(self.wx_obj) dc.BlitPointSize((0, 0), (self._size[0], self._size[1]), self._buf_image, (0, 0))"
550,"def get_gene_panel(self, panel_name, gene_dict): if panel_name is self.gene(panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name, panel_name,","def gene_to_panels(self, case_obj): LOG.info(""Building gene to panels"") gene_dict =  for panel_info in case_obj.get('panels', []): panel_name = panel_info['panel_name'] panel_version = panel_info['version'] panel_obj = self.gene_panel(panel_name, version=panel_version) if not panel_obj: # LOG.warning(""Panel: 0, version 1 does not exist in database"".format(panel_name, panel_version)) for gene in panel_obj['genes']: hgnc_id = gene['hgnc_id'] if hgnc_id not in gene_dict: gene_dict[hgnc_id] = set([panel_name]) continue gene_dict[hgnc_id].add(panel_name) LOG.info(""Gene to panels done"") return gene_dict"
551,def get_module_name(modules): if module_name = module_name = module_name = module_name = module_name = module_name = module_name = module_name = module_name = module_name = module_name = module_name = module_name = module_name = module_name = module_name = module_name = module_name = module_name,"def module_list(path): name, extension = os.path.splitext( path ) return import_re.match(path) and py3compat.isidentifier(name) folder_list = [p for p in folder_list if isfile(pjoin(path, p,'__init__.py')) or is_importable_file(p) ] return [basename(p).split('.')[0] for p in folder_list]"
552,def glm_model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_demo.model_,"def glm(interactive=True, echo=True, testing=False): go() h2o.init() go() prostate = h2o.load_dataset(""prostate"") go() prostate.describe() go() train, test = prostate.split_frame(ratios=[0.70]) go() train[""CAPSULE""] = train[""CAPSULE""].asfactor() test[""CAPSULE""] = test[""CAPSULE""].asfactor() go() from h2o.estimators import H2OGeneralizedLinearEstimator prostate_glm = H2OGeneralizedLinearEstimator(family=""binomial"", alpha=[0.5]) prostate_glm.train(x=[""AGE"", ""RACE"", ""PSA"", ""VOL"", ""GLEASON""], y=""CAPSULE"", training_frame=train) go() prostate_glm.show() go() predictions = prostate_glm.predict(test) predictions.show() go() performance = prostate_glm.model_performance(test) performance.show() _run_demo(demo_body, interactive, echo, testing)"
553,"def switch_on(self, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message)","def turn_on_switch(self, device_id, name): msg = ""!%sF1|Turn On|%s"" % (device_id, name) self._send_message(msg)"
554,"def write_value(ostream, ostream, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_","def write_value(self, ostream, kmip_version=enums.KMIPVersion.KMIP_1_0): try: ostream.write(pack('!Q', self.value)) except Exception: self.logger.error(""Error writing boolean value to buffer"") raise"
555,"def seed_random(self, seed material): if self.get_seed(seed): self.get_seed(seed): self.get_seed(seed)","def seedRandom(self, seed): low_seed = ckbytelist(seed) rv = self.lib.C_SeedRandom(self.session, low_seed) if rv!= CKR_OK: raise PyKCS11Error(rv)"
556,"def decode_object(input_stream, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version): def decode_object(input_stream, input_stream): decode_object(input_stream, input_stream, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip","def read(self, input_stream, kmip_version=enums.KMIPVersion.KMIP_1_0): super(ArchiveResponsePayload, self).read( input_stream, kmip_version=kmip_version ) local_stream = utils.BytearrayStream(input_stream.read(self.length)) if self.is_tag_next(enums.Tags.UNIQUE_IDENTIFIER, local_stream): self._unique_identifier = primitives.TextString( tag=enums.Tags.UNIQUE_IDENTIFIER ) self._unique_identifier.read( local_stream, kmip_version=kmip_version ) self.is_oversized(local_stream)"
557,"def buffer_read_samples(self, samples): buffer = self.read_samples(samples) buffer = self.read_samples(samples) buffer = self.read_samples(samples) buffer = self.read_samples(samples) buffer = self.read_samples(samples) buffer = self.read_samples(samples) buffer = self.read_samples(samples) buffer = self.read_samples(samples) buffer = self.read_samples(samples) buffer = self.read_samples(samples) buffer = self.read_samples(samples) buffer = self.read_samples) buffer = self.read_samples(samples) buffer = self.read_samples) buffer = self.read_samples_samples_samples_samples_samples_samples_samples_samples_samples_samples_samples_samples_sample","def create_buffer(self, bins, repeats, base_buffer_size, max_buffer_size=0): samples = bins * repeats buffer_repeats = 1 buffer_size = math.ceil(samples / base_buffer_size) * base_buffer_size if not max_buffer_size: max_buffer_size = (100 * 1024**2) / 8 if max_buffer_size > 0: max_buffer_size = math.ceil(max_buffer_size / base_buffer_size) * base_buffer_size if buffer_size > max_buffer_size: logger.warning('Required buffer size () will be shrinked to max_buffer_size ()!'.format( buffer_size, max_buffer_size )) buffer_repeats = math.ceil(buffer_size / max_buffer_size) buffer_size = max_buffer_size logger.info('repeats: '.format(repeats)) logger.info('samples:  (time: :.5f s)'.format(samples, samples / self.device.sample_rate)) if max_buffer_size > 0: logger.info('max_buffer_size (samples):  (repeats: :.2f, time: :.5f s)'.format( max_buffer_size, max_buffer_size / bins, max_buffer_size / self.device.sample_rate )) else: logger.info('max_buffer_size (samples): UNLIMITED') logger.info('buffer_size (samples):  (repeats: :.2f, time: :.5f s)'.format( buffer_size, buffer_size / bins, buffer_"
558,"def run_gvcfs(self, gvcfs, fai, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, gvcfs, g","def gatk_genotype_gvcfs(job, gvcfs, ref, fai, ref_dict, annotations=None, emit_threshold=10.0, call_threshold=30.0, unsafe_mode=False): inputs = 'genome.fa': ref, 'genome.fa.fai': fai, 'genome.dict': ref_dict inputs.update(gvcfs) work_dir = job.fileStore.getLocalTempDir() for name, file_store_id in inputs.iteritems(): job.fileStore.readGlobalFile(file_store_id, os.path.join(work_dir, name)) command = ['-T', 'GenotypeGVCFs', '-R', '/data/genome.fa', '--out', 'genotyped.vcf', '-stand_emit_conf', str(emit_threshold), '-stand_call_conf', str(call_threshold)] if annotations: for annotation in annotations: command.extend(['-A', annotation]) for uuid in gvcfs.keys(): command.extend(['--variant', os.path.join('/data', uuid)]) if unsafe_mode: command.extend(['-U', 'ALLOW_SEQ_DICT_INCOMPATIBILITY']) job.fileStore.logToMaster('Running GATK GenotypeGVCFsn' 'Emit threshold: emit_thresholdn' 'Call threshold: call_thresholdnn' 'Annotations:nannotationsnn' 'Samples:nsamplesn'.format(emit_"
559,"def use_parent(self, parent, child, override): use_parent = self.override(self.override) self.override(self.override)","def push(self, override, use_parent=False): current = self.current if use_parent and current: override = current + override _override_ctx_stack.push((self, override))"
560,"def normalize_field_name(self, field_name, tuple): if field_name is not self._____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________","def _normalize_field_name(self, field_name) -> str: if isinstance(field_name, tuple): field_name, _ = field_name return field_name"
561,"def find_java_execution(self, java_execution): if java is not found: if java is not found: if java is not found: if java is not found: if java is not found: if java is not found: if java is not found: if java is not found: if java is not found: if java is not found: if java is not found: if java is not found: if java_execution): if java_execution = java_execution = java_execution = java_execution = java_execution = java_execution = java_execution = java_execution = java_execution = java_execution = java_execution = java_execution = java_execution = java_execution = java_execution = java_execution = java_execution = java_execution = java_execution = java_execution = java_execution = java_execution = java_execution = java_execution = java_","def _find_java(): java = ""java.exe"" if sys.platform == ""win32"" else ""java"" if os.access(java, os.X_OK): return java for path in os.getenv(""PATH"").split(os.pathsep): full_path = os.path.join(path, java) if os.access(full_path, os.X_OK): return full_path if os.getenv(""JAVA_HOME""): full_path = os.path.join(os.getenv(""JAVA_HOME""), ""bin"", java) if os.path.exists(full_path): return full_path if sys.platform == ""win32"": program_folders = [os.path.join(""C:"", ""Program Files"", ""Java""), os.path.join(""C:"", ""Program Files (x86)"", ""Java""), os.path.join(""C:"", ""ProgramData"", ""Oracle"", ""Java"")] for folder in program_folders: for dirpath, dirnames, filenames in os.walk(folder): if java in filenames: return os.path.join(dirpath, java) raise H2OStartupError(""Cannot find Java. Please install the latest JRE fromn"" ""http://www.oracle.com/technetwork/java/javase/downloads/index.html"")"
562,"def authenticate_user_data(self, user_data): self.redirect(user_data) self.redirect(user_data) self.redirect(user_data) self.redirect(user_data) self.redirect(user_data) self.redirect(user_data) self.redirect(user_data) self.redirect(user_data) self.redirect(user_data) self.redirect(user_data) self.redirect(user_data) self.redirect(user_data) self.redirect(user_data) self.redirect(user_data) self.redirect(user_data) self.redirect(user_data) self.redirect(user_data) self.redirect(user_data) self.redirect(user_data) self.redirect(user_data) self.redirect(user_data) self.redirect(user_data) self.redirect(user_data) self.redirect(user_data) self.redirect(user_data) self.redirect(user_data) self.redirect(user_data) self.redirect(user_data) self.redirect(user_data) self.redirect(user_data) self.redirect(user_data) self.redirect","def get_authenticated_user(self, callback): oauth_ns = """" for name, values in self.request.arguments.iteritems(): if name.startswith(""openid.ns."") and  values[-1] == u""http://specs.openid.net/extensions/oauth/1.0"": oauth_ns = name[10:] break token = self.get_argument(""openid."" + oauth_ns + "".request_token"", """") if token: http = httpclient.AsyncHTTPClient() token = dict(key=token, secret="""") http.fetch(self._oauth_access_token_url(token), self.async_callback(self._on_access_token, callback)) else: OpenIdMixin.get_authenticated_user(self, callback)"
563,"def run_tests(tests, reporter, stop_after, stop_after, stop_after, stop_after, stop_after, stop_after, stop_after, stop_after, stop_after, stop_after, stop_after, stop_after, stop_after, stop_after, stop_after, stop_after, stop_after, stop_after, stop_after, stop_after, stop_after, stop_after, stop_after, stop_after, stop_after, stop_after","def run(tests=(), reporter=None, stop_after=None): if reporter is None: reporter = Counter() if stop_after is not None: reporter = _StopAfterWrapper(reporter=reporter, limit=stop_after) locator = ObjectLocator() cases = ( case for test in tests for loader in locator.locate_by_name(name=test) for case in loader.load() ) suite = unittest.TestSuite(cases) getattr(reporter, ""startTestRun"", lambda: None)() suite.run(reporter) getattr(reporter, ""stopTestRun"", lambda: None)() return reporter"
564,"def random_float(self, min, max, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places, decimal_places","def amount(min=1, max=sys.maxsize, decimal_places=2): q = '.%s1' % '0' * (decimal_places - 1) return decimal.Decimal(uniform(min, max)).quantize(decimal.Decimal(q))"
565,"def shrink_trajectories(self, force=True): self.shrink_trajectories(force=True) self.shrink_trajectories(force=True) self.shrink_trajectories(force=True) self.shrink_trajectories(force=True) self.shrink_trajectories(force=True) self.shrink_trajectories(force=True) self.shrink_trajectories(force=True) self.shrink_trajectories_trajectories_trajectories_trajectories_trajectories_trajectories_trajectories_trajectories_trajectories_trajectories_trajectories_trajectories_trajectories_trajectories_trajectories_trajectories_trajectories_trajectories_trajectories_trajectories_trajectories_trajectories_trajectories_trajectories_trajectories_trajectories_trajectories_trajectories)","def f_shrink(self, force=False): if self._stored and not force: raise TypeError('Your trajectory is already stored to disk or database, shrinking is''not allowed.') for param in self._explored_parameters.values(): param.f_unlock() try: param._shrink() except Exception as exc: self._logger.error('Could not shrink %s because of:%s' % (param.v_full_name, repr(exc))) self._explored_parameters =  self._run_information =  self._single_run_ids =  self._add_run_info(0) self._test_run_addition(1)"
566,"def get_bipartite_matrix(self, matrix): if self.matrix(matrix): return self.matrix(matrix) return self.matrix(matrix) return self.matrix(matrix)","def _bipartite_shape(self): return (self._input_dim, self._output_dim, self._input_dim, self._output_dim)"
567,"def slicing_step(self, slicing_step): if self.slicing_step(self.slicing_step): self.slicing_step(self.slicing_step): self.slicing_step(self.slicing_step): self.slicing_step(self.slicing_step)","def _apply_single_step(dist, params_event_ndims, slices, params_overrides): if len(slices) == 1 and slices[0] == Ellipsis: override_dict =  else: override_dict = _slice_params_to_dict(dist, params_event_ndims, slices) override_dict.update(params_overrides) parameters = dict(dist.parameters, **override_dict) new_dist = type(dist)(**parameters) return new_dist"
568,"def transform_state_matrix(self, state_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix)","def transform(self, a, b, c, d, e, f): a0, b0, c0, d0, e0, f0 = self._currentMatrix self._currentMatrix = (a0 * a + c0 * b, b0 * a + d0 * b, a0 * c + c0 * d, b0 * c + d0 * d, a0 * e + c0 * f + e0, b0 * e + d0 * f + f0) a1, b1, c1, d1, e1, f1 = self._currentMatrix self.session._out('%.2f %.2f %.2f %.2f %.2f %.2f cm' % (a1, b1, c1, d1, e1, f1), self.page)"
569,"def delete_model(self, cluster, key): self.get_model(model, key): self.get_model(model, key) self.get_model(model, key) self.get_model(model, key) self.get_model(model, key) self.get_model(model, key) self.get_model(model, key) self.get_model(model, key) self.get_model(model) self.get_model(model, key) self.get_model(model, key) self.get_model(model, key) self.get_model(model, key) self.get_model(model, key) self.get_model(model, key) self.get_model(model, key) self.get_model(model, key) self.get_model(model, key) self.get_model(model) self.get_model(model) self.get_model(model) self.get_model(model) self.get_model(model) self.get_model(model) self.get_model(model) self.get_model(model) self.get_model(model) self.get_model(model) self.get_model(model) self.get_model(model) self.get_model(model) self.get","def delete_model(self, key, ignoreMissingKey=True, timeoutSecs=60, **kwargs): assert key is not None, '""key"" parameter is null' result = self.do_json_request('/3/Models.json/' + key, cmd='delete', timeout=timeoutSecs) if not ignoreMissingKey and 'f00b4r' in result: raise ValueError('Model key not found:'+ key) verboseprint(""delete_model result:"", dump_json(result)) return result"
570,def use_socket_fileobject(socket_fileobject): if socket_fileobject = socket_fileobject = socket_fileobject = socket_fileobject = socket_fileobject = socket_fileobject = socket_fileobject = socket_fileobject = socket_fileobject = socket_fileobject = socket_fileobject = socket_fileobject = socket_fileobject = socket_fileobject = socket_fileobject = socket_fileobject = socket_fileobject = socket_fileobject = socket_fileobject = socket_fileobject = socket_fileobject = socket_fileobject = socket_fileobject = socket_fileobject = socket_fileobject = socket_fileobject = socket_fileobject = socket_fileobject = socket_fileobject = socket_fileobject = socket_,"def setup(self): self.connection = self.request self.rfile = socket._fileobject(self.request, ""rb"", self.rbufsize) self.wfile = socket._fileobject(self.request, ""wb"", self.wbufsize)"
571,"def check_file(self, filename): if file exists, if file exists, if file exists, if file exists, if file exists, if file exists, if file exists, if file exists, if file exists, if file exists, if file exists, if file exists, if file exists, if file exists, if file exists, if file exists, if file exists, if file exists, if file exists, if file exists, if file exists, if filename, if filename, if filename, if filename, if filename, if filename, if filename, if filename, if filename, if filename, if filename, if filename, if filename, if filename, if filename, if filename, if filename, if filename, if filename, if filename, if filename, if filename, if filename, if filename, if filename, if filename, if filename, if filename, if filename, if filename, if filename, if filename, if filename, if filename, if filename, if filename, if","def check_file_output(filename, force): log.debug(""Checking file output"") if os.path.isfile(filename) and not force: if sys.stdin.isatty(): answer = console.ask(""File 0 already exists! Overwrite it? [y/N] "", filename) if answer.lower()!= ""y"": sys.exit() else: log.error(""File 0 already exists, use --force to overwrite it."".format(filename)) sys.exit() return FileOutput(filename)"
572,"def create_dag_run(run_id, run_id, execution_date, state, start_date, external_trigger, external_trigger, session, external_trigger, session, external_trigger, external_trigger, run_id, run_id, run_id, run_id, run_id, run_id, run_id, run_id, run_id, run_id, run_id, external_trigger, external_trigger, external_trigger, external_trigger, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, external_trigger, external_trigger, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session, session","def create_dagrun(self, run_id, state, execution_date, start_date=None, external_trigger=False, conf=None, session=None): return self.get_dag().create_dagrun(run_id=run_id, state=state, execution_date=execution_date, start_date=start_date, external_trigger=external_trigger, conf=conf, session=session)"
573,"def send_command(self, websocket, websocket, websocket, websocket, websocket, websocket, websocket, websocket, websocket, websocket, websocket, websocket, websocket, websocket, websocket, websocket, websocket, websocket, websocket, websocket)","def send(self, cmd): with self.ws_sendlock: self.ws.send(json.dumps(cmd))"
574,"def line_mod(self, line_ori, line_mod, line_mod, line_mod, line_mod, line_mod, line_mod, line_ori, line_mod, line_mod, line_mod, line_mod, line_mod, line_mod, line_ori, line_mod, line_mod, line_mod, line_mod, line_mod, line_mod, line_mod, line_mod, line_ori, line_mod, line_mod): line_mod, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_ori, line_","def log(self, line_mod, line_ori): if self.log_raw_input: self.log_write(line_ori) else: self.log_write(line_mod)"
575,def check_node_line_number(node_line_number): if not yet done: check if not yet done: check if not yet done: check if not yet done: check if not yet done: check if not yet done: check if not yet done: check if not yet done: check if not yet done: check if not yet done: check if not yet done: check if not yet done: check if not yet done: check,"def visit_default(self, node): if not node.is_statement: return if not node.root().pure_python: return prev_sibl = node.previous_sibling() if prev_sibl is not None: prev_line = prev_sibl.fromlineno else: if ( isinstance(node.parent, nodes.TryFinally) and node in node.parent.finalbody ): prev_line = node.parent.body[0].tolineno + 1 else: prev_line = node.parent.statement().fromlineno line = node.fromlineno assert line, node if prev_line == line and self._visited_lines.get(line)!= 2: self._check_multi_statement_line(node, line) return if line in self._visited_lines: return try: tolineno = node.blockstart_tolineno except AttributeError: tolineno = node.tolineno assert tolineno, node lines = [] for line in range(line, tolineno + 1): self._visited_lines[line] = 1 try: lines.append(self._lines[line].rstrip()) except KeyError: lines.append("""")"
576,"def AST_NODE(self, AST_NODE, AST_NODE, AST_NODE, AST_NODE, AST_NODE, AST_NODE, AST_NODE, AST_NODE, AST_NODE, AST_NODE, AST_NODE, AST_NODE, AST_NODE, AST_NODE, AST_NODE, AST_NODE, AST_NODE): AST_NODE = AST_NODE, AST_NODE, AST_NODE, AST_NODE, AST_NODE, AST_NODE, AST_NODE, AST_NODE, AST_NODE, AST_NODE, AST_NODE, AST_NODE)","def _try_to_py_ast(ctx: GeneratorContext, node: Try) -> GeneratedPyAST: assert node.op == NodeOp.TRY try_expr_name = genname(""try_expr"") body_ast = _synthetic_do_to_py_ast(ctx, node.body) catch_handlers = list( map(partial(__catch_to_py_ast, ctx, try_expr_name=try_expr_name), node.catches) ) finallys: List[ast.AST] = [] if node.finally_ is not None: finally_ast = _synthetic_do_to_py_ast(ctx, node.finally_) finallys.extend(map(statementize, finally_ast.dependencies)) finallys.append(statementize(finally_ast.node)) return GeneratedPyAST( node=ast.Name(id=try_expr_name, ctx=ast.Load()), dependencies=[ ast.Try( body=list( chain( body_ast.dependencies, [ ast.Assign( targets=[ast.Name(id=try_expr_name, ctx=ast.Store())], value=body_ast.node, ) ], ) ), handlers=catch_handlers, orelse=[], finalbody=finallys, ) ], )"
577,"def get_issue_data(self, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id) id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id,","def issue(self, issue_id): path = urijoin(""bugs"", str(issue_id)) url_issue = self.__get_url(path) raw_text = self.__send_request(url_issue) return raw_text"
578,"def remove_element(self, i>, i>, i>, i>, i>, i>, i>, i>, i>, i>, i>, i>, i>, i>, i>, i>, i>, i>, i>, i>, i>, i>, i>, i>, i>, i>)",def _merge_element_contents(el): parent = el.getparent() text = el.text or '' if el.tail: if not len(el): text += el.tail else: if el[-1].tail: el[-1].tail += el.tail else: el[-1].tail = el.tail index = parent.index(el) if text: if index == 0: previous = None else: previous = parent[index-1] if previous is None: if parent.text: parent.text += text else: parent.text = text else: if previous.tail: previous.tail += text else: previous.tail = text parent[index:index+1] = el.getchildren()
579,"def render_template(self, source, context): if source is not self.______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________","def render_template_string(source, **context): ctx = _app_ctx_stack.top ctx.app.update_template_context(context) return _render(ctx.app.jinja_env.from_string(source), context, ctx.app)"
580,def enumeration_class(enumeration_class): enumeration_class(enumeration_class): enumeration_class(enumeration_class): enumeration_class(enumeration_class): enumeration_class(enumeration_class)): enumeration_class(enumeration_class): enumeration_class(enumeration_class): enumeration_class(enumeration_class): enumeration_class(enumeration_class): enumeration_class(enumeration_class)): enumeration_class(enumeration_class): enumeration_class): enumeration_class(enumeration_class): enumeration_class): enumeration_class = enumeration_class = enumeration_class = enumeration_class = enumeration_class = enumeration_class = enumeration_class = enumeration_class = enumeration_class = enumeration_class = enumeration,"def is_bit_mask(enumeration, potential_mask): if not isinstance(potential_mask, six.integer_types): return False mask_enumerations = ( CryptographicUsageMask, ProtectionStorageMask, StorageStatusMask ) if enumeration not in mask_enumerations: return False mask = 0 for value in [e.value for e in enumeration]: if (value & potential_mask) == value: mask |= value if mask!= potential_mask: return False return True"
581,"def eval_error(self, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error): eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error, eval_error,","def _helpful_failure(method): @wraps(method) def wrapper(self, val): try: return method(self, val) except: exc_cls, inst, tb = sys.exc_info() if hasattr(inst, '_RERAISE'): _, expr, _, inner_val = Q.__debug_info__ Q.__debug_info__ = QDebug(self, expr, val, inner_val) raise if issubclass(exc_cls, KeyError): exc_cls = QKeyError prettyval = repr(val) if len(prettyval) > 150: prettyval = ""%s instance>"" % (type(val).__name__) msg = ""0nntEncountered when evaluating 12"".format( inst, prettyval, self) new_exc = exc_cls(msg) new_exc._RERAISE = True Q.__debug_info__ = QDebug(self, self, val, val) six.reraise(exc_cls, new_exc, tb) return wrapper"
582,"def update_inner_boundary(self, inner_boundary): if self.init(self.init(self.init)): self.init(self.init(self.init)) self.init(self.init) self.init(self.init) self.init(self.init) self.init(self.init) self.init(self.init(self.init) self.init(self.init(self.init(self.init(self.init(self.init(self.init) self.init) self.init(self.init) self.init) self.init(self.init) self.init(self.init) self.init(self.init) self.init(self.init) self.init(self.init) self.init(self.init) self.init) self.init(self.init) self.init(self.init) self.init(self.init) self.init(self.init) self.init(self.init) self.init(self.init) self.init(self.init) self","def update_internal_boundary_x_y (self, solution_array): nsd_ = self.nsd dtype = solution_array.dtype if nsd_!=len(self.in_lower_buffers) | nsd_!=len(self.out_lower_buffers): print(""Buffers for communicating with lower neighbors not ready"") return if nsd_!=len(self.in_upper_buffers) | nsd_!=len(self.out_upper_buffers): print(""Buffers for communicating with upper neighbors not ready"") return loc_nx = self.subd_hi_ix[0]-self.subd_lo_ix[0] loc_ny = self.subd_hi_ix[1]-self.subd_lo_ix[1] lower_x_neigh = self.lower_neighbors[0] upper_x_neigh = self.upper_neighbors[0] lower_y_neigh = self.lower_neighbors[1] upper_y_neigh = self.upper_neighbors[1] trackers = [] flags = dict(copy=False, track=False) if lower_x_neigh>-1: if self.slice_copy: self.out_lower_buffers[0] = ascontiguousarray(solution_array[1,:]) else: for i in xrange(0,loc_ny+1): self.out_lower_buffers[0][i] = solution_array[1,i] t = self.comm.west.send(self.out_lower_buffers[0], **flags) trackers.append(t) if upper_x_neigh>-1: msg = self.comm.east.recv(copy=False) self.in_upper_buffers[0] = frombuffer(msg,"
583,"def get_engineeringNotationNumber(self, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum,","def peng_power(snum): rsuffix = "" "" if snum[-1].isdigit() else snum[-1] return EngPower(suffix, _SUFFIX_POWER_DICT[suffix])"
584,"def add_new_oid(self, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid, new_oid","def add(self, new_oid): if self._oid_set[0]: oid_ptr = None if isinstance(new_oid, OID): oid_ptr = ffi.addressof(new_oid._oid) elif isinstance(new_oid, ffi.CData): if ffi.typeof(new_oid) == ffi.typeof('gss_OID_desc'): oid_ptr = ffi.addressof(new_oid) elif ffi.typeof(new_oid) == ffi.typeof('gss_OID'): oid_ptr = new_oid if oid_ptr is None: raise TypeError(""Expected a gssapi.oids.OID, got "" + str(type(new_oid))) minor_status = ffi.new('OM_uint32[1]') retval = C.gss_add_oid_set_member(minor_status, oid_ptr, self._oid_set) if GSS_ERROR(retval): raise _exception_for_status(retval, minor_status[0]) else: raise GSSException(""Cannot add a member to this OIDSet, its gss_OID_set is NULL!"")"
585,"def get_file_name(self, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name)","def stream(url, headers, stream_to=None, retry=True): bot.debug(""GET %s"" % url) if DISABLE_SSL_CHECK is True: bot.warning('Verify of certificates disabled! ::TESTING USE ONLY::') response = requests.get(url, headers=headers, verify=not DISABLE_SSL_CHECK, stream=True) if response.status_code in [401, 403]: headers = update_token(headers) return stream(url, headers, stream_to, retry=False) elif response.status_code == 200: content_size = None if 'Content-Length' in response.headers: progress = 0 content_size = int(response.headers['Content-Length']) bot.show_progress(progress,content_size,length=35) chunk_size = 1  20 with open(stream_to,'wb') as filey: for chunk in response.iter_content(chunk_size=chunk_size): filey.write(chunk) if content_size is not None: progress+=chunk_size bot.show_progress(iteration=progress, total=content_size, length=35, carriage_return=False) sys.stdout.write('n') return stream_to bot.error(""Problem with stream, response %s"" %(response.status_code)) sys.exit(1)"
586,"def train_covariance(mu, chol): tfd = tfp.distributions = tfp.distributions = tfp.distributions = tfp.distributions = tfp.distributions = tfp.distributions = tfp.distributions = tfp.distributions = tfp.distributions = tfp.distributions = tfp.distributions = tfp.distributions = tfp.distributions = tfp.distributions = tfp.distributions = tfp.distributions = tfp.distributions = tfp.distributions = tfp.distributions = tfp.distributions = tfp.distributions = tfp.distributions = tfp.distributions = tfp.distributions = tfp.distributions = tfp.distributions = tfp.distributions = tfp.distributions","def matrix_diag_transform(matrix, transform=None, name=None): with tf.name_scope(name or ""matrix_diag_transform""): matrix = tf.convert_to_tensor(value=matrix, name=""matrix"") if transform is None: return matrix diag = tf.linalg.diag_part(matrix) transformed_diag = transform(diag) transformed_mat = tf.linalg.set_diag(matrix, transformed_diag) return transformed_mat"
587,"def generate_data_object(self, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task, task)","def tasks(self, key_prefix=''): for row in self._tasks.get_range(): logger.debug(row) if not row[0].startswith(key_prefix): continue data = json.loads(row[1]['task_data']) data['task_key'] = row[0] yield data"
588,"def verify_signature(self, PKey key): if self.signature(self.signature): self.signature(self.signature) self.signature(self.signature) self.signature(self.signature) self.signature(self.signature) self.signature(self.signature) self.signature(self.signature) self.signature(self.signature) self.signature(self.signature) self.signature(self.signature) self.signature) self.signature(self.signature) self.signature(self.signature) self.signature) self.signature(self.signature) self.signature(self.signature) self.signature(self.signature) self.signature(self.signature) self.signature) self.signature(self.signature) self.signature) self.signature(self.signature) self.signature(self.signature) self.signature(self.signature) self.signature(self.signature) self.signature(self.signature) self.signature(self.signature) self.signature(self.signature) self.signature(self.signature) self.signature(self.signature) self","def verify(self, key): answer = _lib.NETSCAPE_SPKI_verify(self._spki, key._pkey) if answer = 0: _raise_current_error() return True"
589,"def initialize_app(self, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name) app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_","def initialize(self, argv=None): super(BaseParallelApplication, self).initialize(argv) self.to_work_dir() self.reinit_logging()"
590,"def check_s3url(self, s3url, str, rtype, str, s3url, str, s3url, str, rtype, str, s3url, str, s3url, str, s3url, str, s3url, str, s3url, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, s3url, str, str, str, str, s3url, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, s3url, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str,","def check_s3_url(self, s3url): bucket, key = S3Hook.parse_s3_url(s3url) if not self.s3_hook.check_for_bucket(bucket_name=bucket): raise AirflowException( ""The input S3 Bucket  does not exist "".format(bucket)) if key and not self.s3_hook.check_for_key(key=key, bucket_name=bucket) and not self.s3_hook.check_for_prefix( prefix=key, bucket_name=bucket, delimiter='/'): raise AirflowException(""The input S3 Key "" ""or Prefix  does not exist in the Bucket "".format(s3url, bucket)) return True"
591,"def get_command_regex_string(self, regex_string, completer_dict): self.get_command_regex_string(regex_string) self.get_command_regex_string(regex_string) self.get_command_regex_string(regex_string) self.get_command_regex_string(regex_string) self.get_command_regex_string(regex_string) self.get_command_regex_string) self.get_command_regex_string(regex_string) self.get_command_regex_string(regex_string) self.get_command_regex_string) self.get_command_regex_string(regex_string) self.get_string) self.get_string(regex_string) self.get_string) self.get_string(regex_string) self.get_string(regex_string) self.get_string(regex_string) self.get_string) self.get_string(regex_string) self.get_string(regex_string) self.get_string) self.get_string(regex_","def _cmdRegex(self, cmd_grp=None):, completers ])"
592,"def verify_redirect_uri(self, redirect_uri): if self.redirect_uri(self.redirect_uri): self.redirect_uri(self.redirect_uri): self.redirect_uri(self.redirect_uri)","def confirm_redirect_uri(self, client_id, code, redirect_uri, client, *args, **kwargs): client = client or self._clientgetter(client_id) log.debug('Confirm redirect uri for client %r and code %r.', client.client_id, code) grant = self._grantgetter(client_id=client.client_id, code=code) if not grant: log.debug('Grant not found.') return False if hasattr(grant, 'validate_redirect_uri'): return grant.validate_redirect_uri(redirect_uri) log.debug('Compare redirect uri for grant %r and %r.', grant.redirect_uri, redirect_uri) testing = 'OAUTHLIB_INSECURE_TRANSPORT' in os.environ if testing and redirect_uri is None: return True return grant.redirect_uri == redirect_uri"
593,"def load_public_suffix_database(public_suffix_database, public_suffix_database, public_suffix_database, public_suffix_database, public_suffix_database, public_suffix_database, public_suffix_database, public_suffix_database, public_suffix_database, public_suffix_database, public_suffix_database, public_suffix_database, public_suffix_database, public_suffix_database, public_suffix_database)","def load(self): if not PyFunceble.INTERN[""psl_db""]: PyFunceble.INTERN[""psl_db""] = Dict().from_json( File(self.destination).read() )"
594,"def set_dag_run_state(self, dag_id, execution_date): self.set_dag_run_state(dag_id) self.set_dag_run_state(dag_id) self.set_dag_run_state(dag_id) self.set_dag_run_state(dag_id) self.set_dag_run_state(dag_run_state) self.set_dag_run_state) self.set_date(dag_run_state) self.set_date(dag_run_state) self.set_date(dag_run_state) self.set_date(dag_run_state) self.set_date(dag_run_state) self.set_date(dag_run_state) self.set_date(dag_run_state) self.set_date(dag_run_state) self.set_date(dag_run_state) self.set_date(dag_run_state) self.set_date(dag_run_state) self.set_date(dag_run_","def _set_dag_run_state(dag_id, execution_date, state, session=None): DR = DagRun dr = session.query(DR).filter( DR.dag_id == dag_id, DR.execution_date == execution_date ).one() dr.state = state if state == State.RUNNING: dr.start_date = timezone.utcnow() dr.end_date = None else: dr.end_date = timezone.utcnow() session.merge(dr)"
595,"def parse_hpo_phenotypes(hpo_lines, iterable(str)): hpo_phenotypes(hpo_lines, iterable(str)) return hpo_phenotypes(hpo_lines, iterable(str))","def parse_hpo_diseases(hpo_lines): diseases =  LOG.info(""Parsing hpo diseases..."") for index, line in enumerate(hpo_lines): if index == 0: continue if not len(line) > 3: continue disease_info = parse_hpo_disease(line) if not disease_info: continue disease_nr = disease_info['disease_nr'] hgnc_symbol = disease_info['hgnc_symbol'] hpo_term = disease_info['hpo_term'] source = disease_info['source'] disease_id = ""0:1"".format(source, disease_nr) if disease_id not in diseases: diseases[disease_id] =  'disease_nr': disease_nr,'source': source, 'hgnc_symbols': set(), 'hpo_terms': set(),  if hgnc_symbol: diseases[disease_id]['hgnc_symbols'].add(hgnc_symbol) if hpo_term: diseases[disease_id]['hpo_terms'].add(hpo_term) LOG.info(""Parsing done."") return diseases"
596,"def split_text(self, locations): if self.split(text): self.split(text): self.split(text): self.split(text): self.split(text): self.split(text): self.split(text)","def split_with_locations(text, locations): start = 0 for pos, decision in enumerate(locations): if decision == SHOULD_SPLIT: if start!= pos: yield text[start:pos] start = pos if start!= len(text): yield text[start:]"
597,"def _unique_id(self, version=2): if self.unique_id(self.unique_id): self.unique_id = self.unique_id(self.unique_id)","def get_uuid(length=32, version=1): if version == 1: return uuid.uuid1().hex[:length] else: return uuid.uuid4().hex[:length]"
598,def close_sender_directly(self): self.close_sender_directly(self.close_sender_directly) self.close_sender_directly(self.close_sender_directly) self.close_sender_directly(self.close_sender_directly) self.close_sender_directly(self.close_sender_directly) self.close_sender_directly(self.close_sender_directly) self.close_sender_directly(self.close_sender_directly) self.close_sender_directly(self.close_sender_directly) self.close_sender_directly(self.close_sender_directly) self.close_sender_directly(self.close_sender_directly) self.close_sender_directly(self.close_sender_directly) self.close_sender_directly(self.close_sender_directly) self.close_sender_directly(self.close_sender_directly) self.close_sender_directly(self.close_sender_directly) self.close_sender_directly(self.close_sender_directly) self.close_sender,"async def close(self, exception=None): self.running = False if self.error: return if isinstance(exception, ServiceBusError): self.error = exception elif exception: self.error = ServiceBusError(str(exception)) else: self.error = ServiceBusError(""This message handler is now closed."") await self._handler.close_async()"
599,"def parse_args(self, args): self.parse(args) self.parse(args) self.parse(args) self.parse(args) self.parse(args) self.parse(args) self.parse(args) self.parse(args) self.parse(args) self.parse(args) self.parse(args) self.parse(args) self.parse(args) self.parse(args) self.parse(args) self.parse(args) self.parse(args) self.parse(args) self.parse(args) self.parse(args) self.parse(args) self.parse(args) self.parse(args) self.parse(args) self.parse(args) self.parse(args) self.parse(args) self.parse(args) self.parse(args) self.parse(args) self.parse(args) self.pars","def parse_args(): usage = ""Usage: create_concordance infile> [outfile>]"" description = ""Simple Concordance Generator"" argparser = argparse.ArgumentParser( usage=usage, description=description) argparser.add_argument( 'infile', type=argparse.FileType('r'), help=""File read in to create concordance"") argparser.add_argument( 'outfile', nargs='?', type=argparse.FileType('w'), default=sys.stdout, help=""File to write concordance to. "" ""Default is stdout"") argparser.add_argument( '--word', nargs=""?"", const=str, help=""Display a word in concordance"") args = argparser.parse_args() return args"
600,"def write_tag_data(self, tag_data): if tag_data is not self.get_tag(tag_data): self.get_tag(tag_data) self.get_tag(tag_data) self.get_tag(tag_data) self.get_tag(tag_data) self.get_tag(tag_data) self.get_tag(tag_data) self.get_tag(tag_data) self.get_tag(tag_data) self.get_tag(tag_data) self.get_tag(tag_data) self.get_tag(tag_data) self.get_tag(tag_data) self.get_tag(tag_data) self.get_tag(tag_data) self.get_tag(tag_data) self.get_tag(tag_data) self.get_tag(tag_data) self.get_tag(tag_data) self.get_tag(tag_data) self.get_tag(tag_data) self.get_tag(tag_data) self.get_tag(tag_data) self.get_tag(tag_data) self.get_tag(tag_data) self.get_tag(tag_data) self.get_tag(tag_data) self.get_tag_","def _inject(self, fileobj): fileobj.seek(0) page = OggPage(fileobj) while not page.packets[0].startswith(b""x81theora""): page = OggPage(fileobj) old_pages = [page] while not (old_pages[-1].complete or len(old_pages[-1].packets) > 1): page = OggPage(fileobj) if page.serial == old_pages[0].serial: old_pages.append(page) packets = OggPage.to_packets(old_pages, strict=False) packets[0] = b""x81theora"" + self.write(framing=False) new_pages = OggPage.from_packets(packets, old_pages[0].sequence) OggPage.replace(fileobj, old_pages, new_pages)"
601,"def Theano(self, variables): if self.____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________",def variables(self): result = self.inputs seen = set(i.name for i in result) for loss in self.losses: for v in loss.variables: if v.name not in seen: result.append(v) seen.add(v.name) return result
602,"def match_intervals_to(intervals_from, intervals_to, intervals_to, intervals_to, intervals_to, intervals_to, intervals_to, intervals_to, intervals_to, intervals_to, intervals_to, intervals_to, intervals_to, intervals_to, intervals_to, intervals_to, intervals_to, intervals_to, intervals_to, intervals_to, intervals_ma, interval_ma, interval_ma, interval_ma, interval_ma, interval_ma, interval_ma, interval_ma) interval_ma, interval_ma, interval_ma, interval_ma, interval_ma, interval_ma, interval_ma, interval_ma, interval_ma, interval_ma, interval_ma, interval_ma, interval_ma, interval_ma, interval_ma, interval_ma, interval_ma, interval_ma, interval_ma, interval_ma, interval_ma, interval_ma, interval_ma, interval_ma, interval_ma, interval_ma, interval_ma, interval_ma, interval_ma, interval_","def match_intervals(intervals_from, intervals_to, strict=True): if len(intervals_from) == 0 or len(intervals_to) == 0: raise ParameterError('Attempting to match empty interval list') valid_intervals(intervals_from) valid_intervals(intervals_to) try: return __match_intervals(intervals_from, intervals_to, strict=strict) except ParameterError: six.reraise(ParameterError, ParameterError('Unable to match intervals with strict='.format(strict)), sys.exc_info()[2])"
603,"def tensor_version(self, tensor): if self.get_tensor(tensor): self.get_tensor(tensor): self.get_tensor(tensor): self.get_tensor(tensor)","def _maybe_assert_valid_sample(self, samples): if not self.validate_args: return samples with tf.control_dependencies([ assert_util.assert_near( 1., tf.linalg.norm(tensor=samples, axis=-1), message='samples must be unit length'), assert_util.assert_equal( tf.shape(input=samples)[-1:], self.event_shape_tensor(), message=('samples must have innermost dimension matching that of''self.mean_direction')), ]): return tf.identity(samples)"
604,"def get_session_number(self, session_number): self.get_session_number(session_number) self.get_session_number(session_number) self.get_session_number(session_number) self.get_session_number(session_number) self.get_session_number(session_number) self.get_session_number(session_number) self.get_session_number(session_number) self.get_session_number) self.get_session_number(session_number) self.get_session_number(session_number) self.get_session_number(session_number) self.get_session_number) self.get_session_number(session_number) self.get_session_number) self.get_session_number(session_number) self.get_session_number(session_number) self.get_session_number(session_number) self.get_session_number(session_number) self.get_session_number(session_number) self.get_session_number(session_number) self.get_session_number(session_number) self.get","def new_session(self, conn=None):, (datetime.datetime.now(),)) self.session_number = cur.lastrowid"
605,"def load_file_object(self, file_object): self.load_file(file_object) self.load_file(file_object) self.load_file(file_object) self.load_file(file_object) self.load_file(file_object) self.load_file(file_object) self.load_file(file_object) self.load_file(file_object) self.load_file(file_object) self.load_file(file_object) self.load_file(file_object) self.load_file(file_object) self.load_file(file_object) self.load_file(file_object) self.load_file(file_object) self.load_file(file_object) self.load_file(file_object) self.load_file(file_object) self.load_file(file_object) self.load_file(file_object) self.load_file(file_object) self.load_file(file_object) self.load_file(file_object) self.load_file(file_object) self.load_file(file_object) self.load_file(file_object) self.load_file(file_object) self.load_file(file_object) self.load_","def fload(self): if hasattr(self, 'fobj') and self.fobj is not None: self.fobj.close() if hasattr(self.src, ""read""): self.fobj = self.src else: self.fobj = open(self.fname)"
606,"def plugin_key(self, plugin, key): if self.key = self.key = self.key = self.key = self.key = self.key = self.key","def get_plugin_option(self, plugin, key): if plugin in self.plugins: plugin = self.plugins[plugin] return plugin.get_option(key)"
607,"def iop_file_writer(file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name, file_name)","def _write_iop_to_file(self, iop, file_name): lg.info('Writing ::'+ file_name) f = open(file_name, 'w') for i in scipy.nditer(iop): f.write(str(i) + 'n')"
608,"def type(self, block, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements) sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-elements, sub-e","def infer_block(self, body, diagnostic=None): for e in body: e.infer_node = InferNode(parent=self.infer_node) e.infer_type(diagnostic=diagnostic)"
609,"def group_cols(self, group_cols, aggregations): group_cols = self.group_cols = self.group_cols = self.group_cols = self.group_cols = self.group_cols = self.group_cols = self.group_cols = self.group_cols = self.group_cols = self.group_cols","def groupby(df, *, group_cols: Union[str, List[str]], aggregations: Dict[str, Union[str, List[str]]]): df = df.groupby(group_cols, as_index=False).agg(aggregations) if df.columns.nlevels == 2: level_0 = df.columns.get_level_values(0) level_1 = df.columns.get_level_values(1) new_columns = [(f'x_y' if x else y) for (x, y) in zip(level_1, level_0)] df.columns = new_columns return df"
610,def receive_nok(self): self.get_nok(self.get_nok) self.get_nok(self.get_nok) self.get_nok(self.get_nok) self.get_nok(self.get_nok) self.get_nok(self.get_nok) self.get_nok(self.get_nok) self.get_nok(self.get_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_nok_,"def receive_nak_rebinding(self, pkt): logger.debug(""C3.1. Received NAK?, in RENEWING state."") if self.process_received_nak(pkt): logger.debug(""C3.1: T. Received NAK, in RENEWING state, "" ""raise INIT."") raise self.INIT()"
611,"def cycle_detector(self, sequence_a, start_state): if sequence_a is None: if sequence_a is None: if sequence_a is None: if sequence_a is None: if sequence_a is None: if sequence_a is None: if sequence_a is None: if sequence_a is None: if sequence_a is None: if sequence_a is None: if sequence_a is None: if sequence_a is None: if sequence_a is None: if sequence_a is None: sequence_a is None: sequence_a is None: sequence_a is None: sequence_a is None: sequence_a is None: sequence_a is None: sequence_a is None: sequence_a is None: sequence_a is None: sequence_a is None: sequence_a is None: sequence_a is None: sequence_a is None: sequence_a is None: sequence_a is None: sequence_a is None: sequence_a is None: sequence_a is None: sequence_a is None: sequence_a is None: sequence_a is None: sequence_a is None: sequence_a is None: sequence_a is None: sequence_a is None: sequence_a is None: sequence_a is None: sequence_a is None","def gosper(seqs, f=None, start=None, key=lambda x: x): tab = [] for c, value in enumerate(seqs[0], start=1): yield value try: e = tab.index(key(value)) raise CycleDetected( period=c - ((((c >> e) - 1) | 1)  e)) except ValueError: try: tab[(c  (c - 1)).bit_length() - 1] = key(value) except IndexError: tab.append(value)"
612,"def get_extra_options(session, prepped_request, extra_options): extra_options = extra_options.get_extra_options(session, prepped_request, prepped_request, extra_options, extra_options, extra_options, extra_options, extra_options, extra_options, extra_options, extra_options, extra_options)","def run_and_check(self, session, prepped_request, extra_options): extra_options = extra_options or  try: response = session.send( prepped_request, stream=extra_options.get(""stream"", False), verify=extra_options.get(""verify"", True), proxies=extra_options.get(""proxies"", ), cert=extra_options.get(""cert""), timeout=extra_options.get(""timeout""), allow_redirects=extra_options.get(""allow_redirects"", True)) if extra_options.get('check_response', True): self.check_response(response) return response except requests.exceptions.ConnectionError as ex: self.log.warn(str(ex) +'Tenacity will retry to execute the operation') raise ex"
613,def read_namespaced_token(namespaced_token): read_namespaced_token(namespaced_token): read_namespaced_token(namespaced_token),"def _read_namespaced( ctx: ReaderContext, allowed_suffix: Optional[str] = None ) -> Tuple[Optional[str], str]: ns: List[str] = [] name: List[str] = [] reader = ctx.reader has_ns = False while True: token = reader.peek() if token == ""/"": reader.next_token() if has_ns: raise SyntaxError(""Found '/'; expected word character"") elif len(name) == 0: name.append(""/"") else: if ""/"" in name: raise SyntaxError(""Found '/' after '/'"") has_ns = True ns = name name = [] elif ns_name_chars.match(token): reader.next_token() name.append(token) elif allowed_suffix is not None and token == allowed_suffix: reader.next_token() name.append(token) else: break ns_str = None if not has_ns else """".join(ns) name_str = """".join(name) if ns_str is None: if ""/"" in name_str and name_str!= ""/"": raise SyntaxError(""'/' character disallowed in names"") assert ns_str is None or len(ns_str) > 0 return ns_str, name_str"
614,"def generate_html_report(self, morfs, morfs, morfs, morfs, morfs, morfs, morfs, morfs, morfs, morfs, morfs)","def report(self, morfs): assert self.config.html_dir, ""must give a directory for html reporting"" self.status.read(self.config.html_dir) m = Hasher() m.update(self.config) these_settings = m.digest() if self.status.settings_hash()!= these_settings: self.status.reset() self.status.set_settings_hash(these_settings) if self.config.extra_css: self.extra_css = os.path.basename(self.config.extra_css) self.report_files(self.html_file, morfs, self.config.html_dir) if not self.files: raise CoverageException(""No data to report."") self.index_file() self.make_local_static_report_files() return self.totals.pc_covered"
615,"def create_tuple(self, str, tuple, str, tuple, str, tuple, str, tuple, str, tuple, str, tuple, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, tuple, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str,","def _best_order(g): if u.depth  0: return if not u.parents: result.append((u.name, u.parents)) u.depth = -1 return b = (u.name, []) result.append(b) u.depth = -1 d = 0 for v in sorted((g.get(p) for p in u.parents), key=lambda v: v.depth): n0 = len(result) _explore(v) n1 = len(result) b[1].extend(['_']*d + [v.name]) d = n1 - n0 - 1 g = _depth(g) result = [] for u in sorted(g.values(), key=lambda v: v.depth, reverse=True): _explore(u) return tuple(reversed(result))"
616,"def global_loop_iterations(self, dimension, dimension): if dimension is None: if dimension is None: if dimension is None: if dimension is None: if dimension is None: if dimension is None: if dimension is None: if dimension is None: if dimension is None: if dimension is None: if dimension is None: if dimension is None: if dimension is None: if dimension is None: if dimension is None: if dimension is None: if dimension is None: if dimension is None: if dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None: dimension is None","def iteration_length(self, dimension=None): total_length = 1 if dimension is not None: loops = [self._loop_stack[dimension]] else: loops = reversed(self._loop_stack) for var_name, start, end, incr in loops: length = end-start total_length = total_length*length return self.subs_consts(total_length)"
617,"def _resolve_request(self, method, path): self.resolve_request(method, method, path) self.resolve_request(path) self.resolve_request(path) self.resolve_request(path) self.resolve_request(path) self.resolve_request(path) self.resolve_request(path) self.resolve_request(path) self.resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve_resolve","def _resolve_non_literal_route(self, method, path): for route_dict in (self._wildcard, self._regex): if method in route_dict: for route in reversed(route_dict[method]): callback_data = route.match(path) if callback_data is not None: return callback_data return None"
618,"def set_focus(self.focus, self.separate, self.bkgsub, self.calibrated, self.calibrated, self.focus): self.focus = self.focus = self.focus = self.focus = self.focus = self.focus = self.focus = self.focus = self.focus = self.focus = self.focus = self.focus = self.focus = self.focus = self.focus = self.focus = self.focus = self.focus = self.focus = self.focus = self.focus","def set_focus(self, focus_stage=None, samples=None, subset=None): if samples is not None: subset = self.make_subset(samples) if subset is None: subset = 'All_Analyses' samples = self._get_samples(subset) if focus_stage is None: focus_stage = self.focus_stage else: self.focus_stage = focus_stage for s in samples: self.data[s].setfocus(focus_stage)"
619,"def load_data(filename, file_format, file_format, file_format, file_format, file_format, file_format, file_format, file_format, file_format, file_format, file_format, file_format, file_format, file_format, file_format, file_format, file_format, file_format, file_format, file_format, file_format, file_format, file_format, file_format, file_format): file_format, file_format, file_format, file_format, file_format, file_format, file_format, file_format, file_format, file_format, file_format, file_format, file_format)","def load(file, file_format=None, **kwargs): if file_format is None and is_str(file): file_format = file.split('.')[-1] if file_format not in file_handlers: raise TypeError('Unsupported format: '.format(file_format)) handler = file_handlers[file_format] if is_str(file): obj = handler.load_from_path(file, **kwargs) elif hasattr(file,'read'): obj = handler.load_from_fileobj(file, **kwargs) else: raise TypeError('""file"" must be a filepath str or a file-object') return obj"
620,"def compute_baese_information_criterion(self, criterion): def compute_baese_information_criterion(criterion): def compute_baese_information_criterion(criterion)","def compute_bic(self, D, means, labels, K, R): D = vq.whiten(D) Rn = D.shape[0] M = D.shape[1] if R == K: return 1 mle_var = 0 for k in range(len(means)): X = D[np.argwhere(labels == k)] X = X.reshape((X.shape[0], X.shape[-1])) for x in X: mle_var += distance.euclidean(x, means[k]) #print x, means[k], mle_var mle_var /= float(R - K) l_D = - Rn/2. * np.log(2*np.pi) - (Rn * M)/2. * np.log(mle_var) -  (Rn - K) / 2. + Rn * np.log(Rn) - Rn * np.log(R) p = (K-1) + M * K + mle_var #print ""BIC:"", l_D, p, R, K return l_D - p / 2. * np.log(R)"
621,"def sigproc_keyword(self, keyword, value_str, value_str, value_str, value_str, value_str, value_str, value_str, value_str, value_str, value_str, value_str, value_str, value_str, value_str, value_str, value_str, value_str, value_str, value_str, value_str)","def to_sigproc_keyword(keyword, value=None): keyword = bytes(keyword) if value is None: return np.int32(len(keyword)).tostring() + keyword else: dtype = header_keyword_types[keyword] dtype_to_type = b'l' : np.int32, b'str' : str, b'd' : np.float64, b'angle' : to_sigproc_angle value_dtype = dtype_to_type[dtype] if value_dtype is str: return np.int32(len(keyword)).tostring() + keyword + np.int32(len(value)).tostring() + value else: return np.int32(len(keyword)).tostring() + keyword + value_dtype(value).tostring()"
622,"def download_results(self, config): config = self.config(config) config = self.config(config) config = self.config(config) config = self.config(config) config = self.config(config)","def patch_config(self, config): if config.get(""monitoring""): if config[""monitoring""].get(""expvar""): self.expvar = config[""monitoring""][""expvar""].get(""enabled"") if config[""monitoring""][""expvar""].get(""port""): self.expvar_port = config[""monitoring""][""expvar""].get(""port"") else: self.expvar_port = self.DEFAULT_EXPVAR_PORT else: config[""monitoring""] =  ""expvar"":  ""enabled"": True,   self.expvar = True self.expvar_port = self.DEFAULT_EXPVAR_PORT for pool in config['pools']: if pool.get('ammo', ).get('file', ''): self.ammofile = pool['ammo']['file'] pool['ammo']['file'] = resource_manager.resource_filename( self.ammofile ) if not pool.get('result') or 'phout' not in pool.get('result', ).get('type', ''): logger.warning('Seems like pandora result file not specified... adding defaults') pool['result'] = dict( destination=self.DEFAULT_REPORT_FILE, type='phout', ) return config"
623,"def build_upstream_pipeline(process_descriptions, process_descriptions, task_pipeline, process_descriptions, process_descriptions, process_descriptions, process_descriptions, process_descriptions, process_descriptions, process_descriptions, process_descriptions, process_descriptions, process_descriptions, process_descriptions, process_descriptions, process_descriptions, process_descriptions, process_descriptions, task_pipeline, task_pipeline, task_pipeline, task_pipeline, task_pipeline, task_pipeline, task_pipeline, task_pipeline, task_pipeline, task_pipeline, task_pipeline, task_pipeline, task_pipeline, task_pipeline, task_pipeline, task_pipeline, task_pipeline, task_pipeline, task_pipeline, task_pipeline, task_pipeline, task_pipeline, task_pipeline, task_pipeline, task_pipeline, task_pipeline, task_pipeline, task_pipeline, task_pipeline, task_pipeline, task_pipeline, task_pipeline, task_pipeline, task_pipeline, task_pipeline, task_pipeline, task_","def build_downstream(self, process_descriptions, task, all_tasks, task_pipeline, count_forks, total_tasks, forks): if task in process_descriptions: if process_descriptions[task][2] is not None: if len(process_descriptions[task][2].split(""|"")) > 1: local_forks = process_descriptions[task][2].split(""|"") for local_fork in local_forks: if local_fork in total_tasks: count_forks += 1 task_pipeline.append(process_descriptions[task][2]) self.define_pipeline_string( process_descriptions, local_fork, False, True, count_forks, total_tasks, forks ) return task_pipeline else: if process_descriptions[task][2] in total_tasks: task_pipeline.append(process_descriptions[task][2].split(""|"")[0]) self.build_downstream( process_descriptions, process_descriptions[task][2].split(""|"")[0], all_tasks, task_pipeline, count_forks, total_tasks, forks ) return task_pipeline else: return task_pipeline"
624,def _loggabor(envelope): if envelope = envelope = envelope = envelope = envelope = envelope = envelope = envelope = envelope = envelope = envelope = envelope = envelope = envelope = envelope = envelope = envelope = envelope = envelope = envelope = envelope = envelope = envelope = envelope = envelop = envelop = envelop = envelop = envelop = envelop = envelop = envelop = envelop = envelop = envelop = envelop = envelop = envelop = envelop = envelop = envelop = envelop = envelop = envelop = envelop = envelop = envelop = envelop = envelop = envelop = envelop = envelop = envelop = envelop = envelop = envelop = envelop = envelop = envelop = envelop = envelop = envelop = envelop = envelop = envelop,"def loggabor(self, x_pos, y_pos, sf_0, B_sf, theta, B_theta, preprocess=True): env = np.multiply(self.band(sf_0, B_sf), self.orientation(theta, B_theta)) if not(x_pos==0.) and not(y_pos==0.): env = env.astype(np.complex128) * self.trans(x_pos*1., y_pos*1.) if preprocess : env *= self.f_mask env /= np.sqrt((np.abs(env)**2).mean()) env *= np.sqrt(2.) return env"
625,"def create_distribution(self, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params)","def params_size(event_size, num_components, name=None): with tf.compat.v1.name_scope( name, 'CategoricalMixtureOfOneHotCategorical_params_size', [event_size, num_components]): return MixtureSameFamily.params_size( num_components, OneHotCategorical.params_size(event_size, name=name), name=name)"
626,"def get_backing_adressbook(self, name): self.get_backing_adressbook(name) self.get_backing_adressbook(name) self.get_backing_adressbook(name) self.get_backing_adressbook(name) self.get_backing_adressbook(name) self.get_backing_adressbook(name) self.get_backing_adressbook(name) self.get_backing_adressbook(name) self.get_backing_adressbook(name) self.get_backing_adressbook(name) self.get_backing_adressbook(name) self.get_backing_adressbook(name) self.get_backing_adressbook(name) self.get_backing_adressbook(name) self.get_backing_adressbook(name) self.get_backing_adressbook(name) self.get_backing_adressbook(name) self.get_backing_adressbook(name) self.get_backing_adressbook(name) self.get_backing_adressbook(name) self.get_backing_adressbook(name) self.get_backing_adressbook","def get_abook(self, name): for abook in self._abooks: if abook.name == name: return abook"
627,def trim_cqt_responses(self): if self.get_cqt_responses(self.get_cqt_responses(self.get_cqt_response(self.get_cqt_response(self.get_cqt_response(self.get_cqt_response(self.get_cqt_response(self.get_cqt_response(self.get_cqt_response(self.get_cqt_response(self.get_cqt_response_cqt_response_cqt_response_cqt_response_cqt_response_cqt_response_cqt_response_cqt_response_cqt_response_response_response_response_response_response_response_response_response_re,"def __trim_stack(cqt_resp, n_bins): max_col = min(x.shape[1] for x in cqt_resp) cqt_resp = np.vstack([x[:, :max_col] for x in cqt_resp][::-1]) return np.ascontiguousarray(cqt_resp[-n_bins:].T).T"
628,"def get_alert(name_or_id, name_or_id, name_or_id, name_or_id, name_or_id, name_or_id, name_or_id, name_or_id, name_or_id, name_or_id, name_or_id, name_or_id, name_or_id, name_or_id): if name_or_id(name_or_id): if name_or_id, name_or_id): if name_or_id, name_or_id): if name_or_id, name_or_id, name_id, name_id, name_id, name_id, name_id, name_id, name_id, name_id, name_id, name_id, name_id, name_id, name_id, name_id, name_id, name_id, name_id, name_id, name_id, name_id, name_id, name_id, name_id, name_id, name_id,","def get(self, name_or_id): return [ tag for tag in self.list_tags() if name_or_id == tag.get('id') or name_or_id == tag.get('name') ]"
629,"def add_workspace_entry(self, workspace, config_file, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace, workspace,","def add(self, name, path): if not (os.path.exists(path)): raise ValueError(""Workspace path %s doesn't exists."" % path) if (self.exists(name)): raise ValueError(""Workspace %s already exists."" % name) self.config[""workspaces""][name] = ""path"": path, ""repositories"":  self.config.write()"
630,"def type_reference(self, Message): if self.type_reference(Message): self.type_reference(Message): self.type_reference(Message): self.type_reference(Message): self.type_reference(Message)","def setup_types(self): def load(t): from TelegramBotAPI.types.type import Type if isinstance(t, str): return Type._type(t) assert issubclass(t, Type) return t self.types = [load(t) for t in self.types]"
631,"def import_sitetree_module(self, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name): app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app","def import_app_sitetree_module(app): module_name = settings.APP_MODULE_NAME module = import_module(app) try: sub_module = import_module('%s.%s' % (app, module_name)) return sub_module except ImportError: if module_has_submodule(module, module_name): raise return None"
632,"def create_hdl_vector(self, hdl_vector, hdl_vector, hdl_vector, hdl_vector, hdl_vector, hdl_vector, hdl_vector, hdl_vector, hdl_vector, hdl_vector, hdl_vector, hdl_vector, hdl_vector, hdl_vector, hdl_vector, hdl_vector, hdl_vector, hdl_vector, hdl_vector, hdl_vector, hdl_vector, hdl_vector, hdl_vector, hdl_vector, hdl_vector, hdl_vector, hdl_vector, hdl_vector, hdl_vector, hdl_vector, hdl_vector, hdl_vector, hdl_vector, hdl_vector, hdl_vector, hdl_ve","def vec(val, width, signed=None): return Bits(width, signed, forceVector=True).fromPy(val)"
633,def r_[*]: if r'[*]d+(*): if r_[*]d+(*): if r_[*]d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*)d+(*),"def t_address(self, s): r'[*]d+' pos = self.pos self.add_token('ADDRESS', s) self.pos = pos + len(s)"
634,"def filename(self, filename, filename, filename): if filename is python_file(filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename)","def is_python_file(filename): if filename.endswith('.py'): return True try: with open_with_encoding( filename, None, limit_byte_check=MAX_PYTHON_FILE_DETECTION_BYTES) as f: text = f.read(MAX_PYTHON_FILE_DETECTION_BYTES) if not text: return False first_line = text.splitlines()[0] except (IOError, IndexError): return False if not PYTHON_SHEBANG_REGEX.match(first_line): return False return True"
635,"def create_bucket(self, create_bucket, create_bucket, create_bucket, create_bucket, create_bucket, create_bucket, create_bucket, create_bucket, create_bucket, create_bucket, create_bucket, create_bucket, create_bucket, create_bucket, create_bucket, create_bucket, create_bucket, create_bucket, create_bucket, create_bucket, create_bucket, create_bucket, create_bucket, create_bucket, create_bucket, create_bucket, create_bucket, create_bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket, bucket","def create_bucket(self, source): s3url = S3URL(source) message('Creating %s', source) if not self.opt.dry_run: resp = self.s3.create_bucket(Bucket=s3url.bucket) if resp['ResponseMetadata'][""HTTPStatusCode""] == 200: message('Done.') else: raise Failure('Unable to create bucket %s' % source)"
636,"def _convert_string(self, sympy, None): self.convert_string(self.convert_string) self.convert_string(self.convert_string) self.convert_string(self.convert_string) self.convert_string(self.convert_string) self.convert_string(self.convert_string) self.convert_string(self.convert_string) self.convert_string(self.convert_string) self.convert_string(self.convert_string) self.convert_string(self.convert_string) self.convert_string(self.convert_string) self.convert_string(self.convert_string) self.convert_string(self.convert_string) self.convert_string) self.convert_string(self.convert_string) self.convert_string) self.convert_string(self.convert_string) self.convert_string(self.convert_string) self.convert_string(self.convert_string) self.convert_string(self.convert_string) self.convert_string(self.convert_string) self.convert_string(self.convert_string","def string_to_sympy(cls, s): if isinstance(s, int): return sympy.Integer(s) elif isinstance(s, list): return tuple([cls.string_to_sympy(e) for e in s]) elif s is None: return None else: local_dict = c: symbol_pos_int(c) for c in s if c in string.ascii_letters preliminary_expr = parse_expr(s, local_dict=local_dict) local_dict.update( s.name: symbol_pos_int(s.name) for s in preliminary_expr.free_symbols) return parse_expr(s, local_dict=local_dict)"
637,"def link_from_receiver(self, receiver): link = self.get_receiver(receiver)","def request_receiver(self, pn_link): rl = ReceiverLink(self._connection, pn_link) self._links.add(rl) return rl"
638,"def set_like_special_variable(self, name, sep): self.set_like_special_variable(name, sep): self.set_like_special_variable(name, sep) self.set_like_special_variable(name) self.set_like_special_variable(name) self.set_like_special_variable(name) self.set_like_special_variable(name) self.set_like_special_variable(name) self.set_like_special_variable(name) self.set_like_special_variable(name) self.set_like_special_variable(name) self.set_like_special_variable(name) self.set_variable(name) self.set_variable(name) self.set_variable(name) self.set_variable(name) self.set_variable(name) self.set_variable(name) self.set_variable(name) self.set_variable(name) self.set_variable(name) self.set_variable(name) self.set_variable(name) self.set_variable(name) self.set_variable(name) self.set_variable(name) self.set_","def declare_set(self, name, sep=os.pathsep): self._declare_special(name, sep, SetVariable)"
639,"def parse_source(self, source): if self.get_source(source): self.get_source(source): self.get_source(source): self.get_source(source): self.get_source(source): self.get_source(source)","def _raw_parse(self): if self.exclude: self.excluded = self.lines_matching(self.exclude) indent = 0 exclude_indent = 0 excluding = False prev_toktype = token.INDENT first_line = None empty = True tokgen = generate_tokens(self.text) for toktype, ttext, (slineno, _), (elineno, _), ltext in tokgen: if self.show_tokens: print(""%10s %5s %-20r %r"" % ( tokenize.tok_name.get(toktype, toktype), nice_pair((slineno, elineno)), ttext, ltext )) if toktype == token.INDENT: indent += 1 elif toktype == token.DEDENT: indent -= 1 elif toktype == token.NAME and ttext == 'class': self.classdefs.add(slineno) elif toktype == token.OP and ttext == ':': if not excluding and elineno in self.excluded: exclude_indent = indent excluding = True elif toktype == token.STRING and prev_toktype == token.INDENT: self.docstrings.update(range(slineno, elineno+1)) elif toktype == token.NEWLINE: if first_line is not None and elineno!= first_line: rng = (first_line, elineno) for l in range(first_line, elineno+1): self.multiline[l] = rng first_line = None if ttext.strip() and toktype!= tokenize.COMMENT: empty = False if first_line is None: first_line ="
640,"def calculate_start(start, stop, stop): start = start, stop = stop, stop = stop, stop = stop, stop = stop, start = stop, stop = stop, stop = stop, start = stop, stop = stop, stop = stop, start = stop, stop = stop, stop = stop, start = stop, stop = stop, stop = stop, start = stop, stop = stop, stop = stop, start = stop, stop = stop, stop = stop, stop = stop, start = stop, stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = start = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = stop = start = stop = stop = stop = stop = stop = stop = start = stop = stop = stop = stop = stop = stop = stop = stop = start = stop = stop = stop = stop = stop = stop = stop = stop = stop = start = stop = stop = stop = stop = stop =","def _calculate(cls, start=None, end=None): if start and end: time_difference = int(end) - int(start) else: time_difference = PyFunceble.INTERN[""end""] - PyFunceble.INTERN[""start""] data = PyFunceble.OrderedDict() data[""days""] = str(time_difference // (24 * 60 * 60)).zfill(2) data[""hours""] = str((time_difference // (60 * 60)) % 24).zfill(2) data[""minutes""] = str((time_difference % 3600) // 60).zfill(2) data[""seconds""] = str(time_difference % 60).zfill(2) return data"
641,"def get_line_numbers(self, line_numbers): line_numbers = line_numbers.get_line_numbers(line_numbers) return line_numbers(line_numbers)","def branch_lines(self): exit_counts = self.parser.exit_counts() return [l1 for l1,count in iitems(exit_counts) if count > 1]"
642,"def add_task_record(self, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id)","def add_record(self, msg_id, rec): if self._records.has_key(msg_id): raise KeyError(""Already have msg_id %r""%(msg_id)) self._records[msg_id] = rec"
643,"def update_jobs(self.completed, self.dead, self.dead, self.completed, self.dead, self.dead, self.completed, self.dead, self.dead, self.completed, self.dead, self.completed, self.dead, self.dead, self.completed, self.dead, self.report, self.completed, self.dead)","def _update_status(self): srun, scomp, sdead = self._s_running, self._s_completed, self._s_dead running, completed, dead = self._running, self._completed, self._dead for num, job in enumerate(running): stat = job.stat_code if stat == srun: continue elif stat == scomp: completed.append(job) self._comp_report.append(job) running[num] = False elif stat == sdead: dead.append(job) self._dead_report.append(job) running[num] = False running[:] = filter(None, running)"
644,"def convert_character_offset(self, offset, offset, offset, offset, offset, offset, offset, offset, offset, offset, offset, offset, offset, offset, offset, offset, offset)","def char_tokens_to_char_offsets(si_tokens): for token in si_tokens: offset = token.offsets[OffsetType.CHARS] yield offset.first, offset.first + offset.length"
645,"def _display_data(self, data, metadata): self.display_data = self.display_data = self.display_data = self.display_data = self.display_data = self.display_data = self.display_data = self.display_data = self.display_data = self.display_data = self.display_data = self.display_data = self.display_data = self.display_data = self.display_data = self.display_data = self.display_data = self.display_data","def publish(self, source, data, metadata=None): if data.has_key('text/plain'): print(data['text/plain'], file=io.stdout)"
646,"def evaluate_python_expr(self, python_expr, python_expr, python_expr, python_expr, python_expr, python_expr, python_expr, python_expr, python_expr, python_expr, python_expr, python_expr, python_expr, python_expr, python_expr, python_expr, python_expr, python_expr, python_expr, python_expr, python_expr, python_expr, python_expr, python_expr, python_expr, python_expr, python_expr, python_expr, pyth","def ev(self, expr): with self.builtin_trap: return eval(expr, self.user_global_ns, self.user_ns)"
647,"def parse_play_details(self, details, struct): if not struct, return struct(self, details, struct)","def _clean_features(struct): struct = dict(struct) ptypes = ['isKickoff', 'isTimeout', 'isFieldGoal', 'isPunt', 'isKneel', 'isSpike', 'isXP', 'isTwoPoint', 'isPresnapPenalty', 'isPass', 'isRun'] for pt in ptypes: struct[pt] = struct[pt] if pd.notnull(struct.get(pt)) else False struct['callUpheld'] = struct.get('callUpheld') == 'upheld' struct['fgGood'] = struct.get('fgGood') == 'good' struct['isBlocked'] = struct.get('isBlocked') == 'blocked' struct['isComplete'] = struct.get('isComplete') == 'complete' struct['isFairCatch'] = struct.get('isFairCatch') == 'fair catch' struct['isMuffedCatch'] = pd.notnull(struct.get('isMuffedCatch')) struct['isNoPlay'] = ('(no play)' in struct['detail'] and 'penalty enforced in end zone' not in struct['detail'] if struct.get('detail') else False) struct['isOnside'] = struct.get('isOnside') == 'onside' struct['isSack'] = pd.notnull(struct.get('sackYds')) struct['isSafety'] = (struct.get('isSafety') == ', safety' or (struct.get('detail') and 'en"
648,def preprocess_list_of_variables(variables): preprocess_list_of_variables(variables) preprocess_list_of_variables(variables) preprocess_list_of_variables(variables) preprocess_list_ofvariables(variables) preprocess_list_ofvariables(variables) preprocess_list_ofvariables(variables) preprocess_list_ofvariables(variables) preprocess_list_ofvariables) preprocess_list_ofvariables_ofvariables_ofvariables_ofvariables_ofvariables_ofvariables_ofvariables_ofvariables_ofvariables_ofvariables_ofvariables_ofvariables_ofvariables_ofvariables_ofvariables_ofvariables_ofvariables_ofvariables_ofvariables_ofvariables_ofvariables_ofvariable,"def gene_variants(store, variants_query, page=1, per_page=50): variant_count = variants_query.count() skip_count = per_page * max(page - 1, 0) more_variants = True if variant_count > (skip_count + per_page) else False variant_res = variants_query.skip(skip_count).limit(per_page) my_institutes = list(inst['_id'] for inst in user_institutes(store, current_user)) variants = [] for variant_obj in variant_res: if variant_obj['institute'] not in my_institutes: LOG.warning(""Institute  not allowed."".format(variant_obj['institute'])) continue variant_case_obj = store.case(case_id=variant_obj['case_id']) if not variant_case_obj: continue case_display_name = variant_case_obj.get('display_name') variant_obj['case_display_name'] = case_display_name genome_build = variant_case_obj.get('genome_build', '37') if genome_build not in ['37','38']: genome_build = '37' variant_genes = variant_obj.get('genes') if variant_genes is not None: for gene_obj in variant_genes: if not gene_obj['hgnc_id']: continue if gene_obj.get('hgnc_symbol') is None or gene_obj.get('description') is None: hgnc_gene = store.hgnc_gene(gene_obj['hgnc_id'], build=genome_build) if not hgnc_"
649,"def _get_options(self, flags): if self.get_options(options): self.get_options(options): self.get_options(options): self.get_options(options): self.get_options(options): self.get_options(options): self.get_options(options): self.get_options(options): self.get_options(options)","def _options_to_args(**options): flags = [] for name in sorted(options): value = options[name] formatted_flag = '--%s' % name if len(name) > 1 else '-%s' % name formatted_flag = formatted_flag.replace('_', '-') accepts_no_arguments = formatted_flag in NO_ARGUMENT_OPTIONS if value is None or (value is False and accepts_no_arguments): continue flags.append(formatted_flag) if accepts_no_arguments: continue flags.append(six.text_type(value)) return flags"
650,def _reset_state(self): self.set_state(self.set_state) self.set_state(self.set_state) self.set_state(self.set_state) self.set_state(self.set_state) self.set_state(self.set_state) self.set_state(self.set_state) self.set_state(self.set_state) self.set_state(self.set_state) self.set_state(self.set_state) self.set_state) self.set_state) self.set_state(self.set_state) self.set_state(self.set_state) self.set_state(self.set_state) self.set_state) self.set_state) self.set_state(self.set_state) self.set_state) self.set_state(self.set_state) self.set_state) self.set_state(self.set_state) self.set_state(self.set_state) self.set_state(self.set_state) self.set_state(self.set_state) self.set_state(self.set_state) self.set_state(self.set_state) self.set_state(self.set_state,"def unset_state(self): x,y,z = self.obj.pos glTranslatef(-x,-y,-z)"
651,"def Graphviz_position_attribute(self, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position) Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graphviz_position, Graph","def _pos_changed(self, new): w, h = self.component.bounds self.component.position = [ new[0] - (w/2), new[1] - (h/2) ] self.component.request_redraw()"
652,def return_return(self): return self.return(self.return) return self.return(self.return),"def random_str(length=16, only_digits=False): choices = string.digits if not only_digits: choices += string.ascii_uppercase return ''.join(random.SystemRandom().choice(choices) for _ in range(length))"
653,"def check_identity(self, requirements, override_requirements, user_without_foo): self.check_identity(requirements) self.check_identity(requirements) self.check_identity(requirements) self.check_identity(requirements) self.check_identity(requirements) self.check_identity(requirements) self.check_identity(requirements) self.check_identity(requirements) self.check_identity) self.check_identity) self.check_identity(requirements) self.check_identity) self.check_identity(requirements) self.check_identity) self.check_identity(requirements) self.check_identity(requirements) self.check_identity) self.check_identity(requirements) self.check_identity(requirements) self.check_identity(requirements) self.check_identity(requirements) self.check_identity(requirements) self.check_identity(requirements) self.check_identity(requirements) self.check_identity(","def fulfill(self, requirements, identity=None): identity = identity or self._identity_loader() if self.additional.current: all_requirements = chain(iter(self.additional.current), requirements) else: all_requirements = iter(requirements) if self.overrides.current is not None: all_requirements = ( r for r in all_requirements if r not in self.overrides.current ) return all(_call_requirement(r, identity, request) for r in all_requirements)"
654,"def remove_variable(self, config, config): if config is not found, config = self.config(config, config, config, config)","def remove(backend, variable): print('[remove]') settings = read_client_secrets() prefixed = variable prefix = 'SREGISTRY_%s_' %backend.upper() if not variable.startswith(prefix): prefixed = '%s%s' %(prefix, variable) variable = variable.upper() bot.info(variable) if backend in settings: if variable in settings[backend]: del settings[backend][variable] if prefixed in settings[backend]: del settings[backend][prefixed] update_secrets(settings)"
655,"def decode_bytecode(self, bytecode): if bytecode = self.instruction(bytecode) if bytecode = self.instruction(bytecode)","def decode_bytecode(bytecode): bytecode_wnd = memoryview(bytecode) while bytecode_wnd: opcode_id = byte2int(bytecode_wnd[0]) opcode = OPCODE_MAP[opcode_id] if opcode.imm_struct is not None: offs, imm, _ = opcode.imm_struct.from_raw(None, bytecode_wnd[1:]) else: imm = None offs = 0 insn_len = 1 + offs yield Instruction(opcode, imm, insn_len) bytecode_wnd = bytecode_wnd[insn_len:]"
656,"def def callback_functions(self, pin_num, direction, direction): if None: def def callback_functions(pin_num, direction, direction, direction): def callback_functions(pin_num, direction, direction): def callback_functions(pin_num, direction, direction): def callback_functions(pin_num, direction, direction): def callback_functions(pin_num, direction): def callback_functions(pin_num, direction): def callback_functions(pin_num, direction): def callback_functions(pin_num, direction): def callback_functions(pin_num): def callback_functions(pin_num): def callback_functions(pin_num): def callback_functions(pin_num): def callback_functions(pin_num): def callback_functions(pin_num): def callback_functions(pin_num): def callback_functions(pin_num): def callback_functions(pin_num): def callback_functions(pin_num): def callback_functions(pin_num)","def deregister(self, pin_num=None, direction=None): to_delete = [] for i, function_map in enumerate(self.pin_function_maps): if ( pin_num == None or ( function_map.pin_num == pin_num and ( direction == None or function_map.direction == direction ) ) ): to_delete.append(i) for i in reversed(to_delete): del self.pin_function_maps[i]"
657,"def create_job_collection_id(self, cloud_service_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, cloud_service_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id","def create_job_collection(self, cloud_service_id, job_collection_id, plan=""Standard""): _validate_not_none('cloud_service_id', cloud_service_id) _validate_not_none('job_collection_id', job_collection_id) path = self._get_cloud_services_path( cloud_service_id, ""scheduler"", ""jobCollections"") path += '/' + _str(job_collection_id) body = _SchedulerManagementXmlSerializer.create_job_collection_to_xml( plan) return self._perform_put(path, body, as_async=True)"
658,def get_symbol(symbol): symbol(symbol): symbol(symbol) symbol(symbol) symbol(symbol) symbol(symbol) symbol(symbol) symbol(symbol) symbol(symbol) symbol(symbol) symbol(symbol) symbol(symbol) symbol(symbol) symbol(symbol) symbol(symbol) symbol(symbol) symbol) symbol(symbol) symbol(symbol) symbol(symbol) symbol(symbol) symbol(symbol) symbol(symbol) symbol(symbol) symbol(symbol) symbol(symbol) symbol(symbol) s,def symbol(self): if self.color == BLACK: return PIECE_SYMBOLS[self.piece_type].upper() else: return PIECE_SYMBOLS[self.piece_type]
659,"def _backwards_compatibility(self, backwards_compatibility, backwards_compatibility, backwards_compatibility, backwards_compatibility, backwards_compatibility, backwards_compatibility, backwards_compatibility, backwards_compatibility, backwards_compatibility, backwards_compatibility, backwards_compatibility, backwards_compatibility, backwards_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_compatibility_comp","def from_img(cls, *args, **kwargs): with warnings.catch_warnings(): warnings.simplefilter(""always"") w = ""from_img() is deprecated; please use from_image()"" warnings.warn(w) return cls.from_image(*args, **kwargs)"
660,def update_self.config_from_flag(self.config): self.config_from_flag(self.config_from_flag): self.config_from_flag(self.config_from_flag): self.config_from_flag(self.config_from_flag): self.config_from_flag(self.config_from_flag),"def _load_flag(self, cfg): if isinstance(cfg, (dict, Config)): for sec,c in cfg.iteritems(): self.config[sec].update(c) else: raise TypeError(""Invalid flag: %r"" % cfg)"
661,"def _backup_filename(self, backup_filename=None, backup_filename=None, backup_filename=None, backup_filename=None, backup_filename=None, backup_filename=None, backup_filename=None, backup_filename=None, backup_filename=None, backup_filename=None, backup_filename=None, backup_filename=None, backup_filename=None, backup_filename=None, backup_filename=None, backup_filename=None, backup_filename=None, backup_filename=None)","def f_backup(self, **kwargs): self._storage_service.store(pypetconstants.BACKUP, self, trajectory_name=self.v_name, **kwargs)"
662,"def recur_nodes(self, if): if recur_nodes = self.recur_nodes(self.recur_nodes) self.recur_nodes(self.recur_nodes) self.recur_nodes(self.recur_nodes) self.recur_nodes(self.recur_nodes) self.recur_nodes(self.recur_nodes) self.recur_nodes) self.recur_nodes(self.recur_nodes) self.recur_nodes(self.recur_nodes) self.recur_nodes(self.recur_nodes) self.recur_nodes) self.recur_nodes(self.recur_nodes) self.recur_nodes) self.recur_nodes(self.recur_nodes) self.recur_nodes(self.recur_nodes) self.recur_nodes) self.recur_nodes(self.recur_nodes) self.recur_nodes(self.recur_nodes) self.re","def __if_body_to_py_ast( ctx: GeneratorContext, node: Node, result_name: str ) -> GeneratedPyAST: if node.op == NodeOp.RECUR and ctx.recur_point.type == RecurType.LOOP: assert isinstance(node, Recur) return _recur_to_py_ast(ctx, node) elif node.op == NodeOp.DO: assert isinstance(node, Do) if_body = _synthetic_do_to_py_ast(ctx, node.assoc(is_body=True)) return GeneratedPyAST( node=ast.Assign( targets=[ast.Name(id=result_name, ctx=ast.Store())], value=if_body.node ), dependencies=list(map(statementize, if_body.dependencies)), ) else: py_ast = gen_py_ast(ctx, node) return GeneratedPyAST( node=ast.Assign( targets=[ast.Name(id=result_name, ctx=ast.Store())], value=py_ast.node ), dependencies=py_ast.dependencies, )"
663,"def U-turn(self, momentum): if self.state(state, momentum): self.state(state, momentum) self.state(state, momentum) self.state(state, momentum) self.state(state, momentum) self.state(state, momentum) self.state(state, momentum) self.state(state, momentum) self.state(state, momentum) self.state(state, momentum) self.state(state, momentum) self.state(state, momentum) self.state(state) self.state) self.state) self.state(state, momentum) self.state(state, momentum) self.state(state, momentum) self.state(state, momentum) self.state(state, momentum) self.state(state, momentum) self.state) self.state) self.state(state, momentum) self.state) self.state(state, momentum) self.state(state, momentum) self.state) self.state(state, momentum) self.state(state, momentum) self.state(state, momentum) self.state(state, momentum) self.state) self.state(state, momentum) self.state) self.state(state, momentum) self.state) self.state(state, momentum) self.state(state, momentum) self.state(state, momentum) self.state","def _has_no_u_turn(state_one, state_two, momentum): dot_product = sum([ tf.reduce_sum(input_tensor=(s1 - s2) * m) for s1, s2, m in zip(state_one, state_two, momentum) ]) return dot_product > 0"
664,"def notImplementedError(self, notImplementedError, notImplementedError, notImplementedError, notImplementedError, notImplementedError, notImplementedError, notImplementedError, notImplementedError)","def func(self, f, state): message = 'Tried to use unimplemented lens.' raise NotImplementedError(message.format(type(self)))"
665,"def start_server(self, conf_path, storage_factory, storage_cls): self.start_server(self.start_server) self.start_server(self.start_server) self.start_server(self.start_server) self.start_server(self.start_server) self.start_server(self.start_server) self.start_server(self.start_server) self.start_server(self.start_server) self.start_server(self.start_server) self.start_server(self.start_server) self.start_server(self.start_server) self.start_server(self.start_server) self.start_server(self.start_server) self.start_server(self.start_server) self.start_server(self.start_server) self.start_server(self.start_server) self.start_server(self.start_server) self.start_server(self.start_server) self.start_server(self.start_server) self.start_server(self.start_server) self.start_server(self.start_server) self.start_server(self.start_server) self.start_server(self.start_server) self.start_server(self.start_","def serve(conf_path, storage_factory=None): flawless.lib.config.init_config(conf_path) if not os.path.exists(config.data_dir_path): os.makedirs(config.data_dir_path) storage_factory = storage_factory or (lambda partition: DiskStorage(partition=partition)) root_logger = logging.getLogger() root_handler = logging.handlers.TimedRotatingFileHandler( filename=config.log_file, when='d', interval=1, backupCount=config.log_days_to_keep) root_logger.setLevel(getattr(logging, config.log_level)) root_logger.addHandler(root_handler) child_pid = os.fork() if child_pid == 0: handler = FlawlessWebServiceHandler(storage_factory=storage_factory) server = SimpleThreadedHTTPServer(('', config.http_port), SimpleRequestHTTPHandler) server.attach_service(handler) server.request_queue_size = 50 try: server.serve_forever() except (KeyboardInterrupt, SystemExit): server.server_close() else: handler = FlawlessThriftServiceHandler(storage_factory=storage_factory) processor = Flawless.Processor(handler) transport = TSocket.TServerSocket(port=config.port) tfactory = TTransport.TFramedTransportFactory() pfactory = TBinaryProtocol.TBinaryProtocolFactory() server = TServer.TThreadedServer(processor, transport, tfactory, pfactory) try: server.serve() except (KeyboardInterrupt, SystemExit): handler.errors_seen.sync() transport.close() os.kill(child_pid, signal"
666,"def symp_representation(self, paranthesis, addition, subtraction, multiplication, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction): symp_representation(symp_representation, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction, subtraction)","def conv_ast_to_sym(self, math_ast): if type(math_ast) is c_ast.ID: return symbol_pos_int(math_ast.name) elif type(math_ast) is c_ast.Constant: return sympy.Integer(math_ast.value) else: op =  '*': operator.mul, '+': operator.add, '-': operator.sub  return op[math_ast.op]( self.conv_ast_to_sym(math_ast.left), self.conv_ast_to_sym(math_ast.right))"
667,"def handle_dsync(self, dsync, dsync, dsync, dsync, dsync, dsync, dsync, dsync, dsync, dsync, dsync, dsync, dsync, dsync, dsync, dsync, dsync, dsync)","def dsync_handler(self, args): self.opt.recursive = True self.opt.sync_check = True self.opt.force = True self.validate('cmd|s3,local|s3,local', args) source = args[1] target = args[2] self.s3handler().dsync_files(source, target)"
668,"def read_object(input_stream, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version,","def read(self, input_stream, kmip_version=enums.KMIPVersion.KMIP_1_0): super(CheckResponsePayload, self).read( input_stream, kmip_version=kmip_version ) local_stream = utils.BytearrayStream(input_stream.read(self.length)) if self.is_tag_next(enums.Tags.UNIQUE_IDENTIFIER, local_stream): self._unique_identifier = primitives.TextString( tag=enums.Tags.UNIQUE_IDENTIFIER ) self._unique_identifier.read( local_stream, kmip_version=kmip_version ) if self.is_tag_next(enums.Tags.USAGE_LIMITS_COUNT, local_stream): self._usage_limits_count = primitives.LongInteger( tag=enums.Tags.USAGE_LIMITS_COUNT ) self._usage_limits_count.read( local_stream, kmip_version=kmip_version ) if self.is_tag_next(enums.Tags.CRYPTOGRAPHIC_USAGE_MASK, local_stream): self._cryptographic_usage_mask = primitives.Integer( tag=enums.Tags.CRYPTOGRAPHIC_USAGE_MASK ) self._cryptographic_usage_mask.read( local_stream, kmip_version=kmip_version ) if self.is_tag_next(enums.Tags.LEASE_TIME, local_stream): self._lease_time = primitives.Interval( tag=enums.Tags.LEASE_TIME ) self._lease_time.read( local_stream, kmip_version=kmip_version ) self.is_oversized(local_stream)"
669,"def cols(self, cols): cols = self.cols(cols) if self.cols(cols)",def num_cols(x): if tf.compat.dimension_value(x.shape[-1]) is not None: return tf.compat.dimension_value(x.shape[-1]) return tf.shape(input=x)[-1]
670,"def update_environemnt_variables_for_student_and_solution_environemnt(self, student_and_solution_environemnt): if self.___________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________","def set_env(state, **kwargs): stu_crnt = state.student_env.context sol_crnt = state.solution_env.context stu_new = stu_crnt.update(kwargs) sol_new = sol_crnt.update(kwargs) return state.to_child( student_env=stu_new, solution_env=sol_new, highlight=state.highlight )"
671,"def create_properties(self, name, force): if self.properties(self.properties): self.properties(self.properties) self.properties(self.properties) self.properties(self.properties) self.properties(self.properties) self.properties(self.properties) self.properties(self.properties) self.properties(self.properties) self.properties(self.properties) self.properties(self.properties) self.properties) self.properties) self.properties(self.properties) self.properties) self.properties(self.properties) self.properties) self.properties(self.properties) self.properties) self.properties) self.properties(self.properties) self.properties) self.properties(self.properties) self.properties) self.properties(self.properties) self.properties) self.properties(self.properties) self.properties) self.properties(self.properties) self.properties","def create_property(self, name:str) -> StatePropertyAccessor: if not name: raise TypeError('BotState.create_property(): BotState cannot be None or empty.') return BotStatePropertyAccessor(self, name)"
672,"def self.total_loss(self.softmax_W, self.softmax_b): self.total_loss = self.total_loss = self.total_loss = self.total_loss = self.total_loss = self.total_loss = self.total_loss = self.total_loss = self.total_loss = self.total_loss = self.total_loss = self.total_loss = self.total_loss = self.total_loss = self.total_loss = self.total_loss = self.total_loss = self.total_loss = self.total_loss = self.total_loss = self.total_loss = self.total_loss = self.total_loss = self.total_loss = self.total_loss = self.total_loss = self.total_loss = self.total_loss = self.total_loss = self.total_loss = self.total_loss = self.total_loss = self.total_loss = self.total_loss = self.","def _build_loss(self, lstm_outputs): batch_size = self.options['batch_size'] unroll_steps = self.options['unroll_steps'] n_tokens_vocab = self.options['n_tokens_vocab'] def _get_next_token_placeholders(suffix): name = 'next_token_id' + suffix id_placeholder = tf.placeholder(DTYPE_INT, shape=(batch_size, unroll_steps), name=name) return id_placeholder self.next_token_id = _get_next_token_placeholders('') if self.bidirectional: self.next_token_id_reverse = _get_next_token_placeholders( '_reverse') softmax_dim = self.options['lstm']['projection_dim'] if self.share_embedding_softmax: self.softmax_W = self.embedding_weights with tf.variable_scope('softmax'), tf.device('/cpu:0'): softmax_init = tf.random_normal_initializer(0.0, 1.0 / np.sqrt(softmax_dim)) if not self.share_embedding_softmax: self.softmax_W = tf.get_variable( 'W', [n_tokens_vocab, softmax_dim], dtype=DTYPE, initializer=softmax_init ) self.softmax_b = tf.get_variable( 'b', [n_tokens_vocab], dtype=DTYPE, initializer=tf.constant_initializer(0.0)) self.individual_train_losses = [] self.individual_eval"
673,"def list_pid_current_running_on_system(pid_current_running_on_system, pid_current_running_on_system, pid_current_running_on_system, pid_current_running_on_system, pid_current_running_on_system, pid_current_running_on_system, pid_current_running_on_system)",def get_pid_list(): pids = [int(x) for x in os.listdir('/proc') if x.isdigit()] return pids
674,"def gcm_notification(self, gcm_notification, gcm_notification, gcm_notification, gcm_notification, gcm_notification, gcm_notification, gcm_notification, gcm_notification, gcm_notification, gcm_notification, gcm_notification, gcm_notification, gcm_notification, gcm_notification, gcm_notification, gcm_notification, gcm_notification) gcm_notification, gcm_notification, gcm_notification, gcm_notification, gcm_notification, gcm_notification, gcm_notification, gcm_notification, gcm_notification, gcm_notification, gcm_notification, gcm_notification, gcm_notification, gcm_notification, gcm_notification, gcm_notification, gcm_notification, gcm_notification, gcm_notification","def gcm_send_message(registration_id, data, encoding='utf-8', **kwargs): messenger = GCMMessenger(registration_id, data, encoding=encoding, **kwargs) return messenger.send_plain()"
675,"def merge_hyperloglog(self, datasketch.HyperLogLog): if self.datasketch.HyperLogLog(self.datasketch.HyperLogLog) self.datasketch.HyperLogLog) self.datasketch.HyperLogLog(self.datasketch.HyperLogLog) self.datasketch.HyperLogLog(self.datasketch.HyperLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLogLog","def merge(self, other): if self.m!= other.m or self.p!= other.p: raise ValueError(""Cannot merge HyperLogLog with different precisions."") self.reg = np.maximum(self.reg, other.reg)"
676,"def GET(self, url, params): url = self.get(url, params)","def get(self, url, params=None, **kwargs): return self.call_api( ""GET"", url, params=params, **kwargs )"
677,def 1x1 convolution = 1x1 convolution = 1x1 convolution = 1x1 convolution = 1x1 convolution = 1x1 convolution = 1x1 convolution = 1x1 convolution = 1x1 convolution = 1x1 convolution = 1x1 convolution = 1x1 convolution = 1x1 convolution = 1x1 convolution = 1x1 convolution = 1x1 convolution = 1x1 convolution = 1x1 convolution = 1x1 convolution = 1x1 convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional convolutional,"def conv1x1(in_planes, out_planes, stride=1): return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
678,"def estimate_skyline(self, gen, gen, gen, gen, gen, gen, gen, gen, gen, gen, gen, gen, gen, gen, gen, gen, gen, gen, gen, gen, gen, gen, gen, gen, gen, gen, gen, gen, gen, gen, gen, gen, gen, gen, gen, gen, gen, gen, gen)","def skyline_empirical(self, gen=1.0, n_points = 20): mergers = self.tree_events[:,1]>0 merger_tvals = self.tree_events[mergers,0] nlineages = self.nbranches(merger_tvals-ttconf.TINY_NUMBER) expected_merger_density = nlineages*(nlineages-1)*0.5 nmergers = len(mergers) et = merger_tvals ev = 1.0/expected_merger_density if 2*n_points>len(expected_merger_density): n_points = len(ev)//4 avg = np.sum(ev)/np.abs(et[0]-et[-1]) dt = et[0]-et[-1] mid_points = np.concatenate(([et[0]-0.5*(et[1]-et[0])], 0.5*(et[1:] + et[:-1]), [et[-1]+0.5*(et[-1]-et[-2])])) self.Tc_inv = interp1d(mid_points[n_points:-n_points], [np.sum(ev[(et>=l)&(etu)])/(u-l+dt/nmergers) for u,l in zip(mid_points[:-2*n_points],mid_points[2*n_points:])]) return interp1d(self.date2dist.to_numdate(self.Tc_inv.x), gen/self.date2dist.clock_rate/self.Tc_inv.y)"
679,"def check_dir(self, location, receiver, receiver): if location is None: if location is None: if location is None: if location is None: if location is None: if location is None: if location is None: if location is None: if location is None: if location is None: if location is None: if location is None: if location is None: if location is None: if location is None: if location is None: if location is None: if location is None: if location is None: if location is None: if location is None: if location is None: if location is None: if location is None: if location is None: if location is None: if location is None: if location is None: if location is None: if location is None: if location is None: if location is None: if location is None: if location is None: if location is None: if receiver is None: if receiver is None: if receiver is None: if receiver is None: if receiver is None: if receiver is None: if receiver is None: if receiver is None: if receiver is None: if receiver is None: if receiver is None: if receiver is None: if","def messages(location, receiver): path = filepath.FilePath(location) def _check(path): messageFiles = path.globChildren('*') for message in messageFiles: if message.basename().endswith('.new'): continue receiver.message(message.getContent()) message.remove() return functools.partial(_check, path)"
680,"def decode(istream, istream, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version)","def read(self, istream, kmip_version=enums.KMIPVersion.KMIP_1_0): super(Certificate, self).read(istream, kmip_version=kmip_version) tstream = BytearrayStream(istream.read(self.length)) self.certificate_type = CertificateType() self.certificate_value = CertificateValue() self.certificate_type.read(tstream, kmip_version=kmip_version) self.certificate_value.read(tstream, kmip_version=kmip_version) self.is_oversized(tstream)"
681,def convert_japan_to_hiragana(self): self.convert_japan_to_hiragana(self.convert_japan_to_hiragana) self.convert_japan_to_hiragana(self.convert_japan_to_hiragana) self.convert_japan_to_hiragana(self.convert_japan_to_hiragana) self.convert_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_japan_,"def hiragana(ctx, app_id, sentence_file, json_flag, sentence, output_type, request_id): app_id = clean_app_id(app_id) sentence = clean_sentence(sentence, sentence_file) api = GoolabsAPI(app_id) ret = api.hiragana( sentence=sentence, output_type=output_type, request_id=request_id ) if json_flag: click.echo(format_json(api.response.json())) return click.echo(ret['converted'])"
682,"def k(self, m, m, k): if k is not found in m, return self.get_k(k, self.get_k)","def get(m, k, default=None): if isinstance(m, IAssociative): return m.entry(k, default=default) try: return m[k] except (KeyError, IndexError, TypeError) as e: logger.debug(""Ignored %s: %s"", type(e).__name__, e) return default"
683,"def QuantumChannel(self, QuantumChannel, QuantumChannel, QuantumChannel, QuantumChannel, QuantumChannel, QuantumChannel, QuantumChannel, QuantumChannel, QuantumChannel, QuantumChannel, QuantumChannel, QuantumChannel, QuantumChannel, QuantumChannel, QuantumChannel, QuantumChannel, QuantumChannel, QuantumChannel, QuantumChannel, QuantumChannel)","def is_unitary(self, atol=None, rtol=None): try: op = self.to_operator() return op.is_unitary(atol=atol, rtol=rtol) except QiskitError: return False"
684,"def _exac_lines(self, exac_lines): if self.exac_lines(self.exac_lines): self.exac_lines(self.exac_lines) return self.exac_lines(self.exac_lines)","def fetch_exac_constraint(): file_name = 'fordist_cleaned_exac_r03_march16_z_pli_rec_null_data.txt' url = ('ftp://ftp.broadinstitute.org/pub/ExAC_release/release0.3/functional_gene_constraint' '/0').format(file_name) LOG.info(""Fetching ExAC genes"") try: exac_lines = fetch_resource(url) except URLError as err: LOG.info(""Failed to fetch exac constraint scores file from ftp server"") LOG.info(""Try to fetch from google bucket..."") url = (""https://storage.googleapis.com/gnomad-public/legacy/exacv1_downloads/release0.3.1"" ""/manuscript_data/forweb_cleaned_exac_r03_march16_z_data_pLI.txt.gz"") exac_lines = fetch_resource(url) return exac_lines"
685,"def compare_list(self, inplace = 0): if self.sort_list(self.sort_list): self.sort_list(self.sort_list) self.sort_list(self.sort_list) self.sort_list(self.sort_list) self.sort_list(self.sort_list) self.sort_list(self.sort_list) self.sort_list(self.sort_list) self.sort_list(self.sort_list) self.sort_list(self.sort_list) self.sort_list(self.sort_list) self.sort_list(self.sort_list) self.sort_list(self.sort_list) self.sort_list(self.sort_list) self.sort_list(self.sort_list) self.sort_list(self.sort_list) self.sort_list(self.sort_list) self.sort_list(self.sort_list) self.sort_list(self.sort_list) self.sort_list(self.sort_list) self.sort_list(self.sort_list) self.sort_list(self.sort_list) self.sort_list(self.sort_list) self.sort_list(self.sort_list) self.sort_list","def sort_compare(lst1, lst2, inplace=1): if not inplace: lst1 = lst1[:] lst2 = lst2[:] lst1.sort(); lst2.sort() return lst1 == lst2"
686,"def dbqpi(self, pinot dbqpi, pinot dbqpi, pinot dbqpi, pinot dbqpi, pinot dbqpi, pinot dbqpi, pinot dbqpi, pinot dbqpi, pinot dbqpi, pinot dbqpi, pinot dbqpi, pinot dbqpi, pinot dbqpi)","def get_conn(self): conn = self.get_connection(self.pinot_broker_conn_id) pinot_broker_conn = connect( host=conn.host, port=conn.port, path=conn.extra_dejson.get('endpoint', '/pql'), scheme=conn.extra_dejson.get('schema', 'http') ) self.log.info('Get the connection to pinot''broker on host'.format(host=conn.host)) return pinot_broker_conn"
687,"def parse_csv_file(self, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_file, csv_","def parse_file(self, file_path, currency) -> List[PriceModel]: contents = self.load_file(file_path) prices = [] for line in contents: price = self.parse_line(line) assert isinstance(price, PriceModel) price.currency = currency prices.append(price) return prices"
688,"def split_locations(text, split_locations): split_locations = split_locations(text, split_locations, split_locations, split_locations, split_locations)","def protect_shorthand(text, split_locations): word_matches = list(re.finditer(word_with_period, text)) total_words = len(word_matches) for i, match in enumerate(word_matches): match_start = match.start() match_end = match.end() for char_pos in range(match_start, match_end): if split_locations[char_pos] == SHOULD_SPLIT and match_end - char_pos > 1: match_start = char_pos word = text[match_start:match_end] if not word.endswith('.'): if (not word[0].isdigit() and split_locations[match_start] == UNDECIDED): split_locations[match_start] = SHOULD_SPLIT continue period_pos = match_end - 1 word_is_in_abbr = word[:-1].lower() in ABBR is_abbr_like = ( word_is_in_abbr or one_letter_long_or_repeating.match(word[:-1]) is not None ) is_digit = False if is_abbr_like else word[:-1].isdigit() is_last_word = i == (total_words - 1) is_ending = is_last_word and (match_end == len(text) or text[match_end:].isspace()) is_not_ending = not is_ending abbreviation_and_not_end = ( len(word) > 1 and is_abbr_like and is_not_ending ) if abbreviation_and_not_end and ( (not is_last_word and word_matches[i+1].group(0)[0].islower()) or (not is_last_word and word_matches[i+1].group(0) in PUNCT_SYMBOLS) or word[0].isupper() or word_is_in_abbr or len(word) == 2)"
689,"def visualize_mdr_grid(self, mdr_instance, fig, mdr_instance, mdr_instance, mdr_instance, mdr_instance, mdr_instance, fig, fig, fig, fig, fig, fig, fig, fig, fig, fig, fig, fig, fig, fig, fig, fig, fig, fig, fig, fig, fig, fig, fig, fig)","def plot_mdr_grid(mdr_instance): fig, splots = plt.subplots(ncols=len(var1_levels), nrows=len(var2_levels), sharey=True, sharex=True) fig.set_figwidth(6) fig.set_figheight(6) for (var1, var2) in itertools.product(var1_levels, var2_levels): class_counts = mdr_instance.class_count_matrix[(var1, var2)] splot = splots[var2_levels.index(var2)][var1_levels.index(var1)] splot.set_yticks([]) splot.set_xticks([]) splot.set_ylim(0, max_count * 1.5) splot.set_xlim(-0.5, 1.5) if var2_levels.index(var2) == 0: splot.set_title('X1 = '.format(var1), fontsize=12) if var1_levels.index(var1) == 0: splot.set_ylabel('X2 = '.format(var2), fontsize=12) bars = splot.bar(left=range(class_counts.shape[0]), height=class_counts, width=0.5, color='black', align='center') bgcolor = 'lightgrey' if mdr_instance.feature_map[(var1, var2)] == 0 else 'darkgrey' splot.set_axis_bgcolor(bgcolor) for index, bar in enumerate(bars): splot.text(index, class_counts[index] + (max_count * 0.1), class_counts[index], ha='center') fig.tight_layout()"
690,"def get_engineeringNotationNumber(self, snum, snum, rtype, snum, snum, rtype, snum, snum, rtype, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum, snum,","def peng_float(snum): rsnum = snum.rstrip() power = _SUFFIX_POWER_DICT["" "" if snum[-1].isdigit() else snum[-1]] return float(snum if snum[-1].isdigit() else snum[:-1]) * power"
691,"def remove_cache_challenge(self, URL): self.cache_challenge(url) self.cache_challenge(url) self.cache_challenge(url) self.cache_challenge(url) self.cache_challenge(url) self.cache_challenge(url) self.cache_challenge(url) self.cache_challenge(url) self.cache_challenge(url) self.cache_challenge(url) self.cache_challenge(url) self.cache_challenge(url) self.cache_challenge(url) self.cache_challenge(url) self.cache_challenge(url) self.cache_challenge(url) self.cache_challenge(url) self.cache_challenge(url) self.cache_challenge(url) self.cache_challenge(url) self.cache_challenge(url) self.cache_challenge(url) self.cache_challenge(url) self.cache_chal",def remove_challenge_for_url(url): if not url: raise ValueError('URL cannot be empty') url = parse.urlparse(url) _lock.acquire() del _cache[url.netloc] _lock.release()
692,"def user_type(user_type, pin, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type): user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_type, user_","def login(self, pin, user_type=CKU_USER): pin1 = ckbytelist(pin) rv = self.lib.C_Login(self.session, user_type, pin1) if rv!= CKR_OK: raise PyKCS11Error(rv)"
693,"def check_if_element_is_in_the_database(self, database): if self.element_is_in_the_database(database): if self.element_is_in_the_database(database): self.element_is_in_the_database(database)","def is_in_database(self): if ( self._authorization() and PyFunceble.INTERN[""file_to_test""] in PyFunceble.INTERN[""whois_db""] and PyFunceble.INTERN[""to_test""] in PyFunceble.INTERN[""whois_db""][PyFunceble.INTERN[""file_to_test""]] ): return True return False"
694,"def plot(analytes, datarange, loglog, datarange, datarange, datarange, datarange, datarange, datarange, loglog, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, loglog, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, loglog, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange, datarange","def calibration_plot(self, analytes=None, datarange=True, loglog=False, ncol=3, srm_group=None, save=True): if isinstance(analytes, str): analytes = [analytes] if analytes is None: analytes = [a for a in self.analytes if self.internal_standard not in a] if srm_group is not None: srm_groups = int(g): t for g, t in self.stdtab.loc[:, ['group', 'gTime']].values try: gTime = srm_groups[srm_group] except KeyError: text = ('Invalid SRM group selection. Valid options are:n' +'Key: Time Centren' + 'n'.join([' :: :.1fs'.format(k, v) for k, v in srm_groups.items()])) print(text) else: gTime = None ncol = int(ncol) n = len(analytes) nrow = calc_nrow(n + 1, ncol) axes = [] if not datarange: fig = plt.figure(figsize=[4.1 * ncol, 3 * nrow]) else: fig = plt.figure(figsize=[4.7 * ncol, 3 * nrow]) self.get_focus() gs = mpl.gridspec.GridSpec(nrows=int(nrow), ncols=int(ncol), hspace=0.35, wspace=0.3) mdict = self.srm_mdict for g, a in zip(gs, analytes): if not datarange: ax = fig.add_axes(g.get_"
695,"def librosa_version(self, librosa, librosa, librosa, librosa, librosa, librosa, librosa, librosa, librosa, librosa, librosa, librosa, librosa, librosa, librosa, librosa, librosa, librosa, librosa, librosa, librosa, librosa, librosa, librosa, librosa, librosa, librosa, librosa, librosa, librosa, librosa, librosa)","def show_versions(): core_deps = ['audioread', 'numpy','scipy','sklearn', 'joblib', 'decorator','six','soundfile','resampy', 'numba'] extra_deps = ['numpydoc','sphinx','sphinx_rtd_theme','sphinxcontrib.versioning','sphinx-gallery', 'pytest', 'pytest-mpl', 'pytest-cov','matplotlib'] print('INSTALLED VERSIONS') print('------------------') print('python: n'.format(sys.version)) print('librosa: n'.format(version)) for dep in core_deps: print(': '.format(dep, __get_mod_version(dep))) print('') for dep in extra_deps: print(': '.format(dep, __get_mod_version(dep))) pass"
696,"def _align(self, cell, row): if _align(cell, row, row, row): if _align(cell): _align(cell, row, row, row): _align(cell, row, row, row): _align(cell, row, row): _align(cell, row, row, row): _align(cell, row, row): _align(cell, row, row, row, row, row, row, row, row, row, row, row, row, row, row, row, row, row, row): _align(cell): _align(cell): _align(cell): _align(cell): _align(cell): _align(cell): _align(cell): _align(cell): _align(cell): _align(cell): __align(cell): __align(cell): __align(cell): __align(cell): __align(cell): __align(cell): __align(cell): __align(","def _mediawiki_cell_attrs(row, colaligns): ""Prefix every cell in a row with an HTML alignment attribute."" alignment = ""left"": '', ""right"": 'align=""right""| ', ""center"": 'align=""center""| ', ""decimal"": 'align=""right""|'row2 = [alignment[a] + c for c, a in zip(row, colaligns)] return row2"
697,"def getinnerframes(self, filename): if self.getinnerframes(filename): self.getinnerframes(filename) self.getinnerframes(filename) self.getinnerframes(filename) self.getinnerframes(filename) self.getinnerframes(filename) self.getinnerframes(filename) self.getinnerframes(filename) self.getinnerframes(filename) self.getinnerframes(filename) self.getinnerframes(filename) self.getinnerframes(filename) self.getinnerframes(filename) self.getinnerframes(filename) self.getinnerframes(filename) self.getinnerframes(filename) self.getinnerframes(filename) self.getinnerframes(filename) self.getinnerframes(filename) self.getinnerframes(filename) self.getinnerframes(filename) self.getinnerframes(filename) self.getinnerframes(filename) self.getinnerframes(filename) self.getinnerframes(filename) self.getinnerframes","def fix_frame_records_filenames(records): fixed_records = [] for frame, filename, line_no, func_name, lines, index in records: better_fn = frame.f_globals.get('__file__', None) if isinstance(better_fn, str): filename = better_fn fixed_records.append((frame, filename, line_no, func_name, lines, index)) return fixed_records"
698,"def find_encoding_problem(encoding): if encoding is not working, encoding is not working, encoding is not working, encoding is not working, encoding is not working, encoding is not working, encoding is not working, encoding is not working, encoding is not working, encoding is not working, encoding is not working, encoding is not working, encoding is not working, encoding is not working, encoding is not working","def process_module(self, module): if module.file_encoding: encoding = module.file_encoding else: encoding = ""ascii"" with module.stream() as stream: for lineno, line in enumerate(stream): self._check_encoding(lineno + 1, line, encoding)"
699,"def check_function_name, docstring, arguments, redefinition, variable names, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, maxlocals, max","def visit_functiondef(self, node): self.stats[node.is_method() and ""method"" or ""function""] += 1 self._check_dangerous_default(node)"
700,"def decorate(self, some_configurable_function, whitelist='a default value'): def decorate(self, some_configurable_function, whitelist='a default value'): def decorate(self, some_configurable_function, whitelist='a default value'): def decorate(self, some_configurable_function, whitelist='a default value'): def decorate(self, some_configurable_function, whitelist='a default value')","def configurable(name_or_fn=None, module=None, whitelist=None, blacklist=None): decoration_target = None if callable(name_or_fn): decoration_target = name_or_fn name = None else: name = name_or_fn def perform_decoration(fn_or_cls): return _make_configurable(fn_or_cls, name, module, whitelist, blacklist) if decoration_target: return perform_decoration(decoration_target) return perform_decoration"
701,"def load_filter(self, filters): self.load_filter(filters) self.load_filter(filters) self.load_filter(filters) self.load_filter(filters) self.load_filter(filters) self.load_filter(filters) self.load_filter(filters) self.load_filter(filters) self.load_filter(filters) self.load_filter(filters) self.load_filter(filters) self.load_filter(filters) self.load_filter(filters) self.load_filter(filters) self.load_filter(filters) self.load_filter(filters) self.load_filter(filters) self.load_filter(filters) self.load_filter(filters) self.load_filter(filters) self.load_filter(filters) self.load_filter(filters) self.load_filter(filters) self.load_filter(filters) self.load_filter(filters) self.load_filter(filters) self.load_filter(filters) self.load_filter(filters) self.load_filter(filters) self.load_filter(filters) self.load_filter(filters) self.load_filter(filters)","def load_filters(): all_filters =  for m in JINJA_FILTERS: if hasattr(m, ""filters""): all_filters.update(m.filters()) return all_filters"
702,"def get_rollup_data(self, relay_name, metric_name, metric_name, metric_name, metric_name, metric_name, metric_name, metric_name, metric_name, metric_name, metric_name, metric_name, metric_name, metric_name, metric_name, metric_name, metric_name, metric_name, metric_name, metric_name, metric_name, metric_name, metric_name, metric_name, metric_name, metric_name, metric_name, metric_name, metric_name, metric_name, metric_name, metric_name, metric_name, metric_name)","def get_metrics_rollups_relay(self, name, relay_name, metric): response = self._perform_get( self._get_get_metrics_rollup_relay_path(name, relay_name, metric), None) return _MinidomXmlToObject.convert_response_to_feeds( response, partial( _ServiceBusManagementXmlSerializer.xml_to_metrics, object_type=MetricRollups ) )"
703,"def send_dependency_telemetry(self, commandName, data): self.get_dependency_telemetry(self.get_dependency_telemetry) self.get_dependency_telemetry(self.get_dependency_telemetry) self.get_dependency_telemetry(self.get_dependency_telemetry) self.get_dependency_telemetry(self.get_dependency_telemetry) self.get_dependency_telemetry(self.get_dependency_telemetry) self.get_dependency_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_telemetry_","def track_dependency(self, name:str, data:str, type:str=None, target:str=None, duration:int=None, success:bool=None, result_code:str=None, properties:Dict[str, object]=None, measurements:Dict[str, object]=None, dependency_id:str=None): self._client.track_dependency(name, data, type, target, duration, success, result_code, properties, measurements, dependency_id)"
704,def resolve_module_dependencies(module_dependencies): if module_dependencies(module_dependencies): if module_dependencies(module_dependencies): if module_dependencies(module_dependencies): if module_dependencies(module_dependencies): if module_dependencies(module_dependencies): if module_dependencies(module_dependencies): if module_dependencies(module_dependencies),"def visit_import(self, node): context_file = node.root().file for name in node.names: relative = modutils.is_relative(name[0], context_file) self._imported_module(node, name[0], relative)"
705,"def connect_to_ddp_handler(self, connect_to_ddp_handler, connect_to_ddp_handler, connect_to_ddp_handler, connect_to_ddp_handler, connect_to_ddp_handler, connect_to_ddp_handler, connect_to_ddp_handler)","def recv_connect(self, version=None, support=None, session=None): del session if self.connection is not None: raise MeteorError( 400, 'Session already established.', self.connection.connection_id, ) elif None in (version, support) or version not in self.versions: self.reply('failed', version=self.versions[0]) elif version not in support: raise MeteorError(400, 'Client version/support mismatch.') else: from dddp.models import Connection cur = connection.cursor() cur.execute('SELECT pg_backend_pid()') (backend_pid,) = cur.fetchone() this.version = version this.support = support self.connection = Connection.objects.create( server_addr='%d:%s' % ( backend_pid, self.ws.handler.socket.getsockname(), ), remote_addr=self.remote_addr, version=version, ) self.pgworker.connections[self.connection.pk] = self atexit.register(self.on_close, 'Shutting down.') self.reply('connected', session=self.connection.connection_id)"
706,"def bind(self, username, password): self.bind(self, password) self.bind(self.bind, password) self.bind(self.bind, password) self.bind(self.bind, password) self.bind(self.bind, password) self.bind(self.bind, password) self.bind(self.bind, password) self.bind(self.bind, password) self.bind(self.bind, password) self.bind(self.bind, password) self.bind(self.bind, password) self.bind(self.bind, password) self.bind(self.bind) self.bind(self.bind) self.bind(self.bind) self.bind(self.bind) self.bind(self.bind) self.bind(self.bind) self.bind(self.bind(self.bind) self.bind(self.bind) self.bind(self.bind) self.bind(self.bind(self.bind) self.bind(self.bind) self.bind(self.bind) self.bind(self.bind(self.bind) self.bind(self.bind(self.bind) self.bind(self.bind) self.bind(self.bind(self.bind) self.bind(self.bind) self.bind(self.bind) self.bind(self","def authenticate(self, username, password): if self.config.get('LDAP_BIND_DIRECT_CREDENTIALS'): result = self.authenticate_direct_credentials(username, password) elif not self.config.get('LDAP_ALWAYS_SEARCH_BIND') and  self.config.get('LDAP_USER_RDN_ATTR') ==  self.config.get('LDAP_USER_LOGIN_ATTR'): result = self.authenticate_direct_bind(username, password) else: result = self.authenticate_search_bind(username, password) return result"
707,"def _backup_file(self, filename): if self.backup_file(filename): self.backup_file(filename) self.backup_file(filename) self.backup_file(filename) self.backup_file(filename) self.backup_file(filename) self.backup_file(filename) self.backup_file(filename) self.backup_file(filename) self.backup_file(filename) self.backup_file) self.backup_file(filename) self.backup_file(filename) self.backup_file(filename) self.backup_file(filename) self.backup_file(filename) self.backup_file(filename) self.backup_file(filename) self.backup_file(filename) self.backup_file(filename) self.backup_file(filename) self.backup_file(filename) self.backup_file(filename) self.backup_file(filename) self.backup_file(filename) self.backup_file(filename) self.backup_file(filename) self.backup_file(filename) self.backup","def _file_in_patch(self, filename, patch, ignore): file = self.quilt_pc + File(os.path.join(patch.get_name(), filename)) if file.exists(): if ignore: return True else: raise QuiltError(""File %s is already in patch %s"" % (filename, patch.get_name())) return False"
708,"def point_record(self, point_record, point_record, point_record, point_record, point_record, point_record, point_record, point_record, point_record, point_record, point_record, point_record, point_record, point_record, point_record, point_record, point_record, point_record, point_record, point_record, point_record, point_record, point_record, point_record, point_record, point_record, point_record, point_record, point_record, point_record, point_record, point_record, point_record)","def from_stream(cls, stream, point_format, count): points_dtype = point_format.dtype point_data_buffer = bytearray(stream.read(count * points_dtype.itemsize)) try: data = np.frombuffer(point_data_buffer, dtype=points_dtype, count=count) except ValueError: expected_bytes_len = count * points_dtype.itemsize if len(point_data_buffer) % points_dtype.itemsize!= 0: missing_bytes_len = expected_bytes_len - len(point_data_buffer) raise_not_enough_bytes_error( expected_bytes_len, missing_bytes_len, len(point_data_buffer), points_dtype, ) else: actual_count = len(point_data_buffer) // points_dtype.itemsize logger.critical( ""Expected  points, there are  ( missing)"".format( count, actual_count, count - actual_count ) ) data = np.frombuffer( point_data_buffer, dtype=points_dtype, count=actual_count ) return cls(data, point_format)"
709,"def _validate_version(self, version=0, version=0, version=0, version=0, version=0, version=0, version=0, version=0, version=0, version=0, version=0, version=0, version=0, version=0, version=0, version=0)","def validate_version(): import leicacam version_string = leicacam.__version__ versions = version_string.split('.', 3) try: for ver in versions: int(ver) except ValueError: print( 'Only integers are allowed in release version,''please adjust current version '.format(version_string)) return None return version_string"
710,"def path_to_zip(path,.zip,.zip,.zip,.zip,.zip,.zip,.zip,.zip,.zip,.zip,.zip,.zip,.zip,.zip,.zip,.zip,.zip,.zip,.zip,.zip,.zip,.zip,.zip)","def correct_maybe_zipped(fileloc): _, archive, filename = re.search( r'((.*.zip))?(.*)'.format(re.escape(os.sep)), fileloc).groups() if archive and zipfile.is_zipfile(archive): return archive else: return fileloc"
711,"def change_container(self, drawing_container, drawing_container, drawing_container, drawing_container, drawing_container, drawing_container, drawing_container, drawing_container)","def _drawing_changed(self, old, new): if old is not None: self.component.remove( old ) if new is not None: self.component.add( new ) w, h = self.component.bounds self.component.position = [ self.pos[0] - (w/2), self.pos[1] - (h/2) ] self.component.request_redraw()"
712,"def endpoint_id(self, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id) endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, endpoint_id, path_id, path_id, path_id, path_id, path_id,","def get_endpoint_path(self, endpoint_id): config = os.path.expanduser(""/.globusonline/lta/config-paths"") if not os.path.exists(config): bot.error('%s not found for a local Globus endpoint.') sys.exit(1) path = None config = [x.split(',')[0] for x in read_file(config)] for path in config: if os.path.exists(path): break if path is None: bot.error('No path was found for a local Globus endpoint.') sys.exit(1) return path"
713,"def df(self, df.length_original(), df.length_unfiltered(), df.length_original(), df.length_unfiltered(), df.length_original(), df.length_unfiltered(), df.length_original(), df.length_unfiltered(), df.length_unfiltered(), df.length_original(), df.length_unfiltered(), df.length_original(), df.length_original(), df.length_unfiltered(), df.length_unfiltered(), df.length_original(), df.length_unfiltered(), df.length_original(), df.length_unfiltered(), df.length_original(), df.length_original(), df.length_original(), df.length_original(), df.length_original(), df.length_original(), df.length_original(), df.length_original(), df.length_original(), df.length_original(), df.length_original(), df.length_original(), d","def extract(self): trimmed = self.trim() if trimmed.filtered: indices = trimmed._filtered_range_to_unfiltered_indices(0, len(trimmed)) return trimmed.take(indices) else: return trimmed"
714,"def grouper('abcdefg', fillvalue='x'): grouper('abcdefg', fillvalue='x'): grouper('abcdefg', fillvalue='x'): grouper('abcdefg', fillvalue='x'): grouper('abcdefg', fillvalue='x'): grouper('abcdefg'): grouper('abcdefg'): grouper('abcdefg'): grouper('abcdefg'): grouper('abcdefg'): grouper('abcdefg'): grouper('abcdefg'): grouper('abcdefg'): grouper('abcdefg'): grouper('abcdefg'): grouper('abcdefg'): grouper('abcdefg'): grouper('abcdefg'): grouper('abcdefg'): grouper('abcdefg'): grouper","def grouper(iterable, n, fillvalue=None): return list(zip_longest(*[iter(iterable)] * n, fillvalue=fillvalue))"
715,"def load_mnist_digits(self, MNIST_digits): self.load_mnist_digits(self.load_mnist_digits) self.load_mnist_digits(self.load_mnist_digits) self.load_mnist_digits(self.load_mnist_digits) self.load_mnist_digits(self.load_mnist_digits) self.load_mnist_digits(self.load_mnist_digits) self.load_mnist_digits) self.load_mnist_digits_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_data_","def load_mnist(flatten=True, labels=False): fn = find('mnist.pkl.gz', 'http://deeplearning.net/data/mnist/mnist.pkl.gz') h = gzip.open(fn, 'rb') if sys.version_info  (3, ): (timg, tlab), (vimg, vlab), (simg, slab) = pickle.load(h) else: (timg, tlab), (vimg, vlab), (simg, slab) = pickle.load(h, encoding='bytes') h.close() if not flatten: timg = timg.reshape((-1, 28, 28, 1)) vimg = vimg.reshape((-1, 28, 28, 1)) simg = simg.reshape((-1, 28, 28, 1)) if labels: return ((timg, tlab.astype('i')), (vimg, vlab.astype('i')), (simg, slab.astype('i'))) return (timg, ), (vimg, ), (simg, )"
716,"def standardize_op_scope(self, scope): self.get_op_scope(self.get_op_scope) self.get_op_scope(self.get_op_scope) self.get_op_scope(self.get_op_scope) self.get_op_scope(self.get_op_scope) self.get_op_scope(self.get_op_scope) self.get_op_scope) self.get_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_op_o","def _name_scope(self, name=None, default_name=None, values=None): with tf.compat.v1.name_scope(self.name): with tf.compat.v1.name_scope( name, default_name, values=values or []) as scope: yield scope"
717,"def serialize_function(self, function_type, serial, function_type): serialize_function(self, function_type): serialize_function(self, function_type): serialize_function(self, function_type): serialize_function(self, function_type): serialize_function(self, function_type): serialize_function(self, function_type): serialize_function(self, function_type): serialize_function(self, self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self)","def deserialize_function(serial, function_type): if function_type == 'function': function = tf.keras.utils.deserialize_keras_object(serial) elif function_type == 'lambda': function = generic_utils.func_load(serial) else: raise TypeError('Unknown function type:', function_type) return function"
718,"def update_operator(self, operator, instruction): if self.apply(self.apply_operator): self.apply(self.apply_operator) self.apply(self.apply_operator) self.apply(self.apply_operator) self.apply(self.apply_operator) self.apply(self.apply_operator) self.apply(self.apply_operator) self.apply_operator) self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append(self.append","def _append_instruction(self, obj, qargs=None): if isinstance(obj, Instruction): mat = None if hasattr(obj, 'to_matrix'): try: mat = obj.to_matrix() except QiskitError: pass if mat is not None: op = self.compose(mat, qargs=qargs) self._data = op.data else: if obj.definition is None: raise QiskitError('Cannot apply Instruction: '.format(obj.name)) for instr, qregs, cregs in obj.definition: if cregs: raise QiskitError( 'Cannot apply instruction with classical registers: '.format( instr.name)) new_qargs = [tup[1] for tup in qregs] self._append_instruction(instr, qargs=new_qargs) else: raise QiskitError('Input is not an instruction.')"
719,"def describe_trait(self, trait): self.trait(trait): self.trait(trait) self.trait(trait) self.trait(trait) self.trait(trait) self.trait(trait) self.trait(trait) self.trait(trait) self.trait(trait) self.trait(trait) self.trait(trait) self.trait(trait) self.trait(trait)","def info(self): if isinstance(self.klass, basestring): klass = self.klass else: klass = self.klass.__name__ result = 'a subclass of'+ klass if self._allow_none: return result +'or None' return result"
720,"def ERROR_State(ERROR_State, ERROR_State, ERROR_State, ERROR_State, ERROR_State, ERROR_State, ERROR_State, ERROR_State, ERROR_State, ERROR_State, ERROR_State, ERROR_State, ERROR_State)","def ERROR(self): logger.debug('In state: ERROR') self.current_state = STATE_ERROR if self.script is not None: self.script.script_init(self.client.lease, self.current_state) self.script.script_go() set_net(self.client.lease) raise self.INIT()"
721,"def verbose(self, verbose, verbose, verbose, verbose, verbose, verbose, verbose, verbose, verbose, verbose, verbose, verbose, verbose, verbose, verbose, verbose, verbose, verbose, verbose, verbose): verbose, verbose, verbose, verbose, verbose, verbose, verbose, verbose, verbose, verbose)","def mnemonic(self, index, verbose=False): if index16: return ['last', '2last', '3last', '4last', 'last-1', 'last+1', 'last-2', 'last+2', 'last-3', 'last+3', '2last-1', '2last+1', '2last-2', '2last+2', '2last-3', '2last+3' ][index] if index16+self.NDIRECT: return str(index-16) #construct strings like ""1xx01-15"" index -= self.NDIRECT+16 hcode = index >> self.NPOSTFIX lcode = index & (1self.NPOSTFIX)-1 if self.NPOSTFIX: formatString = '1012:03b4:+d' else: formatString = '1014:+d' return formatString.format( hcode&1, 'x'*(2+hcode>>1) if hcode13 or verbose else '[*x]'.format(2+hcode>>1), lcode, self.NPOSTFIX, self.NDIRECT+1-(4self.NPOSTFIX))"
722,"def create(self, bucket_name, region_name, region_name, region_name, region_name, region_name, region_name, region_name, region_name, region_name, region_name, region_name, region_name, region_name, region_name, region_name, region_name, region_name, region_name, region_name, region_name, region_name, region_name, region_name, region_name, region_name, region_name, bucket_name, region_name, region_name, region_name, region_name, region_name, region_name, region_name, region_name, region_name, region_name, region_name, region_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket_name, bucket","def create_bucket(self, bucket_name, region_name=None): s3_conn = self.get_conn() if not region_name: region_name = s3_conn.meta.region_name if region_name == 'us-east-1': self.get_conn().create_bucket(Bucket=bucket_name) else: self.get_conn().create_bucket(Bucket=bucket_name, CreateBucketConfiguration= 'LocationConstraint': region_name )"
723,"def delete_record(self, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name): record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record_name, record","def delete_record(cls, record): record.delete() PersistentIdentifier.query.filter_by( object_type='rec', object_uuid=record.id, ).update(PersistentIdentifier.status: PIDStatus.DELETED) cls.delete_buckets(record) db.session.commit()"
724,"def _delete_auth_token(self, auth_token): if self.delete_auth_token(auth_token): self.delete_auth_token(auth_token) self.delete_auth_token(auth_token) self.delete_auth_token(auth_token) self.delete_auth_token(auth_token) self.delete_auth_token) self.delete_auth_token(auth_token) self.delete_token(auth_token) self.delete_token(auth_token) self.delete_token(auth_token) self.delete_token(auth_token) self.delete_token) self.delete_token(auth_token) self.delete_token(auth_token) self.delete_token(auth_token) self.delete_token(auth_token) self.delete_token(auth_token) self.delete_token(","def delete(self, request, *args, **kwargs): auth = get_authorization_header(request).split() if not auth or auth[0].lower()!= b'token': return response.Response(status=status.HTTP_400_BAD_REQUEST) if len(auth) == 1: msg = 'Invalid token header. No credentials provided.' return response.Response(msg, status=status.HTTP_400_BAD_REQUEST) elif len(auth) > 2: msg = 'Invalid token header. Token string should not contain spaces.' return response.Response(msg, status=status.HTTP_400_BAD_REQUEST) try: token = self.model.objects.get(key=auth[1]) except self.model.DoesNotExist: pass else: token.delete() signals.user_logged_out.send( type(self), user=token.user, request=request, ) return response.Response(status=status.HTTP_204_NO_CONTENT)"
725,def ________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________,"def webhooks(self): api_version = self._get_api_version('webhooks') if api_version == '2017-10-01': from.v2017_10_01.operations import WebhooksOperations as OperationClass elif api_version == '2018-02-01-preview': from.v2018_02_01_preview.operations import WebhooksOperations as OperationClass elif api_version == '2018-09-01': from.v2018_09_01.operations import WebhooksOperations as OperationClass else: raise NotImplementedError(""APIVersion  is not available"".format(api_version)) return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))"
726,"def clean_child(self, directory): if self.clean_child(directory): self.clean_child(directory): self.clean_child(directory): self.clean_child(directory) self.clean_child(directory) self.clean_child(directory) self.clean_child(directory) self.clean_child(directory) self.clean_child(directory) self.clean_child(directory) self.clean_child(directory) self.clean_child) self.clean_child) self.clean_child) self.clean_child(directory) self.clean_child) self.clean_child) self.clean_child(directory) self.clean_child) self.clean_child(directory) self.clean_child) self.clean_child(directory) self.clean_child) self.clean_child) self.clean_child(directory) self.clean_child) self.clean_child(directory) self.clean_child(directory) self.clean_child) self.clean_child(directory) self.clean_child(directory) self.clean_child) self.clean_child(directory) self.clean_child) self.clean_child(directory) self.clean_child) self",def cleanup(self): for k in self._children: self._children[k].cleanup() if self._cleanup: self.remove(True)
727,"def create_queue(self, predicate): if self.queue(predicate): self.queue(predicate): self.queue(predicate): self.queue(predicate): self.queue(predicate)","def _build_extra_predicate(self, extra_predicate): if extra_predicate is None: return '' if not isinstance(extra_predicate[1], (list, dict, tuple)): extra_predicate = [extra_predicate[0], (extra_predicate[1], )] extra_predicate = database.escape_query(*extra_predicate) return 'AND (' + extra_predicate + ')'"
728,"def add_item_metadata(self, handle, handle, handle): if handle = self.add_item_metadata(self.add_item_metadata, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle)","def get_item_metadata(self, handle): bucket = self.s3resource.Bucket(self.bucket) metadata =  identifier = generate_identifier(handle) prefix = self.fragments_key_prefix + ''.format(identifier) for obj in bucket.objects.filter(Prefix=prefix).all(): metadata_key = obj.key.split('.')[-2] response = obj.get() value_as_string = response['Body'].read().decode('utf-8') value = json.loads(value_as_string) metadata[metadata_key] = value return metadata"
729,"def select_voxels(n_voxels, dataset): mappable(n_voxels, dataset): mappable(n_voxels, dataset) mappable(n_voxels, dataset) mappable(n_voxels) mappable(n_voxels) mappable(n_voxels) mappable(n_voxels) mappable(n_voxels) mappable(n_voxels) mappable(n_voxels) mappable(n_voxels) mappable(n_voxels) mappable(n_voxels) mappable(n_voxels) mappable(n_voxels) mappable(n_voxels) mappable(n_voxels) mappable(n_voxels) mappable(n_voxels) mappable(n_voxels) mappable(n_voxels) mappable(n_voxels) mappable(n_voxels) mappable(n_voxels) mappable(n_voxels) mappable(n_voxels) mappable(n_voxels) mappable(n","def get_random_voxels(dataset, n_voxels): voxels = np.arange(dataset.masker.n_vox_in_vol) np.random.shuffle(voxels) selected = voxels[0:n_voxels] return dataset.get_image_data(voxels=selected)"
730,"def add_member(self, member): if self.add_member(self.add_member): self.add_member(self.add_member) return self.add_member(self.add_member)","def add_member(self, member_id): members = self.fetch_json( uri_path=self.base_uri + '/idMembers', http_method='POST', query_params='value': member_id ) members_list = [] for member_json in members: members_list.append(self.create_member(member_json)) return members_list"
731,"def size_of_a_footing(self, sl, vertical_load, fos, length_to_width, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, sl, vertical_load, fos, sl, sl, sl, fos, sl, sl, sl, sl, sl, sl, sl, sl, sl, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verbosity, verb","def size_footing_for_capacity(sl, vertical_load, fos=1.0, length_to_width=1.0, verbose=0, **kwargs): method = kwargs.get(""method"",'vesics') depth_to_width = kwargs.get(""depth_to_width"", 0) depth = kwargs.get(""depth"", 0) use_depth_to_width = 0 if not depth: use_depth_to_width = 1 fd = models.FoundationRaft() fd.width =.5 for i in range(50): fd.length = length_to_width * fd.width if use_depth_to_width: fd.depth = depth_to_width * fd.width capacity_method_selector(sl, fd, method) q = fd.q_ult bearing_capacity = q * fd.length * fd.width fs_actual = bearing_capacity / vertical_load if fs_actual  fos: fd.width += 0.5 else: if verbose: log(""fs_actual: "", fs_actual) log(""fd.width: "", fd.width) break width_array = [] fs_array = [] for j in range(11): width_array.append(fd.width) fd.length = length_to_width * fd.width if use_depth_to_width: fd.depth = depth_to_width * fd.width capacity_method_selector(sl, fd, method) q = fd.q_ult capacity = q * fd.length * fd.width fs_array.append(capacity / vertical_load) fd.width ="
732,"def parse_peddy_ped(self, lines, iterable(str)): return self.parse_peddy_ped(self.parse_peddy_ped(self.parse_peddy_ped))","def parse_peddy_ped(lines): peddy_ped = [] header = [] for i,line in enumerate(lines): line = line.rstrip() if i == 0: header = line.lstrip('#').split('t') else: ind_info = dict(zip(header, line.split('t'))) ind_info['PC1'] = convert_number(ind_info['PC1']) ind_info['PC2'] = convert_number(ind_info['PC2']) ind_info['PC3'] = convert_number(ind_info['PC3']) ind_info['het_call_rate'] = convert_number(ind_info['het_call_rate']) ind_info['het_idr_baf'] = convert_number(ind_info['het_idr_baf']) ind_info['het_mean_depth'] = convert_number(ind_info['het_mean_depth']) peddy_ped.append(ind_info) return peddy_ped"
733,def enable_event_loop_integration(app): app = app if app is not app.get(app): app.get(app): app.get(app): app.get(app): app.get(app): app.get(app): app.get(app): app.get(app),"def enable_gtk(self, app=None): import gtk try: gtk.set_interactive(True) self._current_gui = GUI_GTK except AttributeError: from IPython.lib.inputhookgtk import inputhook_gtk self.set_inputhook(inputhook_gtk) self._current_gui = GUI_GTK"
734,"def print_summary(self, model): if model(model) is not a model(model) if model(model) is not a model(model)","def summary(self, header=True): table = [] for model in self.models: model_summary = model._model_json[""output""][""model_summary""] r_values = list(model_summary.cell_values[0]) r_values[0] = model.model_id table.append(r_values) print() if header: print('Grid Summary:') print() H2ODisplay(table, ['Model Id'] + model_summary.col_header[1:], numalign=""left"", stralign=""left"")"
735,"def extract_small_item(self, queue): if self.get_small_item(queue): self.get_small_item(queue): self.get_small_item(queue) return self.get_small_item(queue)","def get(self, default=None): if not self.__data: return default return heapq.heappop(self.__data)"
736,"def tier_rename(self, id_from, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to, id_to,","def rename_tier(self, id_from, id_to): childs = self.get_child_tiers_for(id_from) self.tiers[id_to] = self.tiers.pop(id_from) self.tiers[id_to][2]['TIER_ID'] = id_to for child in childs: self.tiers[child][2]['PARENT_REF'] = id_to"
737,"def remove_value(self, value): self.get_value(value) self.get_value(value) self.get_value(value) self.get_value(value) self.get_value(value) self.get_value(value) self.get_value(value) self.get_value(value) self.get_value(value) self.get_value(value) self.get_value(value) self.get_value(value) self.get_value(value) self.get_value) self.get_value(value) self.get_value(value) self.get_value(value) self.get_value(value) self.get_value(value) self.get_value(value) self.get_value(value) self.get_value(value) self.get_value(value) self.get_value(value) self.get_value(value) self.get_value(value) self.get_value(value) self.get_value(value) self.get_value(value) self.get_value(value) self.get_value(value) self.get_value(value) self.get_value(value) self.get_value(value) self.get_value(value) self.get_value(value) self.","def remove(self, name, value): clone = self._clone() clone._qsl = [qb for qb in self._qsl if qb!= (name, str(value))] return clone"
738,"def create_new_symbol(self, symbol, symbol, symbol, symbol, symbol, symbol, symbol, symbol, symbol, symbol, symbol, symbol, symbol, symbol, symbol, symbol, symbol, symbol, symbol, symbol, symbol, symbol, symbol, symbol, symbol, symbol, symbol, symbol, symbol, symbol, symbol, symbol, symbol, symbol, symbol, symbol, symbol, symbol)","def symbol(name: str, ns: Optional[str] = None, meta=None) -> Symbol: return Symbol(name, ns=ns, meta=meta)"
739,"def parse_datetime(self, epoch_time, datetime): if epoch_time is not epoch_time, if epoch_time is not epoch_time, if epoch_time is not epoch_time, if epoch_time is not epoch_time, if epoch_time is not epoch_time, if epoch_time = epoch_time): epoch_time = epoch_time = epoch_time = epoch_time = epoch_time = epoch_time = epoch_time = epoch_time = epoch_time = epoch_time = epoch_time = epoch_time = epoch_time = epoch_time = epoch_time = epoch_time = epoch_time = epoch_time = epoch_time = epoch_time = epoch_time = epoch_time = epoch_time = epoch_time = epoch_time =","def parse_time_date(self, s): try: ret = parser.parse(str(s)) except ValueError: try: ret = datetime.fromtimestamp(int(s)) except TypeError: ret = None return ret"
740,"def wx.app(app): if app is not found, wx.app(app): if app is not found, wx.app(app): if app is not found, wx.app(app): if app is not found, wx.app(app): if app is not found, wx.app(app): if app is not found, wx.app(app): if app is not found, wx.app(app): wx.app(app): wx.app(app): wx.app(app): wx.app(app): wx.app(app): wx.app(app): wx.app(app): wx.app(app): wx.app(app): wx.app(app): wx.app(app): wx.app(app): wx.app(app): wx.app(app): wx.app(app): wx.app(app): wx.app(app): wx.app(app): wx.app(app): wx.app","def enable_wx(self, app=None): import wx wx_version = V(wx.__version__).version if wx_version  [2, 8]: raise ValueError(""requires wxPython >= 2.8, but you have %s"" % wx.__version__) from IPython.lib.inputhookwx import inputhook_wx self.set_inputhook(inputhook_wx) self._current_gui = GUI_WX import wx if app is None: app = wx.GetApp() if app is None: app = wx.App(redirect=False, clearSigInt=False) app._in_event_loop = True self._apps[GUI_WX] = app return app"
741,"def parse_hook_name(self, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name): hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_name, hook_","def hook_name(self, hook, n): hook.name = self.value(n) hook.listparam = [] return True"
742,"def ndarray(x, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray,","def feed_forward(self, x, **kwargs): regs = regularizers.from_kwargs(self, **kwargs) key = self._hash(regs) if key not in self._functions: outputs, updates = self.build_graph(regs) labels, exprs = list(outputs.keys()), list(outputs.values()) util.log('compiling feed_forward function') self._functions[key] = (labels, theano.function( self.inputs, exprs, updates=updates)) labels, f = self._functions[key] return dict(zip(labels, f(x)))"
743,"def refer_all_vars(self, vars): if self.vars is not self.vars(vars): self.vars(vars) self.vars(vars) self.vars(vars) self.vars(vars) self.vars(vars) self.vars(vars) self.vars(vars) self.vars) self.vars(vars) self.vars(vars) self.vars(vars) self.vars) self.vars) self.vars = self.vars","def refer_all(self, other_ns: ""Namespace""): self._refers.swap(Namespace.__refer_all, other_ns.interns)"
744,"def take_string(self, input_string, keyError): return self.______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________","def mech_from_string(input_string): if not re.match(r'd+(.d+)*$', input_string): if re.match(r'd+( d+)*$', input_string): input_string = ""."".join(input_string[1:-1].split()) else: raise ValueError(input_string) for mech in get_all_mechs(): if input_string == str(mech): return mech raise KeyError(""Unknown mechanism: 0"".format(input_string))"
745,"def _title(self, frame, title): if self.frame(self.frame): self.frame(self.frame) self.frame(self.frame) self.frame(self.frame) self.frame(self.frame) self.frame(self.frame) self.frame(self.frame) self.frame(self.frame) self.frame(self.frame) self.frame(self.frame) self.frame(self.frame) self.frame(self.frame) self.frame(self.frame) self.frame(self.frame(self.frame(self.frame) self.frame(self.frame) self.frame(self.frame) self.frame(self.frame) self.frame(self.frame) self.frame(self.frame(self.frame) self.frame(self.frame) self.frame(self.frame) self.frame(self.frame) self.frame(self.frame) self.frame(self.frame) self.frame(self.frame) self.frame(self.frame) self.frame(self.frame) self.frame(self.frame) self.frame(self.frame) self.frame(self.frame) self.frame(self.frame) self.frame(self.frame) self.frame(self.frame) self.frame(self.frame","def frame_title(self): elements = self._find_xpath(""/html/head/title"") titles = [element.all_text for element in elements] return titles[0] if len(titles) else """""
746,"def generate_model_metrics(train, valid, xval): if test_data is None: if test_data is None: if test_data is None: if test_data is None: if test_data is None: if test_data is None: if test_data is None: if test_data is None: if test_data is None: if test_data is None: if test_data is None: if test_data is None: if test_metrics = if test_metrics = if test_metrics = if test_metrics = if test_metrics = test_metrics = test_metrics = test_metrics = test_metrics = test_metrics = test_metrics = test_metrics = test_metrics = test_metrics = test_metrics = test_metrics = test_metrics = test_metrics = test_metrics = test_metrics = test_metrics = test_data = test_data = test_data = test_data = test_data = test_data = test_data = test_data = test_data = test_data = test_data = test_data =","def model_performance(self, test_data=None, train=False, valid=False, xval=False): return model.model_id: model.model_performance(test_data, train, valid, xval) for model in self.models"
747,def correspond_float_point(self): return self.float_point(self.float_point),"def real(self, nested_scope=None): operation = self.children[0].operation() expr = self.children[1].real(nested_scope) return operation(expr)"
748,"def observable(observable, observable, beta, alphaStar, delta, parallax, muAlphaStar, muDelta, alphaStar, beta, beta, beta, beta, beta, beta, beta, beta, beta, beta, beta, beta, beta, beta, beta, beta, beta, beta, beta, beta, beta, beta, beta, beta, beta, beta, beta, beta, beta, beta, beta, alphaStar, alphaStar, alphaStar, beta, beta, beta, beta, beta, beta, beta, beta, beta, beta, beta, beta)","def errorScalingFactor(observable, beta): if isscalar(beta): index=int(floor(abs(sin(beta))*_numStepsSinBeta)) if index == _numStepsSinBeta: return _astrometricErrorFactors[observable][_numStepsSinBeta-1] else: return _astrometricErrorFactors[observable][index] else: indices = array(floor(abs(sin(beta))*_numStepsSinBeta), dtype=int) indices[(indices==_numStepsSinBeta)] = _numStepsSinBeta-1 return _astrometricErrorFactors[observable][indices]"
749,"def visit_project(self, module): if self.project(modules): self.project(modules): self.project(modules) self.project(modules) self.project(modules) self.project(modules) self.project(modules) self.project(modules) self.project(modules) self.project(modules) self.project(modules) self.project(modules) self.project(modules) self.project(modules) self.project(modules) self.project(modules) self.project(modules) self.project(modules) self.project(modules) self.project(modules) self.project(modules) self.project(modules) self.project(modules) self.project(modules) self.project(modules) self.project(modules) self.project(modules) self.project(modules) self.project(modules) self.project(modules) self.project(modules) self.project(modules) self.project(modules) self.project(modules) self.project(modules) self.project(","def close(self): if self.linter.is_message_enabled(""cyclic-import""): graph = self._import_graph_without_ignored_edges() vertices = list(graph) for cycle in get_cycles(graph, vertices=vertices): self.add_message(""cyclic-import"", args="" -> "".join(cycle))"
750,"def get_absolute_values(column, column, column, column, column, column, column, column, column, column, column, column, column, column, column, column, column, column, column, column, column, column, column, column, column, column, column, column, column, column, column, column, column, column, column, column, column, column, column, column, column, colum","def absolute_values(df, *, column: str, new_column: str = None): new_column = new_column or column df[new_column] = abs(df[column]) return df"
751,def serve_fuzzed_json(self): self.json_fuzzed_json(self.json_fuzzed_json) self.json_fuzzed_json(self.json) self.json_fuzzed_json(self.json) self.json_fuzzed_json(self.json) self.json_fuzzed_json(self.json) self.json_fuzzed_json_fuzzed_json_fuzzed_json_fuzzed_json_fuzzed_json_fuzzed_json_fuzzed_json_fuzzed_json_fuzzed_json_fuzzed_json_fuzzed_json_fuzzed_json_fuzzed_json_fuzzed_json_fuzzed_json_fuzzed_json_fuzzed_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_,"def serve(self): try: fuzzed = self.json.fuzzed if self.config.fuzz_web: self.client_queue.put((request.environ.get('REMOTE_ADDR'), fuzzed)) response.headers.append(""Access-Control-Allow-Origin"", ""*"") response.headers.append(""Accept-Encoding"", ""identity"") response.headers.append(""Content-Type"", self.config.content_type) if self.config.notify: PJFTestcaseServer.send_testcase(fuzzed, '127.0.0.1', self.config.ports[""servers""][""TCASE_PORT""]) yield fuzzed except Exception as e: raise PJFBaseException(e.message if hasattr(e, ""message"") else str(e))"
752,"def pylint_rc_file(self, path): if self.pylint_rc_file(path): self.pylint_rc_file(path): self.pylint_rc_file(path): self.pylint_rc_file(path): self.pylint_rc_file(path)","def find_pylintrc(): if os.path.exists(""pylintrc""): return os.path.abspath(""pylintrc"") if os.path.exists("".pylintrc""): return os.path.abspath("".pylintrc"") if os.path.isfile(""__init__.py""): curdir = os.path.abspath(os.getcwd()) while os.path.isfile(os.path.join(curdir, ""__init__.py"")): curdir = os.path.abspath(os.path.join(curdir, "".."")) if os.path.isfile(os.path.join(curdir, ""pylintrc"")): return os.path.join(curdir, ""pylintrc"") if os.path.isfile(os.path.join(curdir, "".pylintrc"")): return os.path.join(curdir, "".pylintrc"") if ""PYLINTRC"" in os.environ and os.path.exists(os.environ[""PYLINTRC""]): pylintrc = os.environ[""PYLINTRC""] else: user_home = os.path.expanduser("""") if user_home in ("""", ""/root""): pylintrc = "".pylintrc"" else: pylintrc = os.path.join(user_home, "".pylintrc"") if not os.path.isfile(pylintrc): pylintrc = os.path.join(user_home, "".config"", ""pylintrc"") if not os.path.isfile(pylintrc): if os.path.is"
753,"def check_list(self, variables): if self.list(variables): self.list(variables) self.list(variables) self.list(variables) self.list(variables) self.list(variables) self.list(variables) self.list(variables) self.list(variables) self.list(variables) self.list(variables) self.list(variables) self.list(variables) self.list(variables)","def check_list(var, num_terms): if not isinstance(var, list): if isinstance(var, tuple): var = list(var) else: var = [var] for _ in range(1, num_terms): var.append(var[0]) if len(var)!= num_terms: print( '""%s"" has the wrong number of terms; it needs %s. Exiting...' % (var, num_terms)) sys.exit(1) return var"
754,def prune_history_price(self): if self.price(self.price): self.price(self.price) self.price(self.price) self.price(self.price) self.price(self.price) self.price(self.price) self.price(self.price) self.price(self.price) self.price(self.price) self.price(self.price) self.price(self.price) self.price(self.price) self.price(self.price) self.price(self.price) self.price(self.price) self.price(self.price) self.price(self.price) self.price) self.price(self.price) self.price) self.price(self.price) self.price(self.price) self.price(self.price) self.price(self.price) self.price(self.price) self.price) self.price(self.price) self.price(self.price) self.price(self.price) self.price(self.,"def prune_all(self) -> int: from.repositories import PriceRepository repo = PriceRepository() items = repo.query.distinct(dal.Price.namespace, dal.Price.symbol).all() count = 0 for item in items: symbol = SecuritySymbol(item.namespace, item.symbol) deleted = self.prune(symbol) if deleted: count += 1 return count"
755,"def stdout(epm_or_idf_path, simulation_dir_path, stderr, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq)","def run_eplus(epm_or_idf_path, weather_data_or_epw_path, simulation_dir_path, stdout=None, stderr=None, beat_freq=None): simulation_dir_path = os.path.abspath(simulation_dir_path) if not os.path.isdir(simulation_dir_path): raise NotADirectoryError(""Simulation directory does not exist: '%s'."" % simulation_dir_path) if not isinstance(epm_or_idf_path, Epm): epm = Epm.from_idf(epm_or_idf_path) else: epm = epm_or_idf_path simulation_idf_path = os.path.join(simulation_dir_path, CONF.default_model_name + "".idf"") epm.to_idf(simulation_idf_path) simulation_epw_path = os.path.join(simulation_dir_path, CONF.default_model_name + "".epw"") if isinstance(weather_data_or_epw_path, WeatherData): weather_data_or_epw_path.to_epw(simulation_epw_path) else: _copy_without_read_only(weather_data_or_epw_path, simulation_epw_path) temp_epw_path = get_simulated_epw_path() if temp_epw_path is not None: _copy_without_read_only(simulation_epw_path, temp_epw_path) eplus_relative_cmd = get_simulation_base_command() eplus_cmd = os.path.join(CONF.eplus_base_dir_path, eplus_relative_cmd) id"
756,def seed_internal_state(self): self.seed_internal_state(self.seed_internal_state) self.seed_internal_state(self.seed_internal_state) self.seed_internal_state(self.seed_internal_state) self.seed_internal_state(self.seed_internal_state) self.seed_internal_state(self.seed_internal_state(self.seed_internal_state) self.seed_internal_state) self.seed_internal_state) self.seed_internal_state(self.seed_internal_state) self.seed_internal_state) self.seed_internal_state) self.seed_internal_state(self.seed_internal_state) self.seed_internal_state) self.seed_internal_state) self.seed_internal_state(self.seed_internal_state) self.seed_internal_state) self.seed_internal_state(self.seed_internal_state),"def seed(self, values): if not values: seed_ids = [int, str, random, self, values, self.__class__] random.shuffle(seed_ids) values = list(map(id, seed_ids)) + [time.time(), os.urandom(512)] mash = Mash() self.c = 1 self.s0 = mash(' ') self.s1 = mash(' ') self.s2 = mash(' ') for val in values: self.s0 -= mash(val) if self.s0  0: self.s0 += 1 self.s1 -= mash(val) if self.s1  0: self.s1 += 1 self.s2 -= mash(val) if self.s2  0: self.s2 += 1"
757,"def get_metric_definition(self, metric): self.get_metric_definition(self.get_metric_definition) self.get_metric_definition(self.get_metric_definition) self.get_metric_definition(self.get_metric_definition) self.get_metric_definition(self.get_metric_definition) self.get_metric_definition(self.get_metric_definition) self.get_metric_definition) self.get_metric_definition(self.get_metric_definition) self.get_metric_definition(self.get_metric_definition) self.get_metric_definition(self.get_metric_definition) self.get_metric_definition) self.get_metric_definition(self.get_metric_definition) self.get_metric_definition self.get_metric_definition self.get_metric_definition self.get_metric_definition self.get_metric_definition self.get_metric_definition self.get_metric_definition self.get_metric_definition self.get_metric_definition self.",def _call_api(self): self._url = self.form_url() if self._headers is not None: logging.debug(self._headers) if self._data is not None: logging.debug(self._data) if len(self._get_url_parameters()) > 0: logging.debug(self._get_url_parameters()) result = self._methods[self._method]() if not self.good_response(result.status_code): logging.error(self._url) logging.error(self._method) if self._data is not None: logging.error(self._data) logging.error(result) self._api_result = result
758,"def print_stats(known_file_paths, known_file_paths, known_file_paths, known_file_paths, known_file_paths, known_file_paths, known_file_paths, known_file_paths, known_file_paths, known_file_paths, known_file_paths, known_file_paths, known_file_paths, known_file_paths, known_file_paths, known_file_paths, known_file_paths)","def _log_file_processing_stats(self, known_file_paths): headers = [""File Path"", ""PID"", ""Runtime"", ""Last Runtime"", ""Last Run""] rows = [] for file_path in known_file_paths: last_runtime = self.get_last_runtime(file_path) file_name = os.path.basename(file_path) file_name = os.path.splitext(file_name)[0].replace(os.sep, '.') if last_runtime: Stats.gauge( 'dag_processing.last_runtime.'.format(file_name), last_runtime ) processor_pid = self.get_pid(file_path) processor_start_time = self.get_start_time(file_path) runtime = ((timezone.utcnow() - processor_start_time).total_seconds() if processor_start_time else None) last_run = self.get_last_finish_time(file_path) if last_run: seconds_ago = (timezone.utcnow() - last_run).total_seconds() Stats.gauge( 'dag_processing.last_run.seconds_ago.'.format(file_name), seconds_ago ) rows.append((file_path, processor_pid, runtime, last_runtime, last_run)) rows = sorted(rows, key=lambda x: x[3] or 0.0) formatted_rows = [] for file_path, pid, runtime, last_runtime, last_run in rows: formatted_rows.append((file_path, pid, "":.2fs"".format(runtime) if runtime else None, "":.2fs"".format(last_runtime) if last_runtime else None, last_run.strftime(""%Y-%m-%dT"
759,"def mutate_object(self, type): self.mutate_object(self.mutate_object(self.mutate_object(self.mutate_object)) self.mutate_object(self.mutate_object) self.mutate_object(self.mutate_object) self.mutate_object(self.mutate_object) self.mutate_object(self.mutate_object) self.mutate_object(self.mutate_object) self.mutate_object(self.mutate_object) self.mutate_object(self.mutate_object) self.mutate_object(self.mutate_object) self.mutate_object(self.mutate_object) self.mutate_object) self.mutate_object(self.mutate_object) self.mutate_object) self.mutate_object(self.mutate_object) self.mutate_object(self.mutate_object) self.mutate_object(self.mutate_object) self.mutate_object(self.mutate_object) self.mutate_object(self.mutate_object) self.mutate_object(self.mutate_object) self.mutate_object(self.mutate_object) self.mutate_","def mutate_object_decorate(self, func): def mutate(): obj = func() return self.Mutators.get_mutator(obj, type(obj)) return mutate"
760,"def normalize_data2d(data2d, axis): if data2d is None: if data2d is None: if data2d is None: if data2d is None: if data2d is None: if data2d is None: if data2d is None: if data2d is None: if data2d is None: if data2d is None: if data2d is None: if data2d is None: axis is None: axis is None: axis is None: axis is None: axis is None: axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis","def zscore(data2d, axis=0): if axis is None: return data2d assert axis in [0,1] z_scored = data2d.apply(lambda x: (x-x.mean())/x.std(ddof=1), axis=operator.xor(1, axis)) return z_scored"
761,"def lower_bound(self, f(x, z) + f(x, z) + f(x, z) + f(x, z) + f(x, z) + f(x, z) + f(x, z) + f(x, z) + f(x, z) + f(x, z) + f(x, z) + f(x, z) + f(x, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q, q,","def entropy_lower_bound(self, name=""entropy_lower_bound""): rwith self._name_scope(name): with tf.control_dependencies(self._assertions): distribution_entropies = [d.entropy() for d in self.components] cat_probs = self._cat_probs(log_probs=False) partial_entropies = [ c_p * m for (c_p, m) in zip(cat_probs, distribution_entropies) ] return tf.add_n(partial_entropies)"
762,"def verify_signature(self, cert, data): if self.signature(self.signature): self.signature(self.signature) self.signature(self.signature) self.signature(self.signature) self.signature(self.signature) self.signature(self.signature) self.signature(self.signature) self.signature(self.signature) self.signature(self.signature) self.signature(self.signature) self.signature(self.signature) self.signature(self.signature) self.signature(self.signature) self.signature(self.signature) self.signature(self.signature) self.signature(self.signature) self.signature(self.signature) self.signature(self.signature) self.signature) self.signature(self.signature) self.signature(self.signature) self.signature) self.signature(self.signature) self.signature(self.signature) self.signature(self.signature) self.signature(self.signature) self.signature(self.signature) self.signature) self.signature(self.signature) self.signature(self","def verify(cert, signature, data, digest): data = _text_to_bytes_and_warn(""data"", data) digest_obj = _lib.EVP_get_digestbyname(_byte_string(digest)) if digest_obj == _ffi.NULL: raise ValueError(""No such digest method"") pkey = _lib.X509_get_pubkey(cert._x509) _openssl_assert(pkey!= _ffi.NULL) pkey = _ffi.gc(pkey, _lib.EVP_PKEY_free) md_ctx = _lib.Cryptography_EVP_MD_CTX_new() md_ctx = _ffi.gc(md_ctx, _lib.Cryptography_EVP_MD_CTX_free) _lib.EVP_VerifyInit(md_ctx, digest_obj) _lib.EVP_VerifyUpdate(md_ctx, data, len(data)) verify_result = _lib.EVP_VerifyFinal( md_ctx, signature, len(signature), pkey ) if verify_result!= 1: _raise_current_error()"
763,"def build_keywrappingspecification(self, args, value): if self.keywrappingspecification(self.keywrappingspecification): self.keywrappingspecification(self.keywrappingspecification) self.keywrappingspecification(self.keywrappingspecification) self.keywrappingspecification(self.keywrappingspecification) self.keywrappingspecification(self.keywrappingspecification) self.keywrappingspecification(self.keywrappingspecification) self.keywrappingspecification(self.keywrappingspecification) self.keywrappingspecification) self.keywrappingspecification(self.keywrappingspecification) self.keywrappingspecification) self.keywrappingspecification(self.keywrappingspecification) self.keywrappingspecification) self.keywrappingspecification(self.keywrappingspecification) self.keywrappingspecificationself.keywrappingspecificationself.keywrappingSpecificationself.keywrappingSpecificationself.keywrappingSpecificationself.keywrappingSpecificationself.keywrappingSpecificationself.keywrappingSpecificationself.keywrap","def _build_key_wrapping_specification(self, value): if value is None: return None if not isinstance(value, dict): raise TypeError(""Key wrapping specification must be a dictionary."") encryption_key_info = self._build_encryption_key_information( value.get('encryption_key_information') ) mac_signature_key_info = self._build_mac_signature_key_information( value.get('mac_signature_key_information') ) key_wrapping_specification = cobjects.KeyWrappingSpecification( wrapping_method=value.get('wrapping_method'), encryption_key_information=encryption_key_info, mac_signature_key_information=mac_signature_key_info, attribute_names=value.get('attribute_names'), encoding_option=value.get('encoding_option') ) return key_wrapping_specification"
764,def extract_file(self): if self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute))))))))))),"def extraction_error(self): % (old_exc, cache_path) ) err.manager = self err.cache_path = cache_path err.original_error = old_exc raise err"
765,"def insert(self, id, obj, idx, obj, obj, idx, obj, obj, idx, obj, obj, idx, obj, obj, idx, obj, obj, idx, obj, obj, idx, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj, obj","def insert(self, id, coordinates, obj=None): p_mins, p_maxs = self.get_coordinate_pointers(coordinates) data = ctypes.c_ubyte(0) size = 0 pyserialized = None if obj is not None: size, data, pyserialized = self._serialize(obj) core.rt.Index_InsertData(self.handle, id, p_mins, p_maxs, self.properties.dimension, data, size)"
766,"def create_table(self, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table)","def write_table(self, table): table_sql, serial_key_sql = super(PostgresDbWriter, self).write_table(table) for sql in serial_key_sql + table_sql: self.execute(sql)"
767,"def confusion_matrix(self, data, metric): if self.matrix(data, metric): self.matrix(data, metric): self.matrix(data, metric)","def confusion_matrix(self, data): return model.model_id: model.confusion_matrix(data) for model in self.models"
768,"def ident_uid(self, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident) ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident, ident,","def _unregister_engine(self, uid): if len(self.targets) == 1: pass self.engine_stream.flush() idx = self.targets.index(uid) self.targets.pop(idx) self.loads.pop(idx) if self.pending[uid]: dc = ioloop.DelayedCallback(lambda : self.handle_stranded_tasks(uid), 5000, self.loop) dc.start() else: self.completed.pop(uid) self.failed.pop(uid)"
769,def transform_quantumChannel(quantumchannel): if q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q =,"def _to_ptm(rep, data, input_dim, output_dim): if rep == 'PTM': return data _check_nqubit_dim(input_dim, output_dim) if rep == 'Operator': return _from_operator('PTM', data, input_dim, output_dim) if rep!= 'SuperOp': data = _to_superop(rep, data, input_dim, output_dim) return _superop_to_ptm(data, input_dim, output_dim)"
770,"def display_time(self, verbose=true, count=true, count=true, count=true, verbose=true, count=true, count=true, count=true, count=true, count=true, count=true, count=true, count=true, count=true, count=true, count=true, count=true, count=true, count=true, count=true)","def displayAll(elapsed, display_amt, est_end, nLoops, count, numPrints): if numPrints > nLoops: display_amt = 1 else: display_amt = round(nLoops / numPrints) if count % display_amt == 0: avg = elapsed / count est_end = round(avg * nLoops) (disp_elapsed, disp_avg, disp_est) = timeUnit(int(round(elapsed)), int(round(avg)), int(round(est_end))) print ""%s%%"" % str(round(count / float(nLoops) * 100)), ""@"" + str(count), totalTime = disp_est[0] unit = disp_est[1] if str(unit) == ""secs"": remain = totalTime - round(elapsed) remainUnit = ""secs"" elif str(unit) == ""mins"": remain = totalTime - round(elapsed) / 60 remainUnit = ""mins"" elif str(unit) == ""hr"": remain = totalTime - round(elapsed) / 3600 remainUnit = ""hr"" print ""ETA: %s %s"" % (str(remain), remainUnit) print return"
771,def return_any_field(models.BigIntegerField()): return if model(models.BigIntegerField(models.BigIntegerField(models.BigIntegerField(models.BigIntegerField(models.BigIntegerField())))),"def any_biginteger_field(field, **kwargs): min_value = kwargs.get('min_value', 1) max_value = kwargs.get('max_value', 10**10) return long(xunit.any_int(min_value=min_value, max_value=max_value))"
772,"def Convenience(self, features, targets, n_samples, n_features, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples)","def fit_transform(self, features, targets): self.fit(features, targets) return self.transform(features)"
773,"def apply_cluster(self, name, subset): self.cluster(self.cluster): self.cluster(self.cluster) self.cluster(self.cluster) self.cluster(self.cluster) self.cluster(self.cluster) self.cluster(self.cluster) self.cluster(self.cluster) self.cluster(self.cluster) self.cluster(self.cluster) self.cluster(self.cluster) self.cluster(self.cluster) self.cluster(self.cluster) self.cluster(self.cluster) self.cluster(self.cluster) self.cluster) self.cluster(self.cluster) self.cluster) self.cluster(self.cluster) self.cluster(self.cluster) self.cluster) self.cluster(self.cluster) self.cluster) self.cluster(self.cluster) self.cluster(self.cluster) self.cluster) self.cluster(self.cluster) self.cluster) self.cluster(self.cluster) self.cluster(self.cluster) self.cluster(self.cluster) self.cluster(self.cluster) self.c","def apply_classifier(self, name, samples=None, subset=None): if samples is not None: subset = self.make_subset(samples) samples = self._get_samples(subset) c = self.classifiers[name] labs = c.classifier.ulabels_ with self.pbar.set(total=len(samples), desc='Applying'+ name +'classifier') as prog: for s in samples: d = self.data[s] try: f = c.predict(d.focus) except ValueError: f = np.array([-2] * len(d.Time)) for l in labs: ind = f == l d.filt.add(name=name + '_:.0f'.format(l), filt=ind, info=name +'' + c.method +'classifier', params=(c.analytes, c.method)) prog.update() return name"
774,"def break_date(dateint, dateint, dateint, dateint, dateint, dateint, dateint, dateint, dateint, dateint, dateint, dateint, dateint, dateint, dateint, dateint, dateint, dateint, dateint, dateint, dateint, dateint, dateint, dateint, dateint, dateint, dateint, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, dateInt, date","def splitEkmDate(dateint): date_str = str(dateint) dt = namedtuple('EkmDate', ['yy','mm', 'dd', 'weekday', 'hh','minutes','ss']) if len(date_str)!= 14: dt.yy = dt.mm = dt.dd = dt.weekday = dt.hh = dt.minutes = dt.ss = 0 return dt dt.yy = int(date_str[0:2]) dt.mm = int(date_str[2:4]) dt.dd = int(date_str[4:6]) dt.weekday = int(date_str[6:8]) dt.hh = int(date_str[8:10]) dt.minutes = int(date_str[10:12]) dt.ss = int(date_str[12:14]) return dt"
775,"def pprint_base(self, exceptions): pprint_base(exceptions): pprint_base(exceptions): pprint_base(exceptions): pprint_base(exceptions): pprint_base(exceptions)","def _exception_pprint(obj, p, cycle): if obj.__class__.__module__ in ('exceptions', 'builtins'): name = obj.__class__.__name__ else: name = '%s.%s' % ( obj.__class__.__module__, obj.__class__.__name__ ) step = len(name) + 1 p.begin_group(step, name + '(') for idx, arg in enumerate(getattr(obj, 'args', ())): if idx: p.text(',') p.breakable() p.pretty(arg) p.end_group(step, ')')"
776,def frobenius_norm(data - WH||): frobenius_norm(data - WH||): frobenius_norm(data - WH||): frobenius_norm(data - WH||) frobenius_norm(data - WH||) frobenius_norm(data - WH||) frobenius_norm(data - WH||) frobenius_norm(data - WH||) frobenius_norm(data - WH||) frobenius_norm(data) frobenius_norm(data) frobenius_norm(data) frobenius_norm(data) frobenius_norm(data) frobenius_norm(data) frobenius_norm(data) frobenius_norm(data) frobenius_norm(data) frobenius_norm(data) frobenius_norm(data) frobenius_norm(data) frobenius_norm(,"def frobenius_norm(self): if hasattr(self,'H') and hasattr(self,'W') and not scipy.sparse.issparse(self.data): err = np.sqrt( np.sum((self.data[:,:] - np.dot(self.W, self.H))**2 )) else: err = -123456 return err"
777,"def handle_relation(self, content_object, tagged_item): self.get_relation(self.get_relation(self.get_relation)) self.get_relation(self.get_relation) self.get_relation(self.get_relation) self.get_relation(self.get_relation) self.get_relation(self.get_relation) self.get_relation(self.get_relation) self.get_relation(self.get_relation) self.get_relation) self.get_relation(self.get_relation) self.get_relation) self.get_relation(self.get_relation) self.get_relation(self.get_relation) self.get_relation(self.get_relation) self.get_relation(self.get_relation) self.get_relation) self.get_relation(self.get_relation) self.get_relation(self.get_relation) self.get_relation(self.get_relation) self.get_relation(self.get_relation) self.get_relation) self.get_relation(self.get_relation) self.get_re","def get_tagged_item_manager(self): rel_name = self.through._meta.get_field('content_object').remote_field.get_accessor_name() return getattr(self.instance, rel_name)"
778,"def generate_tuples(self, startBitAddress, endBitAddress, TransTmpl instance): self.get_tuples(self.get_tuples) self.get_tuples(self.get_tuples) self.get_tuples(self.get_tuples) self.get_tuples(self.get_tuples) self.get_tuples(self.get_tuples) self.get_tuples(self.get_tuples) self.get_tuples(self.get_tuples) self.get_tuples(self.get_tuples) self.get_tuples(self.get_tuples) self.get_tuples(self.get_tuples) self.get_tuples(self.get_tuples) self.get_tuples(self.get_tuples) self.get_tuples(self.get_tuples) self.get_tuples(self.get_tuples) self.get_tuples(self.get_tuples) self.get_tuples(self.get_tuples) self.get_tuples(self.get_tup","def walkFlattenChilds(self) -> Generator[ Union[Tuple[Tuple[int, int], TransTmpl], 'OneOfTransaction'], None, None]: for p in self.possibleTransactions: yield p.walkFlatten(offset=self.offset, shouldEnterFn=self.shouldEnterFn)"
779,"def get_all_values(self, values): self.get_all_values(values) self.get_all_values(values) self.get_all_values(values) self.get_all_values(values) self.get_all_values(values) self.get_all_values(values) self.get_all_values(values) self.get_all_values(values) self.get_all_values(values) self.get_all_values(values) self.get_all_values(values) self.get_all_values(values) self.get_all_values(values) self.get_all_values(values) self.get_all_values(values) self.get_all_values(values) self.get_all_values(values) self.get_all_values(values) self.get_all_values(values) self.get_all_values(values) self.get_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all",def values(self) -> [Signature]: if self.state == StateScope.EMBEDDED and self.parent is not None: return list(self._hsig.values()) + list(self.parent().values()) else: return self._hsig.values()
780,"def ess(self, filter_threshold, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess) ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, ess, es","def effective_sample_size(states, filter_threshold=0., filter_beyond_lag=None, name=None): states_was_list = _is_list_like(states) if not states_was_list: states = [states] filter_beyond_lag = _broadcast_maybelist_arg(states, filter_beyond_lag, 'filter_beyond_lag') filter_threshold = _broadcast_maybelist_arg(states, filter_threshold, 'filter_threshold') with tf.compat.v1.name_scope(name, 'effective_sample_size'): ess_list = [ _effective_sample_size_single_state(s, ml, mlt) for (s, ml, mlt) in zip(states, filter_beyond_lag, filter_threshold) ] if states_was_list: return ess_list return ess_list[0]"
781,"def send_int(self, INT, KILL): if self.int(INT, KILL): if self.int(KILL): if self.int(KILL): self.int(KILL): self.int(KILL): self.int(KILL): self.int(KILL): self.int(KILL): self.int(KILL): self.int(KILL)","def interrupt_then_kill(self, delay=2.0): try: self.signal(SIGINT) except Exception: self.log.debug(""interrupt failed"") pass self.killer = ioloop.DelayedCallback(lambda : self.signal(SIGKILL), delay*1000, self.loop) self.killer.start()"
782,"def approxEqual(float1, float2[, tol=1e-18, rel=1e-7]): approxEqual(float1, float2[, tol=1e-18, rel=1e-7]): approxEqual(obj1, float2[, tol=1e-18, rel=1e-7]): approxEqual(obj1, float2[, tol=1e-18, rel=1e-7])","def approxEqual(x, y, *args, **kwargs): if not (type(x) is type(y) is float): methodname = '__approxEqual__' for a,b in ((x, y), (y, x)): try: method = getattr(a, methodname) except AttributeError: continue else: result = method(b, *args, **kwargs) if result is NotImplemented: print ""WARNING: NotImplemented approxEqual for types"" continue return bool(result) return _float_approxEqual(x, y, *args, **kwargs)"
783,def glob_pATTERN(glob_pATTERN): glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_pATTERN = glob_,"def _translate_glob(pat): translated_parts = [] for part in _iexplode_path(pat): translated_parts.append(_translate_glob_part(part)) os_sep_class = '[%s]' % re.escape(SEPARATORS) res = _join_translated(translated_parts, os_sep_class) return'resZ(?ms)'.format(res=res)"
784,"def convert_boltArrayLocal(self, axis, SparkContext): if axis = self.spark(axis, SparkContext): self.spark(axis, SparkContext) self.spark(axis) self.spark(axis) self.spark(axis) self.spark(axis) self.spark(axis) self.spark(axis) self.spark(axis) self.spark(axis) self.spark(axis) self.spark(axis) self.spark(axis) self.spark(axis) self.spark(axis) self.spark(axis) self.spark(axis) self.spark(axis) self.spark(axis) self.spark(axis) self.spark(axis) self.spark(axis) self.spark(axis) self.spark(axis) self.spark(axis) self.spark(axis) self.spark(axis) self.spark(axis) self.spark(axis) self.spark(axis) self.spark(axis) self.spark(axis) self.spark(axi","def tordd(self, sc, axis=0): from bolt import array return array(self.toarray(), sc, axis=axis).tordd()"
785,"def create_memory_tar(self, output_folder, sha256 sum): if self.create_memory_tar is not self.create_memory_tar: self.create_memory_tar: self.create_memory_tar: self.create_memory_tar: self.create_memory_tar: self.create_memory_tar: self.create_memory_tar: self.create_memory_tar: self.create_memory_tar: self.create_memory_tar: self.create_memory_tar: self.create_memory_tar: self.create_memory_tar: self.create_memory_tar: self.create_memory_tar: self.create_memory_tar: self.create_tar: self.create_tar: self.create_tar: self.create_tar: self.create_tar: self.create_tar: self.create_tar: self.create_tar: self.create_tar: self.create_tar: self.create_tar: self.create_tar: self.create_tar: self.create_tar: self.create_tar: self.create_tar: self.create_tar: self","def create_tar(files, output_folder=None): if output_folder is None: output_folder = tempfile.mkdtemp() finished_tar = None additions = [] contents = [] for entity in files: info = tarfile.TarInfo(name=entity['name']) info.mode = entity['mode'] info.mtime = int(datetime.datetime.now().strftime('%s')) info.uid = entity[""uid""] info.gid = entity[""gid""] info.uname = entity[""uname""] info.gname = entity[""gname""] filey = io.StringIO() content = None try: info.size = filey.write(entity['content']) content = io.BytesIO(entity['content'].encode('utf8')) except Exception: info.size = int(filey.write(entity['content'].decode('utf-8'))) content = io.BytesIO(entity['content'].encode('utf8')) pass if content is not None: addition = 'content': content, 'info': info additions.append(addition) contents.append(content) if len(additions) > 0: hashy = get_content_hash(contents) finished_tar = ""%s/sha256:%s.tar.gz"" % (output_folder, hashy) if os.path.exists(finished_tar): msg = ""metadata file %s already exists "" % finished_tar msg += ""will over-write."" bot.debug(msg) tar = tarfile.open(finished_tar, ""w:gz"") for a in additions: tar.addfile(a[""info""], a[""content""]) tar.close() else:"
786,"def build_MACSignatureKeyInformation(self, args, value): if self.___________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________","def _build_mac_signature_key_information(self, value): if value is None: return None if not isinstance(value, dict): raise TypeError( ""MAC/signature key information must be a dictionary."" ) cryptographic_parameters = value.get('cryptographic_parameters') if cryptographic_parameters: cryptographic_parameters = self._build_cryptographic_parameters( cryptographic_parameters ) mac_signature_key_information = cobjects.MACSignatureKeyInformation( unique_identifier=value.get('unique_identifier'), cryptographic_parameters=cryptographic_parameters ) return mac_signature_key_information"
787,"def expdecay_despiker(self, exponent, exponent, noise_despiker, win, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter) maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter, maxiter,","def despike(self, expdecay_despiker=True, exponent=None, noise_despiker=True, win=3, nlim=12., maxiter=3): if not hasattr(self, 'despiked'): self.data['despiked'] = Bunch() out =  for a, v in self.focus.items(): if 'time' not in a.lower(): sig = v.copy() if expdecay_despiker: if exponent is not None: sig = proc.expdecay_despike(sig, exponent, self.tstep, maxiter) else: warnings.warn('exponent is None - either provide exponent, or run at analysenlevel to automatically calculate it.') if noise_despiker: sig = proc.noise_despike(sig, int(win), nlim, maxiter) out[a] = sig self.data['despiked'].update(out) self.data['total_counts'] = sum(self.data['despiked'].values()) self.setfocus('despiked') return"
788,def read_confusion_matrices(self): if self.read_confusion_matrices(self.read_confusion_matrices): self.read_confusion_matrices(self.read_confusion_matrices) self.read_confusion_matrices(self.read_confusion_matrices) self.read_confusion_matrices(self.read_confusion_matrices) self.read_confusion_matrices(self.read_confusion_matrices),"def read_cms(cms=None, domains=None): assert_is_type(cms, [list]) return [ConfusionMatrix(cm, domains) for cm in cms]"
789,def send_message(self): self.get_message(message): self.get_message(message) self.get_message(message) self.get_message(message) self.get_message(message) self.get_message(message) self.get_message(message) self.get_message(message) self.get_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message,"def send(client, sender, targets, msg_name, dest_name=None, block=None): dest_name = msg_name if dest_name is None else dest_name def _send(targets, m_name): msg = globals()[m_name] return com.send(targets, msg) client[sender].apply_async(_send, targets, msg_name) return client[targets].execute('%s=com.recv()'%dest_name, block=None)"
790,"def decorator(self, decorator): if decorator fails: decorator = self.decorator(decorator) if decorator fails: decorator = self.decorator(decorator)","def catch(logger): def decorator(func): @wraps(func) def wrapper(*args, **kwargs): try: return func(*args, **kwargs) except Exception: logger.warning(f""Exception raised in decorator: func.__name__"") return wrapper return decorator"
791,"def list_files(path, path, globstring): if path is not path, if path is not path, if path is not path, if path is not path, if path is not path, if path is not path, if path is not path, if path is not path, if path is not path, if path is not path, if path is not path, if path is not path, if path is not path, if path is not path, if path = path = path, if path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path = path","def list(self, path): if ""*"" in path: return self.connection.glob(path) else: return self.connection.walk(path)"
792,"def sys.excepthook(self.verbose_crash=True, self.verbose_crash=True, self.verbose_crash=True, self.verbose_crash=True, self.verbose_crash=True, self.verbose_crash=True, self.verbose_crash=True, self.verbose_crash=True)","def excepthook(self, etype, evalue, tb): if self.verbose_crash: return self.crash_handler(etype, evalue, tb) else: return crashhandler.crash_handler_lite(etype, evalue, tb)"
793,def highlight_common_sql_words(sql_words): sql_words(sql_words): sql_words(sql_words): sql_words(sql_words): sql_words(sql_words): sql_words(sql_words),"def pformat_sql_html(sql): sql = escape(sql) sql = RE_SQL_NL.sub(u'br>n1', sql) sql = RE_SQL.sub(u'strong>1/strong>', sql) return sql"
794,"def handle_uncompressed_data(self, uncompressed_data): if self.get_uncompressed_data(self.get_uncompressed_data): self.get_uncompressed_data(self.get_uncompressed_data): self.get_uncompressed_data(self.get_uncompressed_data)","def uncompressed(self): ISUNCOMPRESSED = self.verboseRead( BoolCode('UNCMPR', description='Is uncompressed?')) if ISUNCOMPRESSED: self.verboseRead(FillerAlphabet(streamPos=self.stream.pos)) print('Uncompressed data:') self.output += self.stream.readBytes(self.MLEN) print(outputFormatter(self.output[-self.MLEN:])) return ISUNCOMPRESSED"
795,"def regression_score(y_actual, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_actual, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_","def h2o_r2_score(y_actual, y_predicted, weights=1.): ModelBase._check_targets(y_actual, y_predicted) numerator = (weights * (y_actual - y_predicted) ** 2).sum() denominator = (weights * (y_actual - _colmean(y_actual)) ** 2).sum() if denominator == 0.0: return 1. if numerator == 0. else 0. return 1 - numerator / denominator"
796,"def calculate_covariance_matrix(x, y, binby=E): if x, y, binby=E, df.cov(x, y, binby=E): df.cov(x, y, binby=E): df.cov(x, y, binby=E): df.cov(x, y, binby=E): df.cov(x, y, binby=E): df.cov(x): df.cov(x): df.cov(x): df.cov(x): df.cov(x): df.cov(x): df.cov(x): df.cov(x): df.cov(x): df.cov(x): df.cov(x): df.cov(x): df.cov(x): df.cov(x): df.cov(x): df.cov(x): df.cov(x): df.cov(x): df.cov(x): df.cov(x","def cov(self, x, y=None, binby=[], limits=None, shape=default_shape, selection=False, delay=False, progress=None): selection = _ensure_strings_from_expressions(selection) if y is None: if not _issequence(x): raise ValueError(""if y argument is not given, x is expected to be sequence, not %r"", x) expressions = x else: expressions = [x, y] N = len(expressions) binby = _ensure_list(binby) shape = _expand_shape(shape, len(binby)) progressbar = vaex.utils.progressbars(progress) limits = self.limits(binby, limits, selection=selection, delay=True) @delayed def calculate(expressions, limits): task = tasks.TaskStatistic(self, binby, shape, limits, weights=expressions, op=tasks.OP_COV, selection=selection) self.executor.schedule(task) progressbar.add_task(task, ""covariance values for %r"" % expressions) return task @delayed def finish(values): N = len(expressions) counts = values[..., :N] sums = values[..., N:2 * N] with np.errstate(divide='ignore', invalid='ignore'): means = sums / counts meansxy = means[..., None] * means[..., None, :] counts = values[..., 2 * N:2 * N + N**2] sums = values[..., 2 * N + N**2:] shape = counts.shape[:-1] + (N, N) counts = counts.reshape(shape) sums = sums.reshape(shape) with np.errstate(divide='ignor"
797,"def get_output_file_name(self, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name): output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file_name, output_file","def outfile_input(extension=None): fileok = False while not fileok: filename = string_input('File name? ') if extension: if not filename.endswith(extension): if extension.startswith('.'): filename = filename + extension else: filename = filename + '.' + extension if os.path.isfile(filename): choice = choice_input(prompt=filename + 'already exists. Overwrite?', options=['y', 'n']) if choice == 'y': try: nowtime = time.time() with open(filename, 'a') as f: os.utime(filename, (nowtime, nowtime)) fileok = True except IOError: print('Write permission denied on'+ filename +  '. Try again.') except PermissionError: print('Write permission denied on'+ filename +  '. Try again.') except FileNotFoundError: print(filename + ': directory not found. Try again.') else: choice = choice_input( prompt=filename +'does not exist. Create it?', options=['y', 'n']) if choice == 'y': try: nowtime = time.time() with open(filename, 'w') as f: os.utime(filename, (nowtime, nowtime)) fileok = True except IOError: print('Write permission denied on'+ filename +  '. Try again.') except PermissionError: print('Write permission denied on'+ filename +  '. Try again.') except FileNotFoundError: print(filename + ': directory not found. Try again.') return filename"
798,def contact_anniversary(contact_anniversary): if contact_anniversary(contact_anniversary): if contact_anniversary(contact_anniversary): if contact_anniversary(contact_anniversary): if contact_anniversary(contact_anniversary): if contact_anniversary(contact_anniversary): if contact_anniversary(contact_anniversary): if contact_anniversary(contact_anniversary(contact_anniversary)): if contact_anniversary(contact_anniversary): if contact_anniversary(contact_anniversary): if contact_anniversary(contact_anniversary): if contact_anniversary(contact_anniversary): if contact_anniversary(contact_anniversary): if contact_anniversary(contact_anniversary): if contact_anniversary): if contact_anniversary(contact_anniversary): if contact_anniversary(contact_anniversary): if contact_anniversary): if contact_anniversary(contact_anniversary): if contact_anniversary(contact_anniversary): if contact_,"def get_anniversary(self): try: if self.vcard.anniversary.params.get(""VALUE"")[0] == ""text"": return self.vcard.anniversary.value except (AttributeError, IndexError, TypeError): pass try: return helpers.string_to_date(self.vcard.anniversary.value) except (AttributeError, ValueError): try: return helpers.string_to_date(self.vcard.x_anniversary.value) except (AttributeError, ValueError): pass return None"
799,"def hvac_mode(auto, auxHeatOnly, cool, heatOnly, cool, heatOnly, cool, heatOnly, cool, heatOnly, cool, heatOnly, cool, heatOnly, cool, heatOnly, cool, heatOnly, cool, heatOnly, cool, heatOnly, cool, heatOnly, cool, heatOnly, coolOnly, coolOnly, coolOnly, coolOnly, coolOnly, coolOnly, coolOnly)","def set_hvac_mode(self, index, hvac_mode): body = ""selection"": ""selectionType"": ""thermostats"", ""selectionMatch"": self.thermostats[index]['identifier'], ""thermostat"":  ""settings"":  ""hvacMode"": hvac_mode   log_msg_action = ""set HVAC mode"" return self.make_request(body, log_msg_action)"
800,"def get_grid_point(self, grid_point): if self.grid_point(grid_point): self.grid_point(grid_point) self.grid_point(grid_point) self.grid_point(grid_point) self.grid_point(grid_point) self.grid_point(grid_point) self.grid_point(grid_point) self.grid_point(grid_point) self.grid_point(grid_point) self.grid_point(grid_point) self.grid_point(grid_point) self.grid_point) self.grid_point) self.grid_point(grid_point) self.grid_point) self.grid_point(grid_point) self.grid_point) self.grid_point(grid_point) self.grid_point) self.grid_point(grid_point) self.grid_point) self.grid_point) self.grid_point(grid_point) self.grid_point(grid_point) self.grid_point) self.grid_point(grid_point) self.grid_point) self","def occupancy(grid, points, spacing=0.01): distances = ((grid[:,None,:] - points[None,:,:])**2).sum(axis=2) occupied = (distances  spacing).sum(axis=1) return occupied"
801,def start_monitoring_operation_policy_files(self): if self.operation_policy_files(self.operation_policy_files): self.operation_policy_files(self.operation_policy_files) self.operation_policy_files(self.operation_policy_files) self.operation_policy_files(self.operation_policy_files) self.operation_policy_files(self.operation_policy_files) self.operation_policy_files) self.operation_policy_files,"def run(self): self.initialize_tracking_structures() if self.live_monitoring: self.logger.info(""Starting up the operation policy file monitor."") while not self.halt_trigger.is_set(): time.sleep(1) self.scan_policies() self.logger.info(""Stopping the operation policy file monitor."") else: self.scan_policies()"
802,def install_candidate(self): if self.install_candidate(self.install_candidate): self.install_candidate(self.install_candidate) return self.install_candidate(self.install_candidate),"def _link_package_versions(self, link, search): platform = get_platform() version = None if link.egg_fragment: egg_info = link.egg_fragment ext = link.ext else: egg_info, ext = link.splitext() if not ext: self._log_skipped_link(link, 'not a file') return if ext not in SUPPORTED_EXTENSIONS: self._log_skipped_link( link, 'unsupported archive format: %s' % ext) return if ""binary"" not in search.formats and ext == wheel_ext: self._log_skipped_link( link, 'No binaries permitted for %s' % search.supplied) return if ""macosx10"" in link.path and ext == '.zip': self._log_skipped_link(link,'macosx10 one') return if ext == wheel_ext: try: wheel = Wheel(link.filename) except InvalidWheelFilename: self._log_skipped_link(link, 'invalid wheel filename') return if (pkg_resources.safe_name(wheel.name).lower()!= search.canonical): self._log_skipped_link( link, 'wrong project name (not %s)' % search.supplied) return if not wheel.supported(): self._log_skipped_link( link, 'it is not compatible with this Python') return comes_from = getattr(link, ""comes_from"", None) if ( ( not platform.startswith('win') and not platform.startswith('macosx') and not platform == 'cli' ) and comes_from is not None and urllib_parse.urlparse( comes_from.url ).netloc.endswith(PyPI.netloc)): if not wheel.supported("
803,"def render_tag(self, tag): if tag is self.get_value(tag): self.get_value(tag): self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.","def render_tag(self, context, *tag_args, **tag_kwargs): if self.as_var: return BaseAssignmentNode.render_tag(self, context, *tag_args, **tag_kwargs) else: return BaseInclusionNode.render_tag(self, context, *tag_args, **tag_kwargs)"
804,"def parse_headers(self, baz=qux, foo=bar, baz=qux, foo=bar, baz=qux, foo=bar, baz=qux, foo=bar, baz=qux, baz=qux, foo=bar, baz=qux, foo=bar, baz=qux, foo=qux, foo=qux, baz=qux, baz=qux, baz=qux, baz=qux, baz=qux, baz=qux, baz=qux, baz=qux, baz=qux, baz=qux, baz=qux, baz=qux, baz=qux, baz=qux, baz=qux, baz=qux, baz=qux, baz=qux, baz=qux, baz=qux, baz=qux, baz=qux, baz=qu","def parse_headers(self, headers): for name, value in _parse_keyvalue_list(headers): self.headers[name] = value"
805,"def decode_span(self, message, encoding): if self.span(self.span): self.span(self.span): self.span(self.span): self.span(self.span) self.span(self.span) self.span(self.span) self.span(self.span)","def detect_span_version_and_encoding(message): if isinstance(message, six.string_types): if six.PY2: message = six.b(message) else: message = message.encode('utf-8') if len(message)  2: raise ZipkinError(""Invalid span format. Message too short."") if six.byte2int(message) = 16: if six.byte2int(message) == 10 and six.byte2int(message[1:2])!= 0: return Encoding.V2_PROTO3 return Encoding.V1_THRIFT str_msg = message.decode('utf-8') if str_msg[0] == '[': span_list = json.loads(str_msg) if len(span_list) > 0: for span in span_list: if any(word in span for word in _V2_ATTRIBUTES): return Encoding.V2_JSON elif ( 'binaryAnnotations' in span or ( 'annotations' in span and 'endpoint' in span['annotations'] ) ): return Encoding.V1_JSON return Encoding.V2_JSON raise ZipkinError(""Unknown or unsupported span encoding"")"
806,"def get_team_split(self, year): if self.split(self.split(self.split)): self.split(self.split(self.split(self.split))","def off_splits(self, year): doc = self.get_year_doc('_splits'.format(year)) tables = doc('table.stats_table') dfs = [sportsref.utils.parse_table(table) for table in tables.items()] dfs = [ df.assign(split=df.columns[0]).rename(columns=df.columns[0]:'split_value') for df in dfs ] if not dfs: return pd.DataFrame() return pd.concat(dfs).reset_index(drop=True)"
807,"def storm_date(self, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date): storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_date, storm_","def _unpack_storm_date(date): year = (date & 0x7f) + 2000 day = (date >> 7) & 0x01f month = (date >> 12) & 0x0f return ""%s-%s-%s"" % (year, month, day)"
808,"def module_name(self, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name): module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_","def module(self): from warnings import warn warn(DeprecationWarning('modules were deprecated in favor of''blueprints. Use request.blueprint''instead.'), stacklevel=2) if self._is_old_module: return self.blueprint"
809,"def self(self, other, Kraus): if self is not a valid scalar if self is not a valid scalar if self is not a valid scalar if self is not a valid scalar if self is not a valid scalar if self is not a valid scalar if self is not a valid scalar if self is not a valid scalar if self is not","def multiply(self, other): if not isinstance(other, Number): raise QiskitError(""other is not a number"") if isinstance(other, complex) or other  0: return Kraus(Choi(self).multiply(other)) val = np.sqrt(other) kraus_r = None kraus_l = [val * k for k in self._data[0]] if self._data[1] is not None: kraus_r = [val * k for k in self._data[1]] return Kraus((kraus_l, kraus_r), self._input_dim, self._output_dim)"
810,"def treetime_tree(self, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree) treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, treetime_tree, tree","def timetree(params): if params.relax is None: relaxed_clock_params = None elif params.relax==[]: relaxed_clock_params=True elif len(params.relax)==2: relaxed_clock_params='slack':params.relax[0], 'coupling':params.relax[1] dates = utils.parse_dates(params.dates) if len(dates)==0: print(""No valid dates -- exiting."") return 1 if assure_tree(params, tmp_dir='timetree_tmp'): print(""No tree -- exiting."") return 1 outdir = get_outdir(params, '_treetime') gtr = create_gtr(params) infer_gtr = params.gtr=='infer' ########################################################################## ########################################################################## aln, ref, fixed_pi = read_if_vcf(params) is_vcf = True if ref is not None else False branch_length_mode = params.branch_length_mode #variable-site-only trees can have big branch lengths, the auto setting won't work. if is_vcf or (params.aln and params.sequence_length): if branch_length_mode == 'auto': branch_length_mode = 'joint' ########################################################################## ##############"
811,"def read_abaqus_inp_file(self, abaqus_inp_file, abaqus_inp_file, abaqus_inp_file, abaqus_inp_file, abaqus_inp_file, abaqus_inp_file, abaqus_inp_file, abaqus_inp_file, abaqus_inp_file, abaqus_inp_file, abaqus_inp_file, abaqus_inp_file, abaqus_inp_file, abaqus_inp_file, abaqus_inp_file, abaqus_inp_file, abaqus_inp_file, abaqus_inp_file, abaqus_inp_file, abaqus_inp_file, abaqus_inp_file, abaqus_inp_file, abaqus_inp_file, abaqus_inp_file, abaqus_inp_file, abaqus_inp_file, abaqus_inp_file, abaqus_inp_file, aba","def read_inp(path): def lineInfo(line): out = ""type"": ""data"" if line[0] == ""*"": if line[1] == ""*"": out[""type""] = ""comment"" out[""text""] = line[2:] else: out[""type""] = ""command"" words = line[1:].split("","") out[""value""] = words[0].strip() out[""options""] =  for word in words[1:]: key, value = [s.strip() for s in word.split(""="")] out[""options""][key] = value return out def elementMapper(inpeltype): if inpeltype == ""t3d2"": return ""Line2"" if inpeltype[:3] in [""cps"", ""cpe"", ""cax""]: if inpeltype[3] == ""3"": return ""tri3"" if inpeltype[3] == ""4"": return ""quad4"" if inpeltype[:3] in [""c3d""]: if inpeltype[3] == ""4"": return ""tetra4"" if inpeltype[3] == ""5"": return ""pyra5"" if inpeltype[3] == ""6"": return ""prism6"" if inpeltype[3] == ""8"": return ""hexa8"" nlabels = [] coords = [] nsets =  elabels = [] etypes = [] connectivity = [] esets =  surfaces =  lines = np.array([l.strip().lower() for l in open(path).readlines()]) lines = [line for line in lines if len(line)!= 0] env, setlabel = None, None for line in lines: d = lineInfo(line) if d[""type""] == ""command"": env ="
812,"def identifier(self, identifier): if identifier(self, identifier): identifier(self, identifier): identifier(self, identifier) identifier(self, identifier) identifier(self, identifier) identifier(self, identifier) identifier(self, identifier) identifier(self, identifier) identifier(self, identifier) identifier(self, identifier) identifier) identifier(self, identifier) identifier(self, identifier) identifier(self, identifier) identifier) identifier(self, identifier) identifier(self, identifier) identifier(self, identifier) identifier(self, identifier) identifier(self, identifier) identifier(self, identifier) identifier(self, identifier) identifier(self, identifier) identifier(self, identifier) identifier(self, identifier) identifier(self, identifier) identifier(self, identifier) identifier(self, identifier) identifier(self, identifier) identifier(","def rule(self, text): self._attempting(text) return concatenation([ self.identifier, ""="", self.expression, "";"", ], ignore_whitespace=True)(text).retyped(TokenType.rule)"
813,"def check_readme(self, readme): if self.get_readme(self.get_readme): self.get_readme(self.get_readme) self.get_readme(self.get_readme) self.get_readme(self.get_readme) self.get_readme(self.get_readme) self.get_readme(self.get_readme) self.get_readme(self.get_readme) self.get_readme(self.get_readme) self.get_readme(self.get_readme) self.get_readme(self.get_readme) self.get_readme(self.get_readme) self.get_readme(self.get_readme) self.get_readme(self.get_readme) self.get_readme(self.get_readme) self.get_readme(self.get_readme) self.get_readme(self.get_readme) self.get_readme(self.get_readme) self.get_readme(self.get_readme) self.get_readme(self.get_readme) self.get_readme(self.get_readme(self.get","def get_readme(self, repo): readme_contents = repo.readme() if readme_contents is not None: self.total_readmes += 1 return 'MD' if self.search_limit >= 28: print 'Hit search limit. Sleeping for 60 sec.' time.sleep(60) self.search_limit = 0 self.search_limit += 1 search_results = self.logged_in_gh.search_code('readme' + 'in:path repo:' + repo.full_name) try: for result in search_results: path = result.path[1:] if '/' not in path and'readme' in path.lower(): self.total_readmes += 1 return path return 'MISS' except (github3.models.GitHubError, StopIteration) as e: return 'MISS'"
814,"def _value_and_gradients_function(self, value_and_gradients_function, f_lim, iteration, failed, failed, failed, failed, num_evals, f_lim, f_lim, iteration, f_lim, f_lim, iteration, iteration): if f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim, f_lim","def bisect(value_and_gradients_function, initial_left, initial_right, f_lim): failed = is_finite(initial_left, initial_right) needs_bisect = (initial_right.df  0) & (initial_right.f > f_lim) bisect_args = _IntermediateResult( iteration=tf.convert_to_tensor(value=0), stopped=failed | needs_bisect, failed=failed, num_evals=tf.convert_to_tensor(value=0), left=initial_left, right=initial_right) return _bisect(value_and_gradients_function, bisect_args, f_lim)"
815,"def build_error(self, url_for, url_for, url_for, url_for, url_for, url_for, url_for, url_for, url_for, url_for, url_for, url_for, url_for, url_for, url_for, url_for, url_for, url_for, url_for, url_for): url_for, url_for, url_for, url_for, url_for, url_for, url_for, url_for, url_for, url_for, url_for, url_for)","def handle_url_build_error(self, error, endpoint, values): exc_type, exc_value, tb = sys.exc_info() for handler in self.url_build_error_handlers: try: rv = handler(error, endpoint, values) if rv is not None: return rv except BuildError as error: pass if error is exc_value: reraise(exc_type, exc_value, tb) raise error"
816,"def complete(self, name_in_ns, name_in_ns, name_in_ns, name_in_ns, name_in_ns, name_in_ns, name_in_ns, name_in_ns, name_in_ns, name_in_ns, name_in_ns, name_in_ns, name_in_ns, name_in_ns, name_in_ns, name_in_ns)","def __complete_alias( self, prefix: str, name_in_ns: Optional[str] = None ) -> Iterable[str]: candidates = filter( Namespace.__completion_matcher(prefix), [(s, n) for s, n in self.aliases] ) if name_in_ns is not None: for _, candidate_ns in candidates: for match in candidate_ns.__complete_interns( name_in_ns, include_private_vars=False ): yield f""prefix/match"" else: for alias, _ in candidates: yield f""alias/"""
817,"def template_package(self, template): if self.package(self.package): self.package(self.package): self.package(self.package) self.package(self.package) self.package(self.package) self.package(self.package) self.package(self.package) self.package(self.package) self.package(self.package) self.package(self.package) self.package) self.package) self.package(self.package) self.package) self.package(self.package) self.package) self.package(self.package) self.package) self.package(self.package) self.package) self.package(self.package) self.package) self.package) self.package(self.package) self.package) self.package(self.package) self.package) self.package(self.package) self.package) self.package(self.package) self.package(self.package) self.package) self.package(self.package) self.package(self.package) self.pack","def get_form_kwargs(self): kwargs = super(FormContainersMixin, self).get_form_kwargs() kwargs.update( 'pack': ""foundation-"".format(self.kwargs.get('foundation_version')) ) return kwargs"
818,def main_program(self): if self.program(self.program): self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program) self.program(self.program) self.program(self.program) self.program) self.program(self.program) self.program(self.program) self.program) self.program(self.program) self.program(self.program) self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program) self.program(self.program) self.program(self.program) self,"def main(argv): global g_script_name global g_parse_log_path g_script_name = os.path.basename(argv[0]) parse_args(argv) if (g_parse_log_path is None): print("""") print(""ERROR: -f not specified"") usage() d = Dataset(g_parse_log_path) d.parse() d.emit_header() for i in range(0, g_num_rows): d.emit_one_row()"
819,"def amari_alpha_of_u(self_normalized, alpha, self_normalized, self_normalized, self_normalized, self_normalized, self_normalized, self_normalized, self_normalized, self_normalized, self_normalized, self_normalized, self_normalized, self_normalized, self_normalized, self_normalized, self_normalized, self_normalized, self_normalized, self_normalized, self_normalized, self_normalized, self_normalized, self_normalized)","def amari_alpha(logu, alpha=1., self_normalized=False, name=None): with tf.compat.v1.name_scope(name, ""amari_alpha"", [logu]): if alpha is None or tf.is_tensor(alpha): raise TypeError(""alpha cannot be None or Tensor type."") if (self_normalized is None or tf.is_tensor(self_normalized)): raise TypeError(""self_normalized cannot be None or Tensor type."") logu = tf.convert_to_tensor(value=logu, name=""logu"") if alpha == 0.: f = -logu elif alpha == 1.: f = tf.exp(logu) * logu else: f = tf.math.expm1(alpha * logu) / (alpha * (alpha - 1.)) if not self_normalized: return f if alpha == 0.: return f + tf.math.expm1(logu) elif alpha == 1.: return f - tf.math.expm1(logu) else: return f - tf.math.expm1(logu) / (alpha - 1.)"
820,"def _initPIN(self, pin): if self.initPIN(pin): self.initPIN(pin): self.initPIN(pin) if self.initPIN(pin): self.initPIN(pin)","def initPin(self, pin): new_pin1 = ckbytelist(pin) rv = self.lib.C_InitPIN(self.session, new_pin1) if rv!= CKR_OK: raise PyKCS11Error(rv)"
821,"def add_item_metadata(self, handle, handle, handle): if handle = self.add_item_metadata(self.add_item_metadata, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle, handle)","def get_item_metadata(self, handle): metadata =  identifier = generate_identifier(handle) prefix = self.fragments_key_prefix + ''.format(identifier) blob_generator = self._blobservice.list_blobs( self.uuid, include='metadata', prefix=prefix ) for blob in blob_generator: metadata_key = blob.name.split('.')[-2] value_as_string = self.get_text(blob.name) value = json.loads(value_as_string) metadata[metadata_key] = value return metadata"
822,def _translate_int(self): if self.get_int(self.get_int(self.get_int(self.get_int(self.get_int(self.get_int(self.get_int(self.get_int(self.get_int(self.get_int(self.get_int(self.get_int(self.get_int(self.get_int(self.get_int(self.get_int(self.get_int(self.get_int(self.get_int(self.get_int(self.get_int(self.get_int(self.get_int)))))))))))),"def f_translate_key(self, key): if isinstance(key, int): if key == 0: key = self.v_name else: key = self.v_name + '_%d' % key return key"
823,"def boolean(self, boolean): self.boolean(self.boolean) self.boolean(self.boolean) self.boolean(self.boolean) self.boolean(self.boolean) self.boolean(self.boolean) self.boolean(self.boolean) self.boolean(self.boolean) self.boolean(self.boolean) self.boolean(self.boolean) self.boolean(self.boolean) self.boolean) self.boolean) self.boolean(self.boolean) self.boolean(self.boolean) self.boolean) self.boolean) self.boolean(self.boolean) self.boolean) self.boolean(self.boolean) self.boolean) self.boolean(self.boolean) self.boolean(self.boolean) self.boolean(self.boolean) self.boolean(self.boolean) self.boolean(self.boolean) self.boolean(self.boolean) self.boolean(self.","def starts_with(ctx, full, part): full = next(string_arg(ctx, full), '') part = next(string_arg(ctx, part), '') yield full.startswith(part)"
824,"def open_documentation(self, documentation): self.documentation(documentation) self.documentation(documentation) self.documentation(documentation) self.documentation(documentation) self.documentation(documentation) self.documentation(documentation) self.documentation(documentation) self.documentation) self.documentation(documentation) self.documentation(documentation) self.documentation) self.documentation(documentation) self.documentation(documentation) self.documentation) self.documentation(documentation) self.documentation(documentation) self.documentation) self.documentation(documentation) self.documentation) self.documentation(documentation) self.documentation(documentation) self.documentation) self.documentation(documentation) self.documentation(documentation) self.documentation) self.documentation(documentation) self.documentation(documentation) self.documentation) self.documentation(documentation) self.documentation(documentation) self.documentation) self.documentation(documentation) self.documentation(documentation) self.documentation(documentation) self.documentation) self.documentation(documentation) self.documentation(documentation) self.documentation(documentation) self.documentation(documentation) self.documentation)","def open_documentation(self): window = Window() html = QtCore.QUrl.fromLocalFile(os.path.join(os.getcwd(), './docs/_build/html/index.html')) #open('./docs/_build/html/index.html').read() #window.show() window.view.load(html) window.show() window.exec_()"
825,"def authenticate_this_page(self, notebook_list, print_view, notebook_list, notebook_list, print_view, notebook_list, notebook_list, notebook_list, print_view, notebook_list, notebook_list, print_view, notebook_list, notebook_list, notebook_list, print_view, notebook_list, notebook_list, print_view, read_only_view, read_only_view, read_only_view, read_only_view, read_only_view, read_only_view, notebook_list, print_view, notebook_list, notebook_list, print_view, notebook_list, notebook_list, notebook_list, print_view, notebook_list, notebook_list, notebook_list, notebook_list, notebook_list, notebook_list, notebook_list, notebook_list, notebook_list, notebook_list, notebook_list, notebook_list, notebook_list, notebook_list, notebook_list, notebook_list, notebook_list, notebook_list, notebook_list, notebook_list, notebook_list, notebook_list, notebook_list, notebook_list, notebook_list, notebook_list, notebook_list, notebook_list, print_list, print_list, print_list, print_list, print_list, print_list, print_list,","def authenticate_unless_readonly(f, self, *args, **kwargs): @web.authenticated def auth_f(self, *args, **kwargs): return f(self, *args, **kwargs) if self.application.read_only: return f(self, *args, **kwargs) else: return auth_f(self, *args, **kwargs)"
826,def add_days(days): ds_add(days): ds_add(days): ds_add(days): ds_add(days): ds_add(days): ds_add(days): ds_add(days): ds_add(days): ds_add(days): ds_add(days): ds_add(days): ds_add(days): ds_add(days): ds_add(days): ds_add(days): ds_add(days): ds_add(days): ds_add(days): ds_add(days): ds_add(days): ds_add(days): ds_add(days): ds_add(days): ds_add(days): ds_add(days): ds_add(days): ds_add(days): ds_add(days): ds_add(days,"def ds_add(ds, days): ds = datetime.strptime(ds, '%Y-%m-%d') if days: ds = ds + timedelta(days) return ds.isoformat()[:10]"
827,"def update_inert(self, _update_inert, _update_inert, _update_inert, _update_inert, _update_inert, _update_inert, _update_inert, _update_inert, _update_inert, _update_inert, _update_inert, _update_inert, _update_inert, _update_inert, _update_inert, _update_inert, _update_inert, _update_inert, _update_inert, _update_inert, _update_inert, _update_inert, _update_inert, _update_inert)","def _update_value_inert(self, index, value): field_descriptor = self._table._dev_descriptor.get_field_descriptor(index) value = field_descriptor.deserialize(value, index) if isinstance(value, Link): current_link = self._data.get(index) if current_link is not None: current_link.unregister() if isinstance(value, RecordHook): current_record_hook = self._data.get(index) if current_record_hook is not None: current_record_hook.unregister() if isinstance(value, ExternalFile): current_external_file = self._data.get(index) if current_external_file is not None: current_external_file._dev_unregister() if value in (None, NONE_RECORD_HOOK, NONE_LINK, NONE_EXTERNAL_FILE): self._dev_set_none_without_unregistering(index, check_not_required=False) return old_hook = None if index == 0 and not self._table._dev_auto_pk: old_hook = self._data.get(0) self._data[index] = value if old_hook is not None: self._table._dev_record_pk_was_updated(old_hook.target_value)"
828,"def store metric in data tree and calc offset signs sign  0 is CYAN, sign > 1 is YELLOW, sign == 0 is WHITE, sign == 0 is WHITE, sign == 0 is WHITE, sign == 0 is WHITE, sign == 0 is WHITE, sign == 0 is WHITE, sign == 0 is WHITE, sign == 0 is WHITE, sign == WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE = WHITE =","def __handle_data_items(self, host, data): for metric, value in data.iteritems(): if value == '': self.sign[host][metric] = -1 self.data[host][metric] = value else: if not self.data[host].get(metric, None): self.sign[host][metric] = 1 elif float(value) > float(self.data[host][metric]): self.sign[host][metric] = 1 elif float(value)  float(self.data[host][metric]): self.sign[host][metric] = -1 else: self.sign[host][metric] = 0 self.data[host][metric] = ""%.2f"" % float(value)"
829,"def get_web3_provider(self, provider, timeout): self.get_web3_provider(self.get_web3_provider) self.get_web3_provider(self.get_web3_provider) self.get_web3_provider(self.get_web3_provider) self.get_web3_provider(self.get_web3_provider) self.get_web3_provider(self.get_web3_provider) self.get_web3_provider) self.get_web3_provider(self.get_web3_provider) self.get_web3_provider(self.get_web3_provider) self.get_web3_provider(self.get_web3_provider) self.get_web3_provider(self.get_web3_provider) self.get_web3_provider(self.get_web3_provider) self.get_timeout(self.get_timeout) self.get_timeout(self.get_timeout) self.get_timeout(self.get_timeout) self.get_timeout(self.get_timeout) self.get_timeout_timeout_timeout_timeout","def get_slow_provider(self, timeout: int): if isinstance(self.w3_provider, AutoProvider): return HTTPProvider(endpoint_uri='http://localhost:8545', request_kwargs='timeout': timeout) elif isinstance(self.w3_provider, HTTPProvider): return HTTPProvider(endpoint_uri=self.w3_provider.endpoint_uri, request_kwargs='timeout': timeout) else: return self.w3_provider"
830,"def pull_requests(self, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull","def _reqs(self, tag): return [ (tag, i) for i in self.client.get_pulls(*tag.split('/')) ]"
831,def handle_request(self): self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request) self.request(self.request) self.request) self.request(self.request) self.request) self.request(self.request) self.request) self.request(self.request) self.request) self.request(self.request) self.request) self.request(self.request) self.request) self.request(self.request) self.request(self.request) self.request) self.request(self.request) self.request(self.request) self.request) self.request(self.request) self.request,"async def request(self, method, url, **kwargs): rate_limiter = RateLimiter(max_calls=59, period=60, callback=limited) async with rate_limiter: if not self.token: raise UnauthorizedDetected('UnauthorizedDetected (status code: 401): No TOKEN provided') headers =  'User-Agent': self.user_agent, 'Content-Type': 'application/json'  if 'json' in kwargs: kwargs['data'] = to_json(kwargs.pop('json')) kwargs['headers'] = headers headers['Authorization'] = self.token for tries in range(5): async with self.session.request(method, url, **kwargs) as resp: log.debug('%s %s with %s has returned %s', method, url, kwargs.get('data'), resp.status) data = await json_or_text(resp) if 300 > resp.status >= 200: return data if resp.status == 429: fmt = 'We are being rate limited. Retrying in %.2f seconds (%.3f minutes).' retry_after = json.loads(resp.headers.get('Retry-After')) mins = retry_after / 60 log.warning(fmt, retry_after, mins) is_global = True if is_global: self._global_over.clear() await asyncio.sleep(retry_after, loop=self.loop) log.debug('Done sleeping for the rate limit. Retrying...') if is_global: self._global_over.set() log.debug('Global rate limit is now"
832,"def _print_variables(self, variables): if self.get_variables(variables): self.get_variables(variables): self.get_variables(variables): self.get_variables(variables)","def print_variables_info(self, output_file=sys.stdout): table = (' name | type size n' + '---------+-------------------------n') for name, var_info in list(self.variables.items()): table += ':>8 | :>6!s:10n'.format(name, var_info[0], var_info[1]) print(prefix_indent('variables: ', table), file=output_file)"
833,"def run_process(self, file_path, pickle_dags, pickle_dags, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies, zombies,","def _launch_process(result_queue, file_path, pickle_dags, dag_id_white_list, thread_name, zombies): def helper(): log = logging.getLogger(""airflow.processor"") stdout = StreamLogWriter(log, logging.INFO) stderr = StreamLogWriter(log, logging.WARN) set_context(log, file_path) try: sys.stdout = stdout sys.stderr = stderr settings.configure_orm() threading.current_thread().name = thread_name start_time = time.time() log.info(""Started process (PID=%s) to work on %s"", os.getpid(), file_path) scheduler_job = SchedulerJob(dag_ids=dag_id_white_list, log=log) result = scheduler_job.process_file(file_path, zombies, pickle_dags) result_queue.put(result) end_time = time.time() log.info( ""Processing %s took %.3f seconds"", file_path, end_time - start_time ) except Exception: log.exception(""Got an exception! Propagating..."") raise finally: sys.stdout = sys.__stdout__ sys.stderr = sys.__stderr__ settings.dispose_orm() p = multiprocessing.Process(target=helper, args=(), name=""-Process"".format(thread_name)) p.start() return p"
834,"def config_provider(self, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, id, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict, dict,","def id_to_name(config, short_name): for k, v in list(config.items()): if v.get('id') == short_name: return k raise Exception( 'No provider with id=0 found in the config!'.format(short_name))"
835,"def overrides(self, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method)","def action(self): commandline = ""0 1"".format(self.command, "" "".join(self.arguments)) try: completed_process = subprocess.run(commandline, shell=True) self.exit_status = completed_process.returncode except AttributeError: self.exit_status = subprocess.call(commandline, shell=True)"
836,"def concurrence(self, state, np.array): concurrence = self.state(state, np.array)","def concurrence(state): rho = np.array(state) if rho.ndim == 1: rho = outer(state) if len(state)!= 4: raise Exception(""Concurrence is only defined for more than two qubits"") YY = np.fliplr(np.diag([-1, 1, 1, -1])) A = rho.dot(YY).dot(rho.conj()).dot(YY) w = la.eigh(A, eigvals_only=True) w = np.sqrt(np.maximum(w, 0)) return max(0.0, w[-1] - np.sum(w[0:-1]))"
837,"def check_user(user, str): if str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = str = st","def checkUser(self, user): return not self.conn(""POST"", ""0/GetCredentialType.srf"".format(SkypeConnection.API_MSACC), json=""username"": user).json().get(""IfExistsResult"")"
838,"def Convenience(self, features, class_labels, n_samples, n_features, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_samples, n_sa","def fit_predict(self, features, class_labels): self.fit(features, class_labels) return self.predict(features)"
839,"def serialize_dictionary(self, descriptor): self.get_dictionary(descriptor): self.get_dictionary(descriptor) self.get_dictionary(descriptor) self.get_dictionary(descriptor) self.get_dictionary(descriptor) self.get_dictionary(descriptor) self.get_dictionary(descriptor) self.get_dictionary(descriptor) self.get_dictionary(descriptor) self.get_dictionary(descriptor) self.get_dictionary(descriptor) self.get_dictionary(descriptor) self.get_dictionary(descriptor) self.get_dictionary(descriptor) self.get_dictionary(descriptor) self.get_dictionary(descriptor) self.get_dictionary(descriptor) self.get_dictionary(descriptor) self.get_dictionary(descriptor) self.get_dictionary(descriptor) self.get_dictionary(descriptor) self.get_dictionary(descriptor) self.get_dictionary(descriptor) self.get_dictionary(descriptor) self.get_dictionary(descriptor) self.get_dictionary(de","def to_json(self): d, ps = self._to_json() if len(ps) == 0: return ""name"": d else: return ""name"": d, ""args"": ps"
840,"def extract(self, arguments): self.get(self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get(self.get) self.get(self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.","def get_arguments(self): ApiCli.get_arguments(self) if self.args.metric_name is not None: self._metric_name = self.args.metric_name self.path = ""v1/metrics/0"".format(self._metric_name)"
841,"def push_decorator(self, decorator): self.decorator(decorator) self.decorator(decorator) self.decorator(decorator) self.decorator(decorator) self.decorator(decorator) self.decorator(decorator) self.decorator(decorator) self.decorator(decorator) self.decorator) self.decorator(decorator) self.decorator(decorator) self.decorator) self.decorator(decorator) self.decorator(decorator) self.decorator) self.decorator(decorator) self.decorator) self.decorator(decorator) self.decorator) self.decorator(decorator) self.decorator(decorator) self.decorator) self.decorator(decorator) self.decorator(decorator) self.decorator(decorator) self.decorator) self.decorator(decorator) self.decorator(decorator) self.decorator(decorator) self.decorator) self.decorator(decorator) self.decorator) self","def do_call(self, parser: BasicParser) -> Node: valueparam = [] for v, t in self.param: if t is Node: valueparam.append(parser.rule_nodes[v]) elif type(v) is t: valueparam.append(v) else: raise TypeError( ""Type mismatch expected  got "".format(t, type(v))) if not self.checkParam(self.decorator_class, valueparam): return False decorator = self.decorator_class(*valueparam) global _decorators _decorators.append(decorator) res = self.pt(parser) _decorators.pop() return res"
842,"def fann_output(self, parallel_step_output, parallel_step_output, join_step_output, join_step_output, join_step_output, join_step_output, join_step_output, join_step_output, join_step_output, join_step_output, join_step_output, join_step_output, join_step_output, join_step_output, join_step_output, join_step_output, join_step_output, join_step_output, join_step_output, join_step_output, join_step_output, join_step_output, join_step_output, join_step_output, join_step_output, join_step_output, join_step_output, join_step_output, parallel_step_output, parallel_step_output, parallel_step_output, parallel_step_output, parallel_step_output, parallel_step_output, parallel_step_output, parallel_step_output, parallel_step_output, parallel_step_output, parallel_step_output, parallel_step_output, parallel_step_output, parallel_","def create_joining_subplan( pipeline_def, solid, join_step_key, parallel_steps, parallel_step_output ): check.inst_param(pipeline_def, 'pipeline_def', PipelineDefinition) check.inst_param(solid,'solid', Solid) check.str_param(join_step_key, 'join_step_key') check.list_param(parallel_steps, 'parallel_steps', of_type=ExecutionStep) check.str_param(parallel_step_output, 'parallel_step_output') for parallel_step in parallel_steps: check.invariant(parallel_step.has_step_output(parallel_step_output)) join_step = create_join_step( pipeline_def, solid, join_step_key, parallel_steps, parallel_step_output ) output_name = join_step.step_outputs[0].name return ExecutionValueSubplan( parallel_steps + [join_step], StepOutputHandle.from_step(join_step, output_name) )"
843,"def tautomer_parent(mol, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, mol, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize, skip_standardize,","def tautomer_parent(self, mol, skip_standardize=False): if not skip_standardize: mol = self.standardize(mol) tautomer = self.canonicalize_tautomer(mol) tautomer = self.standardize(tautomer) return tautomer"
844,"def public_tokens(self, dict): if self.tokens(tokens): self.tokens(tokens): self.tokens(tokens)","def list_tokens(self): url = self.url() + ""/nd/resource/public/token/"" req = self.remote_utils.get_url(url) if req.status_code is not 200: raise RemoteDataNotFoundError('Coud not find '.format(req.text)) else: return req.json()"
845,"def get_stats(self, table_id, type): if self.get_stats(table_id, type): self.get_stats(table_id, type): self.get_stats(table_id, type): self.get_stats(table_id, type)","def _get_stats_table(self, table_id, kind='R', summary=False): doc = self.get_main_doc() table_id = 'table#'.format( 'playoffs_' if kind == 'P' else '', table_id) table = doc(table_id) df = sportsref.utils.parse_table(table, flatten=(not summary), footer=summary) return df"
846,"def terminate_line(self, line): if self.get_line(line): self.get_line(line) return self.get_line(line)","def prefix_line_terminator(self, data): for t in self.LINE_TERMINATORS: if data.startswith(t): return t return None"
847,"def __init__(self, __init__, __init__, __init__, __init__, __init__, __init__, __init__, __init__, __init__, __init__, __init__, __init__, __init__, __init__, __init__, _init__, _init__, _init__, _init__, _init__, _init__, _init__, _init__, _init__, _init__, _init__, _init__, _init__, _init__, _init__, _init__, _init__, _init__, _init__, _init__, _init__, _init__, _init__, _init__","def create_init(attrs):.format(args=args, attr_lines=attr_lines, vals=vals) exec(init_code, globals()) return _init"
848,"def create_string(self, human_readable, human_readable, human_readable, human_readable, human_readable, human_readable, human_readable, human_readable, human_readable, human_readable, human_readable, human_readable, human_readable, human_readable, human_readable, human_readable, human_readable, human_readable, human_readable, human_readable, human_readable, human_readable, human_readable, human_readable, human_readable, human_readable, human_readable, human_readable, human_readable)","def lrepr(o, human_readable: bool = False) -> str: core_ns = Namespace.get(sym.symbol(CORE_NS)) assert core_ns is not None return lobj.lrepr( o, human_readable=human_readable, print_dup=core_ns.find(sym.symbol(_PRINT_DUP_VAR_NAME)).value, print_length=core_ns.find( sym.symbol(_PRINT_LENGTH_VAR_NAME) ).value, print_level=core_ns.find( sym.symbol(_PRINT_LEVEL_VAR_NAME) ).value, print_meta=core_ns.find(sym.symbol(_PRINT_META_VAR_NAME)).value, print_readably=core_ns.find( sym.symbol(_PRINT_READABLY_VAR_NAME) ).value, )"
849,"def draw_Y-axis_guidelines(Y-axis_guidelines, Y-axis_guidelines, Y-axis_guidelines, Y-axis_guidelines, Y-axis_guidelines, Y-axis_guidelines, Y-axis_guidelines, Y-axis_guidelines, Y-axis_guidelines, Y-axis_guidelines, Y-axis_guidelines, Y-axis_guidelines, Y-axis_guidelines, Y-axis_guidelines, Y-axis_guidelines, Y-axis_guidelines, Y-axis_guidelines, Y-axis_guidelines, Y-axis_guidelines, Y-axis_guidelines, Y-axis_guidelines, Y-axis_guidelines)","def draw_y_guidelines(self, label_height, count): ""Draw the Y-axis guidelines"" if not self.show_y_guidelines: return for count in range(1, count): move = 'M 0 start hstop'.format( start=self.graph_height - label_height * count, stop=self.graph_width, ) path = 'd': move, 'class': 'guideLines' etree.SubElement(self.graph, 'path', path)"
850,"def rename(self, hstore_keys): self.rename(hstore_keys) self.rename(hstore_keys) self.rename(hstore_keys) self.rename(hstore_keys) self.rename(hstore_keys) self.rename(hstore_keys) self.rename(hstore_keys) self.rename(hstore_keys) self.rename(hstore_keys) self.rename(hstore_keys) self.rename(hstore_keys) self.rename(hstore_keys) self.rename(hstore_keys) self.rename(hstore_keys) self.rename(hstore_keys) self.rename(hstore_keys) self.rename(hstore_keys) self.rename(hstore_keys) self.rename(hstore_keys) self.rename(hstore_keys) self.rename(hstore_keys) self.rename(hstore_keys) self.rename(hstore_keys) self.rename(hstore_keys) self.rename(hstore_keys) self.rename(hstore_","def _rename_hstore_unique(self, old_table_name, new_table_name, old_field, new_field, keys): old_name = self._unique_constraint_name( old_table_name, old_field, keys) new_name = self._unique_constraint_name( new_table_name, new_field, keys) sql = self.sql_hstore_unique_rename.format( old_name=self.quote_name(old_name), new_name=self.quote_name(new_name) ) self.execute(sql)"
851,"def get_logging_level(self, MESSAGELEVEL, MESSAGELEVEL, MESSAGELEVEL, MESSAGELEVEL, MESSAGELEVEL, MESSAGELEVEL, MESSAGELEVEL, MESSAGELEVEL, MESSAGELEVEL, MESSAGELEVEL, MESSAGELEVEL, MESSAGELEVEL, MESSAGELEVEL, MESSAGELEVEL, MESSAGELEVEL, MESSAGELEVEL, MESSAGELEVEL, MESSAGELE","def get_logging_level(): level = os.environ.get(""MESSAGELEVEL"", INFO) if isinstance(level, int): return level if level == ""CRITICAL"": return CRITICAL elif level == ""ABORT"": return ABORT elif level == ""ERROR"": return ERROR elif level == ""WARNING"": return WARNING elif level == ""LOG"": return LOG elif level == ""INFO"": return INFO elif level == ""QUIET"": return QUIET elif level.startswith(""VERBOSE""): return VERBOSE3 elif level == ""LOG"": return LOG elif level == ""DEBUG"": return DEBUG return level"
852,"def get_bot_id(self, bot_id): self.get_bot_id(bot_id) self.get_bot_id(bot_id) self.get_bot_id(bot_id) self.get_bot_id(bot_id) self.get_bot_id(bot_id) self.get_bot_id(bot_id) self.get_bot_id(bot_id) self.get_id(bot_id) self.get_id(bot_id) self.get_id(bot_id) self.get_id) self.get_id(bot_id) self.get_id(bot_id) self.get_id(bot_id) self.get_id) self.get_id(bot_id) self.get_id(bot_id) self.get_id) self.get_id(bot_id) self.get_id) self.get_id(bot_id) self.get_id(bot_id) self.get_id(bot_id) self.get_id) self.get_i","async def get_bot_info(self, bot_id): resp = await self.request('GET', '/bots/'.format(self.BASE, bot_id)) resp['date'] = datetime.strptime(resp['date'], '%Y-%m-%dT%H:%M:%S.%fZ') for k in resp: if resp[k] == '': resp[k] = None return resp"
853,"def generate_topics(n_topics, n_words, n_iters, alpha, beta, weights_df, keys_df, weights_df, weights_df, keys_df, weights_df, weights_df, keys_df, weights_df, weights_df, keys_df, weights_df, keys_df, weights_df, weights_df, weights_df, keys_df, keys_df, keys_df, keys_df, keys_df, keys_df, keys_df, keys_df, keys_df, keys_df, keys_df, keys_df, keys_df, keys_df, keys_df, keys_df, keys_df, keys_df, keys_df, keys_df, keys_df, keys_df, keys_df, keys_df, keys_df, keys_df, keys_df, keys_df, weights_df, keys_df, keys_df, keys_df, keys_df, keys_df, keys_df, keys_d","def run_lda(abstracts, n_topics=50, n_words=31, n_iters=1000, alpha=None, beta=0.001): if abstracts.index.name!= 'pmid': abstracts.index = abstracts['pmid'] resdir = os.path.abspath(get_resource_path()) tempdir = os.path.join(resdir, 'topic_models') absdir = os.path.join(tempdir, 'abstracts') if not os.path.isdir(tempdir): os.mkdir(tempdir) if alpha is None: alpha = 50. / n_topics if not os.path.isdir(absdir): print('Abstracts folder not found. Creating abstract files...') os.mkdir(absdir) for pmid in abstracts.index.values: abstract = abstracts.loc[pmid]['abstract'] with open(os.path.join(absdir, str(pmid) + '.txt'), 'w') as fo: fo.write(abstract) print('Generating topics...') mallet_bin = join(dirname(dirname(__file__)),'resources/mallet/bin/mallet') import_str = ('mallet import-dir''--input absdir''--output outdir/topic-input.mallet''--keep-sequence''--remove-stopwords').format(mallet=mallet_bin, absdir=absdir, outdir=tempdir) train_str = ('mallet train-topics''--input out/topic-input.mallet''--num-topics n_topics"
854,def parse_lines(iterable(str)): lines(iterable(str)): lines(iterable(str)): lines(iterable(str)): lines(iterable(str)): lines(iterable(str)): lines(iterable(str)): lines(iterable(str)): lines(iterable(str)),"def parse_ensembl_exons(lines): header = [] LOG.debug(""Parsing ensembl exons..."") for index, line in enumerate(lines): if index == 0: header = line.rstrip().split('t') continue exon_info = parse_ensembl_line(line, header) chrom = exon_info['chrom'] start = exon_info['exon_start'] end = exon_info['exon_end'] transcript = exon_info['ensembl_transcript_id'] gene = exon_info['ensembl_gene_id'] rank = exon_info['exon_rank'] strand = exon_info['strand'] if strand == 1: start = max(start, exon_info.get('utr_5_end') or -1) end = min(end, exon_info.get('utr_3_start') or float('inf')) elif strand == -1: start = max(start, exon_info.get('utr_3_end') or -1) end = min(end, exon_info.get('utr_5_start') or float('inf')) exon_id = ""-"".join([chrom, str(start), str(end)]) if start > end: raise ValueError(""ERROR: %s"" % exon_id) data =  ""exon_id"": exon_id, ""chrom"": chrom, ""start"": start, ""end"": end, ""transcript"": transcript, ""gene"": gene, ""rank"": rank,  yield data"
855,"def channel_name(self, data, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts,","def _handle_account(self, data, ts): chan_id, channel_short_name, *data = data entry = (channel_short_name, data, ts) self.account.put(entry)"
856,"def split_api_file_path(self, directory, name): if self.get_api_file_path(directory): self.get_api_file_path(directory): self.get_api_file_path(directory): self.get_api_file_path(directory): self.get_api_path(directory)","def split_api_filepath(path): parts = path.rsplit('/', 1) if len(parts) == 1: name = parts[0] dirname = '/' else: name = parts[1] dirname = parts[0] + '/' return from_api_dirname(dirname), name"
857,"def refresh_node(self, node_id, connection_id, node_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_id, connection_","def refresh_session(self, node_id=None): if not node_id: node_id = self.conn.id self.conn.client.hset(self.nodelist_key, node_id, int(time.time() * 1000.))"
858,"def normalize_syntaxError(syntaxError): if syntaxError is not syntaxError, syntaxError is not syntaxError(syntaxError): if syntaxError is not syntaxError, syntaxError is not syntaxError, syntaxError(syntaxError)): syntaxError is not syntaxError is not syntaxError is not syntaxError is not syntaxError is not syntaxError is not syntaxError is not syntaxError is not syntaxError is not syntaxError is not syntaxError is not syntaxError is not syntaxError: syntaxError: syntaxError: syntaxError: syntaxError: syn","def normalize_exception(exc): subs =  'unexpected EOF while parsing': 'invalid syntax', 'parenthesis is never closed': 'invalid syntax',  exc.filename = None exc.lineno = None exc.msg = subs.get(exc.msg, exc.msg) return exc"
859,def _transfer_client(self): if self.transfer_client(self.transfer_client): self.transfer_client(self.transfer_client): self.transfer_client(self.transfer_client),"def init_transfer_client(self): if self._tokens_need_update(): self._update_tokens() access_token = self.transfer['access_token'] authorizer = globus_sdk.RefreshTokenAuthorizer( self.transfer['refresh_token'], self._client, access_token=self.transfer['access_token'], expires_at=self.transfer['expires_at_seconds']) self.transfer_client = globus_sdk.TransferClient(authorizer=authorizer)"
860,"def _accuracy_matrix(self, confusion matrix): if self.aalto.fi(self.aalto.fi): self.aalto.fi(self.aalto.fi): self.aalto.fi(self.aalto.fi)","def accuracy(conf_matrix): total, correct = 0.0, 0.0 for true_response, guess_dict in conf_matrix.items(): for guess, count in guess_dict.items(): if true_response == guess: correct += count total += count return correct/total"
861,"def change_keybind(self, kbname): kbname = self.keybind(kbname) self.keybind(kbname) self.keybind(kbname) self.keybind(kbname) self.keybind(kbname) self.keybind(kbname) self.keybind(kbname) self.keybind(kbname) self.keybind(kbname) self.keybind(kbname) self.keybind(kbname)","def changeKeybind(self,kbname,combo): for key,value in self.keybinds.items(): if kbname in value: del value[value.index(kbname)] break if combo not in self.keybinds: self.keybinds[combo]=[] self.keybinds[combo].append(kbname) self.peng.sendEvent(""peng3d.keybind.change"",""peng"":self.peng,""kbname"":kbname,""combo"":combo)"
862,"def clear_all_filters(self, filter): if filter is not self._filters, filter is not self._filters, filter is not self._filters, filter is not self._filters, filter is not self._filters, filter is not self._filters, filter is not self._filters, filter is not self._filters, filter is not self._filters, filter is not self._filters, filter is not self._filters",def clear(self): self.components =  self.info =  self.params =  self.switches =  self.keys =  self.index =  self.sets =  self.maxset = -1 self.n = 0 for a in self.analytes: self.switches[a] =  return
863,"def locate_webelement(self, WebElement, cache(), cache(), cache(), cache(), cache(), cache(), cache(), cache(), cache(), cache(), cache(), cache(), cache(), cache(), cache(), cache(), cache(), cache(), cache(), cache(), cache(), cache(), cache(), cache(), cache(), cache(), cache(), cache(), cache(), cache(), cache(), cache(), cache(), cache(), cache(), cache()","def locate(self): if self._web_element: return self._web_element else: locator_type, locator_value = self.__locator element = self.driver.find_element(by=locator_type, value=locator_value) self._cache_web_element(element) return element"
864,"def delete_instance(self, metaclass): if self.get_instance(self.get_instance(self.get_instance(self.get_instance(self.get_instance(self.get_instance(self.get_instance(self.get_instance(self.get_instance(self.get_instance(self.get_instance(self.get_instance(self.get_instance(self.get_instance(self.get_instance_instance_instance_instance_instance_instance_instance_instance_instance_instance_instance_instance_instance_instance_instance_instance_instance_instance_instance)))))","def delete(instance, disconnect=True): if not isinstance(instance, Class): raise DeleteException(""the provided argument is not an xtuml instance"") return get_metaclass(instance).delete(instance, disconnect)"
865,"def AST_COMMENTARY(self, AST_COMMENTARY, AST_COMMENTARY, AST_COMMENTARY, AST_COMMENTARY, AST_COMMENTARY, AST_COMMENTARY, AST_COMMENTARY, AST_COMMENTARY, AST_COMMENTARY)",def comments(self): if self._comments is None: self._comments = [c for c in self.grammar.children if c.is_type(TokenType.comment)] return self._comments
866,"def shouldEnterFn(self, shouldEnterFn, shouldUse, shouldEnterFn, shouldEnterFn, shouldUse, shouldEnterFn, shouldEnterFn, shouldEnterFn, shouldEnterFn, shouldEnterFn, shouldEnterFn, shouldEnterFn, shouldEnterFn, shouldUse, shouldEnterFn, shouldEnterFn, shouldEnterFn, shouldEnterFn, shouldEnterFn, shouldEnterFn, shouldEnterFn)","def walkFlatten(self, offset: int=0, shouldEnterFn=_default_shouldEnterFn, otherObjItCtx: ObjIteratorCtx =_DummyIteratorCtx() ) -> Generator[ Union[Tuple[Tuple[int, int], 'TransTmpl'], 'OneOfTransaction'], None, None]: t = self.dtype base = self.bitAddr + offset end = self.bitAddrEnd + offset shouldEnter, shouldYield = shouldEnterFn(self) if shouldYield: yield ((base, end), self) if shouldEnter: if isinstance(t, Bits): pass elif isinstance(t, HStruct): for ch in self.children: with otherObjItCtx(ch.origin.name): yield from ch.walkFlatten( offset, shouldEnterFn, otherObjItCtx) elif isinstance(t, HArray): itemSize = (self.bitAddrEnd - self.bitAddr) // self.itemCnt for i in range(self.itemCnt): with otherObjItCtx(i): yield from self.children.walkFlatten( base + i * itemSize, shouldEnterFn, otherObjItCtx) elif isinstance(t, HUnion): yield OneOfTransaction(self, offset, shouldEnterFn, self.children) elif isinstance(t, HStream): assert len(self.children) == 1 yield StreamTransaction(self, offset, shouldEnterFn, self.children[0]) else: raise TypeError(t)"
867,"def measure_code_coverage(self, start, stop, start, start, start, start, start, stop, start, start, stop, start, start, stop, start, start, start, stop, start, start, start, start, start, start, start, start, start, start, start, start, start, start, start, start, start, start, start, start, start, start, start, start, start, start, start, start, start, start, start, start, stop, start, start, start, start, start, start, start","def start(self): if self.run_suffix: self.data_suffix = self.run_suffix if self.auto_data: self.load() if self.source or self.source_pkgs: self.source_match = TreeMatcher(self.source) else: if self.cover_dir: self.cover_match = TreeMatcher([self.cover_dir]) if self.pylib_dirs: self.pylib_match = TreeMatcher(self.pylib_dirs) if self.include: self.include_match = FnmatchMatcher(self.include) if self.omit: self.omit_match = FnmatchMatcher(self.omit) if self.debug.should('config'): self.debug.write(""Configuration values:"") config_info = sorted(self.config.__dict__.items()) self.debug.write_formatted_info(config_info) if self.debug.should('sys'): self.debug.write(""Debugging info:"") self.debug.write_formatted_info(self.sysinfo()) self.collector.start() self._started = True self._measured = True"
868,"def print_if(self, if, if, if, if, if, if, if, if, if, if, if, if, if, if, if, if, if, if, if, if, if, if, if, if, if, if, if, if, if, if, if, if, if, if, if)","def _qasmif(self, string): if self.control is None: return string return ""if(%s==%d) "" % (self.control[0].name, self.control[1]) + string"
869,"def est_idxs(self, frame_times, est_labels, est_labels, est_labels, est_labels, est_labels, est_labels, est_labels, est_labels, est_idxs, est_idxs, est_labels, est_labels, est_labels, est_labels, est_idxs, est_labels, est_labels, est_labels, est_labels, est_labels, est_labels, est_labels, est_labels, est_labels)","def process_segmentation_level(est_idxs, est_labels, N, frame_times, dur): assert est_idxs[0] == 0 and est_idxs[-1] == N - 1 assert len(est_idxs) - 1 == len(est_labels) est_times = np.concatenate(([0], frame_times[est_idxs], [dur])) silence_label = np.max(est_labels) + 1 est_labels = np.concatenate(([silence_label], est_labels, [silence_label])) est_times, est_labels = remove_empty_segments(est_times, est_labels) assert np.allclose([est_times[0]], [0]) and  np.allclose([est_times[-1]], [dur]) return est_times, est_labels"
870,"def add_child(self, children): self.add_child(children) self.add_child(children) self.add_child(children) self.add_child(children) self.add_child(children) self.add_child(children) self.add_child(children) self.add_child(children) self.add_child(children) self.add_child(children) self.add_child(children) self.add_child(children) self.add_child(children) self.add_child(children) self.add_child(children) self.add_child(children) self.add_child(children) self.add_child(children) self.add_child(children) self.add_child(children) self.add_child(children) self.add_child(children) self.add_child(children) self.add_child(children) self.add_child(children) self.add_child(children) self.add_child(children) self.add_child(children) self.add_child(children) self.add_child(children) self.add_child(children) self.add_child(children) self.add_child(children) self.add_child(children) self.add_child(children) self.add_child(children)","def add_children(G, parent, level, n=2): if level == 0: return for i in range(n): child = parent+str(i) G.add_node(child) G.add_edge(parent,child) add_children(G, child, level-1, n)"
871,def add_gated_default_import(self): if self.import(self.import): self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import(self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self,"def add_default_import(cls, module: str): if module in cls.GATED_IMPORTS: cls.DEFAULT_IMPORTS.swap(lambda s: s.cons(sym.symbol(module)))"
872,"def get_f1_values(train, valid, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, train, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval, xval","def F1(self, thresholds=None, train=False, valid=False, xval=False): return model.model_id: model.F1(thresholds, train, valid, xval) for model in self.models"
873,"def 'a3', 'a4', 'a5', 'letter', 'legal', '11x17', 'letter', 'letter', 'legal', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter'.","def set_page_size(self, layout): self.layout = layout.lower() if self.layout in self.layout_dict: self.page_size = self.layout_dict[self.layout] else: dimensions = self.layout.split('x') if len(dimensions) == 2: self.page_size = (float(dimensions[0]) * 72, float(dimensions[1]) * 72) else: raise IndexError(""Page is two dimensions, given: %s"" % len(dimensions))"
874,"def bytearray(self, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray, bytearray,",def encode16Int(value): value = int(value) encoded = bytearray(2) encoded[0] = value >> 8 encoded[1] = value & 0xFF return encoded
875,"def preprocess_case(self, case_obj, model_obj, model_obj, model_obj, model_obj): case_obj(model_obj, model_obj, model_obj, model_obj, model_obj, model_obj, model_obj, model_obj, model_obj, model_obj, model_obj): case_obj(model_obj, model_obj, model_obj, model_obj, model_obj, model_obj, model_obj, model_obj, model_obj, model_obj, model_obj, model_obj, model_obj, model_obj, model_obj, model_obj, model_obj, model_obj, model_obj, model_obj, model_obj, model_obj, model_obj, model_obj, model_obj, model_obj, model_obj, model_obj, model_obj, model_obj,","def case(store, institute_obj, case_obj): case_obj['individual_ids'] = [] for individual in case_obj['individuals']: try: sex = int(individual.get('sex', 0)) except ValueError as err: sex = 0 individual['sex_human'] = SEX_MAP[sex] pheno_map = PHENOTYPE_MAP if case_obj.get('track', 'rare') == 'cancer': pheno_map = CANCER_PHENOTYPE_MAP individual['phenotype_human'] = pheno_map.get(individual['phenotype']) case_obj['individual_ids'].append(individual['individual_id']) case_obj['assignees'] = [store.user(user_email) for user_email in case_obj.get('assignees', [])] suspects = [store.variant(variant_id) or variant_id for variant_id in case_obj.get('suspects', [])] causatives = [store.variant(variant_id) or variant_id for variant_id in case_obj.get('causatives', [])] distinct_genes = set() case_obj['panel_names'] = [] for panel_info in case_obj.get('panels', []): if not panel_info.get('is_default'): continue panel_obj = store.gene_panel(panel_info['panel_name'], version=panel_info.get('version')) distinct_genes.update([gene['hgnc_id'] for gene in panel_obj.get('genes', []"
876,"def str(self, object): str(self, object): str(self, object) str(self, object) str(self, object) str(self, object) str(self, object) str(self, object) str(self, object) str(self, object) str(self, object) str(self, object) str(self, object)","def converted(self): if self._converted is None: self._converted = self.converter(self.name, self.value) return self._converted"
877,"def post_message(self, roomId, markdown, text, markdown): if self.post_message(request_parameters): self.post_message(request_parameters): self.post_message(request_parameters) self.post_message(request_parameters) self.post_message(request_parameters) self.post_message(request_parameters) self.post_message(request_parameters) self.post_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message","def create(self, roomId=None, toPersonId=None, toPersonEmail=None, text=None, markdown=None, files=None, **request_parameters): check_type(roomId, basestring) check_type(toPersonId, basestring) check_type(toPersonEmail, basestring) check_type(text, basestring) check_type(markdown, basestring) check_type(files, list) if files: if len(files)!= 1: raise ValueError(""The length of the files list is greater "" ""than one (1). The files parameter is a "" ""list, which accepts multiple values to "" ""allow for future expansion, but currently "" ""only one file may be included with the "" ""message."") check_type(files[0], basestring) post_data = dict_from_items_with_values( request_parameters, roomId=roomId, toPersonId=toPersonId, toPersonEmail=toPersonEmail, text=text, markdown=markdown, files=files, ) if not files or is_web_url(files[0]): json_data = self._session.post(API_ENDPOINT, json=post_data) elif is_local_file(files[0]): try: post_data['files'] = open_local_file(files[0]) multipart_data = MultipartEncoder(post_data) headers = 'Content-type': multipart_data.content_type json_data = self._session.post(API_ENDPOINT, headers=headers, data=multipart_data) finally: post_data['files'].file_object.close() else: raise ValueError(""The files parameter does not contain a vaild "" ""URL or path to a local file."") return self._object_factory(OBJECT"
878,"def parse_batch_id(self, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id, batch_id,","def get_batch(): optlist, args = getopt.getopt(sys.argv[1:], '', longopts='batch=') batch = 0 for o, a in optlist: if o == '--batch': batch = int(a) print('Found batch %d' % batch) return batch"
879,"def bit_list(bit_list): bit_list = bit_list(bit_list, bit_list, bit_list, bit_list, bit_list)","def p_bit_list_0(self, program): program[0] = node.IdList([program[1]]) program[1].is_bit = True self.update_symtab(program[1])"
880,"def cache_name_of_the_property(self, name_of_the_property): self.cache(name_of_the_property) self.cache(name_of_the_property) self.cache(name_of_the_property) self.cache(name_of_the_property) self.cache(name_of_the_property) self.cache(name_of_the_property) self.cache(name_of_the_property) self.cache(name_of_property) self.cache(name_of_property) self.cache(name_of_property) self.cache(name_of_property) self.cache(name_of_property) self.cache(name_of_property) self.cache(name_of_property) self.cache(name_of_property) self.cache(name_of_property) self.cache(name_of_property) self.cache(name_of_property) self.cache","def cached(method) -> property: name = ""_"" + method.__name__ @property def wrapper(self): try: return getattr(self, name) except AttributeError: val = method(self) setattr(self, name, val) return val return wrapper"
881,"def argparse(self, docstring): self.argparse(docstring): self.argparse(docstring) self.argparse(docstring) self.argparse(docstring) self.argparse(docstring) self.argparse(docstring) self.argparse(docstring) self.argparse(docstring) self.argparse(docstring) self.argparse(docstring) self.argparse(docstring) self.argparse(docstring) self.argparse(docstring) self.argparse(docstring) self.argparse(docstring) self.argparse(docstring) self.argparse(docstring) self.argparse(docstring) self.argparse(docstring) self.argparse(docstring) self.argparse(docstring) self.argparse(docstring) self.argparse(docstring) self.argparse(docstring) self.argparse(docstring) self.argparse(docstring) self.argparse(docstring) self.argparse(docstring) self.arg","def cli_run(): parser = argparse.ArgumentParser(description='Stupidly simple code answers from StackOverflow') parser.add_argument('query', help=""What's the problem?"", type=str, nargs='+') parser.add_argument('-t','--tags', help='semicolon separated tags -> python;lambda') args = parser.parse_args() main(args)"
882,def hook(self): self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.get(self.get))))) self.get(self.get(self.get(self.get)) self.get(self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get(self.get(self.get(self.get) self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get) self.get(self.get,"def _generate(self, source, name, filename, defer_init=False): return generate(source, self, name, filename, defer_init=defer_init)"
883,"def probability_of_alignment(source_sentences, target_sentences, alignment, params, alignment, params, alignment, params, alignment, params, alignment, params, alignment, params, alignment, params, alignment, params, params, alignment, params, alignment, params, alignment, params, alignment, params, alignment, params, alignment, params, params, alignment, params, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment, alignment","def align_probability(i, j, source_sentences, target_sentences, alignment, params): l_s = sum(source_sentences[i - offset] for offset in range(alignment[0])) l_t = sum(target_sentences[j - offset] for offset in range(alignment[1])) try: m = (l_s + l_t / params.AVERAGE_CHARACTERS) / 2 delta = (l_t - l_s * params.AVERAGE_CHARACTERS) / math.sqrt(m * params.VARIANCE_CHARACTERS) except ZeroDivisionError: delta = infinity return 2 * (1 - norm_cdf(abs(delta))) * params.PRIORS[alignment]"
884,"def paginate_call(self, paginate_results, paginate_results, paginate_results, paginate_results, paginate_results, paginate_results, paginate_results, paginate_results, paginate_results, paginate_results, paginate_results, paginate_results, paginate_results, paginate_results)","def paginate_get(self, url, headers=None, return_json=True, start_page=None): geturl = '%s&page=1' %(url) if start_page is not None: geturl = '%s&page=%s' %(url,start_page) results = [] while geturl is not None: result = self._get(url, headers=headers, return_json=return_json) if isinstance(result, dict): if'results' in result: results = results + result['results'] geturl = result['next'] else: return result return results"
885,"def usls_data(self, data, usls_data): data(data, data, usls_data): data(data, usls_data): data(data, usls_data): data(data, usls_data): data(data, usls_data): data(data, usls_data): data(data, usls_data): data(data, usls_data): data(data, usls_data): usls_data): data(data): data(data): data(data): data(data): data(data): data(data): data(data): data(data): data(data): data(data): data(data): data(data): data(data): data(data): data(data): data(data): data(data): data(data): data(data): data(data): data(data): data(data): data(data): data(data): data(data): data(data): data(data): data(data): data(data): data(data): data(data): data(data): data(data): data(data): data(","def project_usl_with_data(usls_data, metric=None): projection = project_usls_on_dictionary(usls_data) all_terms = set(c for u in usls_data for t in u.objects(Term) for c in t.singular_sequences) if metric is None: metric = lambda e: len(e['posts']) * len(all_terms.intersection(e['table'].singular_sequences)) return sorted(( 'table': table, 'usls': usls, 'posts': list(set(chain.from_iterable(usls_data[u] for u in usls)))  for table, usls in projection.items()), key=metric, reverse=True)"
886,"def receive_signal(self, signal): if self.get_signal(signal): self.get_signal(signal): self.getsignal(signal)","def handle(self, signum, frame): if self.print_method: self.print_method('nProgram received signal %s.' % self.signame) if self.print_stack: import traceback strings = traceback.format_stack(frame) for s in strings: if s[-1] == 'n': s = s[0:-1] self.print_method(s) pass pass if self.b_stop: core = self.dbgr.core old_trace_hook_suspend = core.trace_hook_suspend core.trace_hook_suspend = True core.stop_reason = ('intercepting signal %s (%d)' % (self.signame, signum)) core.processor.event_processor(frame,'signal', signum) core.trace_hook_suspend = old_trace_hook_suspend pass if self.pass_along: if self.old_handler: self.old_handler(signum, frame) pass pass return"
887,def get_statistics(self): self.get_statistics(self.get_statistics): self.get_statistics(self.getstatistics) self.getstatistics(self.getstatistics) self.getstatistics(self.getstatistics) self.getstatistics(self.getstatistics) self.getstatistics(self.getstatistics) self.getstatistics(self.getstatistics) self.getstatistics(self.getstatistics) self.getstatistics) self.getstatistics) self.getstatistics(self.getstatistics) self.getstatistics) self.getstatistics(self.getstatistics) self.getstatistics) self.getstatistics(self.getstatistics) self.getstatistics) self.getstatistics(self.getstatistics) self.getstatistics) self.getstatistics(self.getstatistics) self.getstatistics(self.getstatistics) self.getstatistics) self.getstatistics(self.getstatistics) self.getstatistics(self.getstatistics) self.getstatistics) self.getstat,"def get_statistic_by_name(stat_name): if stat_name == 'ALL': return get_statistic_models() for stat in get_statistic_models(): if stat.__name__ == stat_name: return stat raise Exception, _(""%(stat)s cannot be found."") %'stat': stat_name"
888,"def PUT(self, url, params, files, params, params, params, files, params, params, params, files, params, params, files, params, params, params, files, params, params, params, files, params, params, files, params, params, files, params, files, params): params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, param","def put(self, url, params=None, data=None, files=None, **kwargs): return self.call_api( ""PUT"", url, params=params, data=data, files=files, **kwargs )"
889,"def split_endpoint_name(self, path, endpoint): if 12345: 12345: 12345: 12345: 12345: 12345: 12345: 12345: 12345: 12345: 12345: 12345: 12345: 12345: 12345: 12345: 12345: 12345: 12345: 12345: 12345: 12345: 12345: 12345: 12345: 12345: 12345: 12345: 12345: 12345: 12345:","def parse_endpoint_name(self, endpoint): parts = [x for x in endpoint.split(':') if x] endpoint = parts[0] if len(parts) == 1: path = '' else: path = '/'.join(parts[1:]) return endpoint, path"
890,"def check_args(self, args): return self.args(args)","def _get_samples(dist, z, n, seed): with tf.compat.v1.name_scope('get_samples', values=[z, n]): if (n is None) == (z is None): raise ValueError( 'Must specify exactly one of arguments ""n"" and ""z"". Found:''n = %s, z = %s' % (n, z)) if n is not None: return dist.sample(n, seed=seed) else: return tf.convert_to_tensor(value=z, name='z')"
891,"def main_task(self, start, start, start, start): self.start(self.start, start, start, start, start, start, start, start, start, start): self.start(): self.start(): self.start(): self.start(): self.start(): self.start(): self.start()",def run(self): self._create_socket() self._running = True self._beating = True while self._running: if self._pause: time.sleep(self.time_to_dead) continue since_last_heartbeat = 0.0 self.socket.send(b'ping') request_time = time.time() ready = self._poll(request_time) if ready: self._beating = True self.socket.recv() remainder = self.time_to_dead - (time.time() - request_time) if remainder > 0: time.sleep(remainder) continue else: self._beating = False since_last_heartbeat = time.time() - request_time self.call_handlers(since_last_heartbeat) self._create_socket() continue try: self.socket.close() except: pass
892,"def main_config_value(self, config): if self.config = self.config = self.config_value(config)","def _get_config_or_default(self, key, default, as_type=lambda x: x): if self.main_config.has_option(self.main_section, key): return as_type(self.main_config.get(self.main_section, key)) return default"
893,"def parsed_transcripts(enstid, transcript_lines): parsed_transcripts(enstid, transcript_lines): parsed_transcripts(enstid, transcript_lines): parsed_transcripts(enstid, transcript_lines): parsed_transcripts(enstid, transcript_lines): parsed_transcripts(enstid, transcript_lines): parsed_transcripts(enstid, transcript_lines)","def parse_transcripts(transcript_lines): LOG.info(""Parsing transcripts"") if isinstance(transcript_lines, DataFrame): transcripts = parse_ensembl_transcript_request(transcript_lines) else: transcripts = parse_ensembl_transcripts(transcript_lines) parsed_transcripts =  for tx in transcripts: tx_id = tx['ensembl_transcript_id'] ens_gene_id = tx['ensembl_gene_id'] if not tx_id in parsed_transcripts: tx_info =  'chrom': tx['chrom'], 'transcript_start': tx['transcript_start'], 'transcript_end': tx['transcript_end'],'mrna': set(),'mrna_predicted': set(), 'nc_rna': set(), 'ensembl_gene_id': ens_gene_id, 'ensembl_transcript_id': tx_id,  parsed_transcripts[tx_id] = tx_info tx_info = parsed_transcripts[tx_id] if tx.get('refseq_mrna_predicted'): tx_info['mrna_predicted'].add(tx['refseq_mrna_predicted']) if tx.get('refseq_mrna'): tx_info['mrna'].add(tx['refseq_mrna']) if tx.get('refseq_ncrna'): tx_info['nc_rna"
894,"def make_pandana_network('walk', 'drive', 'two_way', 'two_way', 'two_way', 'two_way', 'two_way', 'two_way', 'two_way', 'two_way', 'two_way', 'two_way', 'two_way', 'two_way', 'two_way', 'two_way', 'one_way', 'one_way', 'one_way', 'one_way', 'one_way', 'one_way', 'one_way', 'one_way', 'one_way', 'one_way', 'one_way', 'one_way', 'one_way', 'one_way', 'one_way', 'one_way', 'one_way', 'one_way', 'one_way', 'one_way', 'one_way', 'one_way', 'one_way', 'one_way', 'one_way', 'one_way'","def pdna_network_from_bbox( lat_min=None, lng_min=None, lat_max=None, lng_max=None, bbox=None, network_type='walk', two_way=True, timeout=180, memory=None, max_query_area_size=50 * 1000 * 50 * 1000): nodes, edges = network_from_bbox(lat_min=lat_min, lng_min=lng_min, lat_max=lat_max, lng_max=lng_max, bbox=bbox, network_type=network_type, two_way=two_way, timeout=timeout, memory=memory, max_query_area_size=max_query_area_size) return Network( nodes['x'], nodes['y'], edges['from'], edges['to'], edges[['distance']])"
895,"def run_multi_command(self, 'command', 'description', 'description', 'write_stdin', 'command', 'description', 'description', 'description', 'description', 'description', 'description', 'description', 'description', 'description', 'description', 'description', 'description', 'write_stdin', 'description', 'description', 'description', 'description', 'description', 'description', 'description', 'description', 'description', 'description', 'description', 'description', 'description', 'description', 'description', 'description', 'description', 'description', 'description', 'description', 'description', 'description', 'description', 'description', 'description', 'description', 'description', 'description', 'description', 'de","def stream_commands(commands, hash_colors=True, parallel=False): def _get_color(string): if hash_colors is True: return get_color_hash(string) else: return DEFAULT_COLOR fixed_commands = [] for command in commands: cmd_text = command['command'] description = command.get('description') color = _get_color(description or '') write_stdin = command.get('write_stdin') description = color(description) if color is not None else description formatter = _format_with_description(description) fixed_commands.append( 'command': cmd_text, 'formatter': formatter, 'write_stdin': write_stdin, 'ignore_empty': True ) stream_command_dicts(fixed_commands, parallel=parallel)"
896,"def get_clinvar_submissions(user, institute, institute, institute, institute, institute, institute, institute, institute, institute, institute, institute, institute, institute, institute, institute, institute, institute, institute, institute, institute, user, institute, institute, institute, institute, institute, user, institute, institute, institute, institute, institute, user, institute, institute, institute, institute, user, institute, institute, institute, institute, user, institute, institute, institute, institute, user, institute, institute, institute, user, institute, institute, user, institute, institute, institute, institute, user, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submissions, submission","def clinvar_submissions(store, user_id, institute_id): submissions = list(store.clinvar_submissions(user_id, institute_id)) return submissions"
897,def kwargs_dictionary_scatter_with_support_for_kwargs_dictionary_scatter_with_support_for_kwargs_dictionary_scatter_with_support_for_kwargs_dictionary_scatter_with_support_for_kwargs_dictionary_scatter_with_support_for_kwargs_dictionary_scatter_with_support_for_kwargs_dictionary_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_scatter_,"def scatter_kwargs(inputs, kwargs, target_gpus, dim=0): inputs = scatter(inputs, target_gpus, dim) if inputs else [] kwargs = scatter(kwargs, target_gpus, dim) if kwargs else [] if len(inputs)  len(kwargs): inputs.extend([() for _ in range(len(kwargs) - len(inputs))]) elif len(kwargs)  len(inputs): kwargs.extend([ for _ in range(len(inputs) - len(kwargs))]) inputs = tuple(inputs) kwargs = tuple(kwargs) return inputs, kwargs"
898,"def _include_coverage(self, sourcefiles): if self.include_coverage(sourcefiles): self.include_coverage(sourcefiles) return self.include_coverage(sourcefiles)","def wantFile(self, file, package=None): if self.coverInclusive: if file.endswith("".py""): if package and self.coverPackages: for want in self.coverPackages: if package.startswith(want): return True else: return True return None"
899,"def recursive_dict_merge(self, other, dicto, other): recursive_dict_merge = self.recursive_dict_merge = self.recursive_dict_merge = self.recursive_dict_merge = self.recursive_dict_merge = self.recursive_dict_merge = self.recursive_dict_merge = self.recursive_dict_merge = self.recursive_dict_merge","def merge(dicto, other): if not isinstance(dicto, Dicto): dicto = Dicto(dicto) if not isinstance(other, Dicto): other = Dicto(other) for k, v in other.__dict__.items(): if k in dicto and isinstance(dicto[k], Dicto) and isinstance(other[k], Dicto): dicto[k] = merge(dicto[k], other[k]) else: dicto[k] = other[k] return dicto"
900,"def get_layout_instance(self, LayoutInstance, LayoutInstance, LayoutInstance, LayoutInstance, LayoutInstance, LayoutInstance, LayoutInstance, LayoutInstance, LayoutInstance, LayoutInstance)",def copy(self): layout_copy = type(self)() layout_copy._p2v = self._p2v.copy() layout_copy._v2p = self._v2p.copy() return layout_copy
901,"def _consume_all_stream(self, stream): if _consume_all_stream(stream): if _consume_all_stream(stream): if _consume_all_stream(stream): _consume_all_stream(stream): _consume_all_stream(stream): _consume_all_stream(stream): _consume_all_stream(stream)",def read_until_eof(self) -> bool: if self.read_eof(): return True self._stream.save_context() while not self.read_eof(): self._stream.incpos() return self._stream.validate_context()
902,"def initialize_profile_dir(self, profile_dir, profile_dir, profile_dir, profile_dir, profile_dir, profile_dir, profile_dir, profile_dir, profile_dir, profile_dir, profile_dir, profile_dir, profile_dir, profile_dir, profile_dir, profile_dir)","def init_profile_dir(self): try: location = self.config.ProfileDir.location except AttributeError: try: p = ProfileDir.find_profile_dir_by_name(self.ipython_dir, self.profile, self.config) except ProfileDirError: if self.auto_create or self.profile=='default': try: p = ProfileDir.create_profile_dir_by_name(self.ipython_dir, self.profile, self.config) except ProfileDirError: self.log.fatal(""Could not create profile: %r""%self.profile) self.exit(1) else: self.log.info(""Created profile dir: %r""%p.location) else: self.log.fatal(""Profile %r not found.""%self.profile) self.exit(1) else: self.log.info(""Using existing profile dir: %r""%p.location) else: try: p = ProfileDir.find_profile_dir(location, self.config) except ProfileDirError: if self.auto_create: try: p = ProfileDir.create_profile_dir(location, self.config) except ProfileDirError: self.log.fatal(""Could not create profile directory: %r""%location) self.exit(1) else: self.log.info(""Creating new profile dir: %r""%location) else: self.log.fatal(""Profile directory %r not found.""%location) self.exit(1) else: self.log.info(""Using existing profile dir: %r""%location) self.profile_dir = p self.config_file_paths.append(p.location)"
903,"def stage_name(self, name, config): if self._name = self._name: self._name = self._name = self._name = self._name = self._name = self._name = self._name = self._name = self._name = self._name = self._name","def init_stage(self, name, config): subconfig = config.get(name, ) ctor = self[name] return ctor(subconfig)"
904,"def check_axes(self, shape, axes): if axes = self.___________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________","def inshape(shape, axes): valid = all([(axis  len(shape)) and (axis >= 0) for axis in axes]) if not valid: raise ValueError(""axes not valid for an ndarray of shape: %s"" % str(shape))"
905,"def id_simplified(self, id): id_simplified(id): id_simplified(id): id_simplified(id): id_simplified(id): id_simplified(id): id_simplified(id): id_simplified(id): id_simplified(id): id_simplified(id): id_simplified(id): id_simplified(id): id_simplified(id): id_simplified(id): id_simplified(id): id_simplified(id): id_simplified(id): id_simplified(id): id_simplified(id): id_simplified(id): id_simplified(id): id_simplified(id): id_simplified(id): id_simplified(id): id_simpl","def _get_readable_id(id_name, id_prefix_to_skip): pos = id_name.find('//') if pos!= -1: pos += 2 if id_prefix_to_skip: pos = id_name.find(id_prefix_to_skip, pos) if pos!= -1: pos += len(id_prefix_to_skip) pos = id_name.find('/', pos) if pos!= -1: return id_name[pos + 1:] return id_name"
906,"def search_raw(self, pattern, pattern): if self.get_range = self.get_range = self.get_range = self.get_range = self.get_range = self.get_range = self.get_range = self.get_range = self.get_range = self.get_range = self.get_range = self.get_range = self.get_range = self.get_range = self.get_range = self.get_range = self.get_range = self.get_range = self.get_range = self.get_range = self.get_range = self.get_range = self.get_range = self.get_range = self.get_range = self.get_range = self.get_range","def search(self, pattern=""*"", raw=True, search_raw=True, output=False): tosearch = ""source_raw"" if search_raw else ""source"" if output: tosearch = ""history."" + tosearch self.writeout_cache() return self._run_sql(""WHERE %s GLOB?"" % tosearch, (pattern,), raw=raw, output=output)"
907,"def create_reference(self, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str)","def run_bwa_index(job, ref_id): job.fileStore.logToMaster('Created BWA index files') work_dir = job.fileStore.getLocalTempDir() job.fileStore.readGlobalFile(ref_id, os.path.join(work_dir,'ref.fa')) command = ['index', '/data/ref.fa'] dockerCall(job=job, workDir=work_dir, parameters=command, tool='quay.io/ucsc_cgl/bwa:0.7.12--256539928ea162949d8a65ca5c79a72ef557ce7c') ids =  for output in ['ref.fa.amb','ref.fa.ann','ref.fa.bwt','ref.fa.pac','ref.fa.sa']: ids[output.split('.')[-1]] = (job.fileStore.writeGlobalFile(os.path.join(work_dir, output))) return ids['amb'], ids['ann'], ids['bwt'], ids['pac'], ids['sa']"
908,"def set_url_file(self, url): url = self.get_url_file(url): url = self.get_url_file(url)","def find_url_file(self): config = self.config if not self.url_file: self.url_file = os.path.join( self.profile_dir.security_dir, self.url_file_name )"
909,def get_response(self): if self.get_response(self.get_response(self.get_response(self.get_response(self.get_response(self.get_response(self.get_response(self.get_response(self.get_response(self.get_response(self.get_response_response_response_response_response_response_response_response_response_response_response_response_response_response_response_response)),"def get_raw(self, path, **params): url = ensure_trailing_slash(self.url + path.lstrip('/')) return self._request('get', url, params=params).text"
910,"def print_dict(self, dict): self.get_dict(dict): self.get_dict(dict) self.get_dict(dict) self.get_dict(dict) self.get_dict(dict) self.get_dict(dict) self.get_dict(dict) self.get_dict(dict) self.get_dict(dict) self.get_dict(dict) self.get_dict(dict) self.get_dict(dict) self.get_dict(dict) self.get_dict) self.get_dict(dict) self.get_dict(dict) self.get_dict(dict) self.get_dict(dict) self.get_dict(dict) self.get_dict(dict) self.get_dict(dict) self.get_dict(dict) self.get_dict(dict) self.get_dict(dict) self.get_dict(dict) self.get_dict(dict) self.get_dict(dict) self.get_dict(dict) self.get_dict(dict) self.get_dict(dict) self.get_dict(dict) self.get_dict(dict) self.get_dict(dict) self.get_dict(dict) self.get_dict(dict) self.get_dict(dict)","def print_obj(obj, verbose, metadata, mongo_version): if verbose: sys.stdout.write(json_encoder.encode(obj) + 'n') sys.stdout.flush() else: try: ts_time = obj['ts'] operation = obj['op'] doc = None if operation == 'query': if mongo_version  ""3.2"": doc = obj['ns'].split(""."")[-1] query = json_encoder.encode(obj['query']) if 'query' in obj else """" else: if ""query"" in obj: cmd = obj['query'] else: cmd = obj['command'] doc = cmd['find'] query = json_encoder.encode(cmd['filter']) if 'filter' in cmd else """" if'sort' in cmd: query += ', sort:'+ json_encoder.encode(cmd['sort']) query += '. %s returned.' % obj['nreturned'] elif operation == 'update': doc = obj['ns'].split(""."")[-1] if mongo_version  ""3.6"": query = json_encoder.encode(obj['query']) if 'query' in obj else """" query += ','+ json_encoder.encode(obj['updateobj']) else: query = json_encoder.encode(obj['command']['q']) if 'command' in obj and"
911,"def validate_arg(self, sample_arg): arg = self.log_prob(self.log_prob) if self.log_prob(self.log_prob)","def _validate_sample_arg(self, x): with tf.name_scope(""validate_sample_arg""): x_ndims = ( tf.rank(x) if tensorshape_util.rank(x.shape) is None else tensorshape_util.rank(x.shape)) event_ndims = ( tf.size(input=self.event_shape_tensor()) if tensorshape_util.rank(self.event_shape) is None else tensorshape_util.rank(self.event_shape)) batch_ndims = ( tf.size(input=self._batch_shape_unexpanded) if tensorshape_util.rank(self.batch_shape) is None else tensorshape_util.rank(self.batch_shape)) expected_batch_event_ndims = batch_ndims + event_ndims if (isinstance(x_ndims, int) and isinstance(expected_batch_event_ndims, int)): if x_ndims  expected_batch_event_ndims: raise NotImplementedError( ""Broadcasting is not supported; too few batch and event dims "" ""(expected at least, saw )."".format( expected_batch_event_ndims, x_ndims)) ndims_assertion = [] elif self.validate_args: ndims_assertion = [ assert_util.assert_greater_equal( x_ndims, expected_batch_event_ndims, message=(""Broadcasting is not supported; too few "" ""batch and event dims.""), name=""assert_batch_and_event_ndims_large_enough"
912,def ________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________,"def disaster_recovery_configs(self): api_version = self._get_api_version('disaster_recovery_configs') if api_version == '2017-04-01': from.v2017_04_01.operations import DisasterRecoveryConfigsOperations as OperationClass else: raise NotImplementedError(""APIVersion  is not available"".format(api_version)) return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))"
913,def jupyter_notebook_magic_repr(self): if self.jupyter_notebook_magic_repr(self.jupyter_notebook_magic_repr(self.jupyter_notebook_magic_repr(self.jupyter_notebook_magic_repr(self.jupyter_notebook_magic_repr(self.jupyter_magic_repr(self.jupyter_magic_repr(self.jupyter_magic_repr(self.jupyter_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_magic_mag,"def _repr_html_(self): rows, c = '', '' s = 'tr>td>strong>k/strong>/td>td style=""stl"">v/td>/tr>' for k, v in self.__dict__.items(): if k == '_colour': k = 'colour' c = utils.text_colour_for_hex(v) style = 'color:; background-color:'.format(c, v) else: style = 'color:black; background-color:white' if k == 'component': try: v = v._repr_html_() except AttributeError: v = v.__repr__() rows += s.format(k=k, v=v, stl=style) html = 'table>/table>'.format(rows) return html"
914,"def read_result(self, params, params): if self.get_result(self.get_result(self.get_result(self.get_result(self.get_result(self.get_result(self.get_result(self.get_result(self.get_result(self.get_result(self.get_result(self.get_result(self.get_result(self.get_result(self.get_result(self.get_result(self.get_result(self.get_result(self.get_result(self.get_result(self.get_result(self.get_result(self.get_result))))))))","def read(self, path, params=None): return self.handleresult(self.r.get(urljoin(self.url + CRUD_PATH, path), params=params))"
915,"def display_text(self, field): self.get_text(field): self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field) self.gettext(field","def empty_value(self): edit_empty_value = self.config.get('edit_empty_value', False) if edit_empty_value: return edit_empty_value else: return unicode(inplace_settings.INPLACEEDIT_EDIT_EMPTY_VALUE)"
916,"def add_callback(self, positional, keyword): self.add_callback(self.add_callback) self.add_callback(self.add_callback) self.add_callback(self.add_callback) self.add_callback(self.add_callback) self.add_callback(self.add_callback) self.add_callback(self.add_callback) self.add_callback(self.add_callback) self.add_callback(self.add_callback) self.add_callback(self.add_callback) self.add_callback(self.add_callback) self.add_callback(self.add_callback) self.add_callback(self.add_callback) self.add_callback(self.add_callback) self.add_callback(self.add_callback) self.add_callback(self.add_callback) self.add_callback(self.add_callback) self.add_callback(self.add_callback) self.add_callback(self.add_callback) self.add_callback(self.add_callback) self.add_callback(self.add_callback) self.add_","def addAction(self,action,func,*args,**kwargs): if not hasattr(self,""actions""): self.actions =  if action not in self.actions: self.actions[action] = [] self.actions[action].append((func,args,kwargs))"
917,"def close_fridge(self, file): self.close_fridge(self.close_fridge) self.close_fridge(self.close_fridge) self.close_fridge(self.close_fridge) self.close_fridge(self.close_fridge) self.close_fridge(self.close_fridge) self.close_fridge(self.close_fridge) self.close_fridge(self.close_fridge) self.close_fridge(self.close_fridge) self.close_fridge(self.close_fridge) self.close_fridge(self.close_fridge) self.close_fridge(self.close_fridge) self.close_fridge(self.close_fridge) self.close_fridge) self.close_fridge(self.close_fridge) self.close_fridge) self.close_fridge(self.close_fridge) self.close_fridge) self.close_fridge(self.close_fridge) self.close_fridge(self.close_fridge) self.close_fridge) self.close_fridge(self.close_fridge) self.close_fridge(self.close_fridge) self.close_fridge(",def close(self): if not self.closed: self.save() if self.close_file: self.file.close() self.closed = True
918,"def deserialize_data_to_internal_representation(self, session, instance, transient, transient, transient, transient, transient, transient, transient, transient, transient, transient, transient, transient, transient, transient, transient, transient, transient, transient, transient, transient, transient, transient, transient, transient, transient, transient, transient, transient, transient, transient, transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_transient_trans","def load(self, data, session=None, instance=None, transient=False, *args, **kwargs): self._session = session or self._session self._transient = transient or self._transient if not (self.transient or self.session): raise ValueError(""Deserialization requires a session"") self.instance = instance or self.instance try: return super(ModelSchema, self).load(data, *args, **kwargs) finally: self.instance = None"
919,"def draw_submenu(self, submenu): self.draw_submenu(submenu): self.draw_submenu(submenu) self.draw_submenu(submenu) self.draw_submenu) self.draw_submenu(submenu) self.draw_submenu(submenu) self.draw_submenu(submenu) self.draw_submenu) self.draw_submenu(submenu) self.draw_submenu) self.draw_submenu(submenu) self.draw_submenu) self.draw_submenu(submenu) self.draw_submenu) self.draw_submenu(submenu) self.draw_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back_back","def draw(self): if not self.visible: return if not isinstance(self.submenu,Container): glEnable(GL_SCISSOR_TEST) glScissor(*self.pos+self.size) SubMenu.draw(self) if not isinstance(self.submenu,Container): glDisable(GL_SCISSOR_TEST)"
920,"def package_element(self, package_element(self, package_element(self, package_element(self, package_element(self, package_element(self, package_element(self, package_element(self, package_element(self, package_element(self, package_element(self, package_element(self, package_element(self, package_element(self, package_element(self, package_element(self, package_element(self)))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))","def accept_C_C(self, inst): for child in many(inst).PE_PE[8003](): self.accept(child)"
921,"def create_timezone(self, timezone, timezone, timezone, timezone, timezone, timezone, timezone, timezone, timezone, timezone, timezone, timezone, timezone, timezone, timezone, timezone, timezone, timezone, timezone, timezone, timezone, timezone, timezone, timezone, timezone, timezone, timezone, timezone, timezone, timezone, timezone, timezone, timezone, timezone, timezone)","def create_from_pytz(cls, tz_info): zone_name = tz_info.zone utc_transition_times_list_raw = getattr(tz_info, '_utc_transition_times', None) utc_transition_times_list = [tuple(utt.timetuple()) for utt in utc_transition_times_list_raw]  if utc_transition_times_list_raw is not None  else None transition_info_list_raw = getattr(tz_info, '_transition_info', None) transition_info_list = [(utcoffset_td.total_seconds(), dst_td.total_seconds(), tzname) for (utcoffset_td, dst_td, tzname) in transition_info_list_raw]  if transition_info_list_raw is not None  else None try: utcoffset_dt = tz_info._utcoffset except AttributeError: utcoffset = None else: utcoffset = utcoffset_dt.total_seconds() tzname = getattr(tz_info, '_tzname', None) parent_class_name = getmro(tz_info.__class__)[1].__name__ return cls(zone_name, parent_class_name, utc_transition_times_list, transition_info_list, utcoffset, tzname)"
922,"def install_lib(self, install_lib, install_lib, install_lib, install_lib, install_lib, install_lib, install_lib, install_lib, install_lib, install_lib, install_lib, install_lib, install_lib, install_lib, install_lib, install_lib, install_lib, install_lib, install_lib, install_lib, install_lib, install_lib, install_lib, install_lib, install_lib, install_lib, install_lib, install_lib, install_lib, install_lib, install_lib, install_lib, install_lib, install_lib, install_lib, install_lib, install_lib, install_lib)","def run(self): install_lib.install_lib.run(self) if include_dirs: for directory in include_dirs: dest = join(self.install_dir, directory) if sys.version_info >= (3, 0): exclude = ""invalid_encoded_data*"", ""unknown_encoding*"" else: exclude = set() shutil.rmtree(dest, ignore_errors=True) shutil.copytree( directory, dest, ignore=shutil.ignore_patterns(*exclude) )"
923,"def log_user_specific_dir(username, appname): if 'appname' is None: if 'appname' is None: if 'appname' is None: if 'appname' is None: if 'appname' is None: if 'appname' is None: if 'appname' is None: if 'appname' is None: if 'appname' is None: if 'appname' = 'appname' = 'appname' = 'appname' = 'appname' = 'appname' = 'appname' = 'appname' = 'appname' = 'appname' = 'appname' = 'appname' = 'appname' = 'appname' = 'appname' = 'appname' = 'appname' = 'appname' = 'appname' = 'appname = 'appname = 'appname = 'appname = 'appname = 'appname = 'appname = 'appname = 'appname = 'appname = 'appname = 'appname = 'appname = 'appname = 'appname =","def user_log_dir(appname): if WINDOWS: path = os.path.join(user_data_dir(appname), ""Logs"") elif sys.platform == ""darwin"": path = os.path.join(os.path.expanduser('/Library/Logs'), appname) else: path = os.path.join(user_cache_dir(appname), ""log"") return path"
924,"def raise_exception_error(self, error, str|Exception): if self is not self: return self(self, error, self)","def error(self, error): self._error = RuntimeError(error) if isinstance(error, str) else error"
925,"def finalize_closing(self, closing): self.closing(self.closing) self.closing(self.closing) self.closing(self.closing) self.closing(self.closing) self.closing(self.closing) self.closing(self.closing) self.closing(self.closing) self.closing(self.closing) self.closing(self.closing) self.closing(self.closing) self.closing(self.closing) self.closing","def _call_connection_lost(self, error): try: self._protocol.connection_lost(error) finally: os.close(self._fileno) self._fileno = None self._protocol = None self._loop = None"
926,"def get_user_and_org_data(user, org, org, org, org): org = org.get_user(user, org, org, org, org)","def __get_user(self, login): user =  if not login: return user user_raw = self.client.user(login) user = json.loads(user_raw) user_orgs_raw =  self.client.user_orgs(login) user['organizations'] = json.loads(user_orgs_raw) return user"
927,"def compute_args_list(self, args_list, args_list, args_list, args_list, args_list, args_list, args_list, args_list, args_list, args_list, args_list, args_list, args_list, args_list, args_list, args_list, args_list, args_list, args_list, args_list, args_list)","def maybe_call_fn_and_grads(fn, fn_arg_list, result=None, grads=None, check_non_none_grads=True, name=None): with tf.compat.v1.name_scope(name,'maybe_call_fn_and_grads', [fn_arg_list, result, grads]): fn_arg_list = (list(fn_arg_list) if is_list_like(fn_arg_list) else [fn_arg_list]) result, grads = _value_and_gradients(fn, fn_arg_list, result, grads) if not all(r.dtype.is_floating for r in (result if is_list_like(result) else [result])): raise TypeError('Function result must be a Tensor with float''dtype.') if len(fn_arg_list)!= len(grads): raise ValueError('Function args must be in one-to-one correspondence''with grads.') if check_non_none_grads and any(g is None for g in grads): raise ValueError('Encountered None gradient.n''fn_arg_list: n''grads: '.format(fn_arg_list, grads)) return result, grads"
928,"def copy_shallow(self, copy_leaves, with_links, with_links, with_links, with_links, with_links, with_links, with_links, with_links, with_links, with_links, with_links, with_links, with_links, with_links, with_links, with_links, with_links, with_links, with_links, with_links, copy_shallow_copy_shallow_copy_shallow_copy_shallow_copy_shallow_copy_shallow_copy_shallow_copy_shallow_copy_shallow_copy_shallow_copy_shallow_copy_shallow_copy_shallow_copy_shallow_copy_shallow_copy_shallow_copy_shallow_copy_shallow_copy_shallow_copy_____________________________________________________________________","def f_copy(self, copy_leaves=True, with_links=True): new_traj = Trajectory(_copy_traj=True) new_traj._length = self._length new_traj._name = self._name new_traj._timestamp = self._timestamp new_traj._time = self._time new_traj._single_run_ids = self._single_run_ids new_traj._run_information = self._run_information new_traj._updated_run_information = self._updated_run_information new_traj._fast_access = self._fast_access new_traj._shortcuts = self._shortcuts new_traj._iter_recursive = self._iter_recursive new_traj._max_depth = self._max_depth new_traj._auto_load = self._auto_load new_traj._with_links = self._with_links new_traj._environment_hexsha = self._environment_hexsha new_traj._environment_name = self._environment_name new_traj._idx = self._idx new_traj._crun = self._crun new_traj._standard_parameter = self._standard_parameter new_traj._standard_result = self._standard_result new_traj._standard_leaf = self._standard_leaf new_traj._auto_run_prepend = self._auto_run_prepend new_traj._no_clobber = self._no_clobber new_traj._run_started = self._run_started new_traj._run_by_environment = self._run_by_environment new_traj._full_copy = self._full_copy new_traj"
929,def run_application_main(self): if self.run_application.main(self.run_application.main) if self.run_application.main(self.run_application.main) if self.run_application.main(self.run_application.main),"def run(self, args_list=None): self.log.debug(""Application.run: args_list"".format(**locals())) retval = None try: retval = self._run(args_list=args_list) except KeyboardInterrupt: self.log.verbose(""Interrupted"") except SystemExit as exit: self.log.verbose(""Exited"") retval = exit.code except Exception: print(""Uncaught exception"", file=sys.stderr) traceback.print_exc() if ""debug_pdb"" in self.args and self.args.debug_pdb: debugger() retval = Application.UNCAUGHT_EXCEPTION_EXIT raise finally: try: self._atexit() finally: sys.stderr.flush() sys.stdout.flush() sys.exit(retval)"
930,"def fetch_data_from_archive(self, method, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args,","def _fetch_from_archive(self, method, args): if not self.archive: raise ArchiveError(cause=""Archive not provided"") data = self.archive.retrieve(method, args, None) if isinstance(data, nntplib.NNTPTemporaryError): raise data return data"
931,"def save_node(self, network, rm_nodes, rm_nodes, rm_nodes, rm_nodes, rm_nodes, rm_nodes, rm_nodes, rm_nodes, rm_nodes, rm_nodes, rm_nodes, rm_nodes, rm_nodes, rm_nodes, rm_nodes, rm_nodes, rm_nodes, rm_nodes)","def network_to_pandas_hdf5(network, filename, rm_nodes=None): if rm_nodes is not None: nodes, edges = remove_nodes(network, rm_nodes) else: nodes, edges = network.nodes_df, network.edges_df with pd.HDFStore(filename, mode='w') as store: store['nodes'] = nodes store['edges'] = edges store['two_way'] = pd.Series([network._twoway]) store['impedance_names'] = pd.Series(network.impedance_names)"
932,"def magic(self, ifun, automagic): if self.automagic(self.automagic): self.automagic(self.automagic): self.automagic(self.automagic) self.automagic(self.automagic) self.automagic(self.automagic) self.automagic(self.automagic) self.automagic(self.automagic) self.automagic(self.automagic) self.automagic) self.automagic) self.automagic(self.automagic) self.automagic(self.automagic) self.automagic) self.automagic) self.automagic(self.automagic) self.automagic) self.automagic(self.automagic) self.automagic) self.automagic self.automagic self.automagic self.automagic self.automagic self.automagic self.automagic self.automagic self.automagic self.automagic self.automagic self.automagic self.automagic self.automagic self.automagic self.automagic self.automagic self.automagic self.automagic self.automagic","def check(self, line_info): if not self.shell.automagic or not self.shell.find_magic(line_info.ifun): return None if line_info.continue_prompt and not self.prefilter_manager.multi_line_specials: return None head = line_info.ifun.split('.',1)[0] if is_shadowed(head, self.shell): return None return self.prefilter_manager.get_handler_by_name('magic')"
933,"def send_multisig_tx(self, tx_sender_private_key, tx_gas_price, tx_gas_price, tx_gas_price, tx_nonce, tx_nonce, tx_nonce, tx_nonce, tx_nonce, tx_nonce, tx_nonce, tx_nonce, tx_nonce, tx_nonce, tx_nonce, tx_nonce, tx_nonce, tx_nonce, tx_nonce, tx_nonce, tx_nonce, tx_nonce, tx_nonce, tx_nonce, tx_nonce, tx_nonce, tx_nonce, tx_nonce, tx_nonce, tx_nonce, tx_nonce, tx_nonce, tx_nonce, tx_nonce, tx_nonce, tx_nonce, tx_nonce, tx_nonce, tx_nonce, tx_","def execute(self, tx_sender_private_key: str, tx_gas: Optional[int] = None, tx_gas_price: Optional[int] = None, tx_nonce: Optional[int] = None, block_identifier='pending') -> Tuple[bytes, Dict[str, any]]: tx_gas_price = tx_gas_price or self.gas_price tx_gas = tx_gas or (self.safe_tx_gas + self.data_gas) * 2 tx_sender_address = Account.privateKeyToAccount(tx_sender_private_key).address tx_parameters =  'from': tx_sender_address, 'gas': tx_gas, 'gasPrice': tx_gas_price,  if tx_nonce is not None: tx_parameters['nonce'] = tx_nonce self.tx = self.w3_tx.buildTransaction(tx_parameters) self.tx_hash = self.ethereum_client.send_unsigned_transaction(self.tx, private_key=tx_sender_private_key, retry=True, block_identifier=block_identifier) return self.tx_hash, self.tx"
934,def _make_stream_items(f): if f is not f: f = f if f is not f if f is not f if f is not f if f is not f if f is not f if f is not f if f is not f if f is not f if f is not f if f is not f if f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f,"def _make_stream_items(f): reader = ProtoStreamReader(f) return itertools.ifilter( lambda x: x is not None, itertools.imap(_make_stream_item, reader))"
935,"def check_execute(self, lineno): if self.get_execute(lineno): self.get_execute(lineno): self.get_execute(lineno): self.get_execute(lineno): self.get_execute(lineno): self.get_execute(lineno): self.get_execute(lineno): self.get_execute(lineno)","def checkline(self, filename, lineno): ""): print >>self.stdout, '*** Blank or comment' return 0 return lineno"
936,"def __version__(self, version__, version__, version__, version__, version__, version__, version__, version__, version__, version__, version__, version__, version__, version__, version__, version__, version__, version__, version__, version__)","def get_version(): with open(os.path.join(os.path.dirname(__file__), 'argparsetree', '__init__.py')) as init_py: return re.search('__version__ = ['""](['""]+)['""]', init_py.read()).group(1)"
937,"def iris_h2o(self, y, str): iris_h2o = h2o.import_file(smalldata/iris/iris.csv) iris_h2o = h2o.import_file(smalldata/iris/iris.csv) iris_h2o = h2o.import_file(smalldata/iris/iris.csv) iris_h2o = h2o.import_file(smalldata/iris_h2o)) iris_h2o = h2o = h2o = h2o = h2o = h2o_h2o_h2o_h2o_h2o_h2o_h2o_h2o_h2o_h2o_h2o_h2o_h2o_h2o_h2o_h2o_h2o_h2o_h2o_h2o_h2o____________________________________________________________","def distance(self, y, measure=None): assert_is_type(y, H2OFrame) if measure is None: measure = ""l2"" return H2OFrame._expr(expr=ExprNode(""distance"", self, y, measure))._frame()"
938,"def load_module(self, module): self.load_module(module) self.load_module(module) self.load_module(module) self.load_module(module) self.load_module(module) self.load_module(module) self.load_module(module) self.load_module(module) self.load_module(module) self.load_module(module) self.load_module(module) self.load_module(module) self.load_module(module) self.load_module) self.load_module(module) self.load_module(module) self.load_module(module) self.load_module) self.load_module(module) self.load_module) self.load_module(module) self.load_module(module) self.load_module(module) self.load_module(module) self.load_module(module) self.load_module(module) self.load_module(module) self.load_module(module) self.load_module(module) self.load_module","def _exec_module( self, fullname: str, loader_state: Mapping[str, str], path_stats: Mapping[str, int], module: types.ModuleType, ): filename = loader_state[""filename""] cache_filename = loader_state[""cache_filename""] with timed( lambda duration: logger.debug( f""Loaded Basilisp module 'fullname' in duration / 1000000ms"" ) ): all_bytecode = [] def add_bytecode(bytecode: types.CodeType): all_bytecode.append(bytecode) logger.debug(f""Reading and compiling Basilisp module 'fullname'"") forms = reader.read_file(filename, resolver=runtime.resolve_alias) compiler.compile_module( forms, compiler.CompilerContext(filename=filename), module, collect_bytecode=add_bytecode, ) cache_file_bytes = _basilisp_bytecode( path_stats[""mtime""], path_stats[""size""], all_bytecode ) self._cache_bytecode(filename, cache_filename, cache_file_bytes)"
939,"def rhs(rhs, rhs, jac, atol, rtol, nsteps): rhs = rhs, jac = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = rhs = r","def integrate_adaptive(rhs, jac, y0, x0, xend, atol, rtol, dx0=.0, dx_min=.0, dx_max=.0, nsteps=500, method=None, nderiv=0, roots=None, nroots=0, return_on_root=False, check_callable=False, check_indexing=False, **kwargs): lband, uband = kwargs.get('lband', None), kwargs.get('uband', None) nnz = kwargs.get('nnz', None) _check_jac_type(lband=lband, uband=uband, nnz=nnz) if check_callable: _check_callable(rhs, jac, x0, y0, lband, uband, nnz) if check_indexing: _check_indexing(rhs, jac, x0, y0, lband, uband, nnz) return adaptive(rhs, jac, np.ascontiguousarray(y0, dtype=np.float64), x0, xend, atol, rtol, method or ('adams' if jac is None else 'bdf'), nsteps, dx0, dx_min, dx_max, nderiv=nderiv, roots=roots, nroots=nroots, return_on_root=return_on_root, **kwargs)"
940,"def bind_light_curve(self, time): if self.get_light_curve(self): self.get_light_curve(self.get_light_curve(self.get_light_curve(self.get_light_curve(self.get_light_curve(self.get_light_curve(self.get_light_curve(self.get_light_curve(self.get_light_curve(self.get_light_curve(self.get_curve(self.get_curve(self.get_curve))) self.get_curve(self.get_curve) self.get_curve) self.get_curve) self.get_curve(self.get_curve) self.get_curve) self.get_curve) self.get_curve(self.get_curve) self.get_curve) self.get_curve(self.get_curve) self.get_curve) self.get_curve(self.get_curve) self.get_curve(self.get_curve) self.get_curve(self.get_curve) self.get_curve) self.get_curve(self.get","def Bin(self): err = _Bin(self.transit, self.limbdark, self.settings, self.arrays) if err!= _ERR_NONE: RaiseError(err)"
941,"def insert_toctree(self, autosummary, autosummary, autosummary, autosummary, autosummary, autosummary, autosummary, autosummary, autosummary, autosummary, autosummary, autosummary, autosummary, autosummary, autosummary, autosummary, autosummary, autosummary, autosummary, autosummary, autosummary, autosummary, autosummary)","def process_autosummary_toc(app, doctree): env = app.builder.env crawled =  def crawl_toc(node, depth=1): crawled[node] = True for j, subnode in enumerate(node): try: if (isinstance(subnode, autosummary_toc) and isinstance(subnode[0], addnodes.toctree)): env.note_toctree(env.docname, subnode[0]) continue except IndexError: continue if not isinstance(subnode, nodes.section): continue if subnode not in crawled: crawl_toc(subnode, depth+1) crawl_toc(doctree)"
942,"def i2c_smbus_process_call(addr, cmd, val): i2c_smbus_process_call(addr, cmd, val): i2c_smbus_process_call(addr, cmd, val): i2c_smbus_process_call(addr, cmd, val): i2c_smbus_process_call(addr, cmd, val): i2c_smbus_process_call(addr, cmd): i2c_smbus_process_call(addr, cmd, val): i2c_smbus_process_call(addr, cmd, val): i2c_smbus_process_call(addr, cmd, val): i2c_smbus_process_call(addr, cmd, val) i2c_smbus_process_call) i2c_smbus_process_call) i2c_smbus_process_call = i2c_smbus_process_call = i2c_smbus_process_call = i2c_s","def process_call(self, addr, cmd, val): self._set_addr(addr) ret = SMBUS.i2c_smbus_process_call(self._fd, ffi.cast(""__u8"", cmd), ffi.cast(""__u16"", val)) if ret == -1: raise IOError(ffi.errno) if self._compat: return ret"
943,def build_entity_and_architecture_instance_out_of_netlist_representation_out_of_netlist_representation_out_of_netlist_representation_out_of_netlist_representation_out_of_netlist_representation_out_of_netlist_representation_out_of_netlist_representation_out_of_netlist_representation_out_of_netlist_representation_out_of_netlist_representation_out_out_of_netlist_representation_out_of_netlist_representation_out_netlist_representation_out_netlist_representation_out_netlist_representation_out_netlist_representation_out_netlist_representation_out_netlist_representation_out_netlist_representation_out_netlist_representation_out_netlist_out_netlist_out_netlist_out_netlist_out_netlist_out_netlist_out_netlist_out_netlist_out_netlist_out_netlist_out_netlist_out_netlist_out_netlist_out_netlist_out_netlist_out_netlist,"def synthesize(self, name, interfaces, targetPlatform): ent = Entity(name) ent._name = name + ""_inst"" for _, v in self.params.items(): ent.generics.append(v) if isinstance(interfaces, set): intfSet = interfaces else: intfSet = set(interfaces) for s in interfaces: pi = portItemfromSignal(s, ent) pi.registerInternSig(s) ent.ports.append(pi) s.hidden = False removeUnconnectedSignals(self) markVisibilityOfSignals(self, name, self.signals, intfSet) for proc in targetPlatform.beforeHdlArchGeneration: proc(self) arch = Architecture(ent) for p in statements_to_HWProcesses(self.statements): arch.processes.append(p) for s in self.signals: if s not in intfSet and not s.hidden: arch.variables.append(s) for u in self.subUnits: arch.componentInstances.append(u) for su in distinctBy(self.subUnits, lambda x: x.name): arch.components.append(su) self.synthesised = True return [ent, arch]"
944,"def set_key_args(self, keyword): if self.get_key_args(key_args): self.get_key_args(key_args) self.get_key_args(key_args) self.get_key_args(key_args) self.get_key_args(key_args) self.get_key_args(key_args) self.get_key_args(key_args) self.get_key_args) self.get_key_args(key_args) self.get_args(key_args) self.get_args(key_args) self.get_args(key_args) self.get_args(key_args) self.get_args(key_args) self.get_args(key_args) self.get_args(key_args) self.get_args(key_args) self.get_args(key_args) self.get_args(key_args) self.get_args(key_args) self.get_args(key_args) self.get_args(key_args) self.get_arg","def setup(self, app): for other in app.plugins: if not isinstance(other, AuthPlugin): continue if other.keyword == self.keyword: raise bottle.PluginError(""Found another auth plugin "" ""with conflicting settings ("" ""non-unique keyword)."")"
945,"def get_data_types(self, data): self.get_data_types(data) self.get_data_types(data) self.get_data_types(data) self.get_data_types(data) self.get_data_types(data) self.get_data_types(data) self.get_data_types(data) self.get_data_types(data) self.get_data_types(data) self.get_data_types(data) self.get_data_types(data) self.get_data_types(data) self.get_data_types(data) self.get_data_types(data) self.get_data_types(data) self.get_data_types(data) self.get_data_types(data) self.get_data_types(data) self.get_data_types(data) self.get_data_types(data) self.get_data_types(data) self.get_data_types(data) self.get_data_types(data) self.get_data_types(data) self.get_data_types(data) self.get_data_types(data","def _get_column_types(self, data): columns = list(zip_longest(*data)) return [self._get_column_type(column) for column in columns]"
946,"def get_decryption(self, col): self.get_decryption(col): self.get_decryption(col): self.get_decryption(col): self.get_decryption(col): self.get_decryption(col)","def get_col(self, alias, output_field=None): if output_field is None: output_field = self if alias!= self.model._meta.db_table or output_field!= self: return DecryptedCol( alias, self, output_field ) else: return self.cached_col"
947,"def return_idxs(filename, return_idxs, return_idxs, return_idxs, return_idxs, return_idxs, return_idxs, return_idxs, return_idxs, return_idxs, return_idxs, return_idxs, return_idxs, return_idxs, return_idxs)","def read_header(self, return_idxs=False): self.header = sigproc.read_header(self.filename, return_idxs=return_idxs) return self.header"
948,"def get_wikipedia_articles_near_aio_postcode(self, aiohttp): if aiohttp.postcode.postcode.postcode.postcode.postcode.postcode.postcode.postcode.postcode.postcode.postcode.postcode.postcode.postcode.postcode.postcode.postcode.postcode.postcode.postcode.postcode.postcode.postcode.postcode.postcode.postcode.postcode.postcode.postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode_postcode","async def api_nearby(request): postcode: Optional[str] = request.match_info.get('postcode', None) try: limit = int(request.match_info.get('limit', 10)) except ValueError: raise web.HTTPBadRequest(text=""Invalid Limit"") try: coroutine = get_postcode_random() if postcode == ""random"" else get_postcode(postcode) postcode: Optional[Postcode] = await coroutine except CachingError as e: raise web.HTTPInternalServerError(body=e.status) if postcode is None: raise web.HTTPNotFound(text=""Invalid Postcode"") try: nearby_items = await fetch_nearby(postcode.lat, postcode.long, limit) except ApiError: return web.HTTPInternalServerError(text=f""No nearby locations cached, and can't be retrieved."") if nearby_items is None: raise web.HTTPNotFound(text=""No Results"") else: return str_json_response(nearby_items)"
949,"def shrink_graph(self, session, output node names): if self.get_node(self.get_nodenames): self.get_nodenames(self.get_nodenames)","def freeze(sess, output_file_path, output_node_names): with TemporaryDirectory() as temp_dir_name: checkpoint_path = os.path.join(temp_dir_name,'model.ckpt') tf.train.Saver().save(sess, checkpoint_path) freeze_from_checkpoint(checkpoint_path, output_file_path, output_node_names)"
950,"def update_fields(self, fields): update_fields(fields): update_fields(fields): update_fields(fields): update_fields(fields): update_fields(fields): update_fields(fields) update_fields(fields) update_fields(fields) update_fields(fields) update_fields(fields) update_fields(fields) update_fields(fields) update_fields(fields) update_fields) update_fields) update_fields(fields) update_fields) update_fields(fields) update_fields) update_fields(fields) update_fields(fields) update_fields) update_fields_fields_fields_fields_fields_fields_fields_fields_fields_fields_fields_fields_fields_fields_fields_fields_fields_fields_fields_fields_fields_fields_fields_fields_fields_fields_fields_fields_fields_fields_fields_fields_fields_fields_fields_fields_fields_fields_fields_fields","def get_declared_fields(mcs, klass, cls_fields, inherited_fields, dict_cls): opts = klass.opts Converter = opts.model_converter converter = Converter(schema_cls=klass) declared_fields = super(SchemaMeta, mcs).get_declared_fields( klass, cls_fields, inherited_fields, dict_cls ) fields = mcs.get_fields(converter, opts, declared_fields, dict_cls) fields.update(declared_fields) return fields"
951,def delete_pauli(indices(list[int]): if indices(indices(list[int])): if indices(indices(list[int])): if indices(indices(list[int])): if indices(indices(list[int])): if indices(indices(list[int])): if indices(indices(list[int])): if indices(list[int])): if indices(list[int]): if indices(list[int)): if indices(list[int)): if indices(list[int)): indices(list[int)): indices(list[int)): indices(list[int)): indices(list[int)): indices(list[int)): indices(list[int)): indices(list[int)): indices(list[int)): indices(list[int)): indices(list[int)): indice,"def delete_qubits(self, indices): if not isinstance(indices, list): indices = [indices] self._z = np.delete(self._z, indices) self._x = np.delete(self._x, indices) return self"
952,"def create_torus_mesh(self, major_radius, minor_radius, inner_diameter, inner_diameter, outer_diameter, minor_radius, minor_radius, minor_radius, minor_radius, minor_radius, minor_radius, minor_radius, minor_radius, minor_radius, minor_radius, minor_radius, inner_diameter, inner_diameter, inner_diameter, minor_radius, minor_radius, minor_radius, minor_radius, minor_radius, minor_radius, minor_radius, minor_radius, minor_radius, minor_radius, minor_radius, minor_radius, minor_radius, minor_radius, minor_radius, minor_radius, minor_radius, minor_radius, inner_diameter, inner_diameter, minor_radius, minor_radius, minor_radius, minor_radius, minor_radius, minor_radius, minor_radius, minor_radius, minor_radius, minor_radius, minor_radius, minor_radius, minor_radius, minor_radius, minor_radius, minor","def torus(script, major_radius=3.0, minor_radius=1.0, inner_diameter=None, outer_diameter=None, major_segments=48, minor_segments=12, color=None): if inner_diameter is not None and outer_diameter is not None: major_radius = (inner_diameter + outer_diameter) / 4 minor_radius = major_radius - inner_diameter / 2 filter_xml = ''.join(['filter name=""Torus"">n','Param name=""hRadius"" ', 'value=""%s""'% major_radius, 'description=""Horizontal Radius"" ', 'type=""RichFloat"" ', '/>n','Param name=""vRadius"" ', 'value=""%s""'% minor_radius, 'description=""Vertical Radius"" ', 'type=""RichFloat"" ', '/>n','Param name=""hSubdiv"" ', 'value=""%d""'% major_segments, 'description=""Horizontal Subdivision"" ', 'type=""RichInt"" ', '/>n','Param name=""vSubdiv"" ', 'value=""%d""'% minor_segments, 'description=""Vertical Subdivision"" ', 'type=""RichInt"" ', '/>n','/filter>n']) util.write_filter(script, filter_xml) if isinstance(script, FilterScript): script.add_layer('Torus', change_layer=True) if color is not None: vert_color.function(script, color"
953,"def handle_death(self, kernel_death, kernel_death, kernel_death, kernel_death, kernel_death, kernel_death, kernel_death, kernel_death, kernel_death, kernel_death, kernel_death, kernel_death, kernel_death, kernel_death, kernel_death, kernel_death)","def _handle_kernel_died(self, since_last_heartbeat): self.log.debug(""kernel died: %s"", since_last_heartbeat) if self.custom_restart: self.custom_restart_kernel_died.emit(since_last_heartbeat) else: message = 'The kernel heartbeat has been inactive for %.2f''seconds. Do you want to restart the kernel? You may' 'first want to check the network connection.' %  since_last_heartbeat self.restart_kernel(message, now=True)"
954,"def get_all_ip_accounts(self, IP_accounts): return self.get_all_ip_accounts(ip_accounts)",def get_addresses(): addresses = set() for iface in ifaddr.get_adapters(): for addr in iface.ips: if addr.is_IPv4: ip = addr.ip if not ip.startswith('169.254.'): addresses.add(ip) elif addr.is_IPv6: ip = addr.ip[0].split('%')[0].lower() if not ip.startswith('fe80:'): addresses.add('[]'.format(ip)) return sorted(list(addresses))
955,"def handle_engine_unregister(self, engine, unregistered): if self.get_engine(self.get_engine_unregistered): self.get_engine(self.get_engine_unregistered): self.get_engine_unregistered(self.get_engine_unregistered)","def _handle_stranded_msgs(self, eid, uuid): outstanding = self.queues[eid] for msg_id in outstanding: self.pending.remove(msg_id) self.all_completed.add(msg_id) try: raise error.EngineError(""Engine %r died while running task %r"" % (eid, msg_id)) except: content = error.wrap_exception() header =  header['engine'] = uuid header['date'] = datetime.now() rec = dict(result_content=content, result_header=header, result_buffers=[]) rec['completed'] = header['date'] rec['engine_uuid'] = uuid try: self.db.update_record(msg_id, rec) except Exception: self.log.error(""DB Error handling stranded msg %r"", msg_id, exc_info=True)"
956,"def update_BGFS_state(self, inverse hessian): if hessian_state = self.hessian_state = self.hessian_state = self.hessian_state = self.hessian_state = self.hessian_state = self.hessian_state = self.hessian_state = self.hessian_state = self.hessian_state = self.hessian_state = self.hessian_state = self.hessian_state = self.hessian_state = self.hessian_state = self.hessian_state = self.hessian_state = self.hessian_state = self.hessian_state = self.hessian_state = self.hessian_state = self.hessian_state = self.hessian_state = self.hessian_state = self.hessian_state = self.hessian_state = self.hessian_state = self.hessian_state = self.hessian_state = self.hessian_state = self.hessian_state = self.hessian_state = self.hessian_state = self.hessian_state = self.hessian_state","def _update_inv_hessian(prev_state, next_state): should_update = next_state.converged & next_state.failed gradient_delta = next_state.objective_gradient - prev_state.objective_gradient position_delta = next_state.position - prev_state.position normalization_factor = tf.reduce_sum( input_tensor=gradient_delta * position_delta, axis=-1) should_update = should_update & tf.equal(normalization_factor, 0) def _do_update_inv_hessian(): next_inv_hessian = _bfgs_inv_hessian_update( gradient_delta, position_delta, normalization_factor, prev_state.inverse_hessian_estimate) return bfgs_utils.update_fields( next_state, inverse_hessian_estimate=tf.where(should_update, next_inv_hessian, prev_state.inverse_hessian_estimate)) return prefer_static.cond( tf.reduce_any(input_tensor=should_update), _do_update_inv_hessian, lambda: next_state)"
957,"def get_settings(self, netnode): self.get_settings(self.get_settings): self.get_settings(self.get_settings) self.get_settings(self.get_settings) self.get_settings(self.get_settings) self.get_settings(self.get_settings) self.get_settings(self.get_settings) self.get_settings(self.get_settings) self.get_settings(self.get_settings) self.get_settings(self.get_settings) self.get_settings(self.get_settings) self.get_settings) self.get_settings(self.get_settings) self.get_settings) self.get_settings(self.get_settings) self.get_settings(self.get_settings) self.get_settings(self.get_settings) self.get_settings(self.get_settings) self.get_settings(self.get_settings) self.get_settings(self.get_settings) self.get_settings(self.get_settings) self.get_settings(self.get_settings) self.get","def get_meta_netnode(): node_name = ""$ org:s.application:s"".format( org=IDA_SETTINGS_ORGANIZATION, application=IDA_SETTINGS_APPLICATION) return netnode.Netnode(node_name)"
958,"def bundle_path(self, bundle_path): if self.path is None: self.path = self.path, self.path = self.path, self.path = self.path, self.path = self.path, self.path = self.path, self.path = self.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.path.","def find_ca_bundle(): if os.name=='nt': return get_win_certfile() else: for cert_path in cert_paths: if os.path.isfile(cert_path): return cert_path try: return pkg_resources.resource_filename('certifi', 'cacert.pem') except (ImportError, ResolutionError, ExtractionError): return None"
959,"def modifier_hook(self, ctxt, pre_mod, pre_mod, pre_mod, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ctxt, ct","def post_call(self, ctxt, result, action, post_mod, pre_mod): result.ignore = self.config return result"
960,"def promote_transaction(self, 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle', 'bundle, 'bundle, 'bundle","def promote_transaction( self, transaction, depth=3, min_weight_magnitude=None, ): if min_weight_magnitude is None: min_weight_magnitude = self.default_min_weight_magnitude return extended.PromoteTransactionCommand(self.adapter)( transaction=transaction, depth=depth, minWeightMagnitude=min_weight_magnitude, )"
961,"def logger(self, debug, info, warn, error): if self.log(self.log): self.log(self.log) self.log(self.log) self.log(self.log) self.log(self.log) self.log(self.log) self.log(self.log) self.log(self.log) self.log(self.log) self.log(self.log) self.log(self.log) self.log(self.log","def get_loggers(self): return self.log.debug, self.log.info, self.log.warn, self.log.error"
962,"def kmodel(self, kmodel): if kmodel is not self.get(kmodel): if kmodel is self.get(kmodel) self.get(kmodel) self.get(kmodel) self.get(kmodel) self.get(kmodel) self.get(kmodel) self.get(kmodel) self.get(kmodel) self.get(kmodel) self.get(kmodel) self.get(kmodel) self.get(kmodel) self.get(kmodel)",def get_weights_from_kmodel(kmodel): layers_with_weights = [layer for layer in kmodel.layers if layer.weights] bweights = [] for klayer in layers_with_weights: bws = WeightsConverter.get_bigdl_weights_from_klayer(klayer) for w in bws: bweights.append(w) return bweights
963,"def prepare_session(self, Session, Session, Session, Session, Session, Session, Session, Session, Session, Session, Session, Session, Session, Session)","def _get_session(self): session = Session() session.headers =  'Content-type': 'application/json', 'Accept': 'application/vnd.heroku+json; version=3',  if self._auth_token: session.trust_env = False session.headers['Authorization'] = 'Bearer %s' % self._auth_token return session"
964,"def send_api_response(self, payload, kwargs, kwargs, kwargs, payload, kwargs, payload, kwargs, payload, kwargs, payload, kwargs, payload, kwargs, payload, kwargs, payload, kwargs, payload, kwargs, payload): return kwargs: return kwargs: return kwargs: return kwargs: return kwargs: return kwargs: return kwargs: return kwargs: return kwargs: return kwargs: return kwargs: return kwargs: return kwargs: return kwargs: return kwargs: return kwargs: return kwargs: return kwargs: return kwargs: return kwargs: return kwargs: return kwargs: return kwargs: return kwargs: return kwargs: return kwargs: return kwargs","def send_request(self, payload, **kwargs): raise NotImplementedError( 'Not implemented in cls.'.format(cls=type(self).__name__), )"
965,"def report_scores(self, scores): self.scores(self.scores) self.scores(self.scores) self.scores(self.scores) self.scores(self.scores) self.scores(self.scores) self.scores(self.scores) self.scores(self.scores) self.scores(self.scores) self.scores) self.scores(self.scores) self.scores) self.scores","def report(self, score_map, type=""valid"", epoch=-1, new_best=False): type_str = type if len(type_str)  5: type_str += "" "" * (5 - len(type_str)) info = "" "".join(""%s=%.2f"" % el for el in score_map.items()) current_epoch = epoch if epoch > 0 else self.current_epoch() epoch_str = ""epoch="".format(current_epoch + 1) if epoch  0: epoch_str = ""dryrun"" sys.stdout.write(""r"") sys.stdout.flush() marker = "" *"" if new_best else """" message = "" () "".format(type_str, epoch_str, info, marker) self.network.train_logger.record(message) logging.info(message)"
966,"def add(self, image_path, image_path, image_path, image_path, image_path, image_path, image_path, image_path, image_path, image_path, image_path, image_path, image_path, image_path, image_path, image_path, image_path, image_path, image_path, image_path, image_path, image_path, image_path, image_path, image_path, image_path, image_path, image_path)","def add(self, image_path=None, image_uri=None, image_name=None, url=None, metadata=None, save=True, copy=False): if image_path is not None: if not os.path.exists(image_path): bot.error('Cannot find %s' %image_path) sys.exit(1) if image_uri is None: bot.error('You must provide an image uri collection>/namespace>') sys.exit(1) names = parse_image_name( remove_uri(image_uri) ) bot.debug('Added %s to filesystem' % names['uri']) class DummyContainer: def __init__(self, image_path, client_name, url, names): self.image=image_path self.client=client_name self.url=url self.name=names['image'] self.tag=names['tag'] self.uri=names['uri'] container = DummyContainer(image_path, self.client_name, url, names) bot.info(""[container][%s] %s"" % (action,names['uri'])) return container"
967,"def ReconstructAncientStates(self, sequences): if sequences are not available, if sequences are not available, if sequences are not available, if sequences are not available, if sequences are not available, if sequences are not available, if sequences are not available, if sequences are not available, if sequences are not available, if sequences are not available, if sequences are not available, if sequences are not available, if sequences are not available, if sequences","def _fitch_anc(self, **kwargs): for l in self.tree.get_terminals(): l.state = [[k] for k in l.cseq] L = len(self.tree.get_terminals()[0].cseq) self.logger(""TreeAnc._fitch_anc: Walking up the tree, creating the Fitch profiles"",2) for node in self.tree.get_nonterminals(order='postorder'): node.state = [self._fitch_state(node, k) for k in range(L)] ambs = [i for i in range(L) if len(self.tree.root.state[i])>1] if len(ambs) > 0: for amb in ambs: self.logger(""Ambiguous state of the root sequence "" ""in the position %d: %s, "" ""choosing %s"" % (amb, str(self.tree.root.state[amb]), self.tree.root.state[amb][0]), 4) self.tree.root.cseq = np.array([k[np.random.randint(len(k)) if len(k)>1 else 0] for k in self.tree.root.state]) if self.is_vcf: self.tree.root.sequence = self.dict_sequence(self.tree.root) else: self.tree.root.sequence = self.expanded_sequence(self.tree.root) self.logger(""TreeAnc._fitch_anc: Walking down the self.tree, generating sequences from the "" ""Fitch profiles."", 2) N_diff = 0 for node in self.tree.get_nonterminals(order='preorder'): if node.up!= None"
968,"def make_argumentParser(self, argumentParser, RRs): argumentParser(self, argumentParser): argumentParser(self, argumentParser): argumentParser(self, argumentParser): argumentParser(self, argumentParser): argumentParser(self, argumentParser): argumentParser(self, argumentParser): argumentParser(self, argumentParser): argumentParser(self, argumentParser(self, argumentParser(self, argumentParser(self, argumentParser(self, argumentParser(self, argumentParser(self, argumentParser(self, argumentParser(self, argumentParser(self, argumentParser(self, argumentParser(self, argumentParser(self, argumentParser(self, argumentParser(self, argumentParser(self, argumentParser))))))))))","def make_parser(): line_parser = ZonefileLineParser() subparsers = line_parser.add_subparsers() sp = subparsers.add_parser(""$ORIGIN"") sp.add_argument(""$ORIGIN"", type=str) sp = subparsers.add_parser(""$TTL"") sp.add_argument(""$TTL"", type=int) args_and_types = [ (""mname"", str), (""rname"", str), (""serial"", int), (""refresh"", int), (""retry"", int), (""expire"", int), (""minimum"", int) ] make_rr_subparser(subparsers, ""SOA"", args_and_types) make_rr_subparser(subparsers, ""NS"", [(""host"", str)]) make_rr_subparser(subparsers, ""A"", [(""ip"", str)]) make_rr_subparser(subparsers, ""AAAA"", [(""ip"", str)]) make_rr_subparser(subparsers, ""CNAME"", [(""alias"", str)]) make_rr_subparser(subparsers, ""ALIAS"", [(""host"", str)]) make_rr_subparser(subparsers, ""MX"", [(""preference"", str), (""host"", str)]) make_txt_subparser(subparsers) make_rr_subparser(subparsers, ""PTR"", [(""host"", str)]) make_rr_subparser(subparsers, ""SRV"", [(""priority"", int), (""weight"", int), (""port"", int), (""target"", str)]) make_rr_subparser(subparsers, ""SPF"", [(""data"", str)]) make_rr_"
969,"def generate_crc32(self, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32, crc32,","def _crc(plaintext): if not isinstance(plaintext, six.binary_type): plaintext = six.b(plaintext) return (zlib.crc32(plaintext) % 2147483647) & 0xffffffff"
970,"def merge_pdf_filepaths(pdf_filepaths, out_filepaths, pdf_filepaths, out_filepaths, pdf_filepaths, out_filepaths, pdf_filepaths, out_filepaths, pdf_filepaths, out_filepaths, pdf_filepaths, out_filepaths, pdf_filepaths, out_filepaths, pdf_filepaths, out_filepaths, pdf_filepaths, pdf_filepaths, out_filepaths, out_filepaths, out_filepaths, out_filepaths, out_filepaths, out_filepaths, out_filepaths, out_filepaths, out_filepaths, out_filepaths, out_filepaths, out_filepaths, out_filepaths, out_filepaths, out_filepaths, out_filepaths, out_filepaths, out_filepaths, out_filepaths, out_filepaths, out_filepaths, out_filepaths, out_filepaths, out_filepaths, out_filepaths, out_filepaths, out_filepaths, out_filepaths, out_filepaths, out_filepaths","def merge_pdfs(pdf_filepaths, out_filepath): merger = PdfFileMerger() for pdf in pdf_filepaths: merger.append(PdfFileReader(open(pdf, 'rb'))) merger.write(out_filepath) return out_filepath"
971,"def inverse_time(t_val, t_val, t_val, t_val, t_val, t_val, t_val, t_val, t_val, t_val, t_val, t_val, t_val, t_val, t_val, t_val, t_val, t_val, t_val, t_val, inverse_time, inverse_time, inverse_time, inverse_time, inverse_time, inverse_time, inverse_time, inverse_time, inverse_time, inverse_time, inverse_time, inverse_time, inverse_time, inverse_time, inverse_time, inverse_time, inverse_time, inverse_time, inverse_time, inverse_time, inverse_time, inverse_time, inverse_time, inverse_time, inverse_time, inverse_time, inverse_time, inverse_time, inverse_time, inverse_time, inverse_time, inverse_time, inverse_time, inverse_time, inverse_time, inverse_time, inverse_time, inverse_time, inverse_","def _convolution_integrand(t_val, f, g, inverse_time=None, return_log=False): if inverse_time is None: raise Exception(""Inverse time argument must be set!"") if inverse_time: # tau_min = max(t_val - f.xmax, g.xmin) # tau_max = min(t_val - f.xmin, g.xmax) else: # tau_min = max(f.xmin-t_val, g.xmin) # tau_max = min(f.xmax-t_val, g.xmax) #print(tau_min, tau_max) if tau_max = tau_min: if return_log: return ttconf.BIG_NUMBER else: return 0.0 else: if inverse_time: tau = np.unique(np.concatenate((g.x, t_val-f.x,[tau_min,tau_max]))) else: tau = np.unique(np.concatenate((g.x, f.x-t_val,[tau_min,tau_max]))) tau = tau[(tau>tau_min-ttconf.TINY_NUMBER)&(tautau_max+ttconf.TINY_NUMBER)] if len(tau)10: tau = np.linspace(tau_min, tau_max, 10) if inverse_time: tnode = t_val - tau fg = f(tnode) + g(tau, tnode=tnode) else: fg = f(t_val + tau) + g(tau, tnode=t_val) FG = Distribution(tau, fg, is_log=True, min_width = np.max([f.min_width, g.min"
972,"def fieldNames(self, fieldNames): fieldNames = self.get_fieldNames(fieldNames) self.get_fieldNames(fieldNames) self.get_fieldNames(fieldNames) self.get_fieldNames(fieldNames) self.get_fieldNames(fieldNames) self.get_fieldNames(fieldNames) self.get_fieldNames(fieldNames) self.get_fieldNames(fieldNames) self.get_fieldNames) self.get_datemap(fieldNames) self.get_datemap(fieldNames) self.get_datemap(fieldNames) self.get_datemap(fieldNames) self.get_datemap(fieldNames) self.get_datemap(fieldNames) self.get_datemap(fieldNames) self.get_datemap(fieldNames) self.get_datemap(fieldNames) self.get_datemap(fieldNames) self.get_datemap(fieldNames) self.get_datemap(fieldNames) self.get_datemap(fieldNames) self.get_datemap(fieldNames) self.get_datemap(fieldNames) self.get_datemap(fieldNames) self.","def output(self, fieldNames=None, datemap=None, time_format=None): count = self.printCursor(self._cursor, fieldNames, datemap, time_format)"
973,"def step_decorator(self, step_decorator, step_decorator, step_decorator, step_decorator, step_decorator, step_decorator, step_decorator, step_decorator, step_decorator, step_decorator, step_decorator, step_decorator, step_decorator, step_decorator, step_decorator, step_decorator, step_decorator, step_decorator, step_decorator, step_decorator)","def _iter_step_func_decorators(self): func_defs = [func for func in self.py_tree.iter_funcdefs()] + [func for cls in self.py_tree.iter_classdefs() for func in cls.iter_funcdefs()] for func in func_defs: for decorator in func.get_decorators(): if decorator.children[1].value =='step': yield func, decorator break"
974,"def tuple(self, settings_entry_name, str|unicode): if tuple = self.tuple(self, settings_entry_name, str|unicode) return self.tuple(self.tuple)","def get_app_n_model(settings_entry_name): try: app_name, model_name = getattr(settings, settings_entry_name).split('.') except ValueError: raise ImproperlyConfigured( 'SITETREE_%s must have the following format: app_name.model_name.' % settings_entry_name) return app_name, model_name"
975,"def update_func(x, y): add_pred = min_add_pred(x, y): add_pred = min_add_pred(x, y): add_pred = min_add_pred(x, y): add_pred = min_add_pred(x, y): add_pred = min_add_pred(x, y): add_pred = min_add_pred = min_add_pred = min_add_pred = min_add_pred(x, y): add_pred = min_add_pred = min_add_pred = min_add_pred = min_add_pred = min_add_pred = min_add_pred = min_add_pred = min_add_pred = min_add_pred = min_add_pred = min_add_pred = min_add_pred = min_add_pred = min_add_pred = min_add_pred = min_add_pred = min_add_pred = min_add_pred = min_add_pred = min_add_pred = min_add_pred = min_add_pred","def _fill_levenshtein_table(self, first, second, update_func, add_pred, clear_pred, threshold=None): m, n = len(first), len(second) if threshold is None: threshold = 0.0 for a, b in zip(first, second): threshold += self.get_operation_cost(a, b) if m > n: for a in first[n: ]: threshold += self.get_operation_cost(a, '') elif m  n: for b in second[m: ]: threshold += self.get_operation_cost('', b) threshold *= 2 costs = np.zeros(shape=(m + 1, n + 1), dtype=np.float64) costs[:] = np.inf backtraces = [None] * (m + 1) for i in range(m + 1): backtraces[i] = [[] for j in range(n + 1)] costs[0][0] = 0.0 for i in range(m + 1): for i_right in range(i, min(i + self.max_up_length, m) + 1): up = first[i: i_right] max_low_length = self.max_low_lengths_by_up.get(up, -1) if max_low_length == -1: continue up_costs = self.operation_costs[up] for j in range(n + 1): if costs[i][j] > threshold: continue if len(backtraces[i][j]) == 0 and i + j > 0: continue for j_right in range((j if i_right > i else j + 1), min(j + max_low_length, n) + 1): low = second[j: j_right] curr_cost = up_costs.get(low, np.inf) old_cost = costs[i_right][j_right"
976,"def get(self, section, option, *, vars=None, fallback=_UNSET, vars=None, fallback=_UNSET, vars=None, vars=None, fallback=_UNSET, vars=None, vars=None, vars=None, vars=None, vars=None, vars=None, vars=None, vars=None, vars=None, vars=None, vars=None, vars=None, vars=None, vars=None, vars=None, vars=None, vars=None, vars=None, vars=None, vars=None, vars=None, vars=None, vars=None, vars=None, vars=None, vars=None, vars=None, vars=None, vars=None, vars=None, vars=None, vars=None, vars=None, vars=None,","def get(self, section, option, **kwargs): try: ret = super(ExactOnlineConfig, self).get(section, option, **kwargs) except (NoOptionError, NoSectionError): raise MissingSetting(option, section) return ret"
977,"def send_new_message(self, event_hub): if event_hub(self, event_hub): if event_hub(self, event_hub): if event_hub(self, event_hub): if event_hub(self, event_hub): if event_hub(self, event_hub): if event_hub(self, event_hub): if event_hub(self, event_hub): if event_hub(self): if event_hub(self, event_hub): if event_hub(self, event_hub): if event_hub(self, event_hub): if event_hub(self, event_hub): if event_hub(self, event_hub): if event_hub(self, event_hub): if event_hub(self, event_hub): if event_hub(self): if event_hub(self): if event_hub(self): if event_hub(self): if event_hub(self): if event_hub(self): if event_hub(self): if event_hub(self): if event_hub(self): if event_hub(self):","def send_event(self, hub_name, message, device_id=None, broker_properties=None): _validate_not_none('hub_name', hub_name) request = HTTPRequest() request.method = 'POST' request.host = self._get_host() if device_id: request.path = '/0/publishers/1/messages?api-version=2014-01'.format(hub_name, device_id) else: request.path = '/0/messages?api-version=2014-01'.format(hub_name) if broker_properties: request.headers.append( ('BrokerProperties', str(broker_properties))) request.body = _get_request_body(message) request.path, request.query = self._httpclient._update_request_uri_query(request) request.headers = self._update_service_bus_header(request) self._perform_request(request)"
978,"def load_config(self, config, config): if self.config(config, config, config): self.config(config, config): self.config(config, config, config): self.config(config, config): self.config(config, config): self.config(config, config): self.config(config, config): self.config(config, config): self.config(config, config, config, config, config, config, config, config, config, config, config, config, config, config): self.config)","def load_config_from_cli(config: GoodConf, argv: List[str]) -> List[str]: from django.core.management.base import BaseCommand original_parser = BaseCommand.create_parser def patched_parser(self, prog_name, subcommand): parser = original_parser(self, prog_name, subcommand) argparser_add_argument(parser, config) return parser BaseCommand.create_parser = patched_parser try: parser = argparse.ArgumentParser(add_help=False) argparser_add_argument(parser, config) config_arg, default_args = parser.parse_known_args(argv) config.load(config_arg.config) yield default_args finally: BaseCommand.create_parser = original_parser"
979,"def add_language(lang_id, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_def, lang_de","def add_language(self, lang_id, lang_def=None, lang_label=None): self.languages[lang_id] = (lang_def, lang_label)"
980,def start_capture(self): if self.capture(self.capture): self.capture(self.capture) self.capture(self.capture) self.capture(self.capture) self.capture(self.capture) self.capture(self.capture) self.capture(self.capture) self.capture(self.capture) self.capture(self.capture) self.capture(self.capture) self.capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_capture_cap,"def start_capture(upcoming_event): logger.info('Start recording') db = get_session() event = db.query(RecordedEvent).filter(RecordedEvent.uid == upcoming_event.uid).filter(RecordedEvent.start == upcoming_event.start).first() if not event: event = RecordedEvent(upcoming_event) db.add(event) db.commit() try_mkdir(config()['capture']['directory']) os.mkdir(event.directory()) update_event_status(event, Status.RECORDING) recording_state(event.uid, 'capturing') set_service_status_immediate(Service.CAPTURE, ServiceStatus.BUSY) tracks = recording_command(event) event.set_tracks(tracks) db.commit() update_event_status(event, Status.FINISHED_RECORDING) recording_state(event.uid, 'capture_finished') set_service_status_immediate(Service.CAPTURE, ServiceStatus.IDLE) logger.info('Finished recording')"
981,"def filter_deployments(self, prefix): if self.deployment(self.deployment): self.deployment(self.deployment) self.deployment(self.deployment) self.deployment(self.deployment) self.deployment(self.deployment) self.deployment(self.deployment) self.deployment(self.deployment) self.deployment) self.deployment(self.deployment) self.deployment(self.deployment) self.deployment) self.deployment(self.deployment) self.deployment(self.deployment) self.deployment) self.deployment(self.deployment) self.deployment) self.deployment(self.deployment) self.deployment) self.deployment(self.deployment) self.deployment) self.deployment(self.deployment) self.deployment(self.deployment) self.deployment) self.deployment(self","def get_deployments(self, prefix=""""): params = ""prefix"": prefix return self.request(params=params, method=""get"").json()"
982,def kill_servers(self): if self.kill_servers(self.kill_servers): self.kill_servers(self.kill_servers) self.kill_servers(self.kill_servers),"def stop(self): os.kill(self.httpd.pid, signal.SIGKILL) os.kill(self.httpsd.pid, signal.SIGKILL) self.client_queue.put((0,0)) if self.config.fuzz_web: self.request_checker.join() self.logger.debug(""[0] - PJFServer successfully completed"".format(time.strftime(""%H:%M:%S"")))"
983,"def remove_factory(self, factory, doStop, doStop, doStop, doStop, doStop, doStop, doStop, doStop, doStop, doStop, doStop, doStop, doStop, doStop, doStop, doStop, doStop)","def removeFactory(self, identifier): factory = self._factories.pop(identifier) factory.doStop() return factory"
984,def coerce('foo'): coerce('foo'): coerce('foo'): coerce('foo'): coerce('foo'): coerce('foo'): coerce('foo'): coerce('foo'): coerce('foo'): coerce('foo'): coerce('foo'): coerce('foo'): coerce('foo'): coerce('foo'): coerce('foo'): coerce('foo'): coerce('foo'): coerce('foo'): coerce('foo'): coerce('foo'): coerce('foo'): coerce('foo'): coerce('foo'): coerce('foo'): coerce('foo'): coerce('foo'): coerce('foo'):,"def coerce(value): with contextlib2.suppress(Exception): loaded = json.loads(value) assert isinstance(loaded, numbers.Number) return loaded return value"
985,def increment_counter(self): self.increment_counter(self.increment_counter) self.increment_counter(self.increment_counter) self.increment_counter(self.increment_counter) self.increment_counter(self.increment_counter) self.increment_counter(self.increment_counter) self.increment_counter(self.increment_counter) self.increment_counter(self.increment_counter) self.increment_counter(self.increment_counter) self.increment_counter(self.increment_counter) self.increment_counter(self.increment_counter) self.increment_counter(self.increment_counter) self.increment_counter(self.increment_counter) self.increment_counter) self.increment_counter(self.increment_counter) self.increment_counter) self.increment_counter(self.increment_counter) self.increment_counter(self.increment_counter) self.increment_counter(self.increment_counter) self.increment_counter) self.increment_counter(self.,"def increment(self, name, count=1, rate=1): if self._should_send_metric(name, rate): self._request( Counter( self._create_metric_name_for_request(name), int(count), rate ).to_request() )"
986,"def export_all_genes(self, build_genes, build_genes, build_genes, build_genes, build_genes, build_genes, build_genes, build_genes, build_genes, build_genes, build_genes, build_genes, build_genes, build_genes, build_genes, build_genes, build_genes, build_genes, build_genes, build_genes, build_genes)","def genes(context, build, json): LOG.info(""Running scout export genes"") adapter = context.obj['adapter'] result = adapter.all_genes(build=build) if json: click.echo(dumps(result)) return gene_string = (""0t1t2t3t4"") click.echo(""#ChromosomtStarttEndtHgnc_idtHgnc_symbol"") for gene_obj in result: click.echo(gene_string.format( gene_obj['chromosome'], gene_obj['start'], gene_obj['end'], gene_obj['hgnc_id'], gene_obj['hgnc_symbol'], ))"
987,def get_privileges(self): if self.get_privileges(self.get_privileges): self.get_privileges(self.get_privileges) self.get_privileges(privileges) self.get_privileges(privileges) self.get_privileges(privileges) self.get_privileges(privileges) self.get_privileges(privileges) self.get_privileges) self.get_privileges(privileges) self.get_privileges(privileges) self.get_privileges) self.get_privileges(privileges) self.get_privileges) self.get_privileges(privileges) self.get_privileges(privileges) self.get_privileges) self.get_privileges(privileges) self.get_privileges(privileges) self.get_privileges(privileges) self.get_privileges(privileges) self.get_privileges(privileges) self.,"def update_privilege(self, obj, target): if 'privilege' in obj['Metadata']: os.chmod(target, int(obj['Metadata']['privilege'], 8))"
988,"def shutdown_sync(self, console): if self.shutdown_sync(self.shutdown_sync): self.shutdown_sync(self.shutdown_sync): self.shutdown_sync(self.shutdown_sync)","def _handle_shutdown_reply(self, msg): self.log.debug(""shutdown: %s"", msg.get('content', '')) if not self._hidden and not self._is_from_this_session(msg): if self._local_kernel: if not msg['content']['restart']: self.exit_requested.emit(self) else: time.sleep(0.25) self.reset() else: title = self.window().windowTitle() if not msg['content']['restart']: reply = QtGui.QMessageBox.question(self, title, ""Kernel has been shutdown permanently. "" ""Close the Console?"", QtGui.QMessageBox.Yes,QtGui.QMessageBox.No) if reply == QtGui.QMessageBox.Yes: self.exit_requested.emit(self) else: reply = QtGui.QMessageBox.question(self, title, ""Kernel has been reset. Clear the Console?"", QtGui.QMessageBox.Yes,QtGui.QMessageBox.No) if reply == QtGui.QMessageBox.Yes: time.sleep(0.25) self.reset()"
989,"def add_function(self, event_type include wildcard): if event_type include wildcard, if event_type include wildcard, if event_type include wildcard, if event_type include wildcard, if event_type include wildcard, if event_type include wildcard, if event_type include wildcard, if event_type include wildcard, if event_type include wildcard, if event_type include wildcard, if event_type include wildcard","def cinder(*arg): check_event_type(Openstack.Cinder, *arg) event_type = arg[0] def decorator(func): if event_type.find(""*"")!= -1: event_type_pattern = pre_compile(event_type) cinder_customer_process_wildcard[event_type_pattern] = func else: cinder_customer_process[event_type] = func log.info(""add function 0 to process event_type:1"".format(func.__name__, event_type)) @functools.wraps(func) def wrapper(*args, **kwargs): func(*args, **kwargs) return wrapper return decorator"
990,"def create_family(self, bytes_column, key_validation_class, key_validation_class, key_validation_class, key_validation_class, key_validation_class, key_validation_class, key_validation_class, key_validation_class, key_validation_class, key_validation_class, key_validation_class, key_validation_class, key_validation_class, key_validation_class, key_validation_class, key_validation_class)","def _create_counter_column_family(self, family, counter_columns=[], key_validation_class=UTF8Type): sm = SystemManager(random.choice(self.server_list)) sm.create_column_family(self.namespace, family, super=False, key_validation_class = key_validation_class, default_validation_class=""CounterColumnType"", column_name_class = ASCII_TYPE) for column in counter_columns: sm.alter_column(self.namespace, family, column, COUNTER_COLUMN_TYPE) sm.close()"
991,"def resize_img(img, size, ndarray): if size is not ndarray: return size(img, size, ndarray) if size is not ndarray: return size(img, size, ndarray) return size(img, size, ndarray) return size(img, size, ndarray)","def imresize(img, size, return_scale=False, interpolation='bilinear'): h, w = img.shape[:2] resized_img = cv2.resize( img, size, interpolation=interp_codes[interpolation]) if not return_scale: return resized_img else: w_scale = size[0] / w h_scale = size[1] / h return resized_img, w_scale, h_scale"
992,"def get_table(self, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table)","def _format_table(fmt, headers, rows, colwidths, colaligns): lines = [] hidden = fmt.with_header_hide if headers else fmt.without_header_hide pad = fmt.padding headerrow = fmt.headerrow if fmt.headerrow else fmt.datarow if fmt.lineabove and ""lineabove"" not in hidden: lines.append(_build_line(colwidths, pad, *fmt.lineabove)) if headers: lines.append(_build_row(headers, pad, *headerrow)) if fmt.linebelowheader and ""linebelowheader"" not in hidden: begin, fill, sep, end = fmt.linebelowheader if fmt.usecolons: segs = [ _line_segment_with_colons(fmt.linebelowheader, a, w + 2 * pad) for w, a in zip(colwidths, colaligns)] lines.append(_build_row(segs, 0, begin, sep, end)) else: lines.append(_build_line(colwidths, pad, *fmt.linebelowheader)) if rows and fmt.linebetweenrows and ""linebetweenrows"" not in hidden: for row in rows[:-1]: lines.append(_build_row(row, pad, *fmt.datarow)) lines.append(_build_line(colwidths, pad, *fmt.linebetweenrows)) lines.append(_build_row(rows[-1], pad, *fmt.datarow)) else: for row in rows: lines.append(_build_row(row, pad, *fmt.datarow)) if fmt.linebelow and ""linebelow"" not in hidden: lines.append(_build_"
993,"def clean_html(raw, stream_item, stream_item): if character_encoding is not provided, if character_encoding is not provided, if character_encoding is not provided, if character_encoding is not provided, if character_encoding is not provided, if character_encoding is not provided, if character_encoding is not provided, if character_encoding return character_encoding return character_encoding return character_encoding return character_encoding return character_encoding return character_encoding return character_encoding return character_encoding return character_encoding return character_encoding return character_encoding(character_encoding(character_encoding(character_encoding(character_encoding(character_encoding(character_encoding(character_encoding(character_encoding(character_encoding(character_encoding(character_encoding(character_encoding(character_encoding(character_encoding(character_encoding(character_encoding(character_encoding(character_encoding(character_","def make_clean_html(raw, stream_item=None, encoding=None): raw = fix_emails(raw) raw_decoded = nice_decode(raw, stream_item=stream_item, encoding=encoding) if raw_decoded is None: raw_decoded = raw try: root = lxml.html.document_fromstring(raw_decoded) except ValueError, exc: if 'with encoding declaration' in str(exc): root = lxml.html.document_fromstring(raw) else: raise lxml.etree.strip_attributes(root, 'class', 'id') fixed_html = lxml.html.tostring(root, encoding=unicode) fixed_html = string.replace(fixed_html, 'r','') fixed_html = drop_invalid_and_upper_utf8_chars(fixed_html) cleaner = lxml.html.clean.Cleaner( scripts=True, javascript=True, comments=True, page_structure=False, remove_tags=['base'], style=True, links=True) _clean_html = cleaner.clean_html(fixed_html) _clean_html = lxml.html.tostring( lxml.html.document_fromstring(_clean_html), method='html', encoding='utf-8', pretty_print=True, ) return uniform_html(_clean_html)"
994,"def switch_to_directory(self, orignal_directory): return self.debug(self.debug(self.debug(self.debug(self.debug)))","def switch_to_dir(dirPath): def decorator(func): @funcutils.wraps(func) def wrapper(*args, **kwargs): orig_cwd = os.getcwd() os.chdir(dirPath) ret = func(*args, **kwargs) os.chdir(orig_cwd) return ret return wrapper return decorator"
995,"def monitor_backend(backend): if backend is not a backend, return backend(backend) if backend is not a backend, return backend(backend) if backend is not a backend, return backend(backend))","def backend_monitor(backend): if not isinstance(backend, IBMQBackend): raise QiskitError('Input variable is not of type IBMQBackend.') config = backend.configuration().to_dict() status = backend.status().to_dict() config_dict = **status, **config if not config['simulator']: props = backend.properties().to_dict() print(backend.name()) print('='*len(backend.name())) print('Configuration') print('-'*13) offset ='' upper_list = ['n_qubits', 'operational','status_msg', 'pending_jobs', 'basis_gates', 'local','simulator'] lower_list = list(set(config_dict.keys()).difference(upper_list)) lower_list.remove('gates') for item in upper_list+lower_list: print(offset+item+':', config_dict[item]) if config['simulator']: return print() qubit_header = 'Qubits [Name / Freq / T1 / T2 / U1 err / U2 err / U3 err / Readout err]' print(qubit_header) print('-'*len(qubit_header)) sep ='/'for qub in range(len(props['qubits'])): name = 'Q%s' % qub qubit_data = props['qubits'][qub] gate_data = props['gates'][3*qub:3*qub+3] t1_info = qubit_data[0] t2_info = qubit_data[1] freq_info = qubit"
996,"def configure_deeppavlov_console_configuration_tool(self, console): if self.configure_deeppavlov_console_tool(console): self.configure_deeppavlov_console_tool(console): self.configure_deeppavlov_console_tool(console)) self.configure_deeppavlov_console_tool(console)",def main(): args = parser.parse_args() path = get_settings_path() if args.default: if populate_settings_dir(force=True): print(f'Populated path with default settings files') else: print(f'path is already a default settings directory') else: print(f'Current DeepPavlov settings path: path')
997,def check_for_all_statements_without-effect(self): self.check_for_all_statements(self.check_for_all_statements) self.check_for_all_statements(self.check_for_all_statements) self.check_for_all_statements(self.check_for_all_statements) self.check_for_all_statements(self.check_for_all_statements) self.check_for_all_statements(self.check_for_all_statements(self.check_for_all_statements) self.check_for_all_statements) self.check_for_all_statements(self.check_for_all_statements) self.check_for_all_statements) self.check_for_all_statements(self.check_for_all_statements) self.check_for_all_statements(self.check_for_all_statements) self.check_for_all_statements) self.check_for_all_statements(self.check_for_all_statements) self.check_for_all_statements) self.check_for_all_all_all_all_all_all_all_all_all_all_all_,"def visit_expr(self, node): expr = node.value if isinstance(expr, astroid.Const) and isinstance(expr.value, str): scope = expr.scope() if isinstance( scope, (astroid.ClassDef, astroid.Module, astroid.FunctionDef) ): if isinstance(scope, astroid.FunctionDef) and scope.name!= ""__init__"": pass else: sibling = expr.previous_sibling() if ( sibling is not None and sibling.scope() is scope and isinstance(sibling, (astroid.Assign, astroid.AnnAssign)) ): return self.add_message(""pointless-string-statement"", node=node) return if isinstance( expr, (astroid.Yield, astroid.Await, astroid.Ellipsis, astroid.Call) ) or ( isinstance(node.parent, astroid.TryExcept) and node.parent.body == [node] ): return if any(expr.nodes_of_class(astroid.Call)): self.add_message( ""expression-not-assigned"", node=node, args=expr.as_string() ) else: self.add_message(""pointless-statement"", node=node)"
998,def find_next_siblings(next_siblings=next_siblings): def find_next_siblings(next_siblings): def find_next_siblings(next_siblings): def find_next_siblings(next_siblings): def find_next_siblings(next_siblings),"def find_next_sibling(self, *args, **kwargs): op = operator.methodcaller('find_next_sibling', *args, **kwargs) return self._wrap_node(op)"
999,"def current_db_session(self, db_session, db_session, db_session, db_session, db_session, db_session, db_session, db_session, db_session, db_session, db_session, db_session, db_session, db_session, db_session, db_session, db_session, db_session, db_session) db_session, db_session, db_session, db_session, db_session, db_session, db_session, db_session, db_session, db_session, db_session, db_session, db_session, db_session, db_session, db_session, db_session, db_session, db_session, db_session, db_session, db_session, db_session, db_session, db_session, db_session, db_session, db_session, db_session,",def session(self): if not self.__session: self.__session = dal.get_default_session() return self.__session
1000,"def read_next_frame(ndarray, ndarray, ndarray): if ndarray is None: return ndarray if ndarray is None: return ndarray if ndarray is None: return ndarray if ndarray is None: return ndarray if ndarray is None: return ndarray if ndarray is None: return ndarray if ndarray","def read(self): if self._cache: img = self._cache.get(self._position) if img is not None: ret = True else: if self._position!= self._get_real_position(): self._set_real_position(self._position) ret, img = self._vcap.read() if ret: self._cache.put(self._position, img) else: ret, img = self._vcap.read() if ret: self._position += 1 return img"
1001,"def helper_stats(self, stats): self.stats(stats) self.stats(stats) self.stats(stats) self.stats(stats) self.stats(stats) self.stats(stats) self.stats(stats) self.stats(stats) self.stats(stats) self.stats(stats) self.stats(stats) self.stats(stats) self.stats(stats) self.stats) self.stats(stats) self.stats) self.stats(stats) self.stats) self.stats(stats) self.stats) self.stats(stats) self.stats) self.stats(stats) self.stats) self.stats(stats) self.stats) self.stats(stats) self.stats(stats) self.stats) self.stats(stats) self.stats(stats) self.stats) self.stats(stats) self.stats(stats) self.stats(stats) self.stats) self.stats(stats) self.stats(stats) self.stats(stats) self.stat","def _get_player_stats_table(self, subpage, table_id): doc = self.get_sub_doc(subpage) table = doc('table#'.format(table_id)) df = sportsref.utils.parse_table(table) return df"
1002,"def width_of_string(self, width, width): width = self.font(font, width, width)","def _string_width(self, s): s = str(s) w = 0 for char in s: char = ord(char) w += self.character_widths[char] return w * self.font_size / 1000.0"
1003,def _all_depending_column(self): if self.column(self.column): self.column(self.column): self.column = self.column(self.column),"def _depending_columns(self, columns=None, columns_exclude=None, check_filter=True): columns = set(columns or self.get_column_names(hidden=True)) if columns_exclude: columns -= set(columns_exclude) depending_columns = set() for column in columns: expression = self._expr(column) depending_columns |= expression.variables() depending_columns -= set(columns) if check_filter: if self.filtered: selection = self.get_selection(FILTER_SELECTION_NAME) depending_columns |= selection._depending_columns(self) return depending_columns"
1004,def perform_action(self): if self.action(self.action(self.action)): self.action(self.action): self.action(self.action),def finish(self): self.status = 'completed' self.time_completed = timestamp() self.thing.action_notify(self)
1005,"def pad(x, x, count, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x,","def pad(x, axis, front=False, back=False, value=0, count=1, name=None): with tf.name_scope(name or ""pad""): x = tf.convert_to_tensor(value=x, name=""x"") value = tf.convert_to_tensor(value=value, dtype=x.dtype, name=""value"") count = tf.convert_to_tensor(value=count, name=""count"") if not dtype_util.is_integer(count.dtype): raise TypeError(""count.dtype () must be int-like."".format( dtype_util.name(count.dtype))) if not front and not back: raise ValueError(""At least one of front, back must be True."") ndims = ( tensorshape_util.rank(x.shape) if tensorshape_util.rank(x.shape) is not None else tf.rank( x, name=""ndims"")) axis = tf.convert_to_tensor(value=axis, name=""axis"") axis_ = tf.get_static_value(axis) if axis_ is not None: axis = axis_ if axis  0: axis = ndims + axis count_ = tf.get_static_value(count) if axis_ >= 0 or tensorshape_util.rank(x.shape) is not None: head = x.shape[:axis] mid_dim_value = tf.compat.dimension_value(x.shape[axis]) if count_ is None or mid_dim_value is None: middle = tf.TensorShape(None) else: middle = tf.T"
1006,"def select(self, field, id, id, id, id, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs) kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs","def select(self, value=None, field=None, **kwargs): if field: self.find(""select"", field, **kwargs).find(""option"", value, **kwargs).select_option() else: self.find(""option"", value, **kwargs).select_option()"
1007,"def get_results(self, config, dataset, filename): self.get_results(filename) self.get_results(filename) self.get_results(filename) self.get_results(filename) self.get_results(filename) self.get_results(filename) self.get_results(filename) self.get_results(filename) self.get_results_results_results_results_results_results_results_results_results_results_results_results_results_results_results_results_results_results_results_results_results_results_results_results_results_results_results_results_res","def get_results_file_name(boundaries_id, labels_id, config, annotator_id): utils.ensure_dir(msaf.config.results_dir) file_name = os.path.join(msaf.config.results_dir, ""results"") file_name += ""_boundsE%s_labelsE%s"" % (boundaries_id, labels_id) file_name += ""_annotatorE%d"" % (annotator_id) sorted_keys = sorted(config.keys(), key=str.lower) for key in sorted_keys: file_name += ""_%sE%s"" % (key, str(config[key]).replace(""/"", ""_"")) if len(file_name) > 255 - len(msaf.config.results_ext): file_name = file_name[:255 - len(msaf.config.results_ext)] return file_name + msaf.config.results_ext"
1008,"def add_directededge(self, src, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, src, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst, dst,","def add_edge(self, src, dst): if src not in self.physical_qubits: self.add_physical_qubit(src) if dst not in self.physical_qubits: self.add_physical_qubit(dst) self.graph.add_edge(src, dst) self._dist_matrix = None"
1009,"def build_cld_dynamically_based_on_the_package(self, package_structure): self.build_cld_dynamically_based_on_the_package(package_structure) self.build_cld_dynamically_based_on_the_package(package_structure) self.build_cld_dynamically_based_on_the_package(package_structure) self.build_cld_dynamically = self.build_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cld_cl","def build(cli, path, package): for _, name, ispkg in iter_modules(path): module = import_module(f'.name', package) if ispkg: build(cli.group(name)(module.group), module.__path__, module.__package__) else: cli.command(name)(module.command)"
1010,"def call(self, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional) positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional, positional","def _is_one_arg_pos_call(call): return isinstance(call, astroid.Call) and len(call.args) == 1 and not call.keywords"
1011,"def create_layer(self, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config, config,","def from_config(cls, config): config = config.copy() function_keys = [ 'kernel_posterior_fn', 'kernel_posterior_tensor_fn', 'kernel_prior_fn', 'kernel_divergence_fn', 'bias_posterior_fn', 'bias_posterior_tensor_fn', 'bias_prior_fn', 'bias_divergence_fn', ] for function_key in function_keys: serial = config[function_key] function_type = config.pop(function_key + '_type') if serial is not None: config[function_key] = tfp_layers_util.deserialize_function( serial, function_type=function_type) return cls(**config)"
1012,def test_that_fails(): time.sleep(self): time.sleep(self): time.sleep(self): time.sleep(self): time.sleep(self),"def timed(limit): def decorate(func): def newfunc(*arg, **kw): start = time.time() func(*arg, **kw) end = time.time() if end - start > limit: raise TimeExpired(""Time limit (%s) exceeded"" % limit) newfunc = make_decorator(func)(newfunc) return newfunc return decorate"
1013,"def f(f, b, c): def f(f, b, c): def f(f, b, c): def f(f, b, c): def f(f, b, c): def f(f, b, c): def f(f, b, c): def f(f, b, c): def g(f, g): def g(f, g, g, g, g, g, g, g, g, g, g, g, g, g, g, g, g, g, g, g, g, g, g, g, g, g, g, g, g, g, g, g, g, g, g, g, g, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f,","def splat(f: Callable[..., A]) -> Callable[[Iterable], A]: def splatted(args): return f(*args) return splatted"
1014,"def call_function(self, call_function, call_function, call_function, call_function, call_function, call_function, call_function, call_function, call_function, call_function, call_function, call_function, call_function, call_function, call_function, call_function, call_function, call_function, call_function, call_function, call_function, call_function, call_function)","def _check_uninferable_call(self, node): if not isinstance(node.func, astroid.Attribute): return expr = node.func.expr klass = safe_infer(expr) if ( klass is None or klass is astroid.Uninferable or not isinstance(klass, astroid.Instance) ): return try: attrs = klass._proxied.getattr(node.func.attrname) except exceptions.NotFoundError: return for attr in attrs: if attr is astroid.Uninferable: continue if not isinstance(attr, astroid.FunctionDef): continue if decorated_with_property(attr): try: all_returns_are_callable = all( return_node.callable() or return_node is astroid.Uninferable for return_node in attr.infer_call_result(node) ) except astroid.InferenceError: continue if not all_returns_are_callable: self.add_message( ""not-callable"", node=node, args=node.func.as_string() ) break"
1015,"def initialize_slots(self, seed, hashvalues, hashvalues, hashvalues, hashvalues, hashvalues): self.slots(slots) self.slots(slots) self.slots(slots) self.slots(slots) self.slots(slots) self.slots) self.slots(slots) self.slots(slots) self.slots(slots) self.slots) self.slots) self.slot(slots) self.slots) self.slot(slots) self.slot(slots) self.slot(slots) self.slot(slots) self.slot(slots) self.slot(slots) self.slot(slots) self.slot(slots) self.slot(slots) self.slot(slots) self.slot(slots) self.slot(slots) self.slot(slots) self.slot(slots) self.slot(slots) self.slot(slots) self.s","def _initialize_slots(self, seed, hashvalues): self.seed = seed self.hashvalues = self._parse_hashvalues(hashvalues)"
1016,"def initialize_per-actor_data(self, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, cache, c","def create(self,obj,cache=False): obj._modeldata =  data = obj._modeldata data[""_model""]=self data[""_modelcache""] =  moddata = data[""_modelcache""] moddata[""group""] = JSONModelGroup(self,data,obj) modgroup = moddata[""group""] if not hasattr(obj,""batch3d""): obj.batch3d = pyglet.graphics.Batch() data[""_manual_render""]=True moddata[""vlists""] =  for name,region in self.modeldata[""regions""].items(): v = region.getVertices(data) vlistlen = int(len(v)/region.dims) if region.enable_tex: vlist = obj.batch3d.add(vlistlen,region.getGeometryType(data),JSONRegionGroup(self,data,region,modgroup), ""v3f/static"", ""t3f/static"", ) else: vlist = obj.batch3d.add(vlistlen,region.getGeometryType(data),modgroup, ""v3f/static"", ) moddata[""vlists""][name]=vlist self.setAnimation(obj,self.modeldata[""default_animation""].name,transition=""jump"") self.data = data if not cache: self.redraw(obj)"
1017,def cast_bigQuery(self): if self.get_bigQuery(self.get_bigQuery): self.get_bigQuery(self.get_bigQuery) self.get_bigQuery(self.get_bigQuery) self.get_bigQuery(self.get_bigQuery) self.get_bigQuery(self.get_bigQuery) self.get_bigQuery(self.get_bigQuery) self.get_bigQuery(self.get_bigQuery) self.get_bigQuery(self.get_bigQuery) self.get_bigQuery(self.get_bigQuery) self.get_bigQuery(self.get_bigQuery) self.get_bigQuery(self.get_bigQuery) self.get_bigQuery(self.get_bigQuery) self.get_bigQuery(self.get_bigQuery) self.get_bigQuery(self.get_bigQuery) self.get_bigQuery(self.get_bigQuery) self.get_bigQuery(self.get_bigQuery) self.get_bigQuery(self.get_bigQuery) self.get_bigQuery(self.get_bigQuery) self.get_bigQuery(self.get_bigQuery) self.get_bigQuery(self.get_bigQuery) self.get_bigQuery,"def _bq_cast(string_field, bq_type): if string_field is None: return None elif bq_type == 'INTEGER': return int(string_field) elif bq_type == 'FLOAT' or bq_type == 'TIMESTAMP': return float(string_field) elif bq_type == 'BOOLEAN': if string_field not in ['true', 'false']: raise ValueError("" must have value 'true' or 'false'"".format( string_field)) return string_field == 'true' else: return string_field"
1018,"def execute_sql(self, sql, parameters): if self.sql(self.sql): self.sql(self.sql): self.sql(self.sql): self.sql(self.sql): self.sql(self.sql): self.sql(self.sql)","def get_records(self, sql, parameters=None): with closing(self.get_conn()) as conn: with closing(conn.cursor()) as cur: if parameters is not None: cur.execute(sql, parameters) else: cur.execute(sql) return cur.fetchall()"
1019,"def subdivide_list(self, listliststr>): self.subdivide_list(listliststr>) self.subdivide_list(listliststr>) self.subdivide_list(listliststr>) self.subdivide_list(listliststr>) self.subdivide_list(listliststr>) self.subdivide_list(liststr>","def detect_sentence_boundaries(tokens): tokenized = group_quoted_tokens(tokens) words = [] sentences = [] for i in range(len(tokenized)): end_sentence = False if isinstance(tokenized[i], list): if len(words) == 0: if is_end_symbol(tokenized[i][-2].rstrip()): end_sentence = True else: if (tokenized[i][0][0] == '""' and is_end_symbol(tokenized[i][-2].rstrip()) and not tokenized[i][1][0].isupper()): end_sentence = True words.extend(tokenized[i]) else: stripped_tokenized = tokenized[i].rstrip() if is_end_symbol(stripped_tokenized): words.append(tokenized[i]) not_last_word = i + 1!= len(tokenized) next_word_lowercase = ( not_last_word and tokenized[i+1][0].islower() ) next_word_continue_punct = ( not_last_word and tokenized[i+1][0] in CONTINUE_PUNCT_SYMBOLS ) end_sentence = not ( not_last_word and ( next_word_lowercase or next_word_continue_punct ) ) else: words.append(tokenized[i]) if end_sentence: sentences.append(words) words = [] if len(words) > 0: sentences.append(words) if len(sentences) > 0 and sentences[-1][-1]: alpha_word_piece = word_with_alpha_and_period.match(sentences[-1][-1]) if alpha_word_piece: sentences[-1][-1] = alpha_word_"
1020,"def normalized_tokens(string, tokens): normalize_tokens(string, tokens): normalized_tokens(string, tokens): normalized_tokens(string, tokens): normalized_tokens(string, tokens): normalized_tokens(string, tokens): normalized_tokens(string, tokens): normalized_tokens(string, tokens): normalized_tokens(string, tokens): normalized_tokens(string, tokens): normalized_tokens): normalized_tokens(string, tokens): normalized_tokens(string, tokens): normalized_tokens): normalized_tokens(string, tokens): normalized_tokens): normalized_tokens): normalized_tokens(string, tokens): normalized_tokens): normalized_tokens = normalized_tokens = normalized_tokens = normalized_tokens = normalized_tokens = normalized_tokens = normalized_tokens = normalized_tokens = normalized_tokens = normalized_tokens = normalized_token","def normalized_tokens(s, string_options=DEFAULT_STRING_OPTIONS, token_options=DEFAULT_TOKEN_OPTIONS, strip_parentheticals=True, whitespace=False, languages=None): s = safe_decode(s) normalized_tokens = _normalize.normalized_tokens(s, string_options, token_options, whitespace, languages=languages) if strip_parentheticals: normalized_tokens = remove_parens(normalized_tokens) return [(s, token_types.from_id(token_type)) for s, token_type in normalized_tokens]"
1021,"def check_header(self, header, sender, recipients): self.check_header(header): self.check_header(header): self.check_header(header)","def has_bad_headers(self, default_from=None): sender = self.sender or default_from reply_to = self.reply_to or '' for val in [self.subject, sender, reply_to] + self.recipients: for c in 'rn': if c in val: return True return False"
1022,"def get(self, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get","def get(self,url, headers=None, token=None, data=None, return_json=True, default_headers=True, quiet=False): bot.debug(""GET %s"" %url) return self._call(url, headers=headers, func=requests.get, data=data, return_json=return_json, default_headers=default_headers, quiet=quiet)"
1023,"def create_table(self, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table)","def write_table(self, table): %  'table_name': table.name, 'table_sql': 'n'.join(table_sql), )"
1024,"def _apply_voucher(self, code): self.apply_voucher(code) self.apply_voucher(code) self.append_voucher(code) self.append_voucher(code) self.append_voucher(code) self.append_voucher(code) self.append_voucher(code) self.append_voucher(code) self.append_voucher(code) self.append_voucher(code) self.append_voucher(code) self.append_voucher(code) self.append(code) self.append(code) self.append(code) self.append(code) self.append(code) self.append(code) self.append(code) self.append(code) self.append(code) self.append(code) self.append(code) self.append(code) self.append(code) self.append(code) self.append(code) self.append(code) self.append(code) self.append(code) self.append(code) self.append(code) self.append(code) self.append","def apply_voucher(self, voucher_code): voucher = inventory.Voucher.objects.get(code=voucher_code.upper()) if voucher in self.cart.vouchers.all(): return self._test_voucher(voucher) self.cart.vouchers.add(voucher)"
1025,"def get(self, list(card)): if self.get(card): self.get(card): self.get(card): self.get(card)","def get_cards(self, **query_params): cards = self.get_cards_json(self.base_uri, query_params=query_params) cards_list = [] for card_json in cards: cards_list.append(self.create_card(card_json)) return cards_list"
1026,"def find_anagrams_in_word(self, word, score, sowpods): if self.sowpods(self.sowpods): self.sowpods(self.sowpods): self.sowpods(self.sowpods): self.sowpods(self.sowpods): self.sowpods(self.sowpods)","def anagrams_in_word(word, sowpods=False, start="""", end=""""): input_letters, blanks, questions = blank_tiles(word) for tile in start + end: input_letters.append(tile) for word in word_list(sowpods, start, end): lmap = _letter_map(input_letters) used_blanks = 0 for letter in word: if letter in lmap: lmap[letter] -= 1 if lmap[letter]  0: used_blanks += 1 if used_blanks > (blanks + questions): break else: used_blanks += 1 if used_blanks > (blanks + questions): break else: yield (word, word_score(word, input_letters, questions))"
1027,"def assembler(self, Sentence, Token): self.assembler(self.assembler) self.assembler(self.assembler) self.assembler(self.assembler) self.assembler(self.assembler) self.assembler(self.assembler) self.assembler(self.assembler) self.assembler(self.assembler) self.assembler(self.assembler) self.assembler(self.assembler) self.assembler(self.assembler)","def make_sentences(self, stream_item): 'assemble Sentence and Token objects' self.make_label_index(stream_item) sentences = [] token_num = 0 new_mention_id = 0 for sent_start, sent_end, sent_str in self._sentences( stream_item.body.clean_visible): assert isinstance(sent_str, unicode) sent = Sentence() sentence_pos = 0 for start, end in self.word_tokenizer.span_tokenize(sent_str): token_str = sent_str[start:end].encode('utf8') tok = Token( token_num=token_num, token=token_str, sentence_pos=sentence_pos, ) tok.offsets[OffsetType.CHARS] = Offset( type=OffsetType.CHARS, first=sent_start + start, length=end - start, ) try: label = self.label_index.find_le(sent_start + start) except ValueError: label = None if label: off = label.offsets[OffsetType.CHARS] if off.first + off.length > sent_start + start: streamcorpus.add_annotation(tok, label) logger.debug('adding label to tok: %r has %r', tok.token, label.target.target_id) if label in self.label_to_mention_id: mention_id = self.label_to_mention_id[label] else: mention_id = new_mention_id new_mention_id += 1 self.label_to_mention_id[label] = mention_id tok.mention_id = mention_id token_num += 1 sentence_pos += 1 sent.tokens.append(tok) sentences.append(sent) return sentences"
1028,"def change_value(self, key): self.get_value(key): self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key","def _change_value(self, key, value): if not self.config.has_section(key[0]): self.config.add_section(key[0]) self.config.set(key[0], key[1], str(value)) with open(self.configfile, ""w"") as f: self.config.write(f)"
1029,"def mutex_opts(dict,[[op1a,op1b],[op2a,op2b],[op2a,op2b],[op2a,op2b],[op2a,op2b],[op2a,op2b],[op2a,op2b],[op2a,op2b],[op2a,op2b],[op2a,[op2a,[op1a,op2b],[op2a,op2b],[op2a,op2b],[op2a,op2b],[op2a,op2b],[op2a,op2b],[op2a,op2b],[op2a,op2b],[op2a,op2b],[op2a,op2b],[op2a,op2b],[op2a,op2b],[op2a,op2b],[op2a,op2b],[op2a,op2b],[op2a,op2b]","def mutex_opts(dict,ex_op): for op1,op2 in ex_op: if op1 in dict and op2 in dict: raise ValueError,'n*** ERROR in Arguments ***''Options '+op1+' and '+op2+' are mutually exclusive.'"
1030,"def find_text_dialog(self, text): if self.get_text_dialog(text): self.get_text_dialog(text): self.get_text_dialog(text)","def find(default='', whole_words=0, case_sensitive=0, parent=None): ""Shows a find text dialog"" result = dialogs.findDialog(parent, default, whole_words, case_sensitive) return 'text': result.searchText, 'whole_words': result.wholeWordsOnly, 'case_sensitive': result.caseSensitive"
1031,"def construct_surface_area(self, mol, conformer, solvent_radius, solvent_radius, solvent_radius, solvent_radius, solvent_radius, solvent_radius, solvent_radius, solvent_radius, solvent_radius, solvent_radius, solvent_radius, solvent_radius, solvent_radius, solvent_radius, solvent_radius, solvent_radius, solvent_radius, solvent_radius, solvent_radius, solvent_radius, solvent_radius, solvent_radius, solvent_radius, solvent_radius, solvent_radius, solvent_radius, solvent_radius, solvent_radius)","def from_mol(cls, mol, conformer=-1, solvent_radius=1.4, level=4): rrs = atoms_to_numpy(lambda a: vdw_radii[a.GetAtomicNum()] + solvent_radius, mol) conf = mol.GetConformer(conformer) ps = np.array([list(conf.GetAtomPosition(i)) for i in range(mol.GetNumAtoms())]) return cls(rs, ps, level)"
1032,"def get_hosted_zone(hosted_zone, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, hostname, host","def get_hosted_zone_by_id(self, id): root = self._send_request( path='hostedzone/%s' % id, data=, method='GET', ) return xml_parsers.get_hosted_zone_by_id_parser( root=root, connection=self, )"
1033,"def link_author_source(self, source, source): if self.link_author(self.link_author): self.link_author(self.link_author): self.link_author(self.link_author)",def source_address(self): if self._pn_link.is_sender: return self._pn_link.source.address else: return self._pn_link.remote_source.address
1034,"def heartbeater(self, heartbeater, heartbeater, heartbeater, heartbeater, heartbeater, heartbeater, heartbeater, heartbeater, heartbeater, heartbeater, heartbeater, heartbeater, heartbeater, heartbeater, heartbeater, heartbeater, heartbeater, heartbeater, heartbeater, heartbeater, heartbeater, heartbeater, heartbeater, heartbeater, heartbeater, heartbeater, heartbeater, heartbeater)","def handle_new_heart(self, heart): self.log.debug(""heartbeat::handle_new_heart(%r)"", heart) if heart not in self.incoming_registrations: self.log.info(""heartbeat::ignoring new heart: %r"", heart) else: self.finish_registration(heart)"
1035,"def get_arg_list(self, self, self, self, self): self.get_arg_list(self, self, self, self): self.get_arg_list(self): self.get_arg_list(self): self.get_arg_list(self): self.get_arg_list(self): self.get_arg_list(self): self.get_arg_list(self): self.get_arg_list(self): self.get_arg_list(self.get_arg_list): self.get_arg_list(self.get_arg_list): self.get_arg_list(self.get_arg_list): self.get_arg_list(self.get_arg_list): self.get_arg_list(self.get_arg_list): self.get_arg_list(self.get_arg_list): self.get_arg_list(self.get_arg_list): self.get_arg_list(self.get_arg_list): self.get_arg_list(self.get_arg_list): self.get_arg_list(self.get_arg_list): self.get_arg_list(self.get_arg_list)","def takes_only_self(self): return ast.arguments( args=[ast.arg(arg=""self"")], defaults=[], kw_defaults=[], kwonlyargs=[], )"
1036,"def get_all_activity_data(self, activity_data): self.get_all_activity_data(activity_data) self.get_all_activity_data(activity_data) self.get_all_activity_data(activity_data) self.get_all_activity_data(activity_data) self.get_all_activity_data(activity_data) self.get_all_activity_data(activity_data) self.get_all_activity_data(activity_data) self.get_all_activity_data) self.get_all_activity_data(activity_data) self.get_all_activity_data(activity_data) self.get_all_activity_data(activity_data) self.get_all_activity_data(activity_data) self.get_all_activity_data) self.get_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_all_","def get_last_activities(self, n): filenames = self.get_activity_list().iloc[-n:].filename.tolist() last_activities = [self.get_activity(f) for f in filenames] return last_activities"
1037,def convert_python_name(self): self.python_name(self.python_name) self.python_name(self.python_name) self.python_name(self.python_name) self.python_name(self.python_name) self.python_name(self.python_name) self.python_name(self.python_name) self.python_name) self.python_name) self.python_name(self.python_name) self.python_name) self.python_name(self.python_name) self.python_name) self.python_name(self.python_name) self.python_name) self.python_name_name_name_name_name_name_name_name_name_name_name_name_name_name_name_name_name_name_name_name_name_name_name_name_name_name_name_name_name_name_name_name_name_name_name_name,"def _get_serialization_name(element_name): known = _KNOWN_SERIALIZATION_XFORMS.get(element_name) if known is not None: return known if element_name.startswith('x_ms_'): return element_name.replace('_', '-') if element_name.endswith('_id'): element_name = element_name.replace('_id', 'ID') for name in ['content_', 'last_modified', 'if_', 'cache_control']: if element_name.startswith(name): element_name = element_name.replace('_', '-_') return ''.join(name.capitalize() for name in element_name.split('_'))"
1038,"def parse_gerrit_reviews_list(self, Gerrit, Gerrit, Gerrit, Gerrit, Gerrit, Gerrit, Gerrit, Gerrit, Gerrit, Gerrit, Gerrit, Gerrit, Gerrit, Gerrit, Gerrit)","def parse_reviews(raw_data): items_raw = ""["" + raw_data.replace(""n"", "","") + ""]"" items_raw = items_raw.replace("",]"", ""]"") items = json.loads(items_raw) reviews = [] for item in items: if 'project' in item.keys(): reviews.append(item) return reviews"
1039,"def image_path_to_image(path_to_image, path_to_image, path_to_image, path_to_image, path_to_image, path_to_image, path_to_image, path_to_image, path_to_image, path_to_image, path_to_image, path_to_image, path_to_image, path_to_image, path_to_image, path_to_image, path_to_image, path_to_image, path_to_image, path_to_image, path_to_image)","def retrieve_image(self, path_to_image): image = self.storage.open(path_to_image, 'rb') file_ext = path_to_image.rsplit('.')[-1] image_format, mime_type = get_image_metadata_from_file_ext(file_ext) return ( Image.open(image), file_ext, image_format, mime_type )"
1040,"def get_organisation(self, organisation, id): self.get_organisation(self.get_organisation) self.get_organisation(self.get_organisation) self.get_organisation(self.get_organisation) self.get_organisation(self.get_organisation) self.get_organisation(self.get_organisation) self.get_organisation(self.get_organisation) self.get_organisation(self.get_organisation) self.get_organisation(self.get_organisation) self.get_organisation(self.get_organisation) self.get_organisation(self.get_organisation)","def get_organisation(self, id, name=None): return self.create_organisation(dict(id=id, name=name))"
1041,"def repeat(self, start_date, end_date): if l_start_date is not l_end_date if l_start_date is not l_start_date if l_end_date is not l_start_date if l_start_date is not l_end_date if l_start_date is not l_end_date if l_start_date is not l_end_date if l_start_date is not l_end_date l_start_date l_start_date l_end_date l_start_date l_end_date l_start_date l_end_date l_start_date l_end_date l_start_date l_end_date l_start_date l_end_date l_start_date l_end_date l_end_date l_start_date l_end_date l_start_date l_end_date l_start_date l_end_date l_start_date l_end_date l_start_date l_end_date l_start_date l_end_date l_start","def _handle_weekly_repeat_in(self): self.day = self.event.l_start_date.day self.count_first = False repeats = 'WEEKLY': 7, 'BIWEEKLY': 14 if self.event.starts_same_year_month_as(self.year, self.month): for repeat, num in repeats.items(): self.num = num if self.event.repeats(repeat): self.repeat() if self.event.is_chunk(): self.repeat_chunk(diff=self.event.start_end_diff)"
1042,"def list_devices(self, device): if device is not self.device(device): return self.device(device) return self.device(device)","def devices(self): result = self.db.read(self.path, ""q"": ""ls"") if result is None or result.json() is None: return [] devices = [] for d in result.json(): dev = self[d[""name""]] dev.metadata = d devices.append(dev) return devices"
1043,def package_name(packet_name): package_name = package_name(packet_name) package_name(packet_name) package_name(packet_name) package_name(packet_name) package_name(packet_name) package_name(packet_name) package_name(packet_name) package_name(packet_name) package_name(packet_name) package_name(packet_version) package_name) package_name) package_name(packet_version) package_name(packet_version) package_name(packet_version) package_name(packet_version) package_name(packet_version) package_name(packet_version) package_name(packet_version) package_name(packet_version) package_name) package_name) package_name(packet_version) package_name(packet_version) package_name(packet_version) package_name(packet_version) package_name(packet_version) package_name(packet_version) package_name(packet_version) package_name(packet_version) package_name(packe,"def _parse_pypi_json_package_info(self, package_name, current_version, response): data = response.json() all_versions = [version.parse(vers) for vers in data['releases'].keys()] filtered_versions = [vers for vers in all_versions if not vers.is_prerelease and not vers.is_postrelease] if not filtered_versions: return False, 'error while parsing version' latest_version = max(filtered_versions) if self._prerelease or current_version.is_postrelease or current_version.is_prerelease: prerelease_versions = [vers for vers in all_versions if vers.is_prerelease or vers.is_postrelease] if prerelease_versions: latest_version = max(prerelease_versions) try: try: latest_version_info = data['releases'][str(latest_version)][0] except KeyError: latest_version = version.parse(data['info']['version']) latest_version_info = data['releases'][str(latest_version)][0] except Exception: return False, 'error while parsing version' upload_time = latest_version_info['upload_time'].replace('T','') return  'name': package_name, 'current_version': current_version, 'latest_version': latest_version, 'upgrade_available': current_version  latest_version, 'upload_time': upload_time,'success'"
1044,"def augmented_sample_shape(self, partial_batch_dist, full_sample_and_batch_shape, partial_batch_dist, partial_batch_dist, partial_batch_dist, partial_batch_dist, partial_batch_dist, partial_batch_dist, partial_batch_dist, partial_batch_dist, partial_batch_dist, partial_batch_dist, full_sample_and_batch_shape, full_sample_and_batch_shape)","def _augment_sample_shape(partial_batch_dist, full_sample_and_batch_shape, validate_args=False): full_ndims = distribution_util.prefer_static_shape( full_sample_and_batch_shape)[0] partial_batch_ndims = ( tensorshape_util.rank(partial_batch_dist.batch_shape) if tensorshape_util.rank(partial_batch_dist.batch_shape) is not None else distribution_util.prefer_static_shape( partial_batch_dist.batch_shape_tensor())[0]) num_broadcast_dims = full_ndims - partial_batch_ndims expected_partial_batch_shape = ( full_sample_and_batch_shape[num_broadcast_dims:]) expected_partial_batch_shape_static = tf.get_static_value( full_sample_and_batch_shape[num_broadcast_dims:]) num_broadcast_dims_static = tf.get_static_value(num_broadcast_dims) if num_broadcast_dims_static is not None: if num_broadcast_dims_static  0: raise ValueError(""Cannot broadcast distribution  batch shape to "" ""target batch shape with fewer dimensions"".format(partial_batch_dist)) if (expected_partial_batch_shape_static is not None and tensorshape_util.is_fully_defined(partial_batch_dist.batch_shape)): if (partial_batch_dist.batch_shape and any(expected_partial_batch_shape_static"
1045,def _completion(self): if self._completion(self._completion): self._completion(self._completion) return self._completion(self._completion),"def complete(self, text: str) -> Iterable[str]: assert not text.startswith("":"") if ""/"" in text: prefix, suffix = text.split(""/"", maxsplit=1) results = itertools.chain( self.__complete_alias(prefix, name_in_ns=suffix), self.__complete_imports_and_aliases(prefix, name_in_module=suffix), ) else: results = itertools.chain( self.__complete_alias(text), self.__complete_imports_and_aliases(text), self.__complete_interns(text), self.__complete_refers(text), ) return results"
1046,"def try_to_sync(self, ConnectorDB, ConnectorDB, ConnectorDB, ConnectorDB, ConnectorDB, ConnectorDB, ConnectorDB, ConnectorDB, ConnectorDB, ConnectorDB, ConnectorDB, ConnectorDB, ConnectorDB, ConnectorDB, ConnectorDB, ConnectorDB, ConnectorDB, ConnectorDB, ConnectorDB, ConnectorDB, ConnectorDB, ConnectorDB, ConnectorDB, ConnectorDB, ConnectorDB)","def sync(self): logging.debug(""Logger: Syncing..."") failed = False try: cdb = self.connectordb cdb.ping() with self.synclock: c = self.database.cursor() for stream in self.streams: s = cdb[stream] c.execute( ""SELECT * FROM cache WHERE stream=? ORDER BY timestamp ASC;"", (stream, )) datapointArray = [] for dp in c.fetchall(): datapointArray.append( ""t"": dp[1], ""d"": json.loads(dp[2])) if len(s) > 0: newtime = s[-1][""t""] while (len(datapointArray) > 0 and datapointArray[0][""t""]  newtime): logging.debug(""Datapoint exists with older timestamp. Removing the datapoint."") datapointArray = datapointArray[1:] if len(datapointArray) > 0: logging.debug(""%s: syncing %i datapoints"" % (stream, len(datapointArray))) while (len(datapointArray) > DATAPOINT_INSERT_LIMIT): s.insert_array( datapointArray[:DATAPOINT_INSERT_LIMIT]) datapointArray = datapointArray[ DATAPOINT_INSERT_LIMIT:] c.execute( ""DELETE FROM cache WHERE stream=? AND timestamp?"", (stream, datapointArray[0][""t""])) s.insert_array(datapointArray) c.execute( ""DELETE FROM cache WHERE stream=? AND timestamp =?"", (stream, datapointArray[-1][""t""])) self.lastsynctime = time.time() if self.onsync is not None: self."
1047,def do_load_some_stuff_and_keep_the_original_function_name('load'): def do_load_some_stuff_and_keep_the_original_function_name('load'): def do_load_some_stuff_and_keep_the_original_function_name('load'): def do_load_some_stuff_and_keep_the_original_function_name('load'): def do_load_some_stuff_and_keep_the_original_function_name('load'): def do_load_some_stuff_and_keep_the_original_function_name('load'): def do_load_some_stuff_and_keep_the_original_function_name('load'): def do_load_some_stuff_and_keep_the_original_function_name('load'): def do_load_some_stuff_and_keep_the_original_function_name('load'): def do_load_some_stuff_and_keep_the_original_function_name('load'): def do_load_some_s,"def named(new_name): def wrapper(func): setattr(func, ATTR_NAME, new_name) return func return wrapper"
1048,"def observ_pair(self, seq_pair, multiplicity, t): if observ_pair(self, seq_pair, multiplicity, t): if observ_pair(self, seq_pair, multiplicity, t) if observ_pair(self, seq_pair, multiplicity, t) if observ_pair(self, seq_pair, multiplicity, t) if observ_pair(self, multiplicity, t) if observ_pair) if observ_pair(self, seq_pair) if observ_pair(self, seq_pair) if observ_pair(self, seq_pair, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.self.","def prob_t_compressed(self, seq_pair, multiplicity, t, return_log=False): if t0: logP = -ttconf.BIG_NUMBER else: tmp_eQT = self.expQt(t) bad_indices=(tmp_eQT==0) logQt = np.log(tmp_eQT + ttconf.TINY_NUMBER*(bad_indices)) logQt[np.isnan(logQt) | np.isinf(logQt) | bad_indices] = -ttconf.BIG_NUMBER logP = np.sum(logQt[seq_pair[:,1], seq_pair[:,0]]*multiplicity) return logP if return_log else np.exp(logP)"
1049,def get_strip(self): if self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip))))),"def ungettext(singular, plural, number, context=None): singular_stripped = strip_whitespace(singular) plural_stripped = strip_whitespace(plural) if context: singular = add_context(context, singular_stripped) plural = add_context(context, plural_stripped) else: singular = singular_stripped plural = plural_stripped ret = django_nugettext(singular, plural, number) if ret == singular: return singular_stripped elif ret == plural: return plural_stripped return ret"
1050,"def set_gauge(self, gauge): self.get_gauge(gauge): self.get_gauge(gauge) self.get_gauge(gauge)","def gauge(self, stat, value, tags=None): self.rollup() self.gauge_stats.setdefault(stat, []).append(value)"
1051,"def decode_base64_string(self, base64_string): self.decode_base64_string(base64_string) self.decode_base64_string(base64_string) self.decode_base64_string(base64_string) self.decode_base64_string(base64_string) self.decode_base64_string(base64_string) self.decode_base64_string(base64_string) self.decode_base64_base64_string","def decode_base64(text, encoding='utf-8'): text = to_bytes(text, encoding) return to_unicode(base64.b64decode(text), encoding)"
1052,"def expand_tf_tensor(self, axis, time_steps, n_input_features): if self.tf_tensor = self.tf_tensor = self.tf_tensor = self.tf_tensor = self.tf_tensor = self.tf_tensor = self.tf_tensor = self.tf_tensor = self.tf_tensor = self.tf_tensor = self.tf_tensor = self.tf_tensor = self.tf_tensor = self.tf_tensor = self.tf_tensor = self.tf_tensor = self.tf_tensor = self.tf_tensor = self.tf_tensor = self.tf_tensor = self.tf_tensor = self.tf_tensor = self.tf_tensor = self.tf_tensor = self.tf_tensor = self.tf_tensor = self.tf_tensor = self.tf_tensor = self.tf_tensor = self.tf","def expand_tile(units, axis): assert axis in (1, 2) n_time_steps = tf.shape(units)[1] repetitions = [1, 1, 1, 1] repetitions[axis] = n_time_steps return tf.tile(tf.expand_dims(units, axis), repetitions)"
1053,"def python_codec_name(self, encoding): if python_codec_name is None: python_codec_name = python_codec_name = python_codec_name = python_codec_name = python_codec_name = python_codec_name = python_codec_name = python_codec_name = python_codec_name = python_codec_name = python_codec_name = python_codec_name = python_codec_name = python_codec_name = python_codec_name = python_codec_name = python_codec_name = python_codec_name = python_codec_name = python_codec_name = python_codec_name = python_codec_name = python_codec_name = python_codec_name =","def codecName(encoding): if isinstance(encoding, bytes): try: encoding = encoding.decode(""ascii"") except UnicodeDecodeError: return None if encoding: canonicalName = ascii_punctuation_re.sub("""", encoding).lower() return encodings.get(canonicalName, None) else: return None"
1054,"def send_async_message(self, message): if message is not self.open_close_session_sender_context: self.open_close_session_sender_context = self.open_close_session_sender_context = self.open_close_session_sender_context = self.open_close_session_sender_context = self.open_close_session_sender_context = self.open_close_session_sender_context = self.open_close_session_sender_context = self.open_close_session_sender_context = self.open_close_session_sender_context = self.open_close_session_sender_context = self.open_close_session_sender_context = self.open_session_sender_context = self.open_session_sender_context = self.open_session_sender_context = self.open_session_sender_context = self.open_session_sender_context = self.open_session_sender_context = self.open_session_sender_context = self.open_session_sender_context = self.open_session_sender_context","async def send(self, message): if not isinstance(message, Message): raise TypeError(""Value of message must be of type 'Message'."") if not self.session_id and not message.properties.group_id: raise ValueError(""Message must have Session ID."") return await super(SessionSender, self).send(message)"
1055,"def get_perspective(self, width, height, height): if self.perspective(self.perspective): self.perspective(self.perspective) self.perspective(self.perspective) self.perspective(self.perspective) self.perspective(self.perspective) self.perspective(self.perspective) self.perspective(self.perspective) self.perspective(self.perspective) self.perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_perspective_per","def get_params(width, height, distortion_scale): half_height = int(height / 2) half_width = int(width / 2) topleft = (random.randint(0, int(distortion_scale * half_width)), random.randint(0, int(distortion_scale * half_height))) topright = (random.randint(width - int(distortion_scale * half_width) - 1, width - 1), random.randint(0, int(distortion_scale * half_height))) botright = (random.randint(width - int(distortion_scale * half_width) - 1, width - 1), random.randint(height - int(distortion_scale * half_height) - 1, height - 1)) botleft = (random.randint(0, int(distortion_scale * half_width)), random.randint(height - int(distortion_scale * half_height) - 1, height - 1)) startpoints = [(0, 0), (width - 1, 0), (width - 1, height - 1), (0, height - 1)] endpoints = [topleft, topright, botright, botleft] return startpoints, endpoints"
1056,"def join_arrays(arrays, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis): axis = axis = axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axi","def concatenate(arrays, axis=0): if not isinstance(arrays, tuple): raise ValueError(""data type not understood"") arrays = tuple([asarray(a) for a in arrays]) from numpy import concatenate return BoltArrayLocal(concatenate(arrays, axis))"
1057,"def import_name(self, signature, summary_string, real_name): if self.import_name = self.import_name = self.import_name = self.import_name = self.import_name = self.import_name = self.import_name = self.import_name = self.import_name = self.import_name = self.import_name = self.import_name = self.import_name = self.import_name = self.import_name = self.import_name = self.import_name = self.import_name = self.import_name","def get_items(self, names): env = self.state.document.settings.env prefixes = get_import_prefixes_from_env(env) items = [] max_item_chars = 50 for name in names: display_name = name if name.startswith(''): name = name[1:] display_name = name.split('.')[-1] try: real_name, obj, parent = import_by_name(name, prefixes=prefixes) except ImportError: self.warn('failed to import %s' % name) items.append((name, '', '', name)) continue documenter = get_documenter(obj, parent)(self, real_name) if not documenter.parse_name(): self.warn('failed to parse name %s' % real_name) items.append((display_name, '', '', real_name)) continue if not documenter.import_object(): self.warn('failed to import object %s' % real_name) items.append((display_name, '', '', real_name)) continue sig = documenter.format_signature() if not sig: sig = '' else: max_chars = max(10, max_item_chars - len(display_name)) sig = mangle_signature(sig, max_chars=max_chars) sig = sig.replace('*', r'*') doc = list(documenter.process_doc(documenter.get_doc())) while doc and not doc[0].strip(): doc.pop(0) m = re.search(r""([A-Z][A-Z]*?.s)"", "" "".join(doc).strip()) if m: summary ="
1058,"def initialize_init(self, configuration_dictionary): self.init(config_dictionary) self.init(config_dictionary) self.init(config_dictionary) self.init(config_dictionary) self.init(config_dictionary) self.init(config_dictionary) self.init(config_dictionary) self.init(config_dictionary) self.init(config_dictionary) self.init(config_dictionaries) self.init(config_dictionaries) self.init(config_dictionaries) self.init(config_dictionaries) self.init(config_dictionaries) self.init(config_dictionaries) self.init(config_dictionaries) self.init(config_dictionaries) self.init(config_dictionaries) self.init(config_dictionaries) self.init(config_dictionaries) self.init(config_dictionaries) self.init(config_dictionaries) self.init(config_dictionaries) self.init(config_dictionaries) self.init(config_dictionaries) self.init(config_dictionaries) self.init(config_dictionaries) self.init(config_dictionaries) self.in","def from_config(cls, config): return cls(** 'initializers': [tf.compat.v2.initializers.deserialize(init) for init in config.get('initializers', [])],'sizes': config.get('sizes', []), 'validate_args': config.get('validate_args', False), )"
1059,"def add_source(self, indentations, newlines): if self.get_source(self.get_source): self.get_source(self.get_source): self.get_source(self.get_source): self.get_source(self.get_source)","def add_line(self, line): self.code.append("" "" * self.indent_amount) self.code.append(line) self.code.append(""n"")"
1060,"def remove_all_tags(self, tag): if tag is not self.get_tag(tag): if tag is self.get_tag(tag): if tag is self.get_tag(tag): if tag is self.get_tag(tag): if tag is self.get_tag(tag): if tag is self.get_tag(tag): if tag is self.get_tag(tag): if tag is self.get_tag(tag)","def _strip_tag(tree, tag): for el in tree.iter(): if el.tag == tag: el.getparent().remove(el)"
1061,"def amap(args, list, dict, amap): if amap is not args: if args is not args: if args is not args: if args is not args: if args is not args: if args is not args: if args is not args: if args is not args: if args is not args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args =","def _check_bits(self, args, amap): for wire in args: if wire not in amap: raise DAGCircuitError(""(qu)bit %s[%d] not found"" % (wire[0].name, wire[1]))"
1062,def get_model(self): if self.get_model(self.get_model(self.get_model(self.get_model(self.get_model(self.get_model(self.get_model(self.get_model(self.get_model(self.get_model(self.get_model(self.get_model(self.get_model(self.get_model(self.get_model(self.get_model(self.get_model(self.get_model))))))))),"def get_object_or_none(qs, *args, **kwargs): try: return qs.get(*args, **kwargs) except models.ObjectDoesNotExist: return None"
1063,"def pull_image(self, file_name, file_name): if self.add(self.add(self.add(self.add(self.add(self.add(self.add(self.add(self.add(self.add(self.add(self.add(self.add(self.add(self.add(self.add(self.add(self.add(self.add(self.add(self.add(self.add(self.add))))))))))))))))))))))))))))))))))))))))))))))))))))))","def pull(self, images, file_name=None, save=True, force=False, **kwargs): if not isinstance(images,list): images = [images] bot.debug('Execution of PULL for %s images' % len(images)) finished = [] for image in images: q = parse_image_name(remove_uri(image), lowercase=False) url = ""%s/container/%s/%s:%s"" %(self.base, q['collection'], q['image'], q['tag']) bot.debug('Retrieving manifest at %s' % url) manifest = self._get(url) manifest['selfLink'] = url if ""version"" in manifest: q = parse_image_name('%s@%s' %(q['uri'], manifest['version'])) if file_name is None: file_name = self._get_storage_name(q) file_name = os.path.abspath(file_name) if os.path.exists(file_name) and force is False: bot.error('Image exists! Remove first, or use --force to overwrite') sys.exit(1) show_bar = not bool(self.quiet) image_file = self.download(url=manifest['image'], file_name=os.path.basename(file_name), show_progress=show_bar) if save is True: image_uri = ""%s:%s@%s"" %(manifest['name'], manifest['tag'], manifest['version']) container = self.add(image_path=image_file, image_uri=image_uri, image_name=file_name, metadata=manifest, url=manifest['image']) image_file = container.image if os.path.exists(image_file"
1064,"def n_port(self, n, n, n): if self.port(n): self.port(n): self.port(n): self.port(n): self.port(n): self.port(n): self.port(n)","def random_ports(port, n): for i in range(min(5, n)): yield port + i for i in range(n-5): yield port + random.randint(-2*n, 2*n)"
1065,def set_primary_point_of_interest(prime_point_of-interest): set_prime_point_of-interest(prime_point_of-interest): set_prime_point_of-interest(prime_point_of-interest): set_prime_point_of-interest(prime_point_of-interest): set_prime_point_of-interest(prime_point_of-interest): set_prime_point_of-interest(prime_point_of-interest) set_prime_point_of-interest(prime_point_of-interest(prime_point_of-interest)) set_point_of-interest(prime_point_of-interest)) set_point_of-interest(prime_point_of-interest) set_point_of-interest(prime_point_of-interest) set_point_of-interest(prime_point_of-interest) set_point_of-interest(prime_point_of-interest) set_point_of-interest) set_point_of-interest(prime_point_of-interest) set_point_of-interest(prime_point_of-interest) set_point_of-interest(prime_point_of,"def ppoi(self, value): ppoi = validate_ppoi( value, return_converted_tuple=True ) if ppoi is not False: self._ppoi_value = ppoi self.build_filters_and_sizers(ppoi, self.create_on_demand)"
1066,"def calculate_margin(self, border_left, border_left, border_left, border_left, border_left, border_left, border_left, border_left, border_left, border_left, border_left, border_left, border_left)","def calculate_left_margin(self): bl = 7 if self.rotate_y_labels: max_y_label_height_px = self.y_label_font_size else: label_lengths = map(len, self.get_y_labels()) max_y_label_len = max(label_lengths) max_y_label_height_px = 0.6 * max_y_label_len * self.y_label_font_size if self.show_y_labels: bl += max_y_label_height_px if self.stagger_y_labels: bl += max_y_label_height_px + 10 if self.show_y_title: bl += self.y_title_font_size + 5 self.border_left = bl"
1067,"def _endpoint_state_machine(self, endpoint_state_machine): if self.state_machine(endpoint_state_machine): self.state_machine(endpoint_state_machine) self.state_machine(endpoint_state_machine) self.state_machine(endpoint_state_machine) self.state_machine(endpoint_state_machine) self.state_machine(endpoint_state_machine) self.state_machine(endpoint_state_machine) self.state_machine) self.state_machine(endpoint_state_machine) self.state_machine(endpoint_state_machine) self.state_machine(endpoint_state_machine) self.state_machine(endpoint_state_machine) self.state_machine) self.state_machine(endpoint_machine) self.state_machine) self.state_machine(endpoint_machine) self.state_machine(endpoint_machine) self.state_machine(endpoint_machine) self.state_machine(endpoint_machine) self.state_machine(endpoint_machine) self.state_machine) self.","def _ep_error(self, error): super(Connection, self)._ep_error(error) self._connection_failed(""Protocol error occurred."")"
1068,"def render_part_template(self, context): if self.get_part_template(self, context): self.get_part_template(self.get_part_template(self.get_part_template(self.get_part_template(self.get_part_template(self.get_part_template(self.get_part_template(self.get_part_template(self.get_part_template)))","def render_template(tpl, context): templates = [tpl] if type(tpl)!= list else tpl tpl_instance = None for tpl in templates: try: tpl_instance = template.loader.get_template(tpl) break except template.TemplateDoesNotExist: pass if not tpl_instance: raise Exception('Template does not exist:'+ templates[-1]) return tpl_instance.render(template.Context(context))"
1069,"def write_data(filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_","def write_to_filterbank(self, filename_out): print(""[Filterbank] Warning: Non-standard function to write in filterbank (.fil) format. Please use Waterfall."") n_bytes = int(self.header[b'nbits'] / 8) with open(filename_out, ""wb"") as fileh: fileh.write(generate_sigproc_header(self)) j = self.data if n_bytes == 4: np.float32(j.ravel()).tofile(fileh) elif n_bytes == 2: np.int16(j.ravel()).tofile(fileh) elif n_bytes == 1: np.int8(j.ravel()).tofile(fileh)"
1070,def check_conflicts(self): if self.check_conflicts(self.check_conflicts): self.check_conflicts(self.check_conflicts): self.check_conflicts(self.check_conflicts): self.check_conflicts(self.check_conflicts),"def args_ok(self, options, args): for i in ['erase', 'execute']: for j in ['annotate', 'html','report', 'combine']: if (i in options.actions) and (j in options.actions): self.help_fn(""You can't specify the '%s' and '%s' "" ""options at the same time."" % (i, j)) return False if not options.actions: self.help_fn( ""You must specify at least one of -e, -x, -c, -r, -a, or -b."" ) return False args_allowed = ( 'execute' in options.actions or 'annotate' in options.actions or 'html' in options.actions or 'debug' in options.actions or'report' in options.actions or 'xml' in options.actions ) if not args_allowed and args: self.help_fn(""Unexpected arguments: %s"" % "" "".join(args)) return False if 'execute' in options.actions and not args: self.help_fn(""Nothing to do."") return False return True"
1071,def check_response(self): if self.response(self.response): self.response(self.response): self.response(self.response),"def _check_multi_statement_line(self, node, line): if isinstance(node, nodes.With): return if isinstance(node, nodes.TryExcept) and isinstance( node.parent, nodes.TryFinally ): return if ( isinstance(node.parent, nodes.If) and not node.parent.orelse and self.config.single_line_if_stmt ): return if ( isinstance(node.parent, nodes.ClassDef) and len(node.parent.body) == 1 and self.config.single_line_class_stmt ): return self.add_message(""multiple-statements"", node=node) self._visited_lines[line] = 2"
1072,"def pearson_of_u(self, logu): if pearson_of_u = self._______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________","def pearson(logu, name=None): with tf.compat.v1.name_scope(name, ""pearson"", [logu]): logu = tf.convert_to_tensor(value=logu, name=""logu"") return tf.square(tf.math.expm1(logu))"
1073,def retrieve_all_terms(self): return self.get_terms(self.get_terms(self.get_terms(self.get_terms))),"def _build_pools(self): if self.level >= Topic: self.topics_pool = set(self.topic() for i in range(self.pool_size)) if self.level >= Fact: self.facts_pool = set(self.fact() for i in range(self.pool_size)) if self.level >= Theory: self.theories_pool = set(self.theory() for i in range(self.pool_size)) if self.level >= Text: self.propositions_pool = set(chain.from_iterable((self.topics_pool, self.facts_pool, self.theories_pool)))"
1074,"def create_tfs_connection_context(self, connection_context): self.create_tfs_context(connection_context) self.create_tfs_context(connection_context) self.create_tfs_context(connection_context) self.create_tfs_context(connection_context) self.create_tfs_context(connection_context) self.create_tfs_context(connection_context) self.create_tfs_context(connection_context) self.create_tfs_context(connection_context) self.create_tfs_context(connection_context) self.create_tfs_context(connection_context) self.create_tfs_context(connection_context) self.create_tfs_context(connection_context) self.create_tfs_context) self.create_tfs_context(connection_context) self.create_tfs_context(connection_context) self.create_tfs_context(connection_context) self.create_tfs_context(connection_context) self.create_tfs_context(","def create_tfs_connection(url, token): if token is None: token = os.environ.get('TFS_API_TOKEN', None) tfs_credentials = BasicAuthentication('', token) tfs_connection = VssConnection(base_url=url, creds=tfs_credentials) return tfs_connection"
1075,"def make_requests(self, requests, requests, requests, requests, requests, requests, requests, requests, requests, requests, requests, requests, requests, requests, requests, requests, requests)","def _request(self, req_type, url, **kwargs): logger.debug('%s %s' % (req_type, url)) result = self.session.request(req_type, url, **kwargs) try: result.raise_for_status() except requests.HTTPError: error = result.text try: error = json.loads(error) except ValueError: pass if result.status_code in (401, 403): error_class = LuminosoAuthError elif result.status_code in (400, 404, 405): error_class = LuminosoClientError elif result.status_code >= 500: error_class = LuminosoServerError else: error_class = LuminosoError raise error_class(error) return result"
1076,def _ugly_ugly_dirty_hack(self): if ugly_ugly_dirty_hack(self): ugly_ugly_dirty_hack(self): ugly_ugly_dirty_hack(self): ugly_ugly_dirty_hack(self): ugly_ugly_dirty_hack(self): ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_ugly_u,"def download(self, **kwargs): if 'json_output' in kwargs and kwargs['json_output']: json_output.output(self) elif 'info_only' in kwargs and kwargs['info_only']: if'stream_id' in kwargs and kwargs['stream_id']: stream_id = kwargs['stream_id'] if 'index' not in kwargs: self.p(stream_id) else: self.p_i(stream_id) else: if 'index' not in kwargs: self.p([]) else: stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag'] self.p_i(stream_id) else: if'stream_id' in kwargs and kwargs['stream_id']: stream_id = kwargs['stream_id'] else: stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag'] if 'index' not in kwargs: self.p(stream_id) else: self.p_i(stream_id) if stream_id in self.streams: urls = self.streams[stream_id]['src'] ext = self.streams[stream_id]['container'] total_size = self.streams[stream_id]['size'] else: urls = self.dash_streams[stream_id]['src'] ext = self.dash_streams[stream"
1077,"def create_condition_function_pair(self, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair, condition_function_pair","def _outside_contraction_fn(objective_function, simplex, objective_values, face_centroid, best_index, worst_index, reflected, objective_at_reflected, contraction, shrinkage, batch_evaluate_objective): contracted = face_centroid + contraction * (reflected - face_centroid) objective_at_contracted = objective_function(contracted) is_contracted_acceptable = objective_at_contracted = objective_at_reflected def _accept_contraction(): next_simplex = _replace_at_index(simplex, worst_index, contracted) objective_at_next_simplex = _replace_at_index( objective_values, worst_index, objective_at_contracted) return (False, next_simplex, objective_at_next_simplex, 1) def _reject_contraction(): return _shrink_towards_best(objective_function, simplex, best_index, shrinkage, batch_evaluate_objective) return prefer_static.cond(is_contracted_acceptable, _accept_contraction, _reject_contraction) return _contraction"
1078,"def get_emojis(self, note, emoji): emoji = self.emoji(note, emoji) self.emoji(note, emoji) self.emoji(note, emoji) self.emoji(note, emoji) self.emoji(note, emoji) self.emoji(note, emoji) self.emoji(note, emoji) self.emoji(note, emoji) self.emoji) self.emoji(note, emoji) self.emoji(note, emoji) self.emoji(note, emoji) self.emoji) self.emoji(note, emoji) self.emoji) self.emoji(note, emoji) self.emoji(note, emoji) self.emoji) self.emoji(note, emoji) self.emoji(note, emoji) self.emoji(note, emoji) self.emoji(note, emoji) self.emoji(note, emoji) self.em","def note_emojis(self, item_type, item_id, note_id): payload =  'order_by': 'updated_at','sort': 'asc', 'per_page': PER_PAGE  path = urijoin(item_type, str(item_id), GitLabClient.NOTES, str(note_id), GitLabClient.EMOJI) return self.fetch_items(path, payload)"
1079,def get_cqt_bin_frequency(self): self.get_cqt_bin_frequency(self.get_cqt_bin_frequency) self.get_cqt_bin_frequency(self.get_cqt_bin_frequency) self.get_cqt_bin_frequency(self.get_cqt_bin_frequency) self.get_cqt_bin_frequency(self.get_cqt_bin_frequency(self.get_cqt_bin_frequency_frequency_frequency_frequency_frequency_frequency_frequency_frequency_frequency_frequency_frequency_frequency_frequency_frequency_frequency_frequency_frequency_frequency_frequency_frequency_frequency_frequency_frequency_frequency_frequency_frequency_frequency_frequency_frequency_frequency_f,"def __coord_cqt_hz(n, fmin=None, bins_per_octave=12, **_kwargs): if fmin is None: fmin = core.note_to_hz('C1') return core.cqt_frequencies(n+1, fmin=fmin / 2.0**(0.5/bins_per_octave), bins_per_octave=bins_per_octave)"
1080,"def generate_language_codes(self, language_codes): self.get_language_codes(language_codes) self.get_language_codes(language_codes) self.get_language_codes(language_codes) self.get_language_codes(language_codes) self.get_language_codes(language_codes) self.get_language_codes(language_codes) self.get_language_codes(language_codes) self.get_language_codes) self.get_language_codes(language_codes) self.get_language_codes(language_codes) self.get_language_codes(language_codes) self.get_language_codes(language_codes) self.get_codes(language_codes) self.get_codes) self.get_codes(language_codes) self.get_codes(language_codes) self.get_codes(language_codes) self.get_codes(language_codes) self.get_codes(language_codes) self.get_codes(language_codes) self.get_codes(language_codes) self.get_codes(language_codes) self.get_","def print_table(language): table = translation_table(language) for code, name in sorted(table.items(), key=operator.itemgetter(0)): print(u'language:8 name:u300020'.format( name=name, language=code )) return None"
1081,def get_variable(variables(list)): variants(variables(list))): variants(variables(list)): variants(variables(list)): variants(variables(list)): variants(variables(list)): variants(variables(list)): variants(variables(list)): variants(variables(list)): variants(variables(list)): variants(list)): variants(list): variants(variables(list)): variants(variables(variables(variables(variables(variables(variables(variables(variables(variables(variables(variables(variables(variables(variables(variables(variables(variables)))))))))))))variables(variables(variables(variables(variables(variables(variables(variables(variables))))))))))variables(variables(variables(variables(variables(variables(variables(variables),"def sample_variants(self, variants, sample_name, category ='snv'): LOG.info('Retrieving variants for subject : 0'.format(sample_name)) has_allele = re.compile('1|2') query =  '$and': [ '_id' :  '$in' : variants, 'category' : category,'samples':  '$elemMatch':  'display_name' : sample_name, 'genotype_call':  '$regex' : has_allele    ]  result = self.variant_collection.find(query) return result"
1082,"def cancel(self, block): if self.getEvents(block): self.getEvents(block) self.getEvents(block) self.getEvents(block) self.getEvents(block) self.getEvents(block) self.getEvents(block) self.getEvents(block) self.getEvents(block) self.getEvents(block) self.getEvents(block) self.getEvents(block) self.getEvents(block) self.getEvents(block) self.getEvents(block) self.getEvents(block) self.getEvents(block) self.getEvents(block) self.getEvents(block) self.getEvents(block) self.getEvents(block) self.getEvents(block) self.getEvent(block) self.getEvent(block) self.getEvent(block) self.getEvent(block) self.getEvent(block) self.getEvent(block) self.getEvent(block) self.getEvent(block) self.getEvent(block) self.getEvent(block) self.getEvent(block) self","def cancel(self, block): event = libaio.io_event() try: libaio.io_cancel(self._ctx, byref(block._iocb), byref(event)) except OSError as exc: if exc.errno == errno.EINPROGRESS: return None raise return self._eventToPython(event)"
1083,"def get_events(self, events): self.get_events(events) self.get_events(events) self.get_events(events) self.get_events(events) self.get_events(events) self.get_events(events) self.get_events(events) self.get_events(events) self.get_events(events) self.get_events) self.get_events) self.get_events(events) self.get_events) self.get_events(events) self.get_events) self.get_events(events) self.get_events) self.get_events(events) self.get_events) self.get_events(events) self.get_events(events) self.get_events(events) self.get_events) self.get_events(events) self.get_events(events) self.get_events) self.get_event","def eventsReceived(self, events): if (self.maxsize  1) or (len(self.events)  self.maxsize): self.events.extend(events)"
1084,"def consume_stream(self, read_until, read_until, read_until, read_until, read_until, read_until, read_until, read_until, read_until, read_until, read_until, read_until, read_until, read_until, read_until, read_until, read_until, read_until, read_until)","def read_until(self, c: str, inhibitor='') -> bool: if self.read_eof(): return False self._stream.save_context() while not self.read_eof(): if self.peek_char(inhibitor): self.one_char() self.one_char() if self.peek_char(c): self._stream.incpos() return self._stream.validate_context() self._stream.incpos() return self._stream.restore_context()"
1085,"def create_request_element(self, credentials, method, headers, body, json_input): return json_input if json_input is false: return json_input if json_input is false: return json_input if json_input is false: return json_input if json_input is false: return json_input if json_input is false: return json_input = return json_input = return json_input = return json_input = return json_input = return json_input = return json_input = return json_input = return json_input = return json_input = return json_input = return json_input = return json_input = return json_input = return json_input = return json_input = return json_input = return json_input = return json_input = return json_input = return json_input = return json_input = return json","def request_elements( self, credentials=None, url=None, method='GET', params=None, headers=None, body='', json_input=None, return_json=False ): if json_input: parsed_input = json.loads(json_input) credentials = parsed_input.get('credentials', credentials) url = parsed_input.get('url', url) method = parsed_input.get('method', method) params = parsed_input.get('params', params) headers = parsed_input.get('headers', headers) body = parsed_input.get('body', body) if not credentials and url: raise RequestElementsError( 'To create request elements, you must provide credentials''and URL either as keyword arguments or in the JSON object!') credentials = Credentials.deserialize(self.config, credentials) ProviderClass = credentials.provider_class request_elements = ProviderClass.create_request_elements( ProviderClass.PROTECTED_RESOURCE_REQUEST_TYPE, credentials=credentials, url=url, method=method, params=params, headers=headers, body=body) if return_json: return request_elements.to_json() else: return request_elements"
1086,"def retrune(self, endpoint, base): if retrune = self.retrune = self.retrune = self.retrune = self.retrune = self.retrune = self.retrune","def space(start, stop, num, endpoint=True, log=False, base=10): assert type(start) is int and type(stop) is int and type(num) is int,  ""start, stop and num need to be intergers"" assert num >= 2, ""num has to be atleast 2"" if log: start = math.log(start, base) stop = math.log(stop, base) if endpoint: step_length = float((stop - start)) / float(num - 1) else: step_length = float((stop - start)) / float(num) i = 0 while i  num: if log: yield int(round(base ** (start + i * step_length))) else: yield int(round(start + i * step_length)) i += 1"
1087,"def _fitting_method(self, method, 'trace', 'beta', 'epsilon', 'epsilon', 'epsilon', 'epsilon', 'epsilon', 'epsilon', 'epsilon', 'epsilon', 'epsilon', 'epsilon', 'epsilon', 'epsilon', 'epsilon', 'epsilon', 'epsilon', 'epsilon', 'epsilon', 'epsilon', 'epsilon', 'epsilon', 'epsilon', 'epsilon', 'epsilon', 'epsilon', 'epsilon', 'epsilon', 'epsilon', 'epsilon', 'epsilon', 'epsilon', 'e","def fit_tomography_data(tomo_data, method='wizard', options=None): if isinstance(method, str) and method.lower() in ['wizard', 'leastsq']: trace = __get_option('trace', options) beta = __get_option('beta', options) rho = __leastsq_fit(tomo_data, trace=trace, beta=beta) if method == 'wizard': epsilon = __get_option('epsilon', options) rho = __wizard(rho, epsilon=epsilon) return rho else: raise Exception('Invalid reconstruction method ""%s""' % method)"
1088,"def perform_eigendecompositon(self, left- and right- matrices): self.decompositon(left- and right- matrices) self.decompositon(left- and right- matrices) self.decompositon(left- and right- matrices) self.decompositon(left- and right- matrices) self.decompositon(left- and right- matrices) self.decompositon(left- matrices) self.decompositon(left- matrices) self.decompositon(left- matrices) self.decompositon(left- matrices) self.decompositon(right- matrices) self.decompositon(left- matrices) self.decompositon(right- matrices) self.decompositon(left- matrices) self.decompositon(left- matrices) self.decompositon(left- matrices) self.decompositon(left- matrices) self.decompositon(left- matrices) self.decom","def _eig(self): eigvals, eigvecs = np.linalg.eig(self.Q) self.v = np.real(eigvecs) self.v_inv = np.linalg.inv(self.v) self.eigenvals = np.real(eigvals)"
1089,"def take_decoy(self, target, decoy, decoy, target, decoy, decoy, target, decoy, decoy, target, decoy, decoy, target, decoy, decoy, target, decoy, decoy, target, decoy, decoy, target, decoy, decoy, target, decoy, target, decoy, target, decoy, target, decoy, decoy, target, decoy, target, decoy, target, decoy, target, decoy, target, decoy, target, decoy, target, decoy, target, decoy, target, decoy, target, decoy, target, decoy, target, decoy, target, decoy, target, decoy, target, decoy, target, decoy, target, decoy, target, decoy, decoy, decoy, decoy, decoy, decoy, decoy, decoy, decoy, decoy, decoy, decoy, decoy, decoy, decoy, decoy, decoy, decoy, decoy, de","def error_statistics(target_scores, decoy_scores, parametric, pfdr, pi0_lambda, pi0_method = ""smoother"", pi0_smooth_df = 3, pi0_smooth_log_pi0 = False, compute_lfdr = False, lfdr_trunc = True, lfdr_monotone = True, lfdr_transf = ""probit"", lfdr_adj = 1.5, lfdr_eps = np.power(10.0,-8)): target_scores = to_one_dim_array(target_scores) target_scores = np.sort(target_scores[np.isnan(target_scores)]) decoy_scores = to_one_dim_array(decoy_scores) decoy_scores = np.sort(decoy_scores[np.isnan(decoy_scores)]) if parametric: target_pvalues = pnorm(target_scores, decoy_scores) else: target_pvalues = pemp(target_scores, decoy_scores) pi0 = pi0est(target_pvalues, pi0_lambda, pi0_method, pi0_smooth_df, pi0_smooth_log_pi0) target_qvalues = qvalue(target_pvalues, pi0['pi0'], pfdr) metrics = stat_metrics(target_pvalues, pi0['pi0'], pfdr) error_stat = pd.DataFrame('cutoff': target_scores, 'pvalue': target_pvalues, 'qvalue': target_qvalues,'svalue': metrics['svalue'],"
1090,"def print_user_friendly_contacts_table(vcard_list, carddav_object, parsable, parsable, parsable, parsable, parsable, parsable, parsable, parsable, parsable, parsable, parsable, parsable, parsable, parsable, parsable, parsable, parsable, parsable, parsable, parsable, parsable, parsable, parsable, parsable, parsable, parsable, parsable, parsable, parsable, parsable, parsable)","def list_subcommand(vcard_list, parsable): if not vcard_list: if not parsable: print(""Found no contacts"") sys.exit(1) elif parsable: contact_line_list = [] for vcard in vcard_list: if config.display_by_name() == ""first_name"": name = vcard.get_first_name_last_name() else: name = vcard.get_last_name_first_name() contact_line_list.append('t'.join([vcard.get_uid(), name, vcard.address_book.name])) print('n'.join(contact_line_list)) else: list_contacts(vcard_list)"
1091,"def initialize_b_read(self, ekmmeters.SerialBlock): self.read(self.read_block) self.read_block(self.read_block) self.read_block(self.read_block) self.read_block(self.read_block) self.read_block(self.read_block) self.read_block(self.read_block) self.read_block(self.read_block) self.read_block) self.read_block(self.read_block) self.read_block) self.read_block(self.read_block) self.read_block(self.read_block) self.read_block(self.read_block) self.read_block_block_block_block_block_block_block_block_block_block_block_block_block_block_block_block_block_block_block_block_block_block_block_block_block_block_block_block_block_block_block_block_block_block_block_b","def initFormatB(self): self.m_blk_b[""reserved_5""] = [1, FieldType.Hex, ScaleType.No, """", 0, False, False] self.m_blk_b[Field.Model] = [2, FieldType.Hex, ScaleType.No, """", 0, False, True] self.m_blk_b[Field.Firmware] = [1, FieldType.Hex, ScaleType.No, """", 0, False, True] self.m_blk_b[Field.Meter_Address] = [12, FieldType.String, ScaleType.No, """", 0, False, True] self.m_blk_b[Field.kWh_Tariff_1] = [8, FieldType.Float, ScaleType.KWH, """", 0, False, False] self.m_blk_b[Field.kWh_Tariff_2] = [8, FieldType.Float, ScaleType.KWH, """", 0, False, False] self.m_blk_b[Field.kWh_Tariff_3] = [8, FieldType.Float, ScaleType.KWH, """", 0, False, False] self.m_blk_b[Field.kWh_Tariff_4] = [8, FieldType.Float, ScaleType.KWH, """", 0, False, False] self.m_blk_b[Field.Rev_kWh_Tariff_1] = [8, FieldType.Float, ScaleType.KWH, """", 0, False, False] self.m_blk_b[Field.Rev_kWh_Tariff_2] = [8, FieldType.Float, ScaleType.KWH, """", 0, False, False] self.m_blk_b[Field.Rev_kWh_Tariff_3] = [8, FieldType.Float, ScaleType."
1092,"def get_column_names(virtual=False, hidden=False, hidden=False, regex=False, regex=False, regex=False, regex=False, regex=False, regex=False, regex=False, regex=False, regex=False, regex=False, regex=False, regex=False, regex=False, regex=False, regex=False, regex=False, regex=False)","def get_column_names(self, virtual=True, strings=True, hidden=False, regex=None): if regex and not re.match(regex, name): return False if not virtual and name in self.virtual_columns: return False if not strings and (self.dtype(name) == str_type or self.dtype(name).type == np.string_): return False if not hidden and name.startswith('__'): return False return True return [name for name in self.column_names if column_filter(name)]"
1093,"def list_of_folders(folders, folder_id): if folder_id is not provided, Home folder will be listed._______________________________________________________________________________________________________________________________________________","def list_folder(self, folder_id=None): params = 'folder': folder_id if folder_id else  return self._get('file/listfolder', params=params)"
1094,def export_variants(variants): variants(variants): variants(variants): variants(variants): variants(variants): variants(variants): variants(variants): variants(variants): variants(variants): variants(variants): variants(variants): variants(variants): variants(variants): variants(variants): variants(variants): variants(variant,"def export_mt_variants(variants, sample_id): document_lines = [] for variant in variants: line = [] position = variant.get('position') change = '>'.join([variant.get('reference'),variant.get('alternative')]) line.append(position) line.append(change) line.append(str(position)+change) genes = [] prot_effect = [] for gene in variant.get('genes'): genes.append(gene.get('hgnc_symbol','')) for transcript in gene.get('transcripts'): if transcript.get('is_canonical') and transcript.get('protein_sequence_name'): prot_effect.append(urllib.parse.unquote(transcript.get('protein_sequence_name'))) line.append(','.join(prot_effect)) line.append(','.join(genes)) ref_ad = '' alt_ad = '' for sample in variant['samples']: if sample.get('sample_id') == sample_id: ref_ad = sample['allele_depths'][0] alt_ad = sample['allele_depths'][1] line.append(ref_ad) line.append(alt_ad) document_lines.append(line) return document_lines"
1095,"def get_quote(self, quote): if self.quote(quote): self.quote(quote): self.quote(quote) return self.quote(quote)",def give_another_quote(q): for qc in QUOTES: if qc!= q: return qc else: raise ValueError(u'Could not find a different quote for '.format(q))
1096,"def convert_image(img, keepdim, keepdim, keepdim, keepdim, keepdim, keepdim, keepdim, keepdim, keepdim, keepdim, keepdim, keepdim, keepdim, keepdim, keepdim, keepdim, keepdim, keepdim, keepdim, keepdim, keepdim, keepdim, keepdim, keepdim, keepdim, keepdim)","def bgr2gray(img, keepdim=False): out_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if keepdim: out_img = out_img[..., None] return out_img"
1097,"def build_diameter_filter(self, window, tuple, number, callable, or list-like): get_window(window, window, window): get_window(window, window, window): get_window(window, window): get_window(window): get_window(window): get_window(window): get_window(window): get_window(window): get_window(window): get_window(window): get_window(window): get_window(window): get_window): get_window(window): get_window(window): get_window(window): get_window): get_window(window): get_window(window): get_window(window): get_window(window): get_window(window): get_window(window): get_window(window): get_window(window): get_window(window): get_window(window): get_window(window): get_window(window): get_wind","def diagonal_filter(window, n, slope=1.0, angle=None, zero_mean=False): if angle is None: angle = np.arctan(slope) win = np.diag(get_window(window, n, fftbins=False)) if not np.isclose(angle, np.pi/4): win = scipy.ndimage.rotate(win, 45 - angle * 180 / np.pi, order=5, prefilter=False) np.clip(win, 0, None, out=win) win /= win.sum() if zero_mean: win -= win.mean() return win"
1098,"def set_autosaves(self, command history, autosaves): self.set_autosaves(self.set_autosaves) self.set_autosaves(self.set_autosaves) self.set_autosaves(self.set_autosaves) self.set_autosaves(self.set_autosaves) self.set_autosaves(self.set_autosaves) self.set_autosaves(self.set_autosaves)","def init_history(self): self.history_manager = HistoryManager(shell=self, config=self.config) self.configurables.append(self.history_manager)"
1099,"def extract_required_fields(self, fields): self.get_required_fields(fields) self.get_required_fields(fields) self.get_required_fields(fields) self.get_required_fields(fields) self.get_required_fields(fields) self.get_required_fields(fields) self.get_required_fields(fields) self.get_required_fields(fields) self.get_required_fields(fields) self.get_required_fields(fields) self.get_required_fields(fields) self.get_required_fields(fields) self.get_required_fields(fields) self.get_required_fields(fields) self.get_required_fields(fields) self.get_fields(fields) self.get_fields(fields) self.get_fields(fields) self.get_fields(fields) self.get_fields(fields) self.get_fields) self.get_fields(fields) self.get_fields","def extract_dictionary(self, metrics): new_metrics =  for m in metrics: metric = self.extract_fields(m) new_metrics[m['name']] = metric return new_metrics"
1100,"def limbdark(r, ndarray, limbdark, limbdark, limbdark, limbdark, limbdark, limbdark, limbdark, limbdark, limbdark, limbdark, limbdark, limbdark, limbdark, limbdark, limbdark, limbdark, limbdark, limbdark, limbdark, limbdark, limbdark, limbdark, limbdark, limbdark, limbdark)","def I(r, limbdark): if limbdark.ldmodel == QUADRATIC: u1 = limbdark.u1 u2 = limbdark.u2 return (1-u1*(1-np.sqrt(1-r**2))-u2*(1-np.sqrt(1-r**2))**2)/(1-u1/3-u2/6)/np.pi elif limbdark.ldmodel == KIPPING: a = np.sqrt(limbdark.q1) b = 2*limbdark.q2 u1 = a*b u2 = a*(1 - b) return (1-u1*(1-np.sqrt(1-r**2))-u2*(1-np.sqrt(1-r**2))**2)/(1-u1/3-u2/6)/np.pi elif limbdark.ldmodel == NONLINEAR: raise Exception('Nonlinear model not yet implemented!') else: raise Exception('Invalid limb darkening model.')"
1101,"def get_cursor(self, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor)",def _query_mysql(self): mysql = MySqlHook(mysql_conn_id=self.mysql_conn_id) conn = mysql.get_conn() cursor = conn.cursor() cursor.execute(self.sql) return cursor
1102,def get_status_text(self): self.get_status_text(self.get_status_text) self.get_status_text(self.get_status_text) self.get_status_text(self.get_status_text) self.get_status_text(self.get_status_text) self.get_status_text(self.get_status_text) self.get_status_text(self.get_status_text) self.get_status_text(self.get_status_text) self.get_status_text(self.get_status_text) self.get_status_text(self.get_status_text) self.get_status_text) self.get_status_text(self.get_status_text) self.get_status_text(self.get_status_text) self.get_status_text(self.status_text) self.status_text(self.status_text) self.status_text) self.status_text(self.status_text) self.status_text(self.status_text) self.status_text(self.status_text) self,"def status_text(self): bstr_status_text = c_void_p() _WinHttpRequest._StatusText(self, byref(bstr_status_text)) bstr_status_text = ctypes.cast(bstr_status_text, c_wchar_p) status_text = bstr_status_text.value _SysFreeString(bstr_status_text) return status_text"
1103,"def mark_deployment(self, id): if self.deployment(self.deployment): self.deployment(self.deployment): self.deployment(self.deployment) self.deployment(self.deployment) self.deployment(self.deployment) self.deployment(self.deployment) self.deployment(self.deployment) self.deployment) self.deployment(self.deployment) self.deployment) self.deployment(self.deployment) self.deployment(self.deployment) self.deployment) self.deployment(self.deployment) self.deployment) self.deployment(self.deployment) self.deployment) self.deployment(self.deployment) self.deployment) self.deployment(self.deployment) self.deployment) self.deployment(self.deployment) self.deployment(self.deployment) self.deployment","def fail_deployment(self, id): fail_json = ""DeploymentID"": id return self.request(""fail"", id, json=fail_json, method=""post"").json()"
1104,"def calculate_kl_distribution(a, b, b)): if kl_deterministic_distribution(a, b, b)): if kl_deterministic_distribution(a, b)): if kl_deterministic_distribution(a, b)): if kl_deterministic_distribution(a, b)): if kl_deterministic_distribution(kl_deterministic_distribution(a)): kl_deterministic_distribution(a)): kl_deterministic_distribution(a): kl_deterministic_distribution(a): kl_deterministic_distribution(a): kl_deterministic_distribution(a): kl_deterministic_distribution(a): kl_deterministic_distribution(a): kl_deterministic_distribution(a): kl_deterministic_distribution(a): kl_deterministic_distribution(a): kl_deterministic_distribu","def _kl_deterministic_distribution(a, b, name=None): with tf.name_scope(name or ""kl_deterministic_distribution""): return -b.log_prob(a.loc)"
1105,Python 3 input()/Python 2 raw_input()/Python 2 raw_input()/Python 2 raw_input()/Python 2 raw_input()/Python 2 raw_input(),def string_input(prompt=''): v = sys.version[0] if v == '3': return input(prompt) else: return raw_input(prompt)
1106,"def start(self, start, stop, step): if self.slice(self.slice): self.slice(self.slice) self.slice(self.slice) self.slice(self.slice) self.slice(self.slice) self.slice(self.slice) self.slice(self.slice) self.slice(self.slice) self.slice(self.slice) self.slice(self.slice) self.slice_slice_slice_slice_slice_slice_slice_slice_slice_slice_slice_slice_slice_slice_slice_slice_slice_slice_slice_slice_slice_slice_slice_slice_slice_slice_slice_slice_slice_slice_slice_slice_slice_slice_slice_slice_slice_slice_slice_slice_s","def slicify(slc, dim): if isinstance(slc, slice): start = 0 if slc.start is None else slc.start stop = dim if slc.stop is None else slc.stop step = 1 if slc.step is None else slc.step if start  0: start += dim if stop  0: stop += dim if step > 0: if start  0: start = 0 if stop > dim: stop = dim else: if stop  0: stop = -1 if start > dim: start = dim-1 return slice(start, stop, step) elif isinstance(slc, int): if slc  0: slc += dim return slice(slc, slc+1, 1) else: raise ValueError(""Type for slice %s not recongized"" % type(slc))"
1107,"def get_user_id(user_id, request.rollbar_person, request.rollbar_person, request.rollbar_person, request.rollbar_person, request.rollbar_person, request.rollbar_person, request.rollbar_person, request.rollbar_person, request.rollbar_person, request.rollbar_person, request.rollbar_person, request.rollbar_person, request.rollbar_person, request.rollbar_person, request.rollbar_person","def _build_person_data(request): if hasattr(request, 'rollbar_person'): rollbar_person_prop = request.rollbar_person try: person = rollbar_person_prop() except TypeError: person = rollbar_person_prop if person and isinstance(person, dict): return person else: return None if hasattr(request, 'user'): user_prop = request.user try: user = user_prop() except TypeError: user = user_prop if not user: return None elif isinstance(user, dict): return user else: retval =  if getattr(user, 'id', None): retval['id'] = text(user.id) elif getattr(user, 'user_id', None): retval['id'] = text(user.user_id) if retval.get('id'): username = getattr(user, 'username', None) email = getattr(user, 'email', None) retval.update( 'username': username, 'email': email ) return retval if hasattr(request, 'user_id'): user_id_prop = request.user_id try: user_id = user_id_prop() except TypeError: user_id = user_id_prop if not user_id: return None return 'id': text(user_id)"
1108,"def create_BRIAN_network(self, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj, traj)","def run_net(traj): eqs=traj.eqs namespace = traj.Net.f_to_dict(short_names=True, fast_access=True) neuron=NeuronGroup(traj.N, model=eqs, threshold=traj.Vcut, reset=traj.reset, namespace=namespace) neuron.vm=traj.EL neuron.w=traj.a*(neuron.vm-traj.EL) neuron.Vr=linspace(-48.3*mV,-47.7*mV,traj.N) print('Initial Run') net = Network(neuron) net.run(100*ms, report='text') MSpike=SpikeMonitor(neuron) net.add(MSpike) MStateV = StateMonitor(neuron, variables=['vm'],record=[1,2,3]) net.add(MStateV) print('Measurement run') net.run(500*ms,report='text') traj.v_standard_result = Brian2MonitorResult traj.f_add_result('SpikeMonitor',MSpike) traj.f_add_result('StateMonitorV', MStateV)"
1109,def cast_object(self): if self.get_object(self.get_object(self.get_object(self.get_object(self.get_object(self.get_object(self.get_object(self.get_object(self.get_object(self.get_object(self.get_object(self.get_object(self.get_object(self.get_object(self.get_object(self.get_object(self.get_object(self.get_object(self.get_object(self.get_object(self.get_object(self.get_object))))))))),"def to_number(obj): if isinstance(obj, LiteralWrapper): val = obj.obj elif isinstance(obj, Iterable) and not isinstance(obj, str): val = next(obj, None) else: val = obj if val is None: #FIXME: Should be NaN, not 0 yield 0 elif isinstance(val, str): yield float(val) elif isinstance(val, node): yield float(strval(val)) elif isinstance(val, int) or isinstance(val, float): yield val else: raise RuntimeError('Unknown type for number conversion: '.format(val))"
1110,"def remote_popen(self, cmd, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs","def run_query(self, cmd="""", **kwargs): spark_sql_cmd = self._prepare_command(cmd) self._sp = subprocess.Popen(spark_sql_cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, **kwargs) for line in iter(self._sp.stdout.readline, ''): self.log.info(line) returncode = self._sp.wait() if returncode: raise AirflowException( ""Cannot execute  on. Process exit code:."".format( cmd, self._conn.host, returncode ) )"
1111,def _convert_tag(self): if self.convert_tag(self.convert_tag): self.convert_tag(self.convert_tag) self.convert_tag(self.convert_tag) self.convert_tag(self.convert_tag) self.convert_tag(self.convert_tag) self.convert_tag(self.convert_tag) self.convert_tag(self.convert_tag) self.convert_tag(self.convert_tag) self.convert_tag(self.convert_tag) self.convert_tag(self.convert_tag) self.convert_tag(self.convert_tag) self.convert_tag(self.convert_tag) self.convert_tag(self.convert_tag) self.convert_tag) self.convert_tag(self.convert_tag) self.convert_tag) self.convert_tag(self.convert_tag) self.convert_tag(self.convert_tag) self.convert_tag(self.convert_tag) self.convert_tag(self.convert_tag) self.convert_tag(self.convert_tag) self.convert,"def update_to_v24(self): self.__update_common() if self.__unknown_version == self._V23: converted = [] for frame in self.unknown_frames: try: name, size, flags = unpack('>4sLH', frame[:10]) frame = BinaryFrame.fromData(self, flags, frame[10:]) except (struct.error, error): continue name = name.decode('ascii') converted.append(self.__save_frame(frame, name=name)) self.unknown_frames[:] = converted self.__unknown_version = self._V24 try: date = text_type(self.get(""TYER"", """")) if date.strip(u""x00""): self.pop(""TYER"") dat = text_type(self.get(""TDAT"", """")) if dat.strip(""x00""): self.pop(""TDAT"") date = ""%s-%s-%s"" % (date, dat[2:], dat[:2]) time = text_type(self.get(""TIME"", """")) if time.strip(""x00""): self.pop(""TIME"") date += ""T%s:%s:00"" % (time[:2], time[2:]) if ""TDRC"" not in self: self.add(TDRC(encoding=0, text=date)) except UnicodeDecodeError: pass if ""TORY"" in self: f = self.pop(""TORY"") if ""TDOR"" not in self: try: self.add(TDOR(encoding=0, text=str(f))) except UnicodeDecodeError: pass if ""IPLS"" in self: f = self.pop(""IPLS"") if ""TIPL"" not in self: self.add(TIPL(encoding=f.encoding, people=f.people)) for key in [""RVAD"", ""EQ"
1112,"def move_param_from_idx(self, old_text, new_text, new_text, new_text, old_text, new_text, new_text, new_text, new_text, new_text, old_text, new_text, new_text, new_text, old_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, old_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text, new_text,","def refactor_step(self, old_text, new_text, move_param_from_idx): diffs = [] step, func = self._find_step_node(old_text) if step is None: return diffs step_diff = self._refactor_step_text(step, old_text, new_text) diffs.append(step_diff) params_list_node = func.children[2] moved_params = self._move_param_nodes( params_list_node.children, move_param_from_idx) if params_list_node.children is not moved_params: params_span = self._span_from_pos( params_list_node.children[0].end_pos, params_list_node.children[-1].start_pos) params_list_node.children = moved_params param_code = ''.join(p.get_code() for p in moved_params[1:-1]) diffs.append((params_span, param_code)) return diffs"
1113,"def Generate_meta_analysis_images_for_a_set_of_features(self, dataset, features, threshold, threshold, threshold, threshold, threshold, threshold, threshold, threshold, threshold, threshold, threshold, threshold, threshold, threshold, threshold, threshold, threshold): if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if None: if","def analyze_features(dataset, features=None, image_type='association-test_z', threshold=0.001, q=0.01, output_dir=None, prefix=None): if features is None: features = dataset.get_feature_names() if output_dir is None: result = np.zeros((dataset.masker.n_vox_in_mask, len(features))) for i, f in enumerate(features): ids = dataset.get_studies(features=f, frequency_threshold=threshold) ma = MetaAnalysis(dataset, ids, q=q) if output_dir is None: result[:, i] = ma.images[image_type] else: pfx = f if prefix is None else prefix + '_' + f ma.save_results(output_dir=output_dir, prefix=pfx) if output_dir is None: return result"
1114,def quaternion(axis): axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis = axis,"def quaternion_from_axis_rotation(angle, axis): out = np.zeros(4, dtype=float) if axis == 'x': out[1] = 1 elif axis == 'y': out[2] = 1 elif axis == 'z': out[3] = 1 else: raise ValueError('Invalid axis input.') out *= math.sin(angle/2.0) out[0] = math.cos(angle/2.0) return Quaternion(out)"
1115,"def last_build_info('last_build_info', 'last_build_info', 'last_build_info', 'last_build_info', 'last_build_info', 'last_build_info', 'last_build_info', 'last_build_info', 'last_build_info', 'last_build_info', 'last_build_info', 'last_build_info', 'last_build_info', 'last_build_info', 'last_build_info', 'last_build_info', 'last_build_info', 'last_build_info', 'last_build_info', 'last_build_info', 'last_build_info', 'last_build_info', 'last_build_info', 'last_build_info', 'last_build_info', 'last_build_info', 'last_build_info', 'last_build_info', 'last_build_info', 'last_build_info', 'last_build_info', 'last_build_info', 'last_","def wait_for_build(self, interval=5, path=None): path = path or '' start = time.time() next_log = 0 while True: response = self.get(path)['last_build_info'] if not response: raise ValueError('This project is not building!') if response['stop_time']: if response['success']: return response else: raise LuminosoError(response) elapsed = time.time() - start if elapsed > next_log: logger.info('Still waiting (%d seconds elapsed).', next_log) next_log += 120 time.sleep(interval)"
1116,"def parse_command_line_args(self, command line): if command line = self.params(command line) if command line = self.params(command line)","def nt_quote_arg(arg): result = [] needquote = False nb = 0 needquote = ("" "" in arg) or (""t"" in arg) if needquote: result.append('""') for c in arg: if c == '': nb += 1 elif c == '""': result.append('' * (nb*2) + '""') nb = 0 else: if nb: result.append('' * nb) nb = 0 result.append(c) if nb: result.append('' * nb) if needquote: result.append('' * nb) result.append('""') return ''.join(result)"
1117,"def pipeline_str(self, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, dict, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_str, pipeline_st","def add_unique_identifiers(pipeline_str): for index, val in enumerate(process_names): if ""="" in val: parts = val.split(""="") new_id = ""_="".format(parts[0], index, parts[1]) else: new_id = ""_"".format(val, index) new_process_names.append(new_id) identifiers_to_tags[new_id] = val match_result = lambda match: ""  "".format(match.group()) find = r'[]+'.format(FORK_TOKEN, LANE_TOKEN, CLOSE_TOKEN) pipeline_str_modified = re.sub(find, match_result, pipeline_str_modified) for index, val in enumerate(process_names): find = r'[_]'.format(val).replace("""", """") pipeline_str_modified = re.sub(find, new_process_names[index] + "" "", pipeline_str_modified, 1) return pipeline_str_modified, identifiers_to_tags"
1118,"def get_cloud_sql_proxyrunner(self, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype): rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype, rtype,","def get_sqlproxy_runner(self): if not self.use_proxy: raise AirflowException(""Proxy runner can only be retrieved in case of use_proxy = True"") return CloudSqlProxyRunner( path_prefix=self.sql_proxy_unique_path, instance_specification=self._get_sqlproxy_instance_specification(), project_id=self.project_id, sql_proxy_version=self.sql_proxy_version, sql_proxy_binary_path=self.sql_proxy_binary_path )"
1119,"def decode(self, np.ndarray, n_states, n_steps, n_steps, n_steps, n_steps, n_steps, n_steps, n_steps, n_steps, n_steps, n_steps, n_steps, n_steps, n_steps, n_steps, n_steps, np.ndarray, p_state, p_state, p_state, p_state, p_state, p_state, p_state, p_state, p_state, p_state, p_state, p_state, p_state, p_state, p_state, p_state, p_state, p_state, p_state, p_state, p_state, p_state, p_state, p_state, p_state, p_state, p_state, p_state, p_state, p_state, p_state, p_state, p_state, p_state, p_state, p_state,","def viterbi_binary(prob, transition, p_state=None, p_init=None, return_logp=False): prob = np.atleast_2d(prob) n_states, n_steps = prob.shape if transition.shape == (2, 2): transition = np.tile(transition, (n_states, 1, 1)) elif transition.shape!= (n_states, 2, 2): raise ParameterError('transition.shape=, must be (2,2) or''(n_states, 2, 2)='.format(transition.shape, (n_states))) if np.any(transition  0) or not np.allclose(transition.sum(axis=-1), 1): raise ParameterError('Invalid transition matrix: must be non-negative''and sum to 1 on each row.') if np.any(prob  0) or np.any(prob > 1): raise ParameterError('Invalid probability values: prob must be between [0, 1]') if p_state is None: p_state = np.empty(n_states) p_state.fill(0.5) else: p_state = np.atleast_1d(p_state) if p_state.shape!= (n_states,) or np.any(p_state  0) or np.any(p_state > 1): raise ParameterError('Invalid marginal state distributions: p_state='.format(p_state)) if p_init is None: p_init = np.empty(n_states) p_init.fill(0.5) else: p_init = np.atleast_1d(p_init) if p_init.shap"
1120,"def calculate_md5_hash(self, md5_hash, md5_hash, md5_hash, md5_hash, md5_hash, md5_hash, md5_hash, md5_hash, md5_hash, md5_hash, md5_hash, md5_hash, md5_hash, md5_hash) md5_hash, md5_hash, md5_hash, md5_hash, md5_hash, md5_hash, md5_hash, md5_hash, md5_hash, md5_hash, md5_hash, md5_hash, md5_hash, md5_hash, md5_hash, md5_hash, md5_hash, md5_hash, md5_hash, md5_hash, md5_hash, md5_has",def read_md5(self): f = self.open('rb') try: m = md5() while True: d = f.read(8192) if not d: break m.update(d) finally: f.close() return m.digest()
1121,"def _reset_token(self, token): if self.get_token(token): self.get_token(token) self.get_token(token) self.get_token(token) self.get_token(token) self.get_token(token) self.get_token(token) self.get_token(token) self.get_token(token) self.get_token(token) self.get_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_",def calculate_time_to_reset(self): time_to_reset = self.rate_limit_reset_ts - (datetime_utcnow().replace(microsecond=0).timestamp() + 1) if time_to_reset  0: time_to_reset = 0 return time_to_reset
1122,"def cancel_edit_task(self, ctrl-G, ctrl-G, ctrl-G, ctrl-G, ctrl-G, ctrl-G, ctrl-G, ctrl-G, ctrl-G, ctrl-G, ctrl-G, ctrl-G, ctrl-G): ctrl-G, ctrl-G, ctrl-G, ctrl-G, ctrl-G, ctrl-G, ctrl-G)",def _keyboard_quit(self): if self._temp_buffer_filled : self._cancel_completion() self._clear_temporary_buffer() else: self.input_buffer = ''
1123,"def create_inner_loop_variables(self, inner_loop_variables): self.create_inner_loop_variables(self.create_inner_loop_variables) self.create_inner_loop_variables(self.create_inner_loop_variables) self.create_inner_loop_variables(self.create_inner_loop_variables) self.create_inner_loop_variables(self.create_inner_loop_variables) self.create_inner_loop_variables) self.create_inner_loop_variables(self.create_inner_loop_variables) self.create_inner_loop_variables) self.create_inner_loop_variables_inner_loop_variables_inner_loop_variables_inner_loop_variables_inner_loop_variables_inner_loop_variables_inner_loop_variables_inner_loop_variables_inner_loop_variables_inner_loop_variables_in","def _build_loop_vars(self): from theano.tensor.var import TensorVariable from deepy.core.neural_var import NeuralVariable if not self._loop_vars: self._ordered_out_keys = self._outputs.keys() seq_keys = self._sequences.keys() filled_out_keys = [k for k in self._ordered_out_keys if self._outputs[k]] nonseq_keys = self._non_sequences.keys() dummy_tensors, self._scan_local_vars = get_dummy_args( sequences=[self._sequences[k].tensor for k in seq_keys], outputs_info=[self._outputs[k].tensor for k in self._ordered_out_keys], non_sequences=[self._non_sequences[k].tensor for k in nonseq_keys], **self._kwargs ) dummy_map = dict(zip(seq_keys + filled_out_keys + nonseq_keys, dummy_tensors)) arg_map = self._sequences.copy() arg_map.update(self._outputs) arg_map.update(self._non_sequences) self._loop_vars = LoopVars() for k, dummy_tensor in dummy_map.items(): dummy_var = NeuralVariable(dummy_tensor, dim=arg_map[k].dim()) self._loop_vars[k] = dummy_var"
1124,"def change_default_configure(self, default_configure, default_configure, default_configure, default_configure, default_configure, default_configure, default_configure, default_configure, default_configure, default_configure, default_configure, default_configure, default_configure, default_configure, default_configure, default_configure, default_configure)","def change_default(config): config_file, cf = read_latoolscfg() if config not in cf.sections(): raise ValueError(""n':s' is not a defined configuration."".format(config)) if config == 'REPRODUCE': pstr = ('Are you SURE you want to set REPRODUCE as your default configuration?n' + '... this is an odd thing to be doing.') else: pstr = ('Are you sure you want to change the default configuration from :s'.format(cf['DEFAULT']['config']) + 'to :s?'.format(config)) response = input(pstr + 'n> [N/y]: ') if response.lower() == 'y': cf.set('DEFAULT', 'config', config) with open(config_file, 'w') as f: cf.write(f) print(' Default changed!') else: print(' Done nothing.')"
1125,"def initialize_live_report(self, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_","def _init_live_reports(self, report_id): logger.debug(""Sending initial POST request to  to start report live"" "" update"".format(self.broadcast_address)) try: with open("".metadata.json"") as fh: metadata = [json.load(fh)] except: metadata = [] start_json =  ""data"": ""results"": metadata  try: requests.post( self.broadcast_address, json=""run_id"": report_id, ""report_json"": start_json, ""status"": self.status_info ) except requests.exceptions.ConnectionError: logger.error(colored_print( ""ERROR: Could not establish connection with server. The server"" "" may be down or there is a problem with your internet "" ""connection."", ""red_bold"")) sys.exit(1)"
1126,"def build_folds(n_folds, seed, n_folds, n_folds, n_folds, seed, n_folds, n_folds, seed, n_folds, n_folds, seed, n_folds, n_folds, n_folds, seed, n_folds, n_folds, n_folds, seed, n_folds, n_folds, n_folds, n_folds, n_folds, n_folds, n_folds, n_folds, n_folds, n_folds, n_folds, n_folds, n_folds, n_folds, n_folds, n_folds, n_folds, n_folds, n_folds, n_folds, n_folds, n_folds, n_folds, n_folds, n_folds, n_folds, n_folds, n_folds, n_folds, n_folds, n_folds, n_folds, n_folds","def stratified_kfold_column(self, n_folds=3, seed=-1): return H2OFrame._expr( expr=ExprNode(""stratified_kfold_column"", self, n_folds, seed))._frame()"
1127,"def add_error_report(self, error_report, error_report, error_report, error_report, error_report, error_report, error_report, error_report, error_report, error_report)","def formatError(self, test, err): test.capturedOutput = output = self.buffer self._buf = None if not output: return err ec, ev, tb = err return (ec, self.addCaptureToErr(ev, output), tb)"
1128,"def write_stokes_calibrated_filterbank_file(self, onefile=False): if self.stokes_calibrated_filterbank_file(self.stokes_calibrated_filterbank_file): self.stokes_calibrated_filterbank_file(self.stokes_calibrated_filterbank_file) self.stokes_calibrated_filterbank_file(self.stokes_calibrated_filterbank_file(self.stokes_calibrated_filterbank_file) self.stokes_calibrated_filterbank_file) self.stokes_calibrated_filterbank_file(self.stokes_calibrated_filterbank_file) self.stokes_calibrated_filterbank_file) self.stokes_calibrated_filterbank_file(self.stokes_calibrated_filterbank_file) self.stokes_calibrated_filterbank_file) self.stokes_calibrated_filterbank_file(self.stokes_calibrated_filterbank_file) self.stokes_calibrated_filterbank_file(self.stokes_calibrated_filterbank_file","def calibrate_pols(cross_pols,diode_cross,obsI=None,onefile=True,feedtype='l',**kwargs): #Obtain time sample length, frequencies, and noise diode data obs = Waterfall(diode_cross,max_load=150) cross_dat = obs.data tsamp = obs.header['tsamp'] #Calculate number of coarse channels in the noise diode measurement (usually 8) dio_ncoarse = obs.calc_n_coarse_chan() dio_nchans = obs.header['nchans'] dio_chan_per_coarse = dio_nchans/dio_ncoarse obs = None Idat,Qdat,Udat,Vdat = get_stokes(cross_dat,feedtype) cross_dat = None #Calculate differential gain and phase from noise diode measurements print('Calculating Mueller Matrix variables') gams = gain_offsets(Idat,Qdat,Udat,Vdat,tsamp,dio_chan_per_coarse,feedtype,**kwargs) psis = phase_offsets(Idat,Qdat,Udat,Vdat,tsamp,dio_chan_per_coarse,feedtype,**kwargs) #Clear data arrays to save memory Idat = None Qdat = None Udat = None Vdat = None #Get corrected Stokes parameters print('Opening '+cross_pols) cross_obs = Waterfall(cross_pols,max_load=150) obs_ncoarse = cross_obs.calc_n_coarse_chan() obs_nchans = cross_obs.header['nchans'] obs_chan_per_coarse = obs_nchans/obs_ncoarse print('Grabbing Stokes parameters"
1129,"def has_printout(self, index, not_printed_msg): if not_printed_msg: if not_printed_msg: if not_printed_msg: if not_printed_msg: if not_printed_msg: if not_printed_msg: if not_printed_msg: if not_printed_msg: if not_printed_msg: if not_printed_msg: if not_printed_msg: if not_printed_msg: if not_printed_msg: if not_printed_msg: if not_printed_msg: if not_printed_msg: if not_printed_msg: if not_printed_msg: if not_printed_msg: if not_printed_msg: if not_print_msg: if not_print_msg: if not_print_msg: if not_print_msg: if not_print_msg: if not_print_msg: if not_print_msg: if not_print_msg","def has_printout( state, index, not_printed_msg=None, pre_code=None, name=None, copy=False ): extra_msg = ""If you want to check printouts done in e.g. a for loop, you have to use a check_function('print') chain instead."" state.assert_root(""has_printout"", extra_msg=extra_msg) if not_printed_msg is None: not_printed_msg = ( ""Have you used sol_call to do the appropriate printouts?"" ) try: sol_call_ast = state.ast_dispatcher(""function_calls"", state.solution_ast)[ ""print"" ][index][""node""] except (KeyError, IndexError): raise InstructorError( ""has_printout() couldn't find the  print call in your solution."".format( index, utils.get_ord(index + 1) ) ) out_sol, str_sol = getOutputInProcess( tree=sol_call_ast, process=state.solution_process, context=state.solution_context, env=state.solution_env, pre_code=pre_code, copy=copy, ) sol_call_str = state.solution_ast_tokens.get_text(sol_call_ast) if isinstance(str_sol, Exception): raise InstructorError( ""Evaluating the solution expression  raised error in solution process."" ""Error:  - "".format(sol_call_str, type(out_sol), str_sol) ) _msg = state.build_message(not_printed_msg, ""sol_call"": sol_call_str) has_output(state, out_sol.strip(), pattern=False, no_output_msg=_"
1130,"def append_to_response(self, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_response, append_to_respons","def releases(self, **kwargs): path = self._get_id_path('releases') response = self._GET(path, kwargs) self._set_attrs_to_values(response) return response"
1131,"def append_multiple_frames(self, int_axis): if self.append_multiple_frames(self.append_multiple_frames): self.append_multiple_frames(self.append_multiple_frames) self.append_multiple_frames(self.append_multiple_frames) self.append_multiple_frames(self.append_multiple_frames) self.append_multiple_frames(self.append_multiple_frames) self.append_multiple_frames(self.append_frames) self.append_multiple_frames) self.append_multiple_frames(self.append_frames) self.append_frames(self.append_frames) self.append_frames(self.append_frames) self.append_frames(self.append_frames) self.append_frames(self.append_frames) self.append_frames(self.append_frames) self.append_frames(self.append_frames) self.append_frames(self.append_frames) self.append_","def concat(self, frames, axis=1): if len(frames) == 0: raise ValueError(""Input list of frames is empty! Nothing to concat."") if axis == 1: df = self.cbind(frames) else: df = self.rbind(frames) return df"
1132,"def start_new_qtconsole(self, kernel, kernel): qtconsole = self.qtconsole = self.qtconsole = self.qtconsole = self.qtconsole = self.qtconsole = self.qtconsole = self.qtconsole","def new_qt_console(self, evt=None): return connect_qtconsole(self.ipkernel.connection_file, profile=self.ipkernel.profile)"
1133,def switch_py_funceble_configuration_variable_name(variable_name): if py_funceble.CONFIGURATION(variable_name): if py_funceble.CONFIGURATION(variable_name): if py_funceble.CONFIGURATION(variable_name): if py_funceble.CONFIGURATION(variable_name): py_funceble_funceble_funceble_funceble_funceble_funceble_funceble_funceble_funceble_funceble_funceble_funceble_funceble_funceble_funceble_funceble_funceble_funceble_funceble_funceble_funceble_funceble_funceble_funceble_funceble_funceble_funceble_funceble_funceble_funceble_funceble_funceble_funceble_funceble_funceble_fun,"def switch( cls, variable, custom=False ): if not custom: current_state = dict.get(PyFunceble.CONFIGURATION, variable) else: current_state = variable if isinstance(current_state, bool): if current_state: return False return True to_print = ""Impossible to switch %s. Please post an issue to %s"" raise Exception( to_print % (repr(variable), PyFunceble.LINKS[""repo""] + ""/issues."") )"
1134,"def use_classmethod(self, classmethod, staticmethod): self.get_classmethod(classmethod, staticmethod) self.get_classmethod(classmethod) self.get_classmethod(classmethod) self.get_classmethod(classmethod) self.get_classmethod(classmethod) self.get_classmethod(classmethod) self.get_classmethod(classmethod) self.get_classmethod(classmethod) self.get_classmethod(classmethod) self.get_classmethod(classmethod) self.get_classmethod) self.get_classmethod(classmethod) self.get_classmethod(classmethod) self.get_classmethod(classmethod) self.get_classmethod(classmethod) self.get_classmethod) self.get_classmethod(classmethod) self.get_classmethod(classmethod) self.get_classmethod(classmethod) self.get_classmethod(classmethod) self.get_classmethod(classmethod) self.get_classmethod(classmethod) self.get_classmethod(classmethod) self.get_classmethod(classmethod) self.get_classmethod(classmethod) self.get_classmethod(classmethod) self.get_classmethod(classmethod) self.get_classmethod(classmethod) self.get_classmethod","def _check_classmethod_declaration(self, node): if not isinstance(node.value, astroid.Call): return func = node.value.func if not isinstance(func, astroid.Name) or func.name not in ( ""classmethod"", ""staticmethod"", ): return msg = ( ""no-classmethod-decorator"" if func.name == ""classmethod"" else ""no-staticmethod-decorator"" ) parent_class = node.scope() if not isinstance(parent_class, astroid.ClassDef): return classmeth_arg = node.value.args[0] if not isinstance(classmeth_arg, astroid.Name): return method_name = classmeth_arg.name if any(method_name == member.name for member in parent_class.mymethods()): self.add_message(msg, node=node.targets[0])"
1135,"def _reimplemented_for_IPython_style_prompts(self, prompts): self.reimplemented_for_IPython_style_prompts(self.reimplemented_for_IPython_style_prompts) self.reimplemented_for_IPython_style_prompts(self.reimplemented_for_IPython_style_prompts) self.reimplemented_for_IPython_style_prompts(self.reimplemented_for_IPython_style_prompts) self.reimplemented_for_IPython_style_prompts) self.reimplemented_for_IPython_style_prompts) self.reimplemented_for_IPython_style_prompts","def _show_interpreter_prompt_for_reply(self, msg): content = msg['content'] if content['status'] == 'aborted': if self._previous_prompt_obj: previous_prompt_number = self._previous_prompt_obj.number else: previous_prompt_number = 0 else: previous_prompt_number = content['execution_count'] if self._previous_prompt_obj and  self._previous_prompt_obj.number!= previous_prompt_number: block = self._previous_prompt_obj.block if block.isValid() and block.text(): cursor = QtGui.QTextCursor(block) cursor.movePosition(QtGui.QTextCursor.Right, QtGui.QTextCursor.KeepAnchor, self._previous_prompt_obj.length) prompt = self._make_in_prompt(previous_prompt_number) self._prompt = self._insert_html_fetching_plain_text( cursor, prompt) self._highlighter.rehighlightBlock(cursor.block()) self._previous_prompt_obj = None self._show_interpreter_prompt(previous_prompt_number + 1)"
1136,"def mode(self, mode = None): mode = self.____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________","def _get_mode(mode = None): from Crypto.Cipher import AES AESModeMap =  'CCM': AES.MODE_CCM, 'EAX': AES.MODE_EAX, 'GCM': AES.MODE_GCM, 'OCB': AES.MODE_OCB,  if mode is None: return AESModeMap.keys() return AESModeMap.get(mode)"
1137,if self.widget.pressed: self.vlist_cross.colors = 6*bg else: self.vlist_cross.colors = 6*bg else: self.vlist_cross.colors = 6*bg else: self.vlist_cross.colors = self.vlist_cross.colors = self.vlist_cross.colors = self.vlist_cross.colors = self.vlist_cross.colors = self.vlist_cross.colors,"def redraw_bg(self): sx,sy = self.widget.size x,y = self.widget.pos bx,by = self.border v1 = x, y+sy v2 = x+sx, y+sy v3 = x, y v4 = x+sx, y v5 = x+bx, y+sy-by v6 = x+sx-bx, y+sy-by v7 = x+bx, y+by v8 = x+sx-bx, y+by qb1 = v5+v6+v2+v1 qb2 = v8+v4+v2+v6 qb3 = v3+v4+v8+v7 qb4 = v3+v7+v5+v1 qc = v7+v8+v6+v5 v = qb1+qb2+qb3+qb4+qc self.vlist.vertices = v bg = self.submenu.bg[:3] if isinstance(self.submenu.bg,list) or isinstance(self.submenu.bg,tuple) else [242,241,240] o,i = bg, [min(bg[0]+8,255),min(bg[1]+8,255),min(bg[2]+8,255)] s,h = [max(bg[0]-40,0),max(bg[1]-40,0),max(bg[2]-40,0)], [min(bg[0]+12,255),min(bg[1]+12,255),min(bg[2]+12,255)] if self.borderstyle == ""flat"": if self.widget.pressed: i = s cb1 = i+i+i+i cb2 = i+i+i+i cb"
1138,"def _get_form(self, filename, metadata): if self.get_form(filename, metadata): self.get_form(filename, metadata) self.get_form(filename) self.get_form(filename) self.get_form(filename) self.get_form(filename) self.get_form(filename) self.get_form(filename) self.get_form(filename) self.get_form(filename) self.get_form(filename) self.get_form(filename) self.get_form(filename) self.get_form(filename) self.get_form(filename) self.get_form(filename) self.get_form(filename) self.get_form(filename) self.get_form(filename) self.get_form(filename) self.get_form(filename) self.get_form(filename) self.get_form(filename) self.get_form(filename) self.get_form(filename) self.get_form(filename) self.get_form(filename) self.get_form(filename) self.get_form(filename) self.get_form(filename) self.get_form(","def _loc(form: Union[LispForm, ISeq]) -> Optional[Tuple[int, int]]: try: meta = form.meta line = meta.get(reader.READER_LINE_KW) col = meta.get(reader.READER_COL_KW) except AttributeError: return None else: assert isinstance(line, int) and isinstance(col, int) return line, col"
1139,"def CS_as_dict(self, proj4): if self.as_dict(self.as_dict): self.as_dict(self.as_dict): self.as_dict(self.as_dict): self.as_dict(self.as_dict)","def to_proj4(self, as_dict=False, toplevel=True): if toplevel: string = ""+proj=longlat %s %s +nodef"" % (self.datum.to_proj4(), self.prime_mer.to_proj4()) else: string = ""%s %s"" % (self.datum.to_proj4(), self.prime_mer.to_proj4()) if as_dict: return dict([ entry.lstrip('+').split('=') for entry in string.split() if entry!= ""+no_defs"" ]) else: return string"
1140,"def append_zero(self, value=0, value=0, value=0, value=0, value=0, value=0, value=0, value=0, value=0, value=0, value=0, value=0, value=0, value=0, value=0, value=0, value=0, value=0, value=0, value=0, value=0, value=0)","def _append_zeros_if_too_small(self, value): size_diff = len(value) - len(self.array) if size_diff: self.array = np.append( self.array, np.zeros(size_diff, dtype=self.array.dtype) )"
1141,"def Import_sql_table(self, connection_url, fetch_mode): self.import_sql_table(self.import_sql_table) self.import_sql_table(self.import_sql_table) self.import_sql_table(self.import_sql_table) self.import_sql_table(self.import_sql_table) self.import_sql_table(self.import_sql_table) self.import_sql_table) self.import_sql_table(self.import_sql_table) self.import_sql_table) self.import_sql_table(self.import_sql_table) self.import_sql_table) self.import_sql_table(self.import_sql_table) self.import_sql_table(self.import_sql_table) self.import_sql_table(self.import_sql_table) self.import_sql_table(self.import_sql_table) self.","def import_sql_select(connection_url, select_query, username, password, optimize=True, use_temp_table=None, temp_table_name=None, fetch_mode=None): assert_is_type(connection_url, str) assert_is_type(select_query, str) assert_is_type(username, str) assert_is_type(password, str) assert_is_type(optimize, bool) assert_is_type(use_temp_table, bool, None) assert_is_type(temp_table_name, str, None) assert_is_type(fetch_mode, str, None) p = ""connection_url"": connection_url, ""select_query"": select_query, ""username"": username, ""password"": password, ""use_temp_table"": use_temp_table, ""temp_table_name"": temp_table_name, ""fetch_mode"": fetch_mode j = H2OJob(api(""POST /99/ImportSQLTable"", data=p), ""Import SQL Table"").poll() return get_frame(j.dest_key)"
1142,def make_training_and_evaluation(self): if self._model(self._model): if self._model(self._model): if self._model(self._model),"def train_evaluate_model_from_config(config: Union[str, Path, dict], iterator: Union[DataLearningIterator, DataFittingIterator] = None, *, to_train: bool = True, evaluation_targets: Optional[Iterable[str]] = None, to_validate: Optional[bool] = None, download: bool = False, start_epoch_num: Optional[int] = None, recursive: bool = False) -> Dict[str, Dict[str, float]]: config = parse_config(config) if download: deep_download(config) if to_train and recursive: for subconfig in get_all_elems_from_json(config['chainer'], 'config_path'): log.info(f'Training ""subconfig""') train_evaluate_model_from_config(subconfig, download=False, recursive=True) import_packages(config.get('metadata', ).get('imports', [])) if iterator is None: try: data = read_data_by_config(config) except ConfigError as e: to_train = False log.warning(f'Skipping training. e.message') else: iterator = get_iterator_from_config(config, data) if 'train' not in config: log.warning('Train config is missing. Populating with default values') train_config = config.get('train') if start_epoch_num is not None: train_config['start_epoch_num'] = start_epoch_num if 'evaluation_targets' not in train_config and ('validate_best' in train_config or 'test_best' in train_config): log.warning('""validate_"
1143,"def fetch_messages(self, category, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs)","def fetch_items(self, category, **kwargs): from_date = kwargs['from_date'] logger.info(""Looking for messages from '%s' since %s"", self.url, str(from_date)) mailing_list = HyperKittyList(self.url, self.dirpath) mailing_list.fetch(from_date=from_date) messages = self._fetch_and_parse_messages(mailing_list, from_date) for message in messages: yield message logger.info(""Fetch process completed"")"
1144,"def run_api_command(self, res_id, res_id, params, res_id, res_id, res_id, res_id, params, res_id, res_id, params, res_id, res_id, res_id, res_id, res_id, res_id, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params","def _call(self, res, res_id, params): if res: url = urijoin(self.base_url, res, res_id) else: url = urijoin(self.base_url, res_id) url += self.TJSON logger.debug(""Discourse client calls resource: %s %s params: %s"", res, res_id, str(params)) r = self.fetch(url, payload=params) return r.text"
1145,"def convertEntities(text, convertEntities=BeautifulStoneSoup.XML_ENTITIES, convertEntities=BeautifulStoneSoup.XML_ENTITIES, convertEntities=BeautifulStoneSoup.XML_ENTITIES, convertEntities=BeautifulStoneSoup.XML_ENTITIES, convertEntities=BeautifulStoneSoup.XML_ENTITIES, convertEntities=BeautifulStoneSoup.XML_ENTITIES.XML_ENTITIES.XML_ENTITIES.XML_ENTITIES.XML_ENTITIES.XML_ENTITIES.XML_ENTITIES.XML_ENTITIES.XML_ENTITIES.XML_ENTITIES.XML_ENTITIES.XML_ENTITIES.XML_ENTITIES.XML_ENTITIES.XML_ENTITIES.XML_ENTITIES.XML_ENTITIES.XML_ENTITIES.XML_ENTITIES.XML_ENTITIES.XML_ENTITIES.XML_ENTITIES.XML_ENTITIES.XML_ENTITIES.XML_ENT","def html_entities_to_unicode(text, space_padding=False, safe_only=False): x = match.group(1) if safe_only and x not in ENTITIES_THAT_ARE_SAFE_TO_STRING_PAD: return u'&%s;' % x if x in name2codepoint: # return unichr(name2codepoint[x]) elif x in XML_ENTITIES_TO_SPECIAL_CHARS: return XML_ENTITIES_TO_SPECIAL_CHARS[x] elif len(x) > 0 and x[0] == '#': if len(x) > 1 and x[1] == 'x': return unichr(int(x[2:], 16)) else: return unichr(int(x[1:])) else: # return u'&%s;' % x def convert_to_padded_entitites(match): converted_string = convert_entities(match) num_spaces_needed = len(match.group(0)) - len(converted_string) assert num_spaces_needed >= 0,  'len(%r)!= len(%r)' % (converted_string, match.group(0)) # # num_left = int(num_spaces_needed / 2) num_right = num_spaces_needed - num_left return (''* num_left) + converted_string + (''* num_right) # if space_padding: return tags.sub( convert_to_padded_entitites, text) else: return tags.sub( convert_entities, text)"
1146,def tree_seq(self): if self.seq(self.seq): self.seq(self.seq): self.seq(self.seq): self.seq(self.seq): self.seq(self.seq): self.seq(self.seq),"def add_sequences(self, sequences, cla) -> bool: if not hasattr(sequences, 'parser_tree'): sequences.parser_tree = cla.parser_tree else: oldnode = sequences if isinstance(oldnode.parser_tree, parsing.Seq): oldpt = list(oldnode.parser_tree.ptlist) else: oldpt = [oldnode.parser_tree] oldpt.append(cla.parser_tree) sequences.parser_tree = parsing.Seq(*tuple(oldpt)) return True"
1147,"def Decorate(self, decorate): def decorate(decorate): decorate(decorate): decorate(decorate): decorate(decorate): decorate(decorate): decorate(decorate): decorate(decorate): decorate(decorate): decorate(decorate): decorate(decorate): decorate(decorate): decorate(decorate): decorate(decorate): decorate(decorate): decorate(decorate): decorate(decorate): decorate(decorate): decorate): decorate(decorate): decorate(decorate): decorate(decorate): decorate): decorate(decorate): decorate(decorate): decorate(decorate): decorate): decorate(decorate): decorate(decorate): decorate(decorate): decorate(decorate): decorate(decorate): decorate(decorate): decorate(decorate): decorate): decorate(decorate): decorate(decorate): decorate(decorate): decorate(decorate","def request(self, *args, **kwargs): middleware = args[0] @wraps(middleware) def register_middleware(*args, **kwargs): self.request_middleware.append(middleware) return middleware return register_middleware()"
1148,"def find_dashboard(self, name): if self.dashboard(name): self.dashboard(name): self.dashboard(name) self.dashboard(name) self.dashboard(name) self.dashboard(name) self.dashboard(name) self.dashboard(name) self.dashboard(name) self.dashboard(name) self.dashboard(name) self.dashboard(name) self.dashboard(name) self.dashboard(name) self.dashboard(name) self.dashboard(name) self.dashboard(name) self.dashboard(name) self.dashboard(name) self.dashboard(name) self.dashboard(name) self.dashboard(name) self.dashboard(name) self.dashboard(name) self.dashboard(name) self.dashboard(name) self.dashboard(name) self.dashboard(name) self.dashboard(name) self.dashboard(name) self.dashboard(name) self.dashboard(name) self.dashboard(name) self.dashboard(name) self.dashboard(name) self.","def find_dashboard_by(self, name=None): res = self.get_dashboards() if res[0] is False: return res else: def filter_fn(configuration): return configuration['name'] == name def create_item(configuration): return 'dashboard': configuration dashboards = list(map(create_item, list(filter(filter_fn, res[1]['dashboards'])))) return [True, dashboards]"
1149,"def progress(self, ar.progress, len(ar.progress)): progress = self.__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________",def progress(self): self.wait(0) return len(self) - len(set(self.msg_ids).intersection(self._client.outstanding))
1150,"def target_mentions(self, target_mentions, chain_mentions, target_mentions, target_mentions, target_mentions, target_mentions, chain_mentions, target_mentions, target_mentions, chain_mentions, target_mentions, target_mentions, chain_mentions, target_mentions, target_mentions, chain_mentions, target_mentions, chain_mentions, target_mentions, chain_mentions, target_mentions, chain_mentions, target_mentions, target_mentions, chain_mentions, target_mentions, chain_mentions, target_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions, chain_mentions,","def ANY_mentions(target_mentions, chain_mentions): for name in target_mentions: for chain_ment in chain_mentions: if name in chain_ment: return True return False"
1151,def ________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________,"def query(self, input, params=(), **kwargs): data = dict( input=input, appid=self.app_id, ) data = itertools.chain(params, data.items(), kwargs.items()) query = urllib.parse.urlencode(tuple(data)) url = 'https://api.wolframalpha.com/v2/query?' + query resp = urllib.request.urlopen(url) assert resp.headers.get_content_type() == 'text/xml' assert resp.headers.get_param('charset') == 'utf-8' return Result(resp)"
1152,def export_tsv/csv_tables(export_tsv/csv_tables): export_tsv/csv_tables(export_tsv_tables) export_tsv_tables(export_tsv_tables) export_tsv_tables(export_tsv_tables) export_tsv_tables(export_tsv_tables) export_tsv_tables(export_tsv_tables) export_tsv_tables) export_tsv_tables(export_tsv_tables) export_tsv_csv_tables(export_tsv_csv_tables) export_tsv_csv_csv_csv_csv_csv_csv_csv_csv_csv_csv_csv_csv_csv_csv_csv_csv_csv_csv_csv_csv_csv_csv_csv_csv_csv_csv_csv_c,"def export(infile, outfile, format, outcsv, transition_quantification, max_transition_pep, ipf, ipf_max_peptidoform_pep, max_rs_peakgroup_qvalue, peptide, max_global_peptide_qvalue, protein, max_global_protein_qvalue): if format == ""score_plots"": export_score_plots(infile) else: if outfile is None: if outcsv: outfile = infile.split("".osw"")[0] + "".csv"" else: outfile = infile.split("".osw"")[0] + "".tsv"" else: outfile = outfile export_tsv(infile, outfile, format, outcsv, transition_quantification, max_transition_pep, ipf, ipf_max_peptidoform_pep, max_rs_peakgroup_qvalue, peptide, max_global_peptide_qvalue, protein, max_global_protein_qvalue)"
1153,"def reshape(self, keys, values): self.reshape(keys, values) self.reshape(keys) self.reshape(keys) self.reshape(keys) self.reshape(keys) self.reshape(keys) self.reshape(keys) self.reshape(keys) self.reshape(keys) self.reshape(keys) self.reshape(keys) self.reshape(keys) self.reshape(keys) self.reshape(keys) self.reshape(keys) self.reshape(keys) self.reshape(keys) self.reshape(keys) self.reshape(keys) self.reshape(keys) self.reshape(keys) self.reshape(keys) self.reshape(keys) self.reshape(keys) self.reshape(keys) self.reshape(keys) self.reshape(keys) self.reshape(keys) self.","def _reshapebasic(self, shape): new = tupleize(shape) old_key_size = prod(self.keys.shape) old_value_size = prod(self.values.shape) for i in range(len(new)): new_key_size = prod(new[:i]) new_value_size = prod(new[i:]) if new_key_size == old_key_size and new_value_size == old_value_size: return i return -1"
1154,"def create_interrupt(self, interrupt_handle): self.create(self.create_interrupt) self.create_interrupt(self.create_interrupt) self.create_interrupt(self.create_interrupt) self.create_interrupt(self.create_interrupt) self.create_interrupt(self.create_interrupt) self.create_interrupt(self.create_interrupt) self.create_interrupt(self.create_interrupt) self.create_interrupt(self.create_interrupt) self.create_interrupt(self.create_interrupt) self.create_interrupt(self.create_interrupt) self.create_interrupt(self.create_interrupt) self.create_interrupt) self.create_interrupt(self.create_interrupt) self.create_interrupt) self.create_interrupt(self.create_interrupt) self.create_interrupt(self.create_interrupt) self.create_interrupt(self.create_interrupt) self.create_interrupt(self.create_interrupt) self.create_interrupt(self.create_interrupt) self.create_interrupt(self.create_interrupt) self.create_interrupt(self.create_interrupt) self.create_interrupt(","def create_interrupt_event(): class SECURITY_ATTRIBUTES(ctypes.Structure): _fields_ = [ (""nLength"", ctypes.c_int), (""lpSecurityDescriptor"", ctypes.c_void_p), (""bInheritHandle"", ctypes.c_int) ] sa = SECURITY_ATTRIBUTES() sa_p = ctypes.pointer(sa) sa.nLength = ctypes.sizeof(SECURITY_ATTRIBUTES) sa.lpSecurityDescriptor = 0 sa.bInheritHandle = 1 return ctypes.windll.kernel32.CreateEventA( sa_p, False, False, '')"
1155,"def vpn_sites(self, resource_group_name, virtual_wan_name, vpn_sites, raw=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling","def download( self, resource_group_name, virtual_wan_name, vpn_sites=None, output_blob_sas_url=None, custom_headers=None, raw=False, polling=True, **operation_config): raw_result = self._download_initial( resource_group_name=resource_group_name, virtual_wan_name=virtual_wan_name, vpn_sites=vpn_sites, output_blob_sas_url=output_blob_sas_url, custom_headers=custom_headers, raw=True, **operation_config ) def get_long_running_output(response): if raw: client_raw_response = ClientRawResponse(None, response) return client_raw_response lro_delay = operation_config.get( 'long_running_operation_timeout', self.config.long_running_operation_timeout) if polling is True: polling_method = ARMPolling(lro_delay, **operation_config) elif polling is False: polling_method = NoPolling() else: polling_method = polling return LROPoller(self._client, raw_result, get_long_running_output, polling_method)"
1156,def parser(argv): parser = parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(argv) parser(arg,"def create_argument_parser(self): parser = ArgumentParser( description=self.__doc__, epilog='PyOTA vversion'.format(version=__version__), ) parser.add_argument( '--uri', type=text_type, default='http://localhost:14265/', help=( 'URI of the node to connect to''(defaults to http://localhost:14265/).' ), ) if self.requires_seed: parser.add_argument( '--seed-file', type=text_type, dest='seed_file', help=( 'Path to a file containing your seed in cleartext.''If not provided, you will be prompted to enter''your seed via stdin.' ), ) parser.add_argument( '--testnet', action='store_true', default=False, help='If set, use testnet settings (e.g., for PoW).', ) return parser"
1157,"def observ_node_pair(self, profile_pair, multiplicity): if observ_node_pair(self, profile_pair, multiplicity): if observ_node_pair(self, multiplicity): if observ_node_pair(self, profile_pair, multiplicity): if observ_node_pair(self, multiplicity): if observ_node_pair(self, multiplicity): if observ_node_pair(self, multiplicity): if observ_node_pair(self, multiplicity): if observ_node_pair(self, multiplicity): if observ_node_pair(self, multiplicity) if observ_node_pair(self, multiplicity) if observ_node_node_node_node_node_node_node_node_node_node_node_node_node_node_node_node_node_node_node_node_node","def prob_t_profiles(self, profile_pair, multiplicity, t, return_log=False, ignore_gaps=True): if t0: logP = -ttconf.BIG_NUMBER else: Qt = self.expQt(t) if len(Qt.shape)==3: res = np.einsum('ai,ija,aj->a', profile_pair[1], Qt, profile_pair[0]) else: res = np.einsum('ai,ij,aj->a', profile_pair[1], Qt, profile_pair[0]) if ignore_gaps and (self.gap_index is not None): non_gap_frac = (1-profile_pair[0][:,self.gap_index])*(1-profile_pair[1][:,self.gap_index]) logP = np.sum(multiplicity*np.log(res)*non_gap_frac) else: logP = np.sum(multiplicity*np.log(res)) return logP if return_log else np.exp(logP)"
1158,def np.ndarray(meas_level=1): if np.ndarray(meas_level=1): if np.ndarray(meas_level=1): if np.ndarray(meas_level=1): if np.ndarray(meas_level=1): if np.ndarray(meas_level=1),def format_level_0_memory(memory): formatted_memory = _list_to_complex_array(memory) if not 2 = len(formatted_memory.shape) = 3: raise QiskitError('Level zero memory is not of correct shape.') return formatted_memory
1159,"def read_latex_expression(self, pos, len): self.get_latex_expression(self.get_latex_expression(self.get_latex_expression(self.get_latex_expression(self.get_latex_expression(self.get_latex_expression))))","def get_latex_expression(s, pos, **parse_flags): return LatexWalker(s, **parse_flags).get_latex_expression(pos=pos)"
1160,"def self.parsed_data->self.config(self.parsed_data, self.parsed_data, self.parsed_data, self.parsed_data, self.parsed_data, self.config, self.config, self.config, self.config, self.config, self.config, self.config, self.config, self.config, self.config, self.config, self.config, self.config, self.config, self.config, self.config, self.config, self.config, self.config, self.config, self.config, self.config, self.config)","def _convert_to_config(self): for k, v in vars(self.parsed_data).iteritems(): exec ""self.config.%s = v""%k in locals(), globals()"
1161,"def make_service(self, opt, config,'messages', 'freq', 'config', 'freq', 'config','messages', 'freq', 'config', 'freq', 'config', 'freq', 'config','messages', 'freq', 'config', 'freq', 'freq', 'config','messages', 'freq','messages', 'freq', 'freq', 'freq', 'freq', 'freq', 'freq', 'freq', 'freq', 'freq', 'freq', 'freq', 'freq', 'freq', 'freq', 'freq', 'freq', 'freq', 'freq', 'freq', 'freq', 'freq', 'freq'","def makeService(opt): restarter, path = beatcheck.parseConfig(opt) pool = client.HTTPConnectionPool(reactor) agent = client.Agent(reactor=reactor, pool=pool) settings = Settings(reactor=reactor, agent=agent) states =  checker = functools.partial(check, settings, states, path) httpcheck = tainternet.TimerService(opt['freq'], run, restarter, checker) httpcheck.setName('httpcheck') return heart.wrapHeart(httpcheck)"
1162,"def validate_block_size(self, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size) block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block_size, block","def _validate_block_sizes(block_sizes, bijectors, validate_args): block_sizes_shape = block_sizes.shape if tensorshape_util.is_fully_defined(block_sizes_shape): if (tensorshape_util.rank(block_sizes_shape)!= 1 or (tensorshape_util.num_elements(block_sizes_shape)!= len(bijectors))): raise ValueError( 'block_sizes must be None, or a vector of the same length as''bijectors. Got a Tensor with shape  and bijectors of''length '.format(block_sizes_shape, len(bijectors))) return block_sizes elif validate_args: message = ('block_sizes must be None, or a vector of the same length''as bijectors.') with tf.control_dependencies([ assert_util.assert_equal( tf.size(input=block_sizes), len(bijectors), message=message), assert_util.assert_equal(tf.rank(block_sizes), 1) ]): return tf.identity(block_sizes) else: return block_sizes"
1163,"def remove_all_rows(self, rows, columns): self.remove_all_rows() self.remove_all_rows() self.remove_all_rows() self.remove_all_rows() self.remove_all_rows() self.remove_all_rows() self.remove_all_rows() self.remove_all_rows() self.remove_all_rows()","def clear(self): ""Remove all rows and reset internal structures"" # for i in range(len(self)-1, -1, -1): del self[i] self._key = 0 if hasattr(self._grid_view, ""wx_obj""): self._grid_view.wx_obj.ClearGrid()"
1164,"def convert_quantumcircuit(self, circuit, scale, filename, reverse_bits, dict, str, etc.): if self.get_quantumcircuit( circuit, scale, filename, etc.): self.get_quantumcircuit( circuit, scale, etc.): self.get_quantumcircuit( circuit, scale, etc.): self.get_quantumcircuit( circuit, scale, etc.)","def _generate_latex_source(circuit, filename=None, scale=0.7, style=None, reverse_bits=False, plot_barriers=True, justify=None): qregs, cregs, ops = utils._get_layered_instructions(circuit, reverse_bits=reverse_bits, justify=justify) qcimg = _latex.QCircuitImage(qregs, cregs, ops, scale, style=style, plot_barriers=plot_barriers, reverse_bits=reverse_bits) latex = qcimg.latex() if filename: with open(filename, 'w') as latex_file: latex_file.write(latex) return latex"
1165,"def POST_REQUEST(self, path): return self.pst(path) self.pst(path) self.pst(path) self.pst(path) self.pst(path) self.pst(path) self.pst(path) self.pst(path) self.pst(path) self.pst(path) self.pst(path) self.pst(path) self.pst(path) self.pst(path) self.pst(path) self.pst(path) self.pst(path) self.pst(path) self.pst(path) self.pst(path) self.pst(path) self.pst(path) self.pst(path) self.pst(path) self.pst(path) self.pst(path) self.pst(path) self.pst(path) self.pst(path) self.pst(path) self.pst(path) self.pst(path) self.pst(path) self.pst(path) self.pst(path) self.ps","def post(self, path='', **params): params = jsonify_parameters(params) url = ensure_trailing_slash(self.url + path.lstrip('/')) return self._json_request('post', url, data=params)"
1166,"def deprecate(self): if self.deprecate(self.deprecate(self.deprecate(self.deprecate(self.deprecate(self.deprecate(self.deprecate(self.deprecate(self.deprecate(self.deprecate(self.deprecate(self.deprecate)), self.deprecate(self.deprecate(self.deprecate(self.deprecate(self.deprecate(self.deprecate(self.deprecate(self.deprecate(self.deprecate(self.deprecate)))))))))))","def deprecated_func(func): first_usage = [True] @functools.wraps(func) def wrapper(*args, **kwargs): if first_usage[0]: warnings.warn( ""Call to deprecated function."".format(func.__name__), DeprecationWarning, ) first_usage[0] = False return func(*args, **kwargs) return wrapper"
1167,"def convert_user_type_to_RECORD(user_type, n): if n is None: if n is None: if n is None: if n is None: if n is None: if n is None: if n is None: if n is None: if n is None: if n is None: if n is None: if n is None: if n is None: n is None: n is None: n is None: n is None: n is None: n is None: n is None: n is None: n is None: n is None: n is None: n is None: n is None: n is None: n is None: n is None: n is None: n is None: n is None: n is None: n is None: n is None: n is None: n is None: n is None: n is None: n is None: n is None: n is None: n is None: n is None: n is None: n is None: n is None: n is None: n is None: n is None: n","def convert_user_type(cls, name, value): names = value._fields values = [cls.convert_value(name, getattr(value, name)) for name in names] return cls.generate_data_dict(names, values)"
1168,def self.debugger.intf[-1]: self.debugger.intf[-1]: self.debugger.intf[-1]: self.debugger.intf[-1]: self.debugger.intf[-1]: self.debugger.intf[-1]: self.debugger.intf[-1]: self.debugger.intf[-1]: self.debugger.intf[-1]: self.debugger.intf[-1]: self.debugger.intf[-1]: self.debugger.intf[-1],"def msg(self, msg, opts=): try: return(self.debugger.intf[-1].msg(msg)) except EOFError: pass return None"
1169,"def create_new_instance(self, skype, raw): self.create_new_instance(self.create_new_instance): self.create_new_instance(self.create_new_instance) self.create_new_instance(self.create_new_instance) self.create_new_instance(self.create_new_instance) self.create_new_instance(self.create_new_instance) self.create_new_instance(self.create_new_instance) self.create_new_instance(self.create_new_instance) self.create_new_instance(self.create_new_instance) self.create_new_instance) self.create_new_instance(self.create_instance) self.create_instance(self.create_instance) self.create_instance(self.create_instance) self.create_instance(self.create_instance) self.create_instance(self.create_instance) self.create_instance(self.create_instance) self.create_instance(self.create_instance) self.create_instance(self.create_instance) self.create_instance(self.create_instance)","def fromRaw(cls, skype=None, raw=): return cls(skype, raw, **cls.rawToFields(raw))"
1170,"def transform_item(self.t_chunk, self.t_chunk, self.t_chunk, self.t_chunk, self.t_chunk, self.t_chunk, self.t_chunk, self.t_chunk, self.t_chunk, self.t_chunk, self.t_chunk, self.t_chunk, self.t_chunk, self.t_chunk, self.t_chunk, self.t_chunk)","def _run_incremental_transforms(self, si, transforms): # for transform in transforms: try: stream_id = si.stream_id si_new = transform(si, context=self.context) if si_new is None: logger.warn('transform %r deleted %s abs_url=%r', transform, stream_id, si and si.abs_url) return None si = si_new except TransformGivingUp: # logger.info('transform %r giving up on %r', transform, si.stream_id) except Exception, exc: logger.critical( 'transform %r failed on %r from i_str=%r abs_url=%r', transform, si and si.stream_id, self.context.get('i_str'), si and si.abs_url, exc_info=True) assert si is not None # if not si.stream_time: raise InvalidStreamItem('empty stream_time: %s' % si) if si.stream_id is None: raise InvalidStreamItem('empty stream_id: %r' % si) # if type(si)!= streamcorpus.StreamItem_v0_3_0: raise InvalidStreamItem('incorrect stream item object %r' % type(si)) self.t_chunk.add(si) return si"
1171,"def stop_cluster(self, profile): self.cluster(self.cluster) self.cluster(self.cluster) self.cluster(self.cluster) self.cluster(self.cluster) self.cluster(self.cluster) self.cluster(self.cluster) self.cluster(self.cluster) self.cluster(self.cluster) self.cluster(self.cluster) self.cluster(self.cluster) self.cluster","def stop_cluster(self, profile): self.check_profile(profile) data = self.profiles[profile] if data['status'] =='stopped': raise web.HTTPError(409, u'cluster not running') data = self.profiles[profile] cl = data['controller_launcher'] esl = data['engine_set_launcher'] if cl.running: cl.stop() if esl.running: esl.stop() result =  'profile': data['profile'], 'profile_dir': data['profile_dir'],'status':'stopped'  return result"
1172,"def process_line(self, %%, %%, %%, %%, %%, %%, %%, %%, %%, %%, %%, %%, %%, %%, %%, %%, %%, %%, %%, %%, %%, %%, %%, %%, %%, %%, %%, %%, %%)","def _handle_cell_magic(self, lines): self.processing_cell_magic = True first, _, body = lines.partition('n') magic_name, _, line = first.partition(' ') magic_name = magic_name.lstrip(ESC_MAGIC) self.cell_magic_parts = [body] tpl = 'get_ipython()._run_cached_cell_magic(%r, %r)' tlines = tpl % (magic_name, line) self._store(tlines) self._store(lines, self._buffer_raw,'source_raw') #self._is_complete = last_two_blanks(lines) self._is_complete = last_blank(lines) return self._is_complete"
1173,"def snapshot_list(self, snapshots, snapshots, snapshots, snapshots, snapshots, snapshots, snapshots, snapshots, snapshots, snapshots, snapshots, snapshots, snapshots, snapshots, snapshots, snapshots, snapshots, snapshots, snapshots, snapshots, snapshots, snapshots, snapshots, snapshots, snapshots, snapshots, snapshots, snapshots, snapshots, snapshots, snapshots, snapshots, snapshots, snapshots, snapshots, snapshots, snapshots, snapshots, snapshots)","def list(): snapshots = get_app().get_snapshots() click.echo('n'.join( '%s: %s' % ( s.snapshot_name, humanize.naturaltime(datetime.utcnow() - s.created_at) ) for s in snapshots ))"
1174,"def download_whole_course(self, int->None, int->None, int->None, int->None, int->None, int->None, int->None, int->None, int->None, int->None)","def wanmen_download_by_course(json_api_content, output_dir='.', merge=True, info_only=False, **kwargs): for tIndex in range(len(json_api_content[0]['Topics'])): for pIndex in range(len(json_api_content[0]['Topics'][tIndex]['Parts'])): wanmen_download_by_course_topic_part(json_api_content, tIndex, pIndex, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)"
1175,"def recursive_subdivisions(self, subdivisions): if self.subdivisions = self.subdivisions = self.subdivisions = self.subdivisions = self.subdivisions = self.subdivisions = self.subdivisions = self.subdivisions = self.subdivisions = self.subdivisions = self.subdivisions = self.subdivisions","def sphere_cap(script, angle=1.0, subdivisions=3, color=None): """""" filter_xml = ''.join(['filter name=""Sphere Cap"">n','Param name=""angle"" ', 'value=""%s""'% angle, 'description=""Angle"" ', 'type=""RichFloat"" ', '/>n','Param name=""subdiv"" ', 'value=""%d""'% subdivisions, 'description=""Subdiv. Level"" ', 'type=""RichInt"" ', '/>n','/filter>n']) util.write_filter(script, filter_xml) if isinstance(script, FilterScript): script.add_layer('Sphere Cap', change_layer=True) if color is not None: vert_color.function(script, color=color) return None"
1176,def profile_user_id: profile_user_id if self: self: self: self: self: self: self: self: self: self: self: self: self: self: self: self: self: self: self: self: self: self: self: self: self: self: self: self: self: self: self: self: self: self: self: self: self: self: self: self: self: self: self: self: self: self: self: self:,"def send_profile(self, profile_user_id, user_ids, text='', thread_id=None): profile_id = self.convert_to_user_id(profile_user_id) user_ids = _get_user_ids(self, user_ids) if not isinstance(text, str) and not isinstance(user_ids, (list, str)): self.logger.error('Text must be an string, user_ids must be an list or string') return False if self.reached_limit('messages'): self.logger.info(""Out of messages for today."") return False self.delay('message') if self.api.send_direct_item( 'profile', user_ids, text=text, thread=thread_id, profile_user_id=profile_id ): self.total['messages'] += 1 return True self.logger.info(""Message to user_ids wasn't sent"".format(user_ids=user_ids)) return False"
1177,"def cancel_schedule_messages(self, sequence_numbers, sequence_numbers, sequence_numbers, sequence_numbers, sequence_numbers, sequence_numbers, sequence_numbers, sequence_numbers, sequence_numbers, sequence_numbers, sequence_numbers, sequence_numbers, sequence_numbers, sequence_numbers, sequence_numbers, sequence_numbers, sequence_numbers, sequence_numbers, sequence_numbers)","async def cancel_scheduled_messages(self, *sequence_numbers): if not self.running: await self.open() numbers = [types.AMQPLong(s) for s in sequence_numbers] request_body ='sequence-numbers': types.AMQPArray(numbers) return await self._mgmt_request_response( REQUEST_RESPONSE_CANCEL_SCHEDULED_MESSAGE_OPERATION, request_body, mgmt_handlers.default)"
1178,"def ParentalManyToManyField(self, ParentalManyToManyFields, ParentalManyToManyFields, ParentalManyToManyFields, ParentalManyToManyFields, ParentalManyToManyFields, ParentalManyToManyFields, ParentalManyToManyFields, ParentalManyToManyFields, ParentalManyToManyFields)","def get_all_child_m2m_relations(model): return [ field for field in model._meta.get_fields() if isinstance(field, ParentalManyToManyField) ]"
1179,"def _onOutgoingMessageReceived(self, _onOutgoingMessageReceived, _onOutgoingMessageReceived, _onOutgoingMessageReceived, _onOutgoingMessageReceived, _onOutgoingMessageReceived, _onOutgoingMessageReceived, _onOutgoingMessageReceived, _onOutgoingMessageReceived, _onOutgoingMessageReceived, _onOutgoingMessageReceived, _onOutgoingMessageReceived, _onOutgoingMessageReceived)","def _onOutgoingConnected(self, conn): if self._syncObj.encryptor: conn.setOnMessageReceivedCallback(functools.partial(self._onOutgoingMessageReceived, conn)) conn.recvRandKey = os.urandom(32) conn.send(conn.recvRandKey) else: if not self._selfIsReadonlyNode: conn.send(self._selfNode.address) else: conn.send('readonly') self._onNodeConnected(self._connToNode(conn))"
1180,def calculate_great_circle_distance(self): if self._circle_distance(self._circle_distance): self._circle_distance(self._circle_distance) self._circle_distance(self._circle_distance) self._circle_distance(self._circle_distance) self._circle_distance(self._circle_distance(self._circle_distance) self._circle_distance(self._circle_distance) self._circle_distance(self._circle_distance) self._circle_distance(self._circle_distance) self._circle_distance(self._circle_distance) self._circle_distance) self._circle_distance(self._circle_distance) self._circle_distance) self._circle_distance(self._circle_distance) self._circle_distance(self._circle_distance) self._circle_distance(,"def haversine_distance(point1, point2): lat1, lon1 = point1 lat2, lon2 = point2 lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2]) dlon = lon2 - lon1 dlat = lat2 - lat1 a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2 c = 2 * asin(sqrt(a)) km = 6367 * c return km"
1181,def print_friendly_message(self): self.print_friendly_message(self.print_friendly_message): self.print_friendly_message(self.print_friendly_message) self.print_friendly_message(self.print_friendly_message) self.print_friendly_message(self.print_friendly_message) self.print_friendly_message(self.print_friendly_message) self.print_friendly_message) self.print_friendly_message(self.print_message) self.print_message) self.print_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_message_mes,"def stay_safe(): random = int(choice(str(int(time())))) if not CONFIGURATION[""quiet""] and random % 3 == 0: print(""n"" + Fore.GREEN + Style.BRIGHT + ""Thanks for using PyFunceble!"") print( Fore.YELLOW + Style.BRIGHT + ""Share your experience on "" + Fore.CYAN + ""Twitter"" + Fore.YELLOW + "" with "" + Fore.CYAN + ""#PyFunceble"" + Fore.YELLOW + ""!"" ) print( Fore.GREEN + Style.BRIGHT + ""Have a feedback, an issue or an improvement idea?"" ) print( Fore.YELLOW + Style.BRIGHT + ""Let us know on "" + Fore.CYAN + ""GitHub"" + Fore.YELLOW + ""!"" )"
1182,"def convert_hms_in_seconds(self, hms_in_seconds): if self.hms_in_seconds = self.hms_in_seconds = self.hms_in_seconds = self.hms_in_seconds(self.hms_in_seconds)","def _hms(s): if s == ""-"": return 0 if s.endswith(""ms""): return float(s.rstrip(""ms"")) / 1000 fields = list(map(float, re.split(""[dhms]"", s)[:-1])) if len(fields) == 4: return fields[0] * 24 * 3600 + fields[1] * 3600 + fields[2] * 60 + fields[3] if len(fields) == 3: return fields[0] * 3600 + fields[1] * 60 + fields[2] elif len(fields) == 2: return fields[0] * 60 + fields[1] else: return fields[0]"
1183,"def provide_signifance_for_features_in_dataset_with_anova_using_multiple_hypotesis_testing(self, x, y, threshold): self.get_signifance_for_features_in_dataset_with_anova_using_multiple_hypotesis_testing(self.get_signifance_for_features_in_dataset_with_anova_using_multiple_hypotesis_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_testing_","def feature_importance_report(X, y, threshold=0.001, correcting_multiple_hypotesis=True, method='fdr_bh', alpha=0.1, sort_by='pval'): df = variance_threshold_on_df( pd.DataFrame.from_records(X), threshold=threshold) F, pvals = f_classif(df.values, y) if correcting_multiple_hypotesis: _, pvals, _, _ = multipletests(pvals, alpha=alpha, method=method) df['labels'] = y df_mean = df.groupby('labels').mean().T df_mean['F'] = F df_mean['pval'] = pvals return df_mean.sort_values(sort_by, ascending=True)"
1184,"def _resolve_base_href(resolve_base_href): if'resolve_base_href' is None, if'resolve_base_href' is None, if'resolve_base_href' is None, if'resolve_base_href' is None, if'resolve_base_href' is None, if'resolve_base_href' is None,'resolve_base_href' is None,'resolve_base_href' is None,'resolve_base_href' is None,'resolve_base_href' is None,'resolve_base_href' is None,'resolve_base_href' is None,'resolve_base_href' is None,'resolve_base_href' is None,'resolve_base_href' is None,'resolve_base_href' is None,'resolve_base_href' is None,'resolve_base","def make_links_absolute(self, base_url=None, resolve_base_href=True, handle_failures=None): if base_url is None: base_url = self.base_url if base_url is None: raise TypeError( ""No base_url given, and the document has no base_url"") if resolve_base_href: self.resolve_base_href() if handle_failures == 'ignore': def link_repl(href): try: return urljoin(base_url, href) except ValueError: return href elif handle_failures == 'discard': def link_repl(href): try: return urljoin(base_url, href) except ValueError: return None elif handle_failures is None: def link_repl(href): return urljoin(base_url, href) else: raise ValueError( ""unexpected value for handle_failures: %r"" % handle_failures) self.rewrite_links(link_repl)"
1185,"def add_layer(self, z): if self.add_layer(layer): self.add_layer(layer) self.add_layer(layer) self.add_layer(layer) self.add_layer(layer) self.add_layer(layer) self.add_layer(layer) self.add_layer(layer) self.add_layer(layer) self.add_layer(layer) self.add_layer(layer) self.add_layer(layer) self.add_layer(layer) self.add_layer(layer) self.add_layer(layer) self.add_layer(layer)","def addLayer(self,layer,z=-1): if not isinstance(layer,Layer): raise TypeError(""layer must be an instance of Layer!"") if z==-1: self.layers.append(layer) else: self.layers.insert(z,layer)"
1186,"def unregister_engine(self, engine_name, engine_name, engine_name, engine_name, engine_name, engine_name, engine_name, engine_name, engine_name, engine_name, engine_name, engine_name, engine_name, engine_name, engine_name, engine_name, engine_name, engine_name, engine_name, engine_name, engine_name, engine_name, engine_name, engine_name, engine_name)","def unregister_engine(self, ident, msg): try: eid = msg['content']['id'] except: self.log.error(""registration::bad engine id for unregistration: %r"", ident, exc_info=True) return self.log.info(""registration::unregister_engine(%r)"", eid) uuid = self.keytable[eid] content=dict(id=eid, queue=uuid.decode('ascii')) self.dead_engines.add(uuid) handleit = lambda : self._handle_stranded_msgs(eid, uuid) dc = ioloop.DelayedCallback(handleit, self.registration_timeout, self.loop) dc.start() ############# if self.notifier: self.session.send(self.notifier, ""unregistration_notification"", content=content)"
1187,"def remove_operation_nodes(self, operation_nodes, operation_nodes, operation_nodes, operation_nodes, operation_nodes, operation_nodes, operation_nodes, operation_nodes, operation_nodes, operation_nodes, operation_nodes, operation_nodes, operation_nodes, operation_nodes)","def remove_all_ops_named(self, opname): for n in self.named_nodes(opname): self.remove_op_node(n)"
1188,"def arc(self, arc, arc): if self.get(arc): self.get(arc): self.get(arc): self.get(arc): self.get(arc): self.get(arc)",def arcs_unpredicted(self): possible = self.arc_possibilities() executed = self.arcs_executed() unpredicted = [ e for e in executed if e not in possible and e[0]!= e[1] ] return sorted(unpredicted)
1189,"def match_type(self, type): if self.type(type): self.type(type) return self.type(type) return self.type(type)","def check(self, var): return (isinstance(var, _num_type) and (self._lower_bound is None or var >= self._lower_bound) and (self._upper_bound is None or var = self._upper_bound))"
1190,def mfcc(np.array(np.array(np.array(np.array(np.array(np.array(np.array(np.array(np.array(np.array(np.array(np.array(np.array(np.array(np.array(np.array(np.array(np.array(np.array(np.array(np.array))))))))))),"def compute_features(self): S = librosa.feature.melspectrogram(self._audio, sr=self.sr, n_fft=self.n_fft, hop_length=self.hop_length, n_mels=self.n_mels) log_S = librosa.amplitude_to_db(S, ref=self.ref_power) mfcc = librosa.feature.mfcc(S=log_S, n_mfcc=self.n_mfcc).T return mfcc"
1191,"def include_meta_content_type(self, encoding=true, doctype=true, encoding=true, doctype=true): if encoding=true, encoding=true, doctype=true, encoding=true, doctype=true, encoding=true, doctype=true, encoding=true, doctype=true, encoding=true, doctype=true, encoding=true, encoding=true, encoding=true, encoding=true, encoding=true, encoding=true, encoding=true, encoding=true, encoding=true, encoding=true, encoding=true, encoding=true, encoding=true, encoding=true, encoding=true, encoding=true, encoding=true, encoding=true, encoding=true, encoding=true, encoding=true, encoding=true, encoding=true, encoding=true, encoding=true, encoding","def tostring(doc, pretty_print=False, include_meta_content_type=False, encoding=None, method=""html"", with_tail=True, doctype=None): html = etree.tostring(doc, method=method, pretty_print=pretty_print, encoding=encoding, with_tail=with_tail, doctype=doctype) if method == 'html' and not include_meta_content_type: if isinstance(html, str): html = __str_replace_meta_content_type('', html) else: html = __bytes_replace_meta_content_type(bytes(), html) return html"
1192,"def pyssim(self, pyssim, pyssim, pyssim, pyssim, pyssim, pyssim, pyssim, pyssim, pyssim, pyssim, pyssim, pyssim, pyssim, pyssim, pyssim, pyssim, pyssim, pyssim, pyssim)","def main(): description = 'n'.join([ 'Compares an image with a list of images using the SSIM metric.','Example:','pyssim test-images/test1-1.png ""test-images/*""' ]) parser = argparse.ArgumentParser( prog='pyssim', formatter_class=argparse.RawTextHelpFormatter, description=description) parser.add_argument('--cw', help='compute the complex wavelet SSIM', action='store_true') parser.add_argument( 'base_image', metavar='image1.png', type=argparse.FileType('r')) parser.add_argument( 'comparison_images', metavar='image path with* or image2.png') parser.add_argument('--width', type=int, default=None, help='scales the image before computing SSIM') parser.add_argument('--height', type=int, default=None, help='scales the image before computing SSIM') args = parser.parse_args() if args.width and args.height: size = (args.width, args.height) else: size = None if not args.cw: gaussian_kernel_sigma = 1.5 gaussian_kernel_width = 11 gaussian_kernel_1d = get_gaussian_kernel( gaussian_kernel_width, gaussian_kernel_sigma) comparison_images = glob.glob(args.comparison_images) is_a_single_image = len(comparison_images) == 1 for comparison_"
1193,def check_detached_state(self): self.detached_state(self.detached_state) self.detached_state(self.detached_state) self.detached_state(self.detached_state) self.detached_state(self.detached_state) self.detached_state(self.detached_state) self.detached_state(self.detached_state) self.detached_state) self.detached_state) self.detached_state(self.detached_state) self.detached_state) self.detached_state(self.detached_state) self.detached_state) self.detached_state(self.detached_state) self.detached_state) self.detached_state(self.detached_state) self.detached_state(self.detached_state) self.detached_state) self.detached_state(self.detached_state) self.detached_state(self.detached,"def is_detached(self): cmd_sym = ['git','symbolic-ref', 'HEAD'] try: self._exec(cmd_sym, cwd=self.dirpath, env=self.gitenv) except RepositoryError as e: if e.msg.find(""ref HEAD is not a symbolic ref"") == -1: raise e return True else: return False"
1194,"def iterable(self, prefix): if self.get(prefix): self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(prefix) self.get(","def __complete_refers(self, value: str) -> Iterable[str]: return map( lambda entry: f""entry[0].name"", filter( Namespace.__completion_matcher(value), [(s, v) for s, v in self.refers] ), )"
1195,"def _default_fields(self, default_fields): self.file_request(default_fields) self.file_request(default_fields) self.file_request(default_fields) self.file_request(default_fields) self.file_request(default_fields) self.file_request(default_fields) self.file_request(default_fields) self.file_request(default_fields) self.file_request(default_fields) self.file_request(default_fields) self.file_request(default_fields) self.file_request(default_fields) self.file_request(default_fields) self.file_request(default_fields) self.file_request(default_fields) self.file_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request_request","def _file_default_fields(): return [ files.c.name, files.c.created_at, files.c.parent_name, ]"
1196,"def dag_run(self, dag_id, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, dagRun, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, dagRun, dagRun, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, execution_date, dagRun, dagRun","def get_run(session, dag_id, execution_date): qry = session.query(DagRun).filter( DagRun.dag_id == dag_id, DagRun.external_trigger == False, DagRun.execution_date == execution_date, ) return qry.first()"
1197,"def _get_messages(self, messages): if self.get_messages(messages): self.get_messages(messages): self.get_messages(messages): self.get_messages(messages)","def process_response(self, request, response): if hasattr(request, ""session"") and hasattr(request, ""user"") and request.user.is_authenticated(): msgs = get_messages(request.user) if msgs: for msg, level in msgs: messages.add_message(request, level, msg) return response"
1198,"def environment_title_or_num_frequency(self, environment_title_or_num_frequency, environment_title_or_num_frequency, environment_title_or_num_frequency, environment_title_or_num_frequency, environment_title_or_num_frequency, environment_title_or_num_frequency, environment_title_or_num_frequency, environment_title_or_num_frequency_frequency, environment_title_or_num_frequency)","def get_data(self, environment_title_or_num=-1, frequency=None): if isinstance(environment_title_or_num, int): environment_title = tuple(self._raw_environments.keys())[environment_title_or_num] else: environment_title = environment_title_or_num if environment_title not in self._dfs: raise ValueError(f""No environment named environment_title. Available environments: tuple(self._dfs)."") environment_dfs = self._dfs[environment_title] if frequency is None: for frequency in FREQUENCIES: if environment_dfs[frequency] is not None: break if frequency not in FREQUENCIES: raise ValueError(f""Unknown frequency: frequency. Available frequencies: FREQUENCIES"") return self._dfs[environment_title][frequency]"
1199,"def create_artificial_cutoff_sample_points('num_cut_offs', 'num_cut_offs', 'num_cut_offs', 'num_cut_offs', 'num_cut_offs', 'num_cut_offs', 'num_cut_offs', 'num_cut_offs', 'num_cut_offs', 'num_cut_offs', 'num_cut_offs', 'num_cut_offs', 'num_cut_offs', 'num_cut_offs', 'num_cut_offs', 'num_cut_offs', 'num_cut_offs', 'num_cut_offs', 'num_cut_offs', 'num_cut_offs', 'num_cut_offs', 'num_cut_offs', 'num_cut_offs', 'num_cut_offs', 'num_cut_offs', 'num_cut_offs', 'num_cut_offs', 'num_cut_offs', '","def final_err_table(df, num_cut_offs=51): cutoffs = df.cutoff.values min_ = min(cutoffs) max_ = max(cutoffs) margin = (max_ - min_) * 0.05 sampled_cutoffs = np.linspace(min_ - margin, max_ + margin, num_cut_offs, dtype=np.float32) ix = find_nearest_matches(np.float32(df.cutoff.values), sampled_cutoffs) sampled_df = df.iloc[ix].copy() sampled_df.cutoff = sampled_cutoffs sampled_df.reset_index(inplace=True, drop=True) return sampled_df"
1200,"def x_segments(x_segments, y_segments, center=false, center=false, center=false, center=false, center=false, center=false, center=false, center=false, center=false, center=false, center=false, center=false): x_segments = x_segments = y_segments = y_segments = y_segments = y_segments = y_segments = y_segments = y_segments = y_segments = y_segments = y_segments = y_segments = y_segments = y_segments = y_segments = y_segments = y_segments = y_segments = y_segments = y_segments = y_segments = y_segments = y_segments = y_segments =","def grid(script, size=1.0, x_segments=1, y_segments=1, center=False, color=None): if center: transform.translate(script, value=[size[0]/2, -size[1]/2, 0]) else: transform.translate(script, value=[size[0], 0, 0]) if color is not None: vert_color.function(script, color=color) return None"
1201,"def wrap_with_u_tag(self, span, u): if u_tag has underline style: if u_tag has underline style: if u_tag has underline style: if u_tag has underline style: if u_tag has underline style: if u_tag has underline style: if u_tag has underline style: if u_tag has underline style: if u_tag has underline style: if u_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag_tag","def create_underline(self, tag): style = tag.get('style') if style and 'text-decoration:underline' in style: tag.wrap(self.soup.new_tag('u'))"
1202,"def push_token(self, token, position): self.push_token(self.push_token) self.push_token(self.push_token) self.push_token(self.push_token) self.push_token(self.push_token) self.push_token(self.push_token) self.push_token(self.push_token) self.push_token(self.push_token) self.push_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_token_","def push_token(self, token, position): if _token_followed_by_eol(self._tokens, position): self._cont_stack.append(self._hanging_indent_after_bracket(token, position)) else: self._cont_stack.append(self._continuation_inside_bracket(token, position))"
1203,"def transpose_boltArraySpark(self, axes, axes): self.transpose_boltArraySpark(axes) self.transpose_boltArraySpark(axes) self.transpose_boltArraySpark(axes) self.transpose_boltArraySpark(axes) self.transpose_boltArraySpark(axes) self.transpose_boltArraySpark(axes) self.transpose_boltArraySpark(axes) self.transpose_boltArraySpark(axes) self.transpose_boltArraySpark(axes) self.transpose_boltArraySpark(axes) self.transpose_boltArraySpark(axes) self.transpose_boltArraySpark(axes) self.transpose_boltArraySpark(axes) self.transpose_boltArraySpark(axes) self.transpose_boltArraySpark(axes) self.transpose_boltArraySpark(axes) self.transpose_boltArraySpark(axes) self.transpose_boltArraySpark(axes) self.transpose_","def transpose(self, *axes): new = argpack(axes) old = range(self.ndim) istransposeable(new, old) if new == old: return self._barray def f(v): return v.transpose(new) newrdd = self._barray._rdd.mapValues(f) newshape = self._barray.keys.shape + tuple(self.shape[i] for i in new) return BoltArraySpark(newrdd, shape=newshape).__finalize__(self._barray)"
1204,"def nthresult(self, coll, i): if nthresult(nthresult(nthresult(nthresult(nthresult(nthresult(nthresult(nthresult(nthresult(nthresult(nthresult(nthresult(nthresult(nthresult(nthresult(nthresult(nthresult(nthresult(nthresult(nthresult(nthresult(nthresult))))))))))","def nthrest(coll, i: int): while True: if coll is None: return None if i == 0: return coll i -= 1 coll = rest(coll)"
1205,"def check_function(self, args): self.check_function(args) self.check_function(args) self.check_function(args) self.check_function(args) self.check_function(args) self.check_function(args) self.check_function(args) self.check_function(args) self.check_function(args) self.check_function(args) self.check_function(args) self.check_function(args) self.check_function(args) self.check_function(args) self.check_function(args) self.check_function(args) self.check_function(args) self.check_function(args) self.check_function(args) self.check_function(args) self.check_function(args) self.check_function(args) self.check_function(args) self.check_function(args) self.check_function(args) self.check_function(args) self.check_function(args) self.check_function(args) self.check_function(args) self.check_function(args) self.check_function(args) self.check_function(arg","def check_function( state, name, index=0, missing_msg=None, params_not_matched_msg=None, expand_msg=None, signature=True, ): append_missing = missing_msg is None append_params_not_matched = params_not_matched_msg is None if missing_msg is None: missing_msg = MISSING_MSG if expand_msg is None: expand_msg = PREPEND_MSG if params_not_matched_msg is None: params_not_matched_msg = SIG_ISSUE_MSG stu_out = state.ast_dispatcher(""function_calls"", state.student_ast) sol_out = state.ast_dispatcher(""function_calls"", state.solution_ast) student_mappings = state.ast_dispatcher(""mappings"", state.student_ast) fmt_kwargs =  ""times"": get_times(index + 1), ""ord"": get_ord(index + 1), ""index"": index, ""mapped_name"": get_mapped_name(name, student_mappings),  try: sol_parts = **sol_out[name][index] except KeyError: raise InstructorError( ""check_function() couldn't find a call of %s() in the solution code. Make sure you get the mapping right!"" % name ) except IndexError: raise InstructorError( ""check_function() couldn't find %s calls of %s() in your solution code."" % (index + 1, name) ) try: stu_parts = **stu_out[name][index] except (KeyError, IndexError): _msg = state.build_message(missing_msg, fmt_"
1206,"def decode_object(input_buffer, input_buffer, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version","def read(self, input_buffer, kmip_version=enums.KMIPVersion.KMIP_1_0): super(GetAttributeListRequestPayload, self).read( input_buffer, kmip_version=kmip_version ) local_buffer = utils.BytearrayStream(input_buffer.read(self.length)) if self.is_tag_next(enums.Tags.UNIQUE_IDENTIFIER, local_buffer): self._unique_identifier = primitives.TextString( tag=enums.Tags.UNIQUE_IDENTIFIER ) self._unique_identifier.read( local_buffer, kmip_version=kmip_version ) else: self._unique_identifier = None self.is_oversized(local_buffer)"
1207,def main_program(self): if self.program(self.program): self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program) self.program(self.program) self.program(self.program) self.program) self.program(self.program) self.program(self.program) self.program) self.program(self.program) self.program(self.program) self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program) self.program(self.program) self.program(self.program) self,"def main(argv): global g_script_name g_script_name = os.path.basename(argv[0]) parse_config_file() parse_args(argv) url = 'https://0xdata.atlassian.net/rest/api/2/search?jql='  + 'project+in+(PUBDEV,HEXDEV)'  + '+and+'  +'resolutiondate+>=+' + g_start_date.strftime(""%Y-%m-%d"")  + '+and+'  +'resolution+in+(Done,Fixed)'  + 'order+by+type,component,resolutiondate'  + '&maxResults=1000' r = requests.get(url, auth=(g_user, g_pass)) if (r.status_code!= 200): print(""ERROR: status code is "" + str(r.status_code)) sys.exit(1) j = r.json() issues = j[u'issues'] if len(issues) >= 1000: print(""ERROR: len(issues) >= 1000. Too many issues."") sys.exit(1) last_issue_type_name = """" last_component_name = """" for issue in issues: issue_type_name = get_issue_type_name(issue) component_name = get_issue_component_name(issue) if (issue_type_name!= last_issue_type_name): print("""") print "" last_issue_type_name = issue_type_name if (component_name!= last_component_name): print("""") print ""# last_component_name = component_name key = get_issue_key(issue) summary = get_issue_summary(issue) print ""* "" + key + "": "" + summary"
1208,"def make_request(self, url, kwargs): if self.request(self.request(self.request)): self.request(self.request(self.request)) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request(self.request(self.request) self.request(self.request) self.request(self.request) self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request) self.request(self.request) self.request(self.request) self.request) self.request(self.request) self.request(self.request) self.request(self.request) self.request) self.request(self.request) self.request(self.request) self.request(self.request)","def request(self, method, url, erc, **kwargs): abs_url = self.abs_url(url) kwargs.setdefault('timeout', self.single_request_timeout) while True: response = self._req_session.request(method, abs_url, **kwargs) try: check_response_code(response, erc) except RateLimitError as e: if self.wait_on_rate_limit: warnings.warn(RateLimitWarning(response)) time.sleep(e.retry_after) continue else: raise else: return response"
1209,"def delete_state(self, turn_context, turn_context, turn_context, turn_context, turn_context, turn_context, turn_context, turn_context, turn_context, turn_context, turn_context, turn_context, turn_context, turn_context, turn_context, turn_context, turn_context)","async def delete(self, turn_context: TurnContext) -> None: if turn_context == None: raise TypeError('BotState.delete(): turn_context cannot be None.') turn_context.turn_state.pop(self._context_service_key) storage_key = self.get_storage_key(turn_context) await self._storage.delete( storage_key )"
1210,"def set_stop(self,'set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_stop','set_","def handle_stop(self, signame, set_stop): if set_stop: self.sigs[signame].b_stop = True self.sigs[signame].print_method = self.dbgr.intf[-1].msg self.sigs[signame].pass_along = False else: self.sigs[signame].b_stop = False pass return set_stop"
1211,"def parse_argument(self, decorations, parser): def parse_argument(parser): def parse_argument(parser): def parse_argument(parser)","def construct_parser(magic_func): kwds = getattr(magic_func, 'argcmd_kwds', ) if 'description' not in kwds: kwds['description'] = getattr(magic_func, '__doc__', None) arg_name = real_name(magic_func) parser = MagicArgumentParser(arg_name, **kwds) group = None for deco in magic_func.decorators[::-1]: result = deco.add_to_parser(parser, group) if result is not None: group = result help_text = parser.format_help() if help_text.startswith('usage: '): help_text = help_text.replace('usage: ', '%', 1) else: help_text = '%' + help_text magic_func.__doc__ = help_text return parser"
1212,"def print_status(self, sample, subset, stds): self.print_status = self.print_status(stds) self.print_status(stds) self.print_status(stds) self.print_status(stds) self.print_status(stds) self.print_status(stds) self.print_status(stds) self.print_status(stds) self.print_status(stds) self.print_status(stds) self.print_status(stds) self.print_status(stds) self.print_status(stds) self.print_status(stds) self.print_status_status_status_status_status_status_status_status_status_status_status_status_status_status_status_status_status_status_status_status_status_status_status_status_status_status_status_status_status_status_status_status_status_","def filter_status(self, sample=None, subset=None, stds=False): s = '' if sample is None and subset is None: if not self._has_subsets: s += 'Subset: All Samplesnn' s += self.data[self.subsets['All_Samples'][0]].filt.__repr__() else: for n in sorted(str(sn) for sn in self._subset_names): if n in self.subsets: pass elif int(n) in self.subsets: n = int(n) pass s += 'Subset:'+ str(n) + 'n' s += 'Samples:'+ ', '.join(self.subsets[n]) + 'nn' s += self.data[self.subsets[n][0]].filt.__repr__() if len(self.subsets['not_in_set']) > 0: s += 'nNot in Subset:n' s += 'Samples:'+ ', '.join(self.subsets['not_in_set']) + 'nn' s += self.data[self.subsets['not_in_set'][0]].filt.__repr__() print(s) return elif sample is not None: s += 'Sample:'+ sample + 'n' s += self.data[sample].filt.__repr__() print(s) return elif subset is not None: if isinstance(subset, (str, int, float)): subset = [subset] for n in subset: s +="
1213,"def display_first_and_last_n(self, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n,","def head_and_tail_print(self, n=5): from IPython import display display.display(display.HTML(self._head_and_tail_table(n)))"
1214,"def my_field(self, my_field): if a: aa: 1 ab: 2 b: ba: 3 ba: 4 if aa: 1 ab: 2 b: ba: 3 ba: 4 if aa: 1 ab: 2 b: ba: 3 ba: 4 if aa: 1 ab: 2 b: ba: 3 ba: 4 if aa: 1 ab: 2 b: ba: 3 ba: 4 if aa: 1 aa: 1 aa: 1 aa: 1 aa: 1 aa: 1 aa: 1 aa: 1 aa: 1 aa: 1 aa: 1 aa: 1 aa: 1 aa: 1 aa: 1 aa: 1 aa: 1 aa: 1 aa: 1 aa: 1 aa: 1 aa: 1 aa: 1 aa: 1 aa: 1 aa: 1 aa: 1 aa: 1 aa: 1 aa: 1 aa: 1 aa: 1 aa: 1 aa: 1 ba: 3 aa: 1 ba: 1","def _flatten_dictionary(obj, path, key_name): result = [] if "">"" in key_name: key_name, group_key = key_name.split("">"") else: group_key = None for k, v in obj.items(): if path: if k == path[0]: path.pop(0) if k.startswith(""#""): continue r = _resolve_path(v, list(path)) if isinstance(r, dict): r = [r] for e in r: if group_key: e[group_key] = kk: vv for kk, vv in v.items() if kk not in path e[key_name] = k result.append(e) return result"
1215,def get_stokes_parameter(self): self.stokes_parameter(self.stokes_parameter) self.stokes_parameter(self.stokes_parameter) self.stokes_parameter(self.stokes_parameter) self.stokes_parameter(self.stokes_parameter) self.stokes_parameter(self.stokes_parameter) self.stokes_parameter(self.stokes_parameter) self.stokes_parameter) self.stokes_parameter(self.stokes_parameter) self.stokes_parameter) self.stokes_parameter(self.stokes_parameter) self.stokes_parameter) self.stokes_parameter(self.stokes_parameter) self.stokes_parameter) self.stokes_parameter(self.stokes_parameter) self.stokes_parameter(self.stokes_parameter) self.stokes_parameter) self.stokes_parameter(self.stokes_parameter) self.stokes_parameter(self.stokes_parameter) self.stokes_,"def apply_Mueller(I,Q,U,V, gain_offsets, phase_offsets, chan_per_coarse, feedtype='l'): #Find shape of data arrays and calculate number of coarse channels shape = I.shape ax0 = I.shape[0] ax1 = I.shape[1] nchans = I.shape[2] ncoarse = nchans/chan_per_coarse #Reshape data arrays to separate coarse channels I = np.reshape(I,(ax0,ax1,ncoarse,chan_per_coarse)) Q = np.reshape(Q,(ax0,ax1,ncoarse,chan_per_coarse)) U = np.reshape(U,(ax0,ax1,ncoarse,chan_per_coarse)) V = np.reshape(V,(ax0,ax1,ncoarse,chan_per_coarse)) #Swap axes 2 and 3 to in order for broadcasting to work correctly I = np.swapaxes(I,2,3) Q = np.swapaxes(Q,2,3) U = np.swapaxes(U,2,3) V = np.swapaxes(V,2,3) #Apply top left corner of electronics chain inverse Mueller matrix a = 1/(1-gain_offsets**2) if feedtype=='l': Icorr = a*(I-gain_offsets*Q) Qcorr = a*(-1*gain_offsets*I+Q) I = None Q = None if feedtype=='c': Icorr = a*(I-gain_offsets*V) Vcorr = a*(-1*gain_offsets*I+V) I = None V = None #"
1216,"def calculateExactMatchScore(y_true, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted, y_predicted)","def squad_v2_exact_match(y_true: List[List[str]], y_predicted: List[str]) -> float: EM_total = sum(normalize_answer(prediction) in map(normalize_answer, ground_truth) for ground_truth, prediction in zip(y_true, y_predicted)) return 100 * EM_total / len(y_true) if len(y_true) > 0 else 0"
1217,def add_trit(self): if self.trit(self.trit): self.trit(self.trit): self.trit(self.trit),"def _add_trits(left, right): res = left + right return res if -2  res  2 else (res  0) - (res > 0)"
1218,"def retrieve_resource(self, resource, params, params): self.retrieve_resource(resource, params) self.retrieve_retrieve_retrieve(resource, params) self.retrieve_retrieve_retrieve(resource, params) self.retrieve_retrieve_retrieve_retrieve_retrieve_retrieve_retrieve_retrieve_retrieve_retrieve_retrieve_retrieve_retriev_retriev_retriev_retriev_retriev_retriev_retriev_retriev_retriev_retriev_retriev_retriev_retriev_retriev_retriev_retriev_retriev_retriev_retriev_retriev_retriev_retriev_retriev_retriev_retriev_retriev_retriev_retriev_retriev_retriev_retri","def _call(self, resource, params): url = self.URL % 'base': self.base_url,'resource': resource logger.debug(""Confluence client requests: %s params: %s"", resource, str(params)) while True: r = self.fetch(url, payload=params) yield r.text j = r.json() if '_links' not in j: break if 'next' not in j['_links']: break url = urijoin(self.base_url, j['_links']['next']) params ="
1219,"def Optimize(self, AdaGrad, AdaDelta, AdaDelta, AdaGrad, AdaDelta, AdaDelta, AdaGrad, AdaDelta, AdaDelta, AdaGrad, AdaDelta, AdaGrad, AdaDelta, AdaGrad, AdaDelta, AdaGrad, AdaDelta, AdaDelta, AdaGrad, AdaGrad, AdaGrad)","def ada_family_core(params, gparams, learning_rate = 0.01, eps= 1e-6, rho=0.95, method=""ADADELTA"", beta=0.0, gsum_regularization = 0.0001): _, _, _, args = inspect.getargvalues(inspect.currentframe()) logging.info(""ada_family_core: %s"" % str(args.items())) free_parameters = [] if method == ""FINETUNING_ADAGRAD"": method = ""ADAGRAD"" gsum_regularization = 0 oneMinusBeta = 1 - beta gsums = [theano.shared(np.zeros_like(param.get_value(borrow=True), dtype=FLOATX), name=""gsum_%s"" % param.name) if (method == 'ADADELTA' or method == 'ADAGRAD') else None for param in params] xsums = [theano.shared(np.zeros_like(param.get_value(borrow=True), dtype=FLOATX), name=""xsum_%s"" % param.name) if method == 'ADADELTA' else None for param in params] if method == 'ADAGRAD': for gsum in gsums: gsum.set_value(gsum.get_value() ** 0) updates = OrderedDict() for gparam, param, gsum, xsum in zip(gparams, params, gsums, xsums): if method == 'ADADELTA': updates[gsum] = rho * gsum + (1. - rho) * (gparam **2) dparam = -T.sqrt((xsum + eps) / (up"
1220,"def calculate_unclaimed_credit_notes(self, unclaimed_credit_notes): if self.unclaimed_credit_notes(self.unclaimed_credit_notes): self.unclaimed_credit_notes(self.unclaimed_credit_notes)","def available_credit(context): notes = commerce.CreditNote.unclaimed().filter( invoice__user=user_for_context(context), ) ret = notes.values(""amount"").aggregate(Sum(""amount""))[""amount__sum""] or 0 return 0 - ret"
1221,def hamming_distance(self): hamming_distance(self): hamming_distance(self) hamming_distance(self) hamming_distance(self) hamming_distance(self) hamming_distance(self) hamming_distance(self) hamming_distance(self) hamming_distance(self) hamming_distance(self) hamming distance(self) hamming distance(self) hamming distance(self) hamming distance(self) hamming distance(self) hamming distance(self),"def distance(image_path, other_image_path): image_hash = average_hash(image_path) other_image_hash = average_hash(other_image_path) return hash_distance(image_hash, other_image_hash)"
1222,"def convert_nested_list(self, nested_list): return self.convert(nested_list) self.convert(nested_list) self.convert(nested_list) self.convert(nested_list) self.convert(nested_list) self.convert(nested_list) self.convert(nested_list) self.convert(nested_list) self.convert(nested_list) self.convert(nested_list) self.convert(nested_list)","def _list_to_complex_array(complex_list): arr = np.asarray(complex_list, dtype=np.complex_) if not arr.shape[-1] == 2: raise QiskitError('Inner most nested list is not of length 2.') return arr[..., 0] + 1j*arr[..., 1]"
1223,"def symmetrix_matrix(self, symmetrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix_matrix)","def is_symmetric_matrix(op, rtol=RTOL_DEFAULT, atol=ATOL_DEFAULT): if atol is None: atol = ATOL_DEFAULT if rtol is None: rtol = RTOL_DEFAULT mat = np.array(op) if mat.ndim!= 2: return False return np.allclose(mat, mat.T, rtol=rtol, atol=atol)"
1224,"def slabs_info(self, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info): slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info, slabs_info",def _get_cache_slabs(server_name=None): server_info =  for svr in mc_client.get_slabs(): svr_info = svr[0].split(' ') svr_name = svr_info[0] if server_name and server_name == svr_name: return svr[1] server_info[svr_name] = svr[1] return server_info
1225,"def _path(path1 and path2): if path1 and path2 refer to the same file, return path1 if path1 and path2 refer to the same file if path1 and path2 refer to the same file if path1 and path2 refer to the same file if path1 and path2 refer to the same file if path1 and path2 refer to the same file if path1 and path2 refer to the same file if path1 and path2 refer to the same file if path1","def samefile(path1, path2): info1 = fs.getfileinfo(path1) info2 = fs.getfileinfo(path2) return (info1.dwVolumeSerialNumber == info2.dwVolumeSerialNumber and info1.nFileIndexHigh == info2.nFileIndexHigh and info1.nFileIndexLow == info2.nFileIndexLow)"
1226,"def get_module(self, app_name, module_name, app_name, module_name, app_name, app_name, module_name, app_name, module_name, app_name, app_name, module_name, app_name, app_name, module_name, app_name, app_name, module_name, app_name, app_name, module_name, app_name, app_name, app_name, module_name, app_name, module_name, module_name, app_name, module_name, app_name, module_name, app_name, module_name, app_name, module_name, module_name, app_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, module_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_","def import_app_module(app_name, module_name): name_split = app_name.split('.') if name_split[-1][0].isupper(): app_name = '.'.join(name_split[:-2]) module = import_module(app_name) try: sub_module = import_module('%s.%s' % (app_name, module_name)) return sub_module except: if module_has_submodule(module, module_name): raise return None"
1227,"def format_statevector(vec, list, decimals, decimals): vec = vec, decimals = decimals, decimals = decimals, decimals = decimals, vec = decimals, decimals = decimals, vec = decimals, vec = decimals, decimals = decimals, vec = decimals, decimals = decimals, vec = decimals, decimals = decimals, vec = decimals, decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = vec = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals = decimals =","def format_statevector(vec, decimals=None): num_basis = len(vec) vec_complex = np.zeros(num_basis, dtype=complex) for i in range(num_basis): vec_complex[i] = vec[i][0] + 1j * vec[i][1] if decimals: vec_complex = np.around(vec_complex, decimals=decimals) return vec_complex"
1228,"def send_transfer(self, block_list): self.submit_transfer(block_list) self.submit_transfer(block_list) self.submit_transfer(block_list) self.submit_transfer(block_list) self.submit_transfer(block_list) self.submit_transfer(block_list) self.submit_transfer(block_list) self.submit_transfer(block_list) self.submit_transfer(block_list) self.submit_transfer(block_list) self.submit_transfer(block_list) self.submit_transfer(block_list) self.submit_transfer(block_list) self.submit_transfer(block_list) self.submit_transfer(block_list) self.submit_transfer(block_list) self.submit_transfer(block_list) self.submit_transfer(block_list) self.submit_transfer(block_list) self.submit_transfer(block_list) self.submit_transfer(block_list) self.submit_transfer(block_list) self.submit_transfer(block_list) self.submit_transfer(block","def submit(self, block_list): submitted_count = libaio.io_submit( self._ctx, len(block_list), (libaio.iocb_p * len(block_list))(*[ pointer(x._iocb) for x in block_list ]), ) submitted = self._submitted for block in block_list[:submitted_count]: submitted[addressof(block._iocb)] = (block, block._getSubmissionState()) return submitted_count"
1229,"def open_http(self, URL, values): if self.submit(self.submit(self.submit(self.submit(self.submit(self.submit(self.submit(self.submit(self.submit(self.submit(self.submit(self.submit(self.submit(self.submit(self.submit(self.submit(self.submit(self.submit(self.submit(self.submit(self.submit(self.submit(self.submit(self.submit(self.submit))))))))))","def submit_form(form, extra_values=None, open_http=None): values = form.form_values() if extra_values: if hasattr(extra_values, 'items'): extra_values = extra_values.items() values.extend(extra_values) if open_http is None: open_http = open_http_urllib if form.action: url = form.action else: url = form.base_url return open_http(form.method, url, values)"
1230,"def get_document_in_turn(self, expanded=True, progress=True, expanded=True, expanded=True, expanded=True, expanded=True, expanded=True, expanded=True, expanded=True, expanded=True, expanded=True, expanded=True, expanded=True, expanded=True, expanded=True, expanded=True, expanded=True, expanded=True, expanded=True, progress=True, progress=True, progress=True, progress=True, progress=True, progress=True, progress=True, progress=True, progress=True, progress=True, progress=True, progress=True, progress=True, progress=True, progress=True, progress=True, progress=True, progress=True, progress=True, progress=True, progress=True, progress=True, progress=True, progress=True, progress=True, progress=True, progress=True, progress=True, progress=True, progress=True, progress=True, progress=True, progress=True, progress=True, progress=True, progress=True, progress=True, progress=True, progress=","def iterate_docs(client, expanded=False, progress=False): num_docs = client.get()['document_count'] progress_bar = None try: if progress: progress_bar = tqdm(desc='Downloading documents', total=num_docs) for offset in range(0, num_docs, DOCS_PER_BATCH): response = client.get('docs', offset=offset, limit=DOCS_PER_BATCH) docs = response['result'] for doc in docs: if expanded: for field in UNNECESSARY_FIELDS: doc.pop(field, None) else: doc = field: doc[field] for field in CONCISE_FIELDS if progress: progress_bar.update() yield doc finally: if progress: progress_bar.close()"
1231,"def query_data_object_annotation(self, data_object_annotation): self.data_object_annotation(data_object_annotation) self.data_object_annotation(data_object_annotation) self.data_object_annotation(data_object_annotation) self.data_object_annotation(data_object_annotation) self.data_object_annotation(data_object_annotation) self.data_object_annotation(data_object_annotation) self.data_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_","def data(self, **query): objects = self.cache['objects'] data = self.api.data.get(**query)['objects'] data_objects = [] for d in data: _id = d['id'] if _id in objects: objects[_id].update(d) else: objects[_id] = GenData(d, self) data_objects.append(objects[_id]) for d in data_objects: count += 1 while True: ref_annotation =  remove_annotation = [] for path, ann in d.annotation.items(): if ann['type'].startswith('data:'): _id = ann['value'] if _id not in objects: try: d_tmp = self.api.data(_id).get() except slumber.exceptions.HttpClientError as ex: if ex.response.status_code == 404: continue else: raise ex objects[_id] = GenData(d_tmp, self) annotation = objects[_id].annotation ref_annotation.update(path + '.' + k: v for k, v in annotation.items()) remove_annotation.append(path) if ref_annotation: d.annotation.update(ref_annotation) for path in remove_annotation: del d.annotation[path] else: break return data_objects"
1232,"def instantiate_type(self, type, type, type, type, type, type, type, type, type, type, type, type, type, type, type, type, type, type, type, type, type, type, type)","def instantiate_type(self, typename, base, subtypes): if base not in self.type_factories: raise ArgumentError(""unknown complex base type specified"", passed_type=typename, base_type=base) base_type = self.type_factories[base] #Make sure all of the subtypes are valid for sub_type in subtypes: try: self.get_type(sub_type) except KeyValueException as exc: raise ArgumentError(""could not instantiate subtype for complex type"", passed_type=typename, sub_type=sub_type, error=exc) typeobj = base_type.Build(*subtypes, type_system=self) self.inject_type(typename, typeobj)"
1233,"def list_log(args, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name, container_name)","def list_logs(args, container_name=None): from sregistry.main import Client as cli if len(args.commands) > 0: container_name = args.commands.pop(0) cli.logs(container_name) sys.exit(0)"
1234,def show_all_diseases(self): self.diseases(self.diseases) self.diseases(self.diseases) self.diseases(self.diseases) self.diseases) self.diseases(self.diseases) self.diseases) self.diseases(self.diseases) self.diseases) self.diseases(self.diseases) self.diseases) self.diseases,"def diseases(context): LOG.info(""Running scout view diseases"") adapter = context.obj['adapter'] disease_objs = adapter.disease_terms() nr_diseases = disease_objs.count() if nr_diseases == 0: click.echo(""No diseases found"") else: click.echo(""Disease"") for disease_obj in adapter.disease_terms(): click.echo(""0"".format(disease_obj['_id'])) LOG.info(""0 diseases found"".format(nr_diseases))"
1235,def save_progress_data(self._progress_data): if self._progress_data(self._progress_data): if self._progress_data(self._progress_data): if self._progress_data(self._progress_data): if self._progress_data(self._progress_data): if self._progress_data(self._progress_data): if self._progress_data): if self._progress_data(self._progress_data): if self._progress_data(self._progress_data),"def _store_model_progress(self, res, now): raw_progress, delay = res raw_progress = clamp(raw_progress, 0, self._maxval) self._progress_data.append((now, raw_progress)) if delay  0: delay = self._guess_next_poll_interval() self._next_poll_time = now + clamp(delay, self.MIN_PROGRESS_CHECK_INTERVAL, self.MAX_PROGRESS_CHECK_INTERVAL)"
1236,def get_ordered_dict(self): self.get_ordered_dict(self.get_ordered_dict) self.get_ordered_dict(self.get_ordered_dict) self.get_ordered_dict(self.get_ordered_dict) self.get_ordered_dict(self.get_ordered_dict) self.get_ordered_dict(self.get_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_ordered_,async def receive(self): try: incomming = await self.reader.read(self.buffer_size) except OSError: return [] return _parse_receive(incomming)
1237,"def build_institute(self, internal_id, display_name, sanger_recipients, display_name, display_name, sanger_recipients, display_name, display_name, display_name, sanger_recipients, display_name, display_name, display_name, display_name, display_name, sanger_recipients, display_name, display_name, display_name, sanger_recipients, display_name, sanger_recipients, sanger_recipients, sanger_recipients, sanger_recipients)","def build_institute(internal_id, display_name, sanger_recipients=None, coverage_cutoff=None, frequency_cutoff=None): LOG.info(""Building institute %s with display name %s"", internal_id,display_name) institute_obj = Institute( internal_id=internal_id, display_name=display_name, sanger_recipients=sanger_recipients, coverage_cutoff = coverage_cutoff, frequency_cutoff = frequency_cutoff ) for key in list(institute_obj): if institute_obj[key] is None: institute_obj.pop(key) return institute_obj"
1238,def find_previous_siblings(self): if self.get_previous_siblings(self.get_previous_siblings(self.get_previous_siblings)): self.get_previous_siblings(self.get_previous_siblings),"def find_previous_sibling(self, *args, **kwargs): op = operator.methodcaller('find_previous_sibling', *args, **kwargs) return self._wrap_node(op)"
1239,"def var_find(self, symbol): if var.find(self, symbol) var.find(self, symbol) var.find(self, symbol) var.find(self, symbol) var.find(self, symbol) var.find(self, symbol)","def __var_find_to_py_ast( var_name: str, ns_name: str, py_var_ctx: ast.AST ) -> GeneratedPyAST: return GeneratedPyAST( node=ast.Attribute( value=ast.Call( func=_FIND_VAR_FN_NAME, args=[ ast.Call( func=_NEW_SYM_FN_NAME, args=[ast.Str(var_name)], keywords=[ast.keyword(arg=""ns"", value=ast.Str(ns_name))], ) ], keywords=[], ), attr=""value"", ctx=py_var_ctx, ) )"
1240,"def jQueryDataTables_ThemeRoller(self, version): jQueryDataTables_ThemeRoller = self.jQueryDataTables_ThemeRoller.jQueryDataTables_ThemeRoller.jQueryDataTables_ThemeRoller.jQueryDataTables_ThemeRoller.jQueryDataTables_ThemeRoller.jQueryDataTables_ThemeRoller.jQueryDataTables_ThemeRoller.jQueryDataTables_ThemeRoller.jQueryDataTables_ThemeRoller.jQueryDataTables.jQueryDataTables.jQueryDataTables.jQueryDataTables.jQueryDataTables.jQueryDataTables.jQueryDataTables.jQueryDataTables.jQueryDataTables.jQueryDataTables.jQueryDataTables.jQueryDataTables.jQueryDataTables.jQueryDataTables.jQueryDataTables.jQueryDataTables.jQueryDataTables.jQueryDataTables.jQueryDataTables.jQueryDataTables.jQueryDataTables.jQueryDataTables.jQueryDataTables.jQueryDataTables.jQueryDataTables.jQueryDataTables.jQueryDataTables.jQueryDataTables","def djfrontend_jquery_datatables_themeroller(version=None): if version is None: if not getattr(settings, 'DJFRONTEND_JQUERY_DATATABLES_THEMEROLLER', False): version = getattr(settings, 'DJFRONTEND_JQUERY_DATATABLES_VERSION', DJFRONTEND_JQUERY_DATATABLES_VERSION_DEFAULT) else: version = getattr(settings, 'DJFRONTEND_JQUERY_DATATABLES_THEMEROLLER', DJFRONTEND_JQUERY_DATATABLES_VERSION_DEFAULT) return format_html( 'link rel=""stylesheet"" href=""href=""staticdjfrontend/css/jquery/jquery.dataTables/v/jquery.dataTables_themeroller.min.css"">', static=_static_url, v=version)"
1241,"def draw_layer(self, layer): if layer is not self.redraw(layer): self.redraw(layer) self.redraw(layer) self.redraw(layer) self.redraw(layer) self.redraw(layer) self.redraw(layer) self.redraw(layer) self.redraw(layer) self.redraw(layer) self.redraw(layer) self.redraw(layer) self.redraw(layer) self.redraw(layer) self.redraw(layer)","def redraw_layer(self,name): if name not in self._layers: raise ValueError(""Layer %s not part of widget, cannot redraw"") self._layers[name].on_redraw()"
1242,"def get_sift_predictions(self, genes): self.sift_predictions(self.sift_predictions) self.sift_predictions(self.sift_predictions) self.sift_predictions(self.sift_predictions) self.sift_predictions(self.sift_predictions) self.sift_predictions(self.sift_predictions) self.sift_predictions) self.sift_predictions(self.sift_predictions) self.sift_predictions(self.sift_predictions) self.sift_predictions) self.sift_predictions(self.sift_predictions) self.sift_predictions) self.sift_predictions(self.sift_predictions) self.sift_predictions(self.sift_predictions) self.sift_predictions) self.sift_predictions(self.sift_predictions) self.sift_predictions(self.sift_predictions) self.","def get_predictions(genes): data = 'sift_predictions': [], 'polyphen_predictions': [],'region_annotations': [], 'functional_annotations': []  for gene_obj in genes: for pred_key in data: gene_key = pred_key[:-1] if len(genes) == 1: value = gene_obj.get(gene_key, '-') else: gene_id = gene_obj.get('hgnc_symbol') or str(gene_obj['hgnc_id']) value = ':'.join([gene_id, gene_obj.get(gene_key, '-')]) data[pred_key].append(value) return data"
1243,"def get_curves(self, lib, cls): self.get_curves(cls) self.get_curves(cls) self.get_curves(cls) self.get_curves(cls) self.get_curves(cls) self.get_curves(cls) self.get_curves(cls) self.get_curves(cls) self.get_curves(cls) self.get_curves(cls) self.get_curves(cls) self.get_curves(cls) self.get_curves(cls) self.get_curves(cls) self.get_curves(cls) self.get_curves(cls) self.get_curves(cls) self.get_curves(cls) self.get_curves(cls) self.get_curves(cls) self.get_curves(cls) self.get_curves(cls) self.get_curves(cls) self.get_","def _get_elliptic_curves(cls, lib): if cls._curves is None: cls._curves = cls._load_elliptic_curves(lib) return cls._curves"
1244,def kwargs(in-place): kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwargs = kwarg,"def _add_column_kwargs(self, kwargs, column): if column.nullable: kwargs[""allow_none""] = True kwargs[""required""] = not column.nullable and not _has_default(column) if hasattr(column.type, ""enums""): kwargs[""validate""].append(validate.OneOf(choices=column.type.enums)) if hasattr(column.type, ""length""): try: python_type = column.type.python_type except (AttributeError, NotImplementedError): python_type = None if not python_type or not issubclass(python_type, uuid.UUID): kwargs[""validate""].append(validate.Length(max=column.type.length)) if hasattr(column.type, ""scale""): kwargs[""places""] = getattr(column.type, ""scale"", None)"
1245,"def get_modules(project_abspath, packages_scan, package_scan, package_scan, package_scan, package_scan, package_scan, package_scan, project_abspath, package_scan, package_scan, package_scan, package_scan, project_abspath, package_scan, package_scan, package_scan, package_scan, package_scan, package_scan, package_scan, package_scan_modules_scan_modules_scan_modules_scan_modules_modules_modules_modules_modules_modules_modules_modules_modules_modules_modules_modules_modules_modules_modules_modules_modules_modules_modules_modules_modules_modules_modules_modules_modules_modules_modules_modules_modules_modules_modules_modules_modules_modules_modules_modules_modules_modules_modules_modules_","def get_modules(self): if not self.project_abspath: raise TypeError(""project_abspath can not be empty."") packages_abspath = self.get_package_abspath() for package_abspath in packages_abspath: self.get_module_name(package_abspath) return self._modules"
1246,def delete_video(self): Delete_video(self): Delete_video(self): Delete_video(self): Delete_video(self): Delete_video(self),"def delete(self, *args, **kwargs): api = Api() api.authenticate() api.delete_video(self.video_id) return super(Video, self).delete(*args, **kwargs)"
1247,"def get_video(self, language): if self.get_video(language): self.get_video(language): self.get_video(language)","def videos(self, **kwargs): path = self._get_series_id_season_number_path('videos') response = self._GET(path, kwargs) self._set_attrs_to_values(response) return response"
1248,"def scale(self, identity_multiplier, diag, shift, dtype, validate_args, dtype, scale, dtype, scale, dtype, dtype, scale, identity_multiplier, identity_multiplier, identity_multiplier, shift, scale, identity_multiplier, identity_multiplier, identity_multiplier, diag, shift, scale, dtype, validate_args, validate_args, scale, scale, scale, scale, dtype, scale, dtype, dtype, scale, scale, dtype, scale, dtype, dtype, dtype, dtype, dtype, dtype, identity_multiplier, identity_multiplier, identity_multiplier, identity_multiplier, identity_multiplier, identity_multiplier, identity_multiplier, identity_multiplier, identity_multiplier, identity_multiplier, identity_multiplier, identity_multiplier, identity_multiplier, identity_multiplier, identity_multiplier, identity_multiplier, identity_multiplier, identity_multiplier, identity_multiplier, identity_multiplier, identity_multiplier, identity_multiplier, identity_","def _create_scale_operator(self, identity_multiplier, diag, tril, perturb_diag, perturb_factor, shift, validate_args, dtype): identity_multiplier = _as_tensor(identity_multiplier, ""identity_multiplier"", dtype) diag = _as_tensor(diag, ""diag"", dtype) tril = _as_tensor(tril, ""tril"", dtype) perturb_diag = _as_tensor(perturb_diag, ""perturb_diag"", dtype) perturb_factor = _as_tensor(perturb_factor, ""perturb_factor"", dtype) shape_hint = None if perturb_factor is not None: shape_hint = distribution_util.dimension_size(perturb_factor, axis=-2) if self._is_only_identity_multiplier: if validate_args: return distribution_util.with_dependencies([ assert_util.assert_none_equal( identity_multiplier, tf.zeros([], identity_multiplier.dtype), [""identity_multiplier should be non-zero.""]) ], identity_multiplier) return identity_multiplier scale = distribution_util.make_tril_scale( loc=shift, scale_tril=tril, scale_diag=diag, scale_identity_multiplier=identity_multiplier, validate_args=validate_args, assert_positive=False, shape_hint=shape_hint) if perturb_factor is not None: return tf.linalg.LinearOperatorLowRankUpdate( scale, u=perturb_factor, diag_update=perturb_diag, is_diag_update_positive=perturb_diag is None, is_non_singular=True, is_self_adjoint=True, is_positive_definite=True, is_square="
1249,def find_run(self): if self.get_run(self.get_run(self.get_run(self.get_run(self.get_run(self.get_run(self.get_run(self.get_run(self.get_run(self.get_run(self.get_run(self.get_run(self.get_run(self.get_run(self.get_run(self.get_run(self.get_run(self.get_run(self.get_run(self.get_run(self.get_run))))))))))),"def _split_runs_on_parameters(runs): def _is_dagnode_parameterized(node): return any(isinstance(param, Parameter) for param in node.op.params) out = [] for run in runs: groups = groupby(run, _is_dagnode_parameterized) for group_is_parameterized, gates in groups: if not group_is_parameterized: out.append(list(gates)) return out"
1250,"def format_string(fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string): fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string, fmat_string","def format_string(self, fmat_string): try: return fmat_string.format(**vars(self)) except KeyError as e: raise ValueError('Invalid format string: 0. Instance has no''attribute 1.'.format(repr(fmat_string), repr(e)))"
1251,"def get_queue(self, queue): if self.queue(queue): return self.queue(queue)","def Queue(self, maxsize=0, reducers=None): from.queues import Queue return Queue(maxsize, reducers=reducers, ctx=self.get_context())"
1252,def respond_to_execute(self): if self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute))))))))))))),"def _process_executor_events(self, simple_dag_bag, session=None): TI = models.TaskInstance for key, state in list(self.executor.get_event_buffer(simple_dag_bag.dag_ids).items()): dag_id, task_id, execution_date, try_number = key self.log.info( ""Executor reports execution of %s.%s execution_date=%s "" ""exited with status %s for try_number %s"", dag_id, task_id, execution_date, state, try_number ) if state == State.FAILED or state == State.SUCCESS: qry = session.query(TI).filter(TI.dag_id == dag_id, TI.task_id == task_id, TI.execution_date == execution_date) ti = qry.first() if not ti: self.log.warning(""TaskInstance %s went missing from the database"", ti) continue if ti.try_number == try_number and ti.state == State.QUEUED: msg = (""Executor reports task instance  finished () "" ""although the task says its. Was the task "" ""killed externally?"".format(ti, state, ti.state)) self.log.error(msg) try: simple_dag = simple_dag_bag.get_dag(dag_id) dagbag = models.DagBag(simple_dag.full_filepath) dag = dagbag.get_dag(dag_id) ti.task = dag.get_task(task_id) ti.handle_failure(msg) except Exception: self."
1253,"def expand_node(self, node): if self.node(node): self.node(node): self.node(node): self.node(node): self.node(node): self.node(node): self.node(node): self.node(node): self.node(node)","def expanded_sequence(self, node, include_additional_constant_sites=False): if include_additional_constant_sites: L = self.seq_len else: L = self.seq_len - self.additional_constant_sites return node.cseq[self.full_to_reduced_sequence_map[:L]]"
1254,"def gsea_plot(self, pheno_pos, pheno_neg, pval, pval, fdr, fdr, fdr, fdr, figsize, ofname, ofname, ofname, ofname, ofname, ofname, ofname, ofname, ofname, fdr, fdr, fdr, fdr, fdr): fdr, fdr, fdr, fdr, fdr, fdr, fdr, fdr, fdr, fdr, fdr, fdr, fdr, fdr, fdr, fdr, fdr, fdr, fdr, fdr, fdr, fdr, fdr, fdr, fdr, fdr, fdr, fdr, fdr, fdr, fdr, fdr, fdr, fdr, fdr, fdr, fdr, f","def gseaplot(rank_metric, term, hits_indices, nes, pval, fdr, RES, pheno_pos='', pheno_neg='', figsize=(6,5.5), cmap='seismic', ofname=None, **kwargs): norm = _MidpointNormalize(midpoint=0) #dataFrame of ranked matrix scores x = np.arange(len(rank_metric)) rankings = rank_metric.values phenoP_label = pheno_pos +'(Positively Correlated)' phenoN_label = pheno_neg +'(Negatively Correlated)' zero_score_ind = np.abs(rankings).argmin() z_score_label = 'Zero score at'+ str(zero_score_ind) nes_label = 'NES: '+ "":.3f"".format(float(nes)) pval_label = 'Pval: '+ "":.3f"".format(float(pval)) fdr_label = 'FDR: '+ "":.3f"".format(float(fdr)) im_matrix = np.tile(rankings, (2,1)) plt.rcParams.update('pdf.fonttype':42,'ps.fonttype':42) gs = plt.GridSpec(16,1) if hasattr(sys, 'ps1') and (ofname is None): fig = plt.figure(figsize=figsize) else: fig = Figure(figsize=figsize) canvas = FigureCanvas(fig) ax1 = fig.add_subplot(gs[11:]) module = 'tmp' if ofname is None else ofname.split(""."
1255,"def convert_user_input(self, name, value, allowed_object_type, allowed_object_type, allowed_object_type, allowed_object_type, allowed_object_type, allowed_object_type, allowed_object_type, allowed_object_type, allowed_object_type, allowed_object_type, allowed_object_type, allowed_object_type, allowed_object_type, allowed_object_type, allowed_object_type, allowed_object_type, allowed_object_type, allowed_object_type)","def convert_to_vcard(name, value, allowed_object_type): if isinstance(value, str): if allowed_object_type == ObjectType.list_with_strings: raise ValueError( ""Error: "" + name + "" must not contain a single string."") else: return value.strip() elif isinstance(value, list): if allowed_object_type == ObjectType.string: raise ValueError( ""Error: "" + name + "" must not contain a list."") else: for entry in value: if not isinstance(entry, str): raise ValueError( ""Error: "" + name + "" must not contain a nested list"") return [x.strip() for x in value if x] else: if allowed_object_type == ObjectType.string: raise ValueError( ""Error: "" + name + "" must be a string."") elif allowed_object_type == ObjectType.list_with_strings: raise ValueError( ""Error: "" + name + "" must be a list with strings."") else: raise ValueError( ""Error: "" + name + "" must be a string or a list with strings."")"
1256,"def iterate_over_all_available_recipes(self, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe_name, recipe","def list_recipes(full=False): logger.info(colored_print( ""n===== L I S T O F R E C I P E S =====n"", ""green_bold"")) prefix = ""."".format(recipes.__name__) for importer, modname, _ in pkgutil.iter_modules(recipes.__path__, prefix): _module = importer.find_module(modname).load_module(modname) _recipe_classes = [cls for cls in _module.__dict__.values() if isinstance(cls, type)] for cls in _recipe_classes: recipe_cls = cls() if hasattr(recipe_cls, ""name""): logger.info(colored_print(""=> "".format(recipe_cls.name), ""blue_bold"")) if full: logger.info(colored_print(""t "".format(recipe_cls.__doc__), ""purple_bold"")) logger.info(colored_print(""Pipeline string: n"".format(recipe_cls.pipeline_str), ""yellow_bold"")) sys.exit(0)"
1257,"def get_strings(self, tok_hash, tok_hash, tok_hash, tok_hash, tok_hash, tok_hash, tok_hash, tok_hash, tok_hash, tok_hash, tok_hash, tok_hash, tok_hash, tok_hash, tok_hash, tok_hash, tok_hash, tok_hash_hash_hash_hash_hash_hash_hash_hash_hash_hash_hash_hash_hash_hash_hash_hash_hash_hash_hash_hash_hash_hash_hash_hash_hash_hash_hash_hash_hash_hash_hash_hash_hash_hash_hash_hash_hash___________________________________________________________________________","def invert_hash(self, tok_hash): return [tok_encoded.decode('utf8') for (_, tok_encoded) in self.client.scan_keys(HASH_KEYWORD_INDEX_TABLE, ((tok_hash,), (tok_hash,)))]"
1258,"def aitoff_projection(self, alpha, delta, polar angle, x, y, x, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y","def add_virtual_columns_aitoff(self, alpha, delta, x, y, radians=True): transform = """" if radians else ""*pi/180."" aitoff_alpha = ""__aitoff_alpha_%s_%s"" % (alpha, delta) aitoff_alpha = re.sub(""[a-zA-Z_]"", ""_"", aitoff_alpha) self.add_virtual_column(aitoff_alpha, ""arccos(cos(deltatransform)*cos(alphatransform/2))"".format(**locals())) self.add_virtual_column(x, ""2*cos(deltatransform)*sin(alphatransform/2)/sinc(aitoff_alpha/pi)/pi"".format(**locals())) self.add_virtual_column(y, ""sin(deltatransform)/sinc(aitoff_alpha/pi)/pi"".format(**locals()))"
1259,"def expdecay_despiker(self.despiked, noise_despiker, exponentplot, maxiter, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponentplot, exponent","def despike(self, expdecay_despiker=False, exponent=None, noise_despiker=True, win=3, nlim=12., exponentplot=False, maxiter=4, autorange_kwargs=, focus_stage='rawdata'): if focus_stage!= self.focus_stage: self.set_focus(focus_stage) if expdecay_despiker and exponent is None: if not hasattr(self, 'expdecay_coef'): self.find_expcoef(plot=exponentplot, autorange_kwargs=autorange_kwargs) exponent = self.expdecay_coef time.sleep(0.1) with self.pbar.set(total=len(self.data), desc='Despiking') as prog: for d in self.data.values(): d.despike(expdecay_despiker, exponent, noise_despiker, win, nlim, maxiter) prog.update() self.stages_complete.update(['despiked']) self.focus_stage = 'despiked' return"
1260,"def list_all_profiles(self, ipython_dir, cwd, ipython_dir, cwd, ipython_dir, cwd, ipython_dir, cwd, ipython_dir, cwd, ipython_dir, cwd, ipython_dir, cwd, ipython_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile_profile","def update_profiles(self): for path in [get_ipython_dir(), os.getcwdu()]: for profile in list_profiles_in(path): pd = self.get_profile_dir(profile, path) if profile not in self.profiles: self.log.debug(""Adding cluster profile '%s'"" % profile) self.profiles[profile] =  'profile': profile, 'profile_dir': pd,'status':'stopped'"
1261,def redo_selection_name(self): if self.selection_name(self.selection_name): self.selection_name = self.selection_name(self.selection_name),"def selection_can_redo(self, name=""default""): return (self.selection_history_indices[name] + 1)  len(self.selection_histories[name])"
1262,"def compute_height(height,round): if hex = 0 hex = 0 hex = 0 hex = 0 hex = 0 hex = 0 hex = 0 hex = 0 hex = 0 hex = 0 hex = 0 hex = 0 hex = 0 hex = 0 hex = 0 hex = 0 hex = 0 hex = 0 hex = 0 hex = hex = hex = hex = hex = hex = hex = hex = hex = hex = hex = hex = hex = hex = hex = hex = hex = hex = hex = hex = hex = hex = hex = hex = hex = hex = hex = hex = hex = hex = hex = hex = hex = hex = hex = hex = hex = hex = hex = hex = hex = hex = he","def hr(self): assert len(self), 'no votes, can not determine height' h = set([(v.height, v.round) for v in self.votes]) assert len(h) == 1, len(h) return h.pop()"
1263,"def _filter_path(self, path): return self.filter_path(path)","def get_filtered_path(path_to_image, filename_key, storage): containing_folder, filename = os.path.split(path_to_image) filtered_filename = get_filtered_filename(filename, filename_key) path_to_return = os.path.join(*[ containing_folder, VERSATILEIMAGEFIELD_FILTERED_DIRNAME, filtered_filename ]) path_to_return = path_to_return.replace(' ', '') return path_to_return"
1264,"def process_info_line(self, INFO): INFO = self.________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________","def _process_info(self, info): if 'connect_urls' in info: if info['connect_urls']: connect_urls = [] for connect_url in info['connect_urls']: uri = urlparse(""nats://%s"" % connect_url) srv = Srv(uri) srv.discovered = True should_add = True for s in self._server_pool: if uri.netloc == s.uri.netloc: should_add = False if should_add: connect_urls.append(srv) if self.options[""dont_randomize""] is not True: shuffle(connect_urls) for srv in connect_urls: self._server_pool.append(srv)"
1265,def backup_current_execution_state(self): self.execution_state(self.execution_state) self.execution_state(self.executionstate) self.executionstate(self.executionstate) self.executionstate(self.executionstate) self.executionstate(self.executionstate) self.executionstate(self.executionstate) self.executionstate(self.executionstate) self.executionstate(self.executionstate) self.executionstate) self.executionstate) self.executionstate(self.executionstate) self.executionstate(self.executionstate) self.executionstate) self.executionstate(self.executionstate) self.executionstate) self.executionstate(self.executionstate) self.executionstate) self.executionstate(self.executionstate) self.executionstate) self.executionstate(self.executionstate) self.executionstate(self.executionstate) self.executionstate(self.executionstate) self.executionstate) self.executionstate(self,"def backup(self): if PyFunceble.CONFIGURATION[""auto_continue""]: data_to_backup =  configuration_counter = PyFunceble.INTERN[""counter""][""number""] data_to_backup[PyFunceble.INTERN[""file_to_test""]] =  ""tested"": configuration_counter[""tested""], ""up"": configuration_counter[""up""], ""down"": configuration_counter[""down""], ""invalid"": configuration_counter[""invalid""],  to_save =  to_save.update(self.backup_content) to_save.update(data_to_backup) Dict(to_save).to_json(self.autocontinue_log_file)"
1266,"def extract_bounds_json(self, bounds): self.get_bounds_json(self.get_bounds_json) self.get_bounds_json(self.get_bounds_json) self.get_bounds_json(self.get_bounds_json) self.get_bounds_json(self.get_bounds_json) self.get_bounds_json(self.get_bounds_json) self.get_bounds_json) self.get_bounds_json(self.get_bounds_json) self.get_bounds_json(self.get_bounds_json) self.get_bounds_json(self.get_bounds_json) self.get_bounds_json(self.get_bounds_json) self.get_bounds_json(self.get_bounds_json) self.get_bounds_json(self.get_bounds_json) self.get_bounds_json(self.get_bounds_json) self.get_bounds_json(self.get_bounds_json) self.get_bounds_json(self.get","def json_bounds_to_bounds(bounds_json): f = open(bounds_json) segments = json.load(f)[""bounds""] bounds = [] for segment in segments: bounds.append(segment[""start""]) f.close() return np.asarray(bounds)"
1267,"def set_mainchannel(self, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, k","def set_channels(self, **kwargs): if not self.pid: self.pid = ""_"".format(self.lane, kwargs.get(""pid"")) for i in self.status_channels: if i.startswith(""STATUS_""): self.status_strs.append(""_"".format(i, self.pid)) else: self.status_strs.append(""STATUS__"".format(i, self.pid)) if self.main_forks: logger.debug(""Setting main fork channels: "".format( self.main_forks)) operator = ""set"" if len(self.main_forks) == 1 else ""into"" self.forks = [""n.  n"".format( self.output_channel, operator, "";"".join(self.main_forks))] self._context = **kwargs, **""input_channel"": self.input_channel, ""output_channel"": self.output_channel, ""template"": self.template, ""forks"": ""n"".join(self.forks), ""pid"": self.pid"
1268,def _read_line(self): if self.read_line(self.read_line): self.read_column(self.read_column): self.read_column(self.read_column): self.read_column(self.read_column): self.read_column(self.read_column): self.read_column(self.read_column),"def _with_loc(f: W) -> W: @functools.wraps(f) def with_lineno_and_col(ctx): meta = lmap.map( READER_LINE_KW: ctx.reader.line, READER_COL_KW: ctx.reader.col ) v = f(ctx) try: return v.with_meta(meta) except AttributeError: return v return cast(W, with_lineno_and_col)"
1269,"def generate_team_memberships(self, teamId, max, request_parameters): self.get_team_memberships(self.get_team_memberships) self.get_team_memberships(self.get_team_memberships) self.get_team_memberships(self.get_team_memberships) self.get_team_memberships(self.get_team_memberships) self.get_team_memberships(self.get_team_memberships) self.get_team_memberships) self.get_team_memberships(self.get_team_memberships) self.get_team_memberships(self.get_team_memberships) self.get_team_memberships(self.get_team_memberships) self.get_team_memberships) self.get_team_memberships(self.get_team_memberships) self.get_team_memberships(self.get_team_memberships) self.get_team_memberships(self.get_team_memberships) self.get_team_memberships(self.get_team_memberships) self.get_team_memberships(self.get_","def list(self, teamId, max=None, **request_parameters): check_type(teamId, basestring, may_be_none=False) check_type(max, int) params = dict_from_items_with_values( request_parameters, teamId=teamId, max=max, ) items = self._session.get_items(API_ENDPOINT, params=params) for item in items: yield self._object_factory(OBJECT_TYPE, item)"
1270,"def _get_item(self, item): if self.get_item(item): self.get_item(item): self.get_item(item): self.get_item(item)","def get_instance(self, data): unique_fields = self.unique_fields if not unique_fields: return self.model() filter = dict([(f, data[f]) for f in unique_fields]) try: instance = self.model._default_manager.get(**filter) except self.model.DoesNotExist: return self.model() return instance"
1271,"def _refresh(self, mode): if mode is not self.refresh(mode): if mode is not self.refresh(mode)","def _observe_mode(self, change): block = self.block if block and self.is_initialized and change['type'] == 'update': if change['oldvalue'] =='replace': raise NotImplementedError for c in self.children: block.children.remove(c) c.set_parent(None) self.refresh_items()"
1272,"def test_if_container_group(self, resource_group, name): if self.container_group(resource_group, name): self.container_group(resource_group, name): self.container_group(resource_group, name) self.container_group(resource_group, name) self.container_group(resource_group, name) self.container_group(resource_group, name) self.container_group(resource_group, name) self.container_group, name) self.container_group, name) self.container_group = self.container_group, self.container_group, self.container_group, self.container_group, self.container_group, self.container_group, self.container_group, self.container_group, self.container_group, self.container_group, self.container_group, self.container_group, self.container_group, self.container_group, self.container_group, self.container_group, self.container_group, self.container","def exists(self, resource_group, name): for container in self.connection.container_groups.list_by_resource_group(resource_group): if container.name == name: return True return False"
1273,"def load_unknown_fields(self, valid_data, many, original_data): if self.load(self.load()): self.load(self.load()) self.load(self.load()) self.load(self.load()) self.load(self.load()) self.load(self.load()) self.load(self.load()) self.load(self.load()) self.load(self.load()) self.load(self.load()) self.load(self.load()) self.load(self.load()) self.load(self.load()) self.load(self.load()) self.load(self.load()) self.load(self.load()) self.load(self.load()) self.load(self.load()) self.load(self.load()) self.load(self.load()) self.load(self.load()) self.load(self.load()) self.load(self.load()) self.load(self.load()) self.load(self.load()) self.load() self.load() self.load() self.load() self.load() self.load() self.load() self.load() self.load() self.load() self.load() self.load()","def load_additional_data(self, valid_data, many, original_data): if many: for i, _ in enumerate(valid_data): additional_keys = set(original_data[i]) - set(valid_data[i]) for key in additional_keys: valid_data[i][key] = original_data[i][key] else: additional_keys = set(original_data) - set(valid_data) for key in additional_keys: valid_data[key] = original_data[key] return valid_data"
1274,def encode_arg(arg): arg = encode_arg(arg): arg = encode_arg(arg) arg = encode_arg(arg) arg = encode_arg(arg),"def encode_arg(arg): arg_utf8 = utf8(arg) return ELEM_SEP.join([str(len(str(arg_utf8))), str(arg_utf8)])"
1275,"def straight_ends(radius_limit, radius_limit, axis z, straight_ends, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, axis_z, straight_ends, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius_limit, radius","def bend(script, radius=1, pitch=0, taper=0, angle=0, straght_start=True, straght_end=False, radius_limit=None, outside_limit_end=True): vert_function(script, x_func=x_func, y_func=y_func, z_func=z_func) return None"
1276,"def unfreeze_module(names): if name is None, unfreeze_module(names): if name is None: unfreeze_module(names): unfreeze_module(names): unfreeze_module(names): unfreeze_module(names): unfreeze_module(names): unfreeze_module(names): unfreeze_module(names): unfreeze_module(names): unfreeze_module(names): unfreeze_module(names): unfreeze_module(names): unfreeze_module(names): unfreeze_module(names): unfreeze_module(names): unfreeze_module(names): unfreeze_module(names): unfreeze_module(names): unfreeze_module(names): unfreeze_module(names): unfreeze_module(names): unfreeze_module(names): unfreeze_module(names): unfreeze_module(names): unfreeze_module(names)","def unfreeze(self, names=None): callBigDlFunc(self.bigdl_type, ""unFreeze"", self.value, names) return self"
1277,def filename': if 'filename' is not 'filename': if 'filename' is 'filename': if 'filename' is 'filename': if 'filename' is 'filename': if 'filename' is 'filename': if 'filename' is 'filename': if 'filename' is 'filename': if 'filename' = 'filename' = 'filename' = 'filename' = 'filename' = 'filename' = 'filename' = 'filename' = 'filename' = 'filename' = 'filename' = 'filename' = 'filename' = 'filename' = 'filename' = 'filename' = 'filename' = 'filename' = 'filename' = 'filename' = 'filename = 'filename = 'filename = 'filename = 'filename = 'filename = 'filename = 'filename = 'filename = 'filename = 'filename = 'filename = 'filename = 'filename =,"def canonic(self, filename): if filename == """" + filename[1:-1] + "">"": return filename canonic = self.filename_cache.get(filename) if not canonic: lead_dir = filename.split(os.sep)[0] if lead_dir == os.curdir or lead_dir == os.pardir: canonic = os.path.abspath(os.path.join(self.main_dirname, filename)) else: canonic = os.path.abspath(filename) pass if not os.path.isfile(canonic): canonic = Mclifns.search_file(filename, self.search_path, self.main_dirname) if not canonic: canonic = filename pass canonic = os.path.realpath(os.path.normcase(canonic)) self.filename_cache[filename] = canonic return canonic"
1278,"def _repair_data_buffer(path, lengthof, sizeof): if path is not provided, if path is not provided, return _repair_data_buffer(path, lengthof, sizeof, sizeof)","def new_junction_reparse_buffer(path=None): if path is None: substnamebufferchars = 8000 else: substnamebufferchars = len(path) + 1 class REPARSE_DATA_BUFFER(ctypes.Structure): _fields_ = [(""ReparseTag"", ctypes.c_ulong), (""ReparseDataLength"", ctypes.c_ushort), (""Reserved"", ctypes.c_ushort), (""SubstituteNameOffset"", ctypes.c_ushort), (""SubstituteNameLength"", ctypes.c_ushort), (""PrintNameOffset"", ctypes.c_ushort), (""PrintNameLength"", ctypes.c_ushort), (""SubstituteNameBuffer"", ctypes.c_wchar * substnamebufferchars), (""PrintNameBuffer"", ctypes.c_wchar * 1)] numpathbytes = (substnamebufferchars - 1) * sizeof(ctypes.c_wchar) buffersize = (numpathbytes + (sizeof(ctypes.c_wchar) * 2) + (sizeof(ctypes.c_ushort) * 4)) if path is None: buffer = REPARSE_DATA_BUFFER() buffer.ReparseTag = IO_REPARSE_TAG_MOUNT_POINT else: buffer = REPARSE_DATA_BUFFER( IO_REPARSE_TAG_MOUNT_POINT, buffersize, 0, 0, numpathbytes, numpathbytes + 2, 0, path, """") return (buffer, buffersize + REPARSE_DATA_BUFFER.SubstituteNameOffset.offset)"
1279,def get_variant(compound_info(str)): case_id(str): variant_type(str): variant_type(str): variant_type(str): variant_type(str): variant_type(str): variant_type(str)): variant_type(str): variant_type(str)): variant_type(str): variant_type(str): variant_type(str)): variant_type(str): variant_type(str): variant_type(str)): variant_type(str): variant_type(str): variant_type(str): variant_type(str)): variant_type(str): variant_type(str): variant_type(str): variant_type(str): variant_type(str): variant_type(str)): variant_type(str): variant_type(str): variant_type(str): variant_type(str): variant_type(str): variant_type(,"def parse_compounds(compound_info, case_id, variant_type): compounds = [] if compound_info: for family_info in compound_info.split(','): splitted_entry = family_info.split(':') if splitted_entry[0] == case_id: for compound in splitted_entry[1].split('|'): splitted_compound = compound.split('>') compound_obj =  compound_name = splitted_compound[0] compound_obj['variant'] = generate_md5_key(compound_name.split('_') + [variant_type, case_id]) try: compound_score = float(splitted_compound[1]) except (TypeError, IndexError): compound_score = 0.0 compound_obj['score'] = compound_score compound_obj['display_name'] = compound_name compounds.append(compound_obj) return compounds"
1280,"def remote_popen(self, cmd, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs","def Popen(self, cmd, **kwargs): masked_cmd =''.join(self.cmd_mask_password(cmd)) self.log.info(""Executing command: "".format(masked_cmd)) self.sp = subprocess.Popen( cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, **kwargs) for line in iter(self.sp.stdout): self.log.info(line.strip()) self.sp.wait() self.log.info(""Command exited with return code %s"", self.sp.returncode) if self.sp.returncode: raise AirflowException(""Sqoop command failed: "".format(masked_cmd))"
1281,"def _remove_field(self, field): if self.remove_field(self.remove_field): self.remove_field(self.remove_field)","def remove_field(self, model, field): for key in self._iterate_required_keys(field): self._drop_hstore_required( model._meta.db_table, field, key )"
1282,"def end_dialog(self, parent): return self.resume_dialog() self.resume_dialog() self.resume_dialog(parent)","async def end_dialog(self, result: object = None): await self.end_active_dialog(DialogReason.EndCalled) if self.active_dialog!= None: dialog = await self.find_dialog(self.active_dialog.id) if not dialog: raise Exception(""DialogContext.EndDialogAsync(): Can't resume previous dialog. A dialog with an id of '%s' wasn't found."" % self.active_dialog.id) return await dialog.resume_dialog(self, DialogReason.EndCalled, result) else: return DialogTurnResult(DialogTurnStatus.Complete, result)"
1283,"def read_first_header(self, header): self.get_header(header): self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_","def read_first_header(self): self.file_obj.seek(0) header_dict, pos = self.read_header() self.file_obj.seek(0) return header_dict"
1284,"def load_tree(self, fname, strip_comments, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw) kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw, kw,","def read(fname, encoding='utf8', strip_comments=False, **kw): kw['strip_comments'] = strip_comments with io.open(fname, encoding=encoding) as fp: return load(fp, **kw)"
1285,"def parse_info_table(self, table): if self.________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________","def parse_info_table(table): ret =  for tr in list(table('tr').not_('.thead').items()): th, td = list(tr('th, td').items()) key = th.text().lower() key = re.sub(r'W', '_', key) val = sportsref.utils.flatten_links(td) ret[key] = val return ret"
1286,"def save_images_to_png_file(x, height, width, x, x, x, x, x, x, x, fname, fname, fname, fname, fname, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname","def save_imgs(x, fname): n = x.shape[0] fig = figure.Figure(figsize=(n, 1), frameon=False) canvas = backend_agg.FigureCanvasAgg(fig) for i in range(n): ax = fig.add_subplot(1, n, i+1) ax.imshow(x[i].squeeze(), interpolation=""none"", cmap=cm.get_cmap(""binary"")) ax.axis(""off"") canvas.print_figure(fname, format=""png"") print(""saved %s"" % fname)"
1287,"def check_basenames(name, is_prefix, is_prefix, is_prefix, is_prefix, is_prefix, is_prefix, is_prefix, is_prefix, is_prefix, is_prefix, is_prefix, is_prefix, is_prefix, is_prefix, is_prefix, is_prefix, is_prefix, is_prefix, is_prefix, is_prefix, is_prefixes, is_prefixes, is_prefixes, is_prefixes)","def _find_basename(self, name, basenames, is_prefix=False): ret = """" fileroots = [(os.path.splitext(n)[0], n) for n in basenames] glob = False if name.startswith(""*""): glob = True name = name.strip(""*"") for fileroot, basename in fileroots: if name in fileroot or fileroot in name: for pf in self.module_postfixes: logger.debug( 'Checking if basename  starts with  and ends with '.format( basename, name, pf )) if glob: if name in fileroot and fileroot.endswith(pf): ret = basename break else: if fileroot.startswith(name) and fileroot.endswith(pf): ret = basename break if not ret: for pf in self.module_prefixes: n = pf + name logger.debug('Checking if basename  starts with '.format(basename, n)) if glob: if fileroot.startswith(pf) and name in fileroot: ret = basename break else: if fileroot.startswith(n): ret = basename break if not ret: if is_prefix: logger.debug('Checking if basename  starts with '.format(basename, name)) if basename.startswith(name) or (glob and name in basename): ret = basename else: logger.debug( 'Checking if basename  starts with  and is a test module'.format( basename, name )) if glob: if name in basename and self._is_module_path(basename): ret = basename else: if basename.startswith(name) and self._is_module_path"
1288,"def validate_args(amplitude, length_scale, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args,","def _init_params(self, amplitude, length_scale, validate_args): dtype = util.maybe_get_common_dtype( [amplitude, length_scale]) if amplitude is not None: amplitude = tf.convert_to_tensor( value=amplitude, name='amplitude', dtype=dtype) self._amplitude = _validate_arg_if_not_none( amplitude, tf.compat.v1.assert_positive, validate_args) if length_scale is not None: length_scale = tf.convert_to_tensor( value=length_scale, name='length_scale', dtype=dtype) self._length_scale = _validate_arg_if_not_none( length_scale, tf.compat.v1.assert_positive, validate_args) return dtype"
1289,"def option_group(self, option_group, option_group, option_group, option_group, option_group, option_group, option_group, option_group, option_group, option_group, option_group, option_group, option_group): option_group = option_group, option_group = option_group, option_group = option_group, option_group = option_group, option_group = option_group, option_group = option_group, option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_group = option_","def option_group(self, text): self._attempting(text) return concatenation([ ""["", self.expression, ""]"" ], ignore_whitespace=True)(text).retyped(TokenType.option_group)"
1290,def add right panel widgets: add right panel widgets: add right panel widgets: add right panel widgets: add right panel widgets: add right panel widgets: add right panel widgets: add right panel widgets: add right panel widgets: add right panel widgets: add right panel widgets: add right panel widgets: add right panel widgets: add right panel widgets: add right panel widgets: add right panel widgets: add right panel widgets: add right panel widgets: add right panel widget,"def add_info_widget(self, widget): if not self.screen: self.log.debug(""No screen instance to add widget"") else: self.screen.add_info_widget(widget)"
1291,"def write_data(table, reader, reader, reader, reader, reader, reader, reader, reader, reader, reader, reader, reader, reader, reader, reader, reader, reader, reader, reader, reader, reader, reader, reader, reader, reader, reader, reader, reader, reader, reader, reader, reader, reader, reader, reader, reader, reader, reader, reader, reader, reader","def write_contents(self, table, reader): %  'table_name': table.name, 'column_names': ', '.join(('""%s""' % col['name']) for col in table.columns)) if verbose: tt = time.time start_time = tt() prev_val_len = 0 prev_row_count = 0 for i, row in enumerate(reader.read(table), 1): row = list(row) pr(table, row) try: f_write(u'%sn' % (u't'.join(row))) except UnicodeDecodeError: f_write(u'%sn' % (u't'.join(r.decode('utf-8') for r in row))) if verbose: if (i % 20000) == 0: now = tt() elapsed = now - start_time val = '%.2f rows/sec [%s]'% ((i - prev_row_count) / elapsed, i) print_row_progress('%s%s' % ((""b"" * prev_val_len), val)) prev_val_len = len(val) + 3 start_time = now prev_row_count = i f_write("".nn"") if verbose: print('')"
1292,def dict(self): if self.config(self.config): self.config(self.config): self.config(self.config),"def device_settings(string): if not string: return  settings =  for setting in string.split(','): setting_name, value = setting.split('=') settings[setting_name.strip()] = value.strip() return settings"
1293,"def read_text_line(self, filename): read_text_line(filename): read_text_line(filename) read_text_line(filename) read_text_line(filename) read_text_line(filename) read_text_line(filename) read_text_line(filename) read_text_line(filename) read_text_line(filename) read_text_line(filename) read_text_line(filename) read_text_line(filename) read_line(filename) read_line(filename) read_line(filename) read_line(filename) read_line(filename) read_line(filename) read_line(filename) read_line(filename) read_line(filename) read_line(filename) read_line(filename) read_line(filename) read_line(filename) read_line(filename) read_line(filename) read_line(filename) read_line(filename) read_line(filename) read_line(filename) read_line(filename) read_line(filename) read_line(filename) read_line(filename) read_line(filename) read_line(filename) read",def read_lines_from_file(file_path: str) -> List[str]: with open(file_path) as csv_file: content = csv_file.readlines() return content
1294,"def strip_slashes_from_directories(self, directories): if self.slashes_from_directories(directories): self.slashes_from_directories(directories): self.slashes_from_directories(directories)","def _managers_changed(self, name, old, new): for key in new: if '/' in key: raise ValueError( ""Expected directory names w/o slashes. Got [%s]"" % key ) self.managers = k.strip('/'): v for k, v in new.items()"
1295,"def give_new_nodes(self, id): self.get_new_nodes(id): self.get_new_nodes(id): self.get_new_nodes(id): self.get_new_nodes(id)","def node_factory(**row_factory_kw): if ""__table_editor__"" in row_factory_kw: graph = row_factory_kw[""__table_editor__""].object ID = make_unique_name(""n"", [node.ID for node in graph.nodes]) del row_factory_kw[""__table_editor__""] return godot.node.Node(ID) else: return godot.node.Node(uuid.uuid4().hex[:6])"
1296,"def stmt(self, stmt): if self.stmt(self.stmt): self.stmt(self.stmt) self.stmt(self.stmt) self.stmt(self.stmt)","def in_for_else_branch(parent, stmt): return isinstance(parent, astroid.For) and any( else_stmt.parent_of(stmt) or else_stmt == stmt for else_stmt in parent.orelse )"
1297,"def stop_collect_trace_information(self, trace_information): if trace_information is not collected, stop_collect_trace_information(self, trace_information) if trace_information is not collected, stop_collect_trace_information(self, trace_information) if trace_information is not collected, stop_collect_trace_information(self, trace_information)","def stop(self): #print >>sys.stderr, ""Stopping: %r"" % self._collectors assert self._collectors assert self._collectors[-1] is self self.pause() self.tracers = [] self._collectors.pop() if self._collectors: self._collectors[-1].resume()"
1298,"def WhatsApp_input_validator(self, WhatsApp_input_validator, WhatsApp_input_validator, WhatsApp_input_validator, WhatsApp_input_validator, WhatsApp_input_validator, WhatsApp_input_validator, WhatsApp_input_validator, WhatsApp_input_validator, WhatsApp_input_validator, WhatsApp_input_validator, WhatsApp_input_validator, WhatsApp_input_validator, WhatsApp_input_validator, WhatsApp_input_validator, WhatsApp_input_validator)","def validate_whatsapp(attr, value): if attr in (""from_"", ""to""): if value is not None and ""whatsapp:"" in value: value = value.split(""whatsapp:+"")[-1] check_valid( ""WhatsApp"", attr, value, validus.isint, ""phone number starting with the '+' symbol"", ) elif attr in (""attachments""): check_valid(""WhatsApp"", attr, value, validus.isurl, ""url"")"
1299,"def return(self, int, float, dtype): if dtype is not int, dtype is not int, dtype is not int, dtype is int, dtype is int, dtype is int, dtype is int, dtype is int, dtype is int, dtype is int, dtype is int, dtype is int, dtype is int, dtype =",def get_autotype(arr): try: narr = arr.astype('float') if (narr  sys.maxsize).all() and (narr % 1).sum() == 0: return narr.astype('int') else: return narr except ValueError: return arr
1300,"def setup_vizio_media_player_platform(self, media_player_platform, media_player_platform, media_player_platform, media_player_platform, media_player_platform, media_player_platform, media_player_platform, media_player_platform, media_player_platform, media_player_platform, media_player_platform, media_player_platform, media_player_platform, media_player_platform, media_player_platform, media_player_platform, media_player_platform, media_player_platform, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_player, media_","def setup_platform(hass, config, add_entities, discovery_info=None): host = config.get(CONF_HOST) token = config.get(CONF_ACCESS_TOKEN) name = config.get(CONF_NAME) volume_step = config.get(CONF_VOLUME_STEP) device_type = config.get(CONF_DEVICE_CLASS) device = VizioDevice(host, token, name, volume_step, device_type) if device.validate_setup() is False: _LOGGER.error(""Failed to set up Vizio platform, "" ""please check if host and API key are correct"") return elif (token is None or token == """") and device_type == ""tv"": _LOGGER.error(""Failed to set up Vizio platform, "" ""if device_class is 'tv' then an auth_token needs "" ""to be provided, otherwise if device_class is "" ""'soundbar' then add the right device_class to config"") return if config.get(CONF_SUPPRESS_WARNING): from requests.packages import urllib3 _LOGGER.warning(""InsecureRequestWarning is disabled "" ""because of Vizio platform configuration"") urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning) add_entities([device], True)"
1301,"def CREATE_EVENT_INSTANCE(self, event_specification, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name, variable_name","def p_create_creator_event_statement(self, p): p[0] = CreateCreatorEventNode(variable_name=p[4], event_specification=p[6], key_letter=p[8])"
1302,"def read(self, raw_read, def_buf, def_buf, raw_read, def_buf, raw_read, def_buf, raw_read, def_buf, raw_read, def_buf, raw_read, def_buf, raw_read, def_buf, raw_read, def_buf, raw_read, def_buf, raw_read, def_buf, raw_read, def_buf, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw_read, raw","def crcMeterRead(self, raw_read, def_buf): try: if len(raw_read) == 0: ekm_log(""("" + self.m_context + "") Empty return read."") return False sent_crc = self.calc_crc16(raw_read[1:-2]) logstr = ""("" + self.m_context + "")CRC sent = "" + str(def_buf[""crc16""][MeterData.StringValue]) logstr += "" CRC calc = "" + sent_crc ekm_log(logstr) if int(def_buf[""crc16""][MeterData.StringValue], 16) == int(sent_crc, 16): return True except struct.error: ekm_log(str(sys.exc_info())) for frame in traceback.extract_tb(sys.exc_info()[2]): fname, lineno, fn, text = frame ekm_log(""Error in %s on line %d"" % (fname, lineno)) return False except TypeError: ekm_log(str(sys.exc_info())) for frame in traceback.extract_tb(sys.exc_info()[2]): fname, lineno, fn, text = frame ekm_log(""Error in %s on line %d"" % (fname, lineno)) return False except ValueError: ekm_log(str(sys.exc_info())) for frame in traceback.extract_tb(sys.exc_info()[2]): fname, lineno, fn, text = frame ekm_log(""Error in %s on line %d"""
1303,def fetch_mim_files(api_key): mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim_files = mim,"def fetch_mim_files(api_key, mim2genes=False, mimtitles=False, morbidmap=False, genemap2=False): LOG.info(""Fetching OMIM files from https://omim.org/"") mim2genes_url = 'https://omim.org/static/omim/data/mim2gene.txt' mimtitles_url= 'https://data.omim.org/downloads/0/mimTitles.txt'.format(api_key) morbidmap_url = 'https://data.omim.org/downloads/0/morbidmap.txt'.format(api_key) genemap2_url = 'https://data.omim.org/downloads/0/genemap2.txt'.format(api_key) mim_files =  mim_urls =  if mim2genes is True: mim_urls['mim2genes'] = mim2genes_url if mimtitles is True: mim_urls['mimtitles'] = mimtitles_url if morbidmap is True: mim_urls['morbidmap'] = morbidmap_url if genemap2 is True: mim_urls['genemap2'] = genemap2_url for file_name in mim_urls: url = mim_urls[file_name] mim_files[file_name] = fetch_resource(url) return mim_files"
1304,"def send_trace_statement(self, name, properties): self.trace_statement(self.trace_statement) self.trace_statement(self.trace_statement) self.trace_statement(self.trace_statement) self.trace_statement(self.trace_statement) self.trace_statement(self.trace_statement) self.trace_statement(self.trace_statement) self.trace_statement(self.trace_statement) self.trace_statement(self.trace_statement) self.trace_statement(self.trace_statement) self.trace_statement(self.trace_statement) self.trace_statement(self.trace_statement) self.trace_statement) self.trace_statement(self.trace_statement) self.trace_statement) self.trace_statement(self.trace_statement) self.trace_statement(self.trace_statement) self.trace_statement(self.trace_statement) self.trace_statement(self.trace_statement) self.trace_statement(self.","def track_trace(self, name: str, properties: Dict[str, object]=None, severity=None): self._client.track_trace(name, properties, severity)"
1305,"def non_silent(self, frame_length, hop_length, dtype=bool): non_silent(self, frame_length, hop_length, hop_length, dtype=bool) if self.__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________","def _signal_to_frame_nonsilent(y, frame_length=2048, hop_length=512, top_db=60, ref=np.max): y_mono = core.to_mono(y) mse = feature.rms(y=y_mono, frame_length=frame_length, hop_length=hop_length)**2 return (core.power_to_db(mse.squeeze(), ref=ref, top_db=None) > - top_db)"
1306,"def compile_pattern_list(self.compile_pattern_list, self.expect_list, self.compile_pattern_list, self.compile_pattern_list, self.compile_pattern_list, self.compile_pattern_list, self.compile_pattern_list, self.compile_pattern_list, self.compile_pattern_list, self.compile_pattern_list, self.compile_pattern_list, self.compile_pattern_list, self.compile_pattern_list, self.compile_pattern_list, self.compile_pattern_list, self.compile_pattern_list, self.compile_pattern_list, self.compile_pattern_list, self.compile_pattern_list, self.compile_pattern_list, self.compile_pattern_list, self.compile_pattern_list, self.compile_pattern_list, self.compile_pattern_list, self.compile_pattern_list, self.compile_pattern_list, self.compile_pattern_list, self.compile_pattern_list, self.compile_pattern_list,","def compile_pattern_list(self, patterns): if patterns is None: return [] if not isinstance(patterns, list): patterns = [patterns] compile_flags = re.DOTALL if self.ignorecase: compile_flags = compile_flags | re.IGNORECASE compiled_pattern_list = [] for p in patterns: if isinstance(p, (bytes, unicode)): p = self._cast_buffer_type(p) compiled_pattern_list.append(re.compile(p, compile_flags)) elif p is EOF: compiled_pattern_list.append(EOF) elif p is TIMEOUT: compiled_pattern_list.append(TIMEOUT) elif type(p) is re_type: p = self._prepare_regex_pattern(p) compiled_pattern_list.append(p) else: raise TypeError ('Argument must be one of StringTypes, EOF, TIMEOUT, SRE_Pattern, or a list of those type. %s' % str(type(p))) return compiled_pattern_list"
1307,"def find_settings(self, environment, secrets): if self.set(self.set_settings): self.set_settings(self.set_settings) self.set_settings(self.set_settings) self.set_settings(self.set_settings) self.set_settings(self.set_settings) self.set_settings(self.set_settings) self.set_settings(self.set_settings) self.set_settings(self.set_settings) self.set_settings(self.set_settings) self.set_settings(self.set_settings) self.set_settings(self.set_settings) self.set_settings(self.set_settings) self.set_settings) self.set_settings(self.set_settings) self.set_settings) self.set_settings(self.set_settings) self.set_settings(self.set_settings) self.set_settings(self.set_settings) self.set_settings) self.set_settings(self.set_settings) self.set_settings) self.set_settings(self.set_settings) self.set_setting","def get_setting(self, name, default=None): setting = os.environ.get(name) if setting is None: secrets = read_client_secrets() if self.client_name in secrets: secrets = secrets[self.client_name] if name in secrets: setting = secrets[name] if setting is None and default is not None: setting = default return setting"
1308,def create_schedule(self): self.schedule(self.schedule) self.schedule(self.schedule) self.schedule(self.schedule) self.schedule(self.schedule) self.schedule(self.schedule) self.schedule(self.schedule) self.schedule(self.schedule) self.schedule(self.schedule(self.schedule(self.schedule(self.schedule(self.schedule) self.schedule) self.schedule) self.schedule) self.schedule) self.schedule) self.schedule) self.schedule) self.schedule) self.schedule(self.schedule) self.schedule) self.schedule) self.schedule(self.schedule) self.schedule) self.schedule(self.schedule) self.schedule(self.schedule) self.schedule(self.schedule),"def _create_scheduled_query(self, query, change, scope_unit, scope_count): query_data = 'scheduled_query':  'name': 'ForAnomalyReport', 'query': query, 'threshold_type': '%', 'threshold_value': change, 'time_period': scope_unit.title(), 'time_value': scope_count,   query_url = 'https://logentries.com/rest/account_id/api/scheduled_queries' return self._api_post( url=query_url.format(account_id=self.account_id), data=json.dumps(query_data, sort_keys=True) )"
1309,"def reduce_scenario_set(self, scenarios, numpy.array, reduced_scenario_set, reduced_probability, reduced_scenario_set, reduced_probability, reduced_scenario_set, reduced_scenario_set, reduced_scenario_set, reduced_scenario_set, reduced_scenario_set, reduced_scenario_set, reduced_probability, reduced_scenario_set, reduced_scenario_set, reduced_scenario_set, reduced_scenario_set, reduced_scenario_set, reduced_scenario_set, reduced_scenario_set, reduced_scenario_set, reduced_scenario_set, reduced_scenario_set, reduced_scenario_set, reduced_scenario_set, reduced_scenario_set, reduced_scenario_set, reduced_scenario_set, reduced_scenario_set, reduced_scenario_set, reduced_scenario_set, reduced_scenario_set, reduced_scenario_set, reduced_scenario_","def simultaneous_backward_reduction(scenarios, number_of_reduced_scenarios, probability=None): for scenario_k in range(0, number_of_scenarios): for scenario_j in range(0, number_of_scenarios): c[scenario_k, scenario_j] = distance(scenarios[:, scenario_k], scenarios[:, scenario_j]) for scenario_l in range(0, number_of_scenarios): lowest_value = np.inf for scenario_j in range(0, number_of_scenarios): if scenario_l == scenario_j: continue lowest_value = min(lowest_value, c[scenario_l, scenario_j]) c[scenario_l, scenario_l] = lowest_value z[scenario_l] = probability[scenario_l]*c[scenario_l, scenario_l] J.append(np.argmin(z)) for _ in range(0, number_of_scenarios - number_of_reduced_scenarios - 1): for scenario_l in range(0, number_of_scenarios): for scenario_k in range(0, number_of_scenarios): if scenario_k in J or scenario_k == scenario_l: if scenario_l not in J: lowest_value = np.inf for scenario_j in range(0, number_of_scenarios): if scenario_j not in J and scenario_j!= scenario_l: lowest_value = min(lowest_value, c[scenario_k, scenario_j]) c[scenario_k, scenario_l] = lowest_value for scenario_l in range(0, number_of_scenarios): if scenario_l not in J: summation = 0 for scenario_k in range(0, number_of_"
1310,"def log_output(self, log_output): self.log_output(self.log_output) self.log_output(self.log_output) self.log_output(self.log_output) self.log_output(self.log_output) self.log_output(self.log_output) self.log_output(self.log_output) self.log_output(self.log_output) self.log_output(self.log_output) self.log_output(self.log_output) self.log_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_output_out","def log_output(self, format_dict): if self.shell.logger.log_output: self.shell.logger.log_write(format_dict['text/plain'], 'output') self.shell.history_manager.output_hist_reprs[self.prompt_count] =  format_dict['text/plain']"
1311,"def swap_slot(self, resource_group_name, target_slot): self.swap_slot(self.swap_slot) self.swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_swap_s","def swap_slot_slot( self, resource_group_name, name, slot, target_slot, preserve_vnet, custom_headers=None, raw=False, polling=True, **operation_config): raw_result = self._swap_slot_slot_initial( resource_group_name=resource_group_name, name=name, slot=slot, target_slot=target_slot, preserve_vnet=preserve_vnet, custom_headers=custom_headers, raw=True, **operation_config ) def get_long_running_output(response): if raw: client_raw_response = ClientRawResponse(None, response) return client_raw_response lro_delay = operation_config.get( 'long_running_operation_timeout', self.config.long_running_operation_timeout) if polling is True: polling_method = ARMPolling(lro_delay, **operation_config) elif polling is False: polling_method = NoPolling() else: polling_method = polling return LROPoller(self._client, raw_result, get_long_running_output, polling_method)"
1312,"def update_details(self, orgId, licenses): if self.request_parameters(self.request_parameters): self.request_parameters(self.request_parameters): self.request_parameters(self.request_parameters) self.request_parameters(self.request_parameters) self.request_parameters(self.request_parameters)","def update(self, personId, emails=None, displayName=None, firstName=None, lastName=None, avatar=None, orgId=None, roles=None, licenses=None, **request_parameters): check_type(emails, list) check_type(displayName, basestring) check_type(firstName, basestring) check_type(lastName, basestring) check_type(avatar, basestring) check_type(orgId, basestring) check_type(roles, list) check_type(licenses, list) put_data = dict_from_items_with_values( request_parameters, emails=emails, displayName=displayName, firstName=firstName, lastName=lastName, avatar=avatar, orgId=orgId, roles=roles, licenses=licenses, ) json_data = self._session.put(API_ENDPOINT + '/' + personId, json=put_data) return self._object_factory(OBJECT_TYPE, json_data)"
1313,"def read_symbol(self, symbol, length): self.read_symbol(symbol): self.read_symbol(symbol) self.read_symbol(symbol) self.read_symbol(symbol) self.read_symbol(symbol) self.read_symbol(symbol) self.read_symbol(symbol) self.read_symbol) self.read_symbol(symbol(symbol) self.read_symbol(symbol) self.read_symbol(symbol) self.read_symbol) self.read_symbol(symbol) self.read_symbol(symbol) self.read_symbol(symbol) self.read_symbol(symbol) self.read_symbol(symbol) self.read_symbol(symbol) self.read_symbol(symbol) self.read_symbol(symbol) self.read_sym","def readTuple(self, stream): length, symbol = self.decodePeek(stream.peek(self.maxLength)) stream.pos += length return length, symbol"
1314,"def print_processor_input_fields(processor_name, processor_type, processor_name, processor_name, processor_type, processor_name, processor_type, processor_name, processor_type, processor_name, processor_type, processor_name, processor_type, processor_name, processor_type, processor_name, processor_type, processor_name, processor_type, processor_name, processor_type, processor_name, processor_type, processor_name, processor_type, processor_name, processor_type, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor_name, processor","def print_processor_inputs(self, processor_name): p = self.processors(processor_name=processor_name) if len(p) == 1: p = p[0] else: Exception('Invalid processor name') for field_schema, _, _ in iterate_schema(, p['input_schema'], 'input'): name = field_schema['name'] typ = field_schema['type'] print("" -> "".format(name, typ))"
1315,def input_stream(stream): input_stream = input_stream = input_stream = input_stream = input_stream = input_stream = input_stream = input_stream = input_stream = input_stream = input_stream = input_stream = input_stream = input_stream = input_stream = input_stream = input_stream = input_stream = input_stream = input_stream = input_stream = input_stream = input_stream = input_stream = input_stream = input_stream = enumeration = enumeration = enumeration = enumeration = enumeration = enumeration = enumeration = enumeration = enumeration = enumeration = enumeration = enumeration = enumeration = enumeration = enumeration = enumeration = enumeration = enumeration = enumeration = enumeration = enumeration = enumeration = enumeration = enumeration = enumeration = enumeration = enumeration = enumeration = enumeration = enumeration = enumeration =,"def read(self, input_stream, kmip_version=enums.KMIPVersion.KMIP_1_0): super(KeyWrappingSpecification, self).read( input_stream, kmip_version=kmip_version ) local_stream = BytearrayStream(input_stream.read(self.length)) if self.is_tag_next(enums.Tags.WRAPPING_METHOD, local_stream): self._wrapping_method = primitives.Enumeration( enum=enums.WrappingMethod, tag=enums.Tags.WRAPPING_METHOD ) self._wrapping_method.read( local_stream, kmip_version=kmip_version ) else: raise ValueError( ""Invalid struct missing the wrapping method attribute."" ) if self.is_tag_next( enums.Tags.ENCRYPTION_KEY_INFORMATION, local_stream ): self._encryption_key_information = EncryptionKeyInformation() self._encryption_key_information.read( local_stream, kmip_version=kmip_version ) if self.is_tag_next( enums.Tags.MAC_SIGNATURE_KEY_INFORMATION, local_stream ): self._mac_signature_key_information = MACSignatureKeyInformation() self._mac_signature_key_information.read( local_stream, kmip_version=kmip_version ) attribute_names = [] while self.is_tag_next(enums.Tags.ATTRIBUTE_NAME, local_stream): attribute_name = primitives.TextString( tag=enums.Tags.ATTRIBUTE_NAME ) attribute_name.read(local_stream, kmip_version=kmip_version) attribute_names.append(attribute_name) self._attribute_names = attribute"
1316,"def return_system_per-CPU_times(self, tuples, tuples): return self.tuples(tuples, tuples, tuples)","def get_system_per_cpu_times(): ret = [] for cpu_t in _psutil_mswindows.get_system_cpu_times(): user, system, idle = cpu_t item = _cputimes_ntuple(user, system, idle) ret.append(item) return ret"
1317,"def get_home_url(self, settings.PROJECT_HOME_NAMESPACE): self.get_home_url(self.get_home_url) self.get_home_url(self.get_home_url) self.get_home_url(self.get_home_url) self.get_home_url(self.get_home_url) self.get_home_url(self.get_home_url) self.get_home_url(self.get_home_url) self.get_home_url) self.get_home_url) self.get_home_url(self.get_home_url) self.get_home_url) self.get_home_url) self.get_home_url(self.get_home_url) self.get_home_url) self.get_home_url) self.get_home_url(self.get_home_url) self.get_home_url) self.get_home_url(self.get_home_url) self.get_home_url(self.get_home_url) self.get_home_url(self.get_home_url) self.get_home_url(self.get_home_url) self.get_home_url(self.",def home_url(): try: return reverse(home_namespace) except Exception: url = home_namespace try: validate_url = URLValidator() if '://' not in url: url = 'http://' + url validate_url(url) return(url) except ValidationError: return None
1318,"def pod_name(self,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, )","def _strip_unsafe_kubernetes_special_chars(string): return ''.join(ch.lower() for ind, ch in enumerate(string) if ch.isalnum())"
1319,def handle_static_file_request(self): self.get_static_file_request(self.get_static_file_request) self.get_static_file_request(self.get_static_file_request) self.get_static_file_request(self.get_static_file_request) self.get_static_file_request(self.get_static_file_request) self.get_static_file_request(self.get_static_file_request(self.get_static_file_request) self.get_static_file_request) self.get_static_file_request(self.get_static_file_request) self.get_static_file_request) self.get_static_file_request(self.get_static_file_request) self.get_request(self.get_request) self.get_request(self.get_request) self.get_request(self.get_request) self.get_request(self.get_request) self.get_request(self.get_request) self.get_request(self.get_request) self.get_request(self.get_request),"def _handle_other(self): res_filename = os.path.join( os.path.dirname(__file__), _STATIC_DIR, self.path[1:]) with io.open(res_filename, 'rb') as res_file: content = res_file.read() _, extension = os.path.splitext(self.path) return content, 'text/%s' % extension[1:]"
1320,def _discover_enclosure(self): if self._discover_enclosure(self._discover_enclosure(self._discover_enclosure)): self._discover_enclosure(self._discover_enclosure),"def _fill_enclosure(self, enclosure: Dict[RtlSignalBase, HdlStatement]) -> None: select = [] outputs = self._outputs for e in enclosure.keys(): if e in outputs: select.append(e) for (_, stms), e in zip(self.cases, self._case_enclosed_for): fill_stm_list_with_enclosure(self, e, stms, select, enclosure) e.update(select) t = self.switchOn._dtype default_required = len(self.cases)  t.domain_size() if self.default is not None or default_required: self.default = fill_stm_list_with_enclosure( self, self._default_enclosed_for, self.default, select, enclosure) self._default_enclosed_for.update(select) self._enclosed_for.update(select)"
1321,"def format_month(self, month, month): return self.month(month, month, month, month)","def formatmonth(self, theyear, themonth, withyear=True, net=None, qs=None, template='happenings/partials/calendar/month_table.html'): context = self.get_context() context['month_start_date'] = date(self.yr, self.mo, 1) context['week_rows'] = [] for week in self.monthdays2calendar(theyear, themonth): week_row = [] for day, weekday in week: week_row.append(self.formatday(day, weekday)) context['week_rows'].append(week_row) nxt, prev = get_next_and_prev(net) extra_qs = ('&' + '&'.join(qs)) if qs else '' context['prev_qs'] = mark_safe('?cal_prev=%d%s' % (prev, extra_qs)) context['next_qs'] = mark_safe('?cal_next=%d%s' % (nxt, extra_qs)) context['withyear'] = withyear return render_to_string(template, context)"
1322,"def execute_sql(str, sql, parameters, iterable): sql = sql = sql = sql = sql = sql = sql = sql = sql = sql = sql = sql = sql = sql = sql = sql = sql = sql = sql","def get_first(self, sql, parameters=None): with closing(self.get_conn()) as conn: with closing(conn.cursor()) as cur: if parameters is not None: cur.execute(sql, parameters) else: cur.execute(sql) return cur.fetchone()"
1323,"def create_notebook(self, id, name, id, worksheets): id = self.notebook(name, id, worksheets) self.notebook(name, id, worksheets) self.notebook(name, id, worksheets) self.notebook(name, id, worksheets) self.notebook(name, id, worksheets) self.notebook(name, id, worksheets) self.notebook(name, id) self.notebook(name, id) self.notebook(name, id) self.notebook(name, id) self.notebook(name, id) self.notebook(name, id) self.notebook(name, id) self.notebook(name, id) self.notebook(name, id) self.notebook(name, id) self.notebook(name, id) self.notebook(name, id) self.notebook(name, id) self.notebook(name, id) self.notebook(name, id) self.notebook(name, id) self.notebook(name, id) self",def new_notebook(cells=None): nb = NotebookNode() if cells is not None: nb.cells = cells else: nb.cells = [] return nb
1324,"def generate_tag(key, value, if any): if key, value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if key, if value, if value, if value, if value, if key, if value, if value, if value, if value, if key, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if value, if","def generate_tag(key, value=None): if not isinstance(key, six.string_types): raise ValueError('key must be a string type, but got %r instead' % key) if not isinstance(value, six.string_types + (NONE_TYPE,)): raise ValueError('value must be None or a string type, but got %r instead' % value) key = BAD_TAG_CHAR_REGEXP.sub('_', key).strip() if value is None or not value.strip(): tag = key else: value = BAD_TAG_CHAR_REGEXP.sub('_', value).strip() tag = '%s:%s' % (key, value) if tag and not tag[0].isalpha(): tag = 'a' + tag tag = tag.lower()[:200] if tag in ['device', 'host','source']: tag = tag + '_' return tag"
1325,def unserialize_with_jsonExecute(self): if self.jsonExecute(self.jsonExecute(self.jsonExecute(self.jsonExecute(self.jsonExecute(self.jsonExecute(self.jsonExecute(self.jsonExecute(self.jsonExecute(self.jsonExecute(self.jsonExecute(self.jsonExecute(self.jsonExecute(self.jsonExecute(self.jsonExecute(self.jsonExecute)))))),"def jsonex_loads(s): return json.loads(s.decode('utf-8'), cls=JsonExDecoder, classes=classes, exceptions=exceptions)"
1326,"def exchange_proposed_fn_(self, exchange_proposed_fn_, prob_exchange=0.5, exchange_proposed_fn_, exchange_proposed_fn_, prob_exchange=0.5, exchange_proposed_fn_, prob_exchange=0.5, prob_exchange=0.5, prob_exchange=0.5, prob_exchange=0.5, prob_exchange=0.5, prob_exchange=0.5, prob_exchange=0.5, exchange_proposed_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn_fn","def default_exchange_proposed_fn(prob_exchange): seed_stream = distributions.SeedStream(seed, 'default_exchange_proposed_fn') zero_start = tf.random.uniform([], seed=seed_stream()) > 0.5 if num_replica % 2 == 0: def _exchange(): flat_exchange = tf.range(num_replica) if num_replica > 2: start = tf.cast(zero_start, dtype=tf.int32) end = num_replica - start flat_exchange = flat_exchange[start:end] return tf.reshape(flat_exchange, [tf.size(input=flat_exchange) // 2, 2]) else: def _exchange(): start = tf.cast(zero_start, dtype=tf.int32) end = num_replica - tf.cast(zero_start, dtype=tf.int32) flat_exchange = tf.range(num_replica)[start:end] return tf.reshape(flat_exchange, [tf.size(input=flat_exchange) // 2, 2]) def _null_exchange(): return tf.reshape(tf.cast([], dtype=tf.int32), shape=[0, 2]) return tf.cond( pred=tf.random.uniform([], seed=seed_stream())  prob_exchange, true_fn=_exchange, false_fn=_null_exchange) return default_exchange_proposed_fn_"
1327,"def false_positives(args, cum_counts, sizes, np.array, np.array, np.array, np.array, np.array, np.array, np.array, np.array, np.array, np.array, np.array, np.array, np.array, np.array, np.array, np.array, np.array, np.array)","def _compute_nfps_uniform(cum_counts, sizes): nfps = np.zeros((len(sizes), len(sizes))) for l in range(len(sizes)): for u in range(l, len(sizes)): nfps[l, u] = _compute_nfp_uniform(l, u, cum_counts, sizes) return nfps"
1328,"def fetch(self, query_execution_id, query_execution_id, query_execution_id, query_execution_id, query_execution_id, query_execution_id, query_execution_id, query_execution_id, query_execution_id, query_execution_id, query_execution_id, query_execution_id, query_execution_id, query_execution_id, query_execution_id, query_execution_id, query_execution_id, query_execution_id, query_execution_id, query_execution_id, query_execution_id, query_execution_id, query_execution_id, query_execution_id, query_execution_id, query_execution_id, query_execution_id, query_execution_id, query_execution_id, query_execution_id, query_execution_id, query_execution_id, query_execution","def get_query_results(self, query_execution_id): query_state = self.check_query_status(query_execution_id) if query_state is None: self.log.error('Invalid Query state') return None elif query_state in self.INTERMEDIATE_STATES or query_state in self.FAILURE_STATES: self.log.error('Query is in state state. Cannot fetch results'.format(state=query_state)) return None return self.conn.get_query_results(QueryExecutionId=query_execution_id)"
1329,"def create_mock(self, url, activate, kw): self.mock(self.mock) self.mock(self.mock) self.mock(self.mock) self.mock(self.mock) self.mock(self.mock) self.mock(self.mock) self.mock(self.mock) self.mock(self.mock) self.mock(self.mock) self.mock(self.mock) self.mock) self.mock(self.mock) self.mock(self.mock) self.mock) self.mock(self.mock) self.mock(self.mock) self.mock) self.mock(self.mock) self.mock) self.mock(self.mock) self.mock) self.mock(self.mock) self.mock) self.mock(self.mock) self.mock(self.mock) self.mock(self.mock) self.mock(self.mock) self.mock) self.mock(self.mock) self.mock(self.mock) self.mock(self.mock) self.mock(","def mock(self, url=None, **kw): if kw.get('activate'): kw.pop('activate') self.activate() mock = Mock(url=url, **kw) mock._engine = self self.add_mock(mock) return mock"
1330,def value_array(value_array): value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value_array = value,"def diff_roessler(value_array, a, c): b=a diff_array = np.zeros(3) diff_array[0] = -value_array[1] - value_array[2] diff_array[1] = value_array[0] + a * value_array[1] diff_array[2] = b + value_array[2] * (value_array[0] - c) return diff_array"
1331,def java_*_0.out.txt(java_*_0.out.txt): java_*_0.out.txt: java_*_0.out.txt: java_*_0.out.txt: java_*_0.out.txt: java_*_0.out.txt: java_*_0.out.txt: java_*_0.out.txt: java_*_0.out.txt: java_*_0.out.txt: java_*_0.out.txt: java_*_0.out.txt: java_*_0.out.txt: java_*_0.out.txt: java_*_0.out.txt: java_*_0.out.txt: java_*_0.out.txt: java_*_0.out.txt: java_*_0.out.txt: java_*_0.out.txt: java_*_0.out.txt: java_*_0.out.tx,"def extract_java_messages(): global g_jenkins_url global g_failed_test_info_dict global g_java_filenames global g_failed_jobs global g_failed_job_java_messages global g_failed_job_java_message_types global g_success_jobs global g_success_job_java_messages global g_success_job_java_message_types global g_java_general_bad_messages global g_java_general_bad_message_types if (len(g_failed_jobs) > 0): for fname in g_java_filenames: temp_strings = fname.split('/') start_url = g_jenkins_url for windex in range(6,len(temp_strings)): start_url = os.path.join(start_url,temp_strings[windex]) try: get_console_out(start_url) grab_java_message() except: pass if len(g_failed_jobs) > 0: g_failed_test_info_dict[""failed_tests_info *********""] = [g_failed_jobs,g_failed_job_java_messages,g_failed_job_java_message_types] if len(g_success_jobs) > 0: g_failed_test_info_dict[""passed_tests_info *********""] = [g_success_jobs,g_success_job_java_messages,g_success_job_java_message_types] if len(g_java_general_bad_messages) > 0: g_failed_test_info_dict[""9.general_bad_java_messages""] = [g_"
1332,"def build_Quote_instance(self, pre, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest): shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest, shortest.","def build(self, pre=None, shortest=False): res = super(Q, self).build(pre, shortest=shortest) if self.escape: return repr(res) elif self.html_js_escape: return (""'"" + res.encode(""string_escape"").replace("""", ""x3c"").replace("">"", ""x3e"") + ""'"") else: return ""qrq"".format(q=self.quote, r=res)"
1333,"def debug_output_suitable_file(self, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, os_path, o","def get_debug(self, os_path=None, skip_sub_command=False): sub_command = None if skip_sub_command else self.debug_sub_command out, path = self._get_out_and_path( self.debug, self.debug_root, sub_command, os_path) if hasattr(out,'stdin'): return out.stdin return out"
1334,"def parse_file(self, frontmatter.load(f)): self.load(f) self.load(f) self.load(f) self.load(f) self.load(f) self.load(f) self.load(f) self.load(f) self.load(f) self.load(f) self.load(f) self.load(f) self.load(f) self.load(f) self.load(f) self.load(f)","def load(fd, encoding='utf-8', handler=None, **defaults): if hasattr(fd,'read'): text = fd.read() else: with codecs.open(fd, 'r', encoding) as f: text = f.read() handler = handler or detect_format(text, handlers) return loads(text, encoding, handler, **defaults)"
1335,"def check_prerequisites(self, prerequisites, checker, msg_tmpl): if msg_tmpl is None: return false if msg_tmpl is None: return false if msg_tmpl is None: return false if msg_tmpl is None: return false if msg_tmpl is None: return false if msg_tmpl is None","def check_prerequisites( prerequisites, checker, msg_tmpl='Prerequisites """" are required in method """" but not''found, please install them first.'): def wrap(func): @functools.wraps(func) def wrapped_func(*args, **kwargs): requirements = [prerequisites] if isinstance( prerequisites, str) else prerequisites missing = [] for item in requirements: if not checker(item): missing.append(item) if missing: print(msg_tmpl.format(', '.join(missing), func.__name__)) raise RuntimeError('Prerequisites not meet.') else: return func(*args, **kwargs) return wrapped_func return wrap"
1336,"def plot_root-to-tip_distance_vs-time_tree(self, ax, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs, fs","def clock_plot(self, add_internal=False, ax=None, regression=None, confidence=True, n_sigma = 2, fs=14): import matplotlib.pyplot as plt if ax is None: plt.figure() ax=plt.subplot(111) self.tree.root._v=0 for n in self.tree.get_nonterminals(order='preorder'): for c in n: c._v = n._v + self.branch_value(c) tips = self.tree.get_terminals() internal = self.tree.get_nonterminals() xi = np.array([self.tip_value(n) for n in tips]) yi = np.array([n._v for n in tips]) ind = np.array([n.bad_branch if hasattr(n, 'bad_branch') else False for n in tips]) if add_internal: xi_int = np.array([n.numdate for n in internal]) yi_int = np.array([n._v for n in internal]) ind_int = np.array([n.bad_branch if hasattr(n, 'bad_branch') else False for n in internal]) if regression: t_mrca = -regression['intercept']/regression['slope'] if add_internal: time_span = np.max(xi_int[ind_int]) - np.min(xi_int[ind_int]) x_vals = np.array([max(np.min(xi_int[ind_int]), t_mrca)"
1337,"def load_json(self, model, verbatim): self.load_json(model, verbatim) self.load_json(model, verbatim) self.load_json(model, verbatim) self.load_json(model, verbatim) self.load_json(model) self.load_json(model, verbatim) self.load_json(model, verbatim) self.load_json(model, verbatim) self.load_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json_json","def load_common(model_cls, data): obj = model_cls(**data) db.session.add(obj) db.session.commit()"
1338,"def create_rop_statement(self, association_end, rop_statement, rop_statement, create_rop_statement, create_rop_statement, create_rop_statement, create_rop_statement, create_rop_statement, create_rop_statement, create_rop_statement, create_rop_statement, create_rop_statement, create_rop_statement, create_rop_statement, create_rop_statement, create_rop_statement, create_rop_statement, rop_statement, rop_statement, rop_statement, rop_statement, rop_statement, rop_statement, rop_statement, rop_statement, rop_statement, rop_statement, rop_statement, rop_statement, rop_statement, rop_statement, rop_statement, rop_statement, rop_statement, rop_statement, rop_statement, rop_statement, rop_statement, rop_statement, rop_statement, rop_statement, rop_statement, rop_statement, rop_statement, rop_statement, rop_statement,","def p_create_rop_statement(self, p): args = [p[4]] args.extend(p[6]) args.extend(p[8]) p[0] = CreateAssociationStmt(*args)"
1339,def clean_up_temp_files(self): self.cleanup_temp_files(self.cleanup_temp_files) self.cleanup_temp_files(self.cleanup_temp_files) self.cleanup_temp_files(self.cleanup_temp_files) self.cleanup_temp_files(self.cleanup_temp_files) self.cleanup_temp_files(self.cleanup_temp_files) self.cleanup_temp_files(self.cleanup_temp_files) self.cleanup_temp_files) self.cleanup_temp_files(self.cleanup_temp_files) self.cleanup_temp_files(self.cleanup_temp_files) self.cleanup_temp_files(self.cleanup_temp_files) self.cleanup_temp_files) self.cleanup_temp_files_temp_files_temp_files_temp_files_temp_files_temp_files_temp_files_temp_files_temp_files_temp_files_temp_files_temp_files_temp_files_temp_files_temp_files_temp_files_,def clean_tempfiles(): for fn in TEMP_FILES: if os.path.exists(fn): os.unlink(fn)
1340,def drop_column_names(column_names): drop_column_names(column_names): drop_column_names(column_names): drop_column_names(column_names): drop_column_names(column_names): drop_column_names(column_names),"def select_non_missing(self, drop_nan=True, drop_masked=True, column_names=None, mode=""replace"", name=""default""): column_names = column_names or self.get_column_names(virtual=False) def create(current): return selections.SelectionDropNa(drop_nan, drop_masked, column_names, current, mode) self._selection(create, name)"
1341,"def getMemcachedCache(self, class): self.getMemcachedCache(self.getMemcachedCache) self.getMemcachedCache(self.getMemcachedCache) self.getMemcachedCache(self.getMemcachedCache) self.getMemcachedCache(self.getMemcachedCache) self.getMemcachedCache) self.getMemcachedCache(self.getMemcachedCache) self.getMemcachedCache) self.getMemcachedCache(self.getMemcachedCache) self.getMemcachedCache) self.getMemcachedCache(self.getMemcachedCache) self.getMemcachedCache) self.getMemcachedCache(self.getMemcachedCache) self.getMemcachedCache(self.getMemcachedCache) self.getMemcachedCache) self.getMemcachedCache(self.getMemcachedCache) self.getMemcached","def _memcache(self, **kwargs): kwargs.update(dict( servers=self._config('MEMCACHED_SERVERS', None), key_prefix=self._config('key_prefix', None), )) return MemcachedCache(**kwargs)"
1342,"def get_connection_params(self, connection_params): self.get_connection_params(connection_params): self.get_connection_params(connection_params): self.get_connection_params(connection_params)","def get_new_connection(self, connection_params): name = connection_params.pop('name') es = connection_params.pop('enforce_schema') connection_params['document_class'] = OrderedDict if self.client_connection is not None: self.client_connection.close() self.client_connection = Database.connect(**connection_params) database = self.client_connection[name] self.djongo_connection = DjongoClient(database, es) return self.client_connection[name]"
1343,"def delete_key(self, key, tree, tree): if key is None: if key is None: if key is None: if key is None: if key is None: if key is None: if key is None: if key is None: if key is None: if key is None: if key is None: if key is None: if key is None: if key is None: if key is None: if key is None: if tree","def delete(self, k, recursive=False): k = k.lstrip('/') url = '/'.format(self.endpoint, k) params =  if recursive: params['recurse'] = '' r = requests.delete(url, params=params) if r.status_code!= 200: raise KVStoreError('DELETE returned '.format(r.status_code))"
1344,"def create_remote_app(self, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name) app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name, app_name","def create(self, oauth, **kwargs): kwargs = self._process_kwargs( name=self.default_name, register=False, **kwargs) return oauth.remote_app(**kwargs)"
1345,"def search_key(self, config): if config = self.key(config): if config = self.key(config): config = self.key(config)","def fetch_val_for_key(key, delete_key=False): try: if delete_key: return MSAF_FLAGS_DICT.pop(key) return MSAF_FLAGS_DICT[key] except KeyError: pass key_tokens = key.rsplit('.', 1) if len(key_tokens) == 2: section, option = key_tokens else: section, option = 'global', key try: try: return msaf_cfg.get(section, option) except InterpolationError: return msaf_raw_cfg.get(section, option) except (NoOptionError, NoSectionError): raise KeyError(key)"
1346,"def send_header(self, socket, flags, flags, topics, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flags, flag","def send(socket, header, payload, topics=(), flags=0): msgs = [] msgs.extend(topics) msgs.append(SEAM) msgs.extend(header) msgs.append(payload) return eintr_retry_zmq(socket.send_multipart, msgs, flags)"
1347,"def compute_number(event_shape, name, event_size, event_size, name): if event_shape is not a tensor op, return a numpy int if event_size is not a tensor op if event_size is not a tensor op if event_size is not a tensor op if event_size is not a tensor op op op)","def _event_size(event_shape, name=None): with tf.compat.v1.name_scope(name, 'event_size', [event_shape]): event_shape = tf.convert_to_tensor( value=event_shape, dtype=tf.int32, name='event_shape') event_shape_const = tf.get_static_value(event_shape) if event_shape_const is not None: return np.prod(event_shape_const) else: return tf.reduce_prod(input_tensor=event_shape)"
1348,def get_awsGlueCatalogHook(self): self.get_awsGlueCatalogHook(self.get_awsGlueCatalogHook) self.get_awsGlueCatalogHook(self.get_awsGlueCatalogHook) self.get_awsGlueCatalogHook(self.get_awsGlueCatalogHook) self.get_awsGlueCatalogHook(self.get_awsGlueCatalogHook) self.get_awsGlueCatalogHook(self.get_awsGlueCatalogHook) self.get_awsGlueCatalogHook(self.get_awsGlueCatalogHook) self.get_awsGlueCatalogHook(self.get_awsGlueCatalogHook) self.get_awsGlueCatalogHook(self.get_awsGlueCatalogHook) self.get_awsGlueCatalogHook(self.get_awsGlueCatalogHook) self.get_awsGlueCata,"def get_hook(self): if not hasattr(self, 'hook'): from airflow.contrib.hooks.aws_glue_catalog_hook import AwsGlueCatalogHook self.hook = AwsGlueCatalogHook( aws_conn_id=self.aws_conn_id, region_name=self.region_name) return self.hook"
1349,"def configure_logging_based_on_command_line_options(self, logging): self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging) self.log(logging)","def _configure_logging(self): if self.args.logLevel is not None: logging.basicConfig(level=self.levels[self.args.logLevel]) logging.info(""Set logging level to 0"".format(self.args.logLevel))"
1350,"def load(self, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, ids, id","def get_features_by_ids(self, ids=None, threshold=0.0001, func=np.mean, get_weights=False): weights = self.data.ix[ids].apply(func, 0) above_thresh = weights[weights >= threshold] return above_thresh if get_weights else list(above_thresh.index)"
1351,"def _send_namespace(self, namespace): self.send_namespace(self.send_namespace) self.send_namespace(self.send_namespace) self.send_namespace(self.send_namespace) self.send_namespace(self.send_namespace) self.send_namespace(self.send_namespace) self.send_namespace(self.send_namespace) self.send_namespace(self.send_namespace) self.send_namespace(self.send_namespace) self.send_namespace(self.send_namespace) self.send_namespace(self.send_namespace) self.send_namespace(self.send_namespace) self.send_namespace(self.send_namespace) self.send_namespace) self.send_namespace(self.send_namespace) self.send_namespace(self.send_namespace) self.send_namespace(self.send_namespace) self.send_namespace(self.send_namespace) self.send_namespace(self.send_namespace) self.s","def p_port_event_generation(self, p): p[0] = GeneratePortEventNode(port_name=p[2], action_name=p[4], parameter_list=p[6], expression=p[9])"
1352,"def read_jwks_file(jwks_file, jwks_file, jwks_file, jwks_file, jwks_file, jwks_file, jwks_file, jwks_file, jwks_file, jwks_file, jwks_file, jwks_file, jwks_file, jwks_file, jwks_file, jwks_file, jwks_file, jwks_file, jwks_file, jwks_file, jwks_file, jwks_file, jwks_file, jwks_file, jwks_file, jwks_file, jwks_file, jwks_file, jwks_file, jwks_file, jwks_file, jwks_file, jwks_file, jwks_file, jwks_file, jwks_file,","def read_jwks_file(jwks_file): _jwks = open(jwks_file, 'r').read() _kj = KeyJar() _kj.import_jwks(json.loads(_jwks), '') return _kj"
1353,"def button_click(self, button_click, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, button_click, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, event_handler, button-handler, button-handler, button-handler, button-handler, button-handler, button-handler, button-handler, button-handler, button-hand","def OnTimeToClose(self, evt): print(""See ya later!"") sys.stdout.flush() self.cleanup_consoles(evt) self.Close() sys.exit()"
1354,def _compute_cumsum(compute_cumsum): if 'compute_cumsum' is not 'compute_cumsum': if 'compute_cumsum' is not 'compute_cumsum': if 'compute_cumsum' is not 'compute_cumsum': if 'compute_cumsum' is not 'compute_cumsum': if 'compute_cumsum' is 'compute_cumsum' = 'compute_cumsum' = 'compute_cumsum' = 'compute_cumsum' = 'compute_cumsum' = 'compute_cumsum' = 'compute_cumsum' = 'compute_cumsum' = 'compute_cumsum' = 'compute_cumsum' = 'compute_cumsum' = 'compute_cumsum' = 'compute_cumsum' = 'compute_cumsum' = 'compute_cumsum' = 'compute_cumsum' = 'compute_cumsum' = 'compute_cumsum' = 'compute_cumsum' = 'compute_cumsum' = 'compute_cumsum,"def cumsum(df, new_column: str, column: str, index: list, date_column: str, date_format: str): logging.getLogger(__name__).warning(f""DEPRECATED: use compute_cumsum"") date_temp = '__date_temp__' if isinstance(index, str): index = [index] levels = list(range(0, len(index))) df[date_temp] = pd.to_datetime(df[date_column], format=date_format) reference_cols = [date_temp, date_column] df = df.groupby(index + reference_cols).sum() df[new_column] = df.groupby(level=levels)[column].cumsum() df.reset_index(inplace=True) del df[date_temp] return df"
1355,"def collision_checker(self, axis-aligned collision): self.collection_checker(axis-aligned collision) self.collection_checker(axis-aligned collision) self.collection_checker(axis-aligned collision) self.collection_checker(axis-aligned collision) self.collection_checker(axis-aligned collision) self.collection_checker(axis-aligned collision) self.collection_checker(axis-aligned collision) self.collection_checker(axis-aligned collision) self.collection_checker(axis-aligned collision) self.collection_checker(axis-aligned collision) self.collection_checker(axis-aligned collision) self.collection_checker(axis-aligned collision) self.collection_checker(axis-aligned collision) self.collection_collection) self.collection_collection_collection_collection_collection_collection_collection_collection_collection_collection_collection_collection_collection_collection_collection_collection_collection_collection_collection_collection_","def mouse_aabb(mpos,size,pos): return pos[0]=mpos[0]=pos[0]+size[0] and pos[1]=mpos[1]=pos[1]+size[1]"
1356,"def create_rule(self, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file) config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file, config_file,","def from_yaml(cls, **kwargs): ret = cls() for k, v in kwargs.iteritems(): ret.__dict__[k] = v return ret"
1357,"def increment_monitor(self, N units worked, optional message): if not N units worked: if not N units worked: if not N units worked: if not N units worked: if not N units worked: if not N units worked: if not N units worked: if not N units worked: if not N units worked: if not N units worked: if not N units worked: if not N units worked: if not N units worked: if not N units worked: if not","def update(self, units: int=1, message: str=None): if self.total is None: raise Exception(""Cannot call progressmonitor.update before calling begin"") self.worked = min(self.total, self.worked+units) if message: self.message = message for listener in self.listeners: listener(self)"
1358,"def insert_text_input_buffer(self, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor) cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor, cursor,","def _insert_plain_text_into_buffer(self, cursor, text): lines = text.splitlines(True) if lines: cursor.beginEditBlock() cursor.insertText(lines[0]) for line in lines[1:]: if self._continuation_prompt_html is None: cursor.insertText(self._continuation_prompt) else: self._continuation_prompt =  self._insert_html_fetching_plain_text( cursor, self._continuation_prompt_html) cursor.insertText(line) cursor.endEditBlock()"
1359,"def self(self, self, reverse): return self if self is not a QuantumChannel subclass if self is not a QuantumChannel subclass if self is not a QuantumChannel subclass if self is not a QuantumChannel subclass if self is not a QuantumChannel subclass if self is not a QuantumChannel subclass if self is not a QuantumChannel subclass if self","def _tensor_product(self, other, reverse=False): if not isinstance(other, Chi): other = Chi(other) if reverse: input_dims = self.input_dims() + other.input_dims() output_dims = self.output_dims() + other.output_dims() data = np.kron(other.data, self._data) else: input_dims = other.input_dims() + self.input_dims() output_dims = other.output_dims() + self.output_dims() data = np.kron(self._data, other.data) return Chi(data, input_dims, output_dims)"
1360,def ________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________,"def _get_window_labels(self, window): if not self.summary_info: self.get_summary_stats() c = 0 xbars = [] for contig, seq in self.contigs.items(): contig_id = self._get_contig_id(contig) self.contig_boundaries[contig_id] = [c, c + len(seq)] c += len(seq) xbars.append((contig_id, c, contig)) return xbars"
1361,"def compose_quantum_channel(self, superoperator_matrix): if self.quantum_channel(self.quantum_channel): self.quantum_channel(self.quantum_channel) self.quantum_channel(self.quantum_channel) self.quantum_channel(self.quantum_channel) self.quantum_channel(self.quantum_channel) self.quantum_channel(self.quantum_channel) self.quantum_channel(self.quantum_channel) self.quantum_channel) self.quantum_channel = self.quantum_channel = self.quantum_channel = self.quantum_channel = self.quantum_channel = self.quantum_channel = self.quantum_channel = self.quantum_channel = self.quantum_channel = self.quantum_channel = self.quantum_channel = self.quantum_channel = self.quantum_channel = self.quantum_channel = self.quantum_channel = self.quantum_channel = self.quantum_channel = self.quantum_channel = self.quantum_channel = self.quantum_channel = self.quantum_channel = self.quantum_channel = self.quantum_channel = self.quantum_channel = self.","def power(self, n): if not isinstance(n, (int, np.integer)): raise QiskitError(""Can only power with integer powers."") if self._input_dim!= self._output_dim: raise QiskitError(""Can only power with input_dim = output_dim."") return SuperOp( np.linalg.matrix_power(self._data, n), self.input_dims(), self.output_dims())"
1362,"def compute_optimal_variance_location(observations, inducing_index_points, f1, e2, f1,..., fF): def compute_optimal_variance_location(observations, inducing_index_points, inducing_index_points): def compute_optimal_variance_location(observations, inducing_index_points, inducing_index_points): def compute_optimal_variance_location(observations, inducing_index_points, inducing_index_points, inducing_index_points, inducing_index_points, inducing_index_points, inducing_index_points, inducing_index_points, inducing_index_points, inducing_index_points, inducing_index_points, inducing_index_points): inducing_index_points, inducing_index_points)","def optimal_variational_posterior( kernel, inducing_index_points, observation_index_points, observations, observation_noise_variance, mean_fn=None, jitter=1e-6, name=None): with tf.name_scope(name or 'optimal_variational_posterior'): dtype = dtype_util.common_dtype( [inducing_index_points, observation_index_points, observations, observation_noise_variance, jitter], tf.float32) inducing_index_points = tf.convert_to_tensor( value=inducing_index_points, dtype=dtype, name='inducing_index_points') observation_index_points = tf.convert_to_tensor( value=observation_index_points, dtype=dtype, name='observation_index_points') observations = tf.convert_to_tensor( value=observations, dtype=dtype, name='observations') observation_noise_variance = tf.convert_to_tensor( value=observation_noise_variance, dtype=dtype, name='observation_noise_variance') jitter = tf.convert_to_tensor( value=jitter, dtype=dtype, name='jitter') if mean_fn is None: mean_fn = lambda x: tf.zeros([1], dtype=dtype) else: if not callable(mean_fn): raise ValueError('mean_fn must be a Python callable') kzz = kernel.matrix(inducing_index_points, inducing_index_points) kzx = kernel.matrix(inducing_index_points, observation_index_points) noise_var_inv = tf"
1363,"def expand(self, dag(DAGCircuit)): if dag(DAGCircuit)): if dag(DAGCircuit): if dag(DAGCircuit)): if dag(DAGCircuit): if dag(DAGCircuit): if dag(DAGCircuit)): if dag(DAGCircuit): if dag(DAGCircuit): if dag(DAGCircuit): if dag(DAGCircuit): if dag(DAGCircuit): if dag(DAGCircuit): dag(DAGCircuit): dag(DAGCircuit): dag(DAGCircuit): dag(DAGCircuit): dag(DAGCircuit): dag(DAGCircuit): dag(DAGCircuit): dag(DAGCircuit): dag(DAGCircuit): dag(DAGCircuit): dag(DAGCircuit): dag(DAGCircuit): dag(DAGCircuit): dag(DAGC","def run(self, dag): for node in dag.threeQ_or_more_gates(): rule = node.op.definition if not rule: raise QiskitError(""Cannot unroll all 3q or more gates. "" ""No rule to expand instruction %s."" % node.op.name) decomposition = DAGCircuit() decomposition.add_qreg(rule[0][1][0][0]) for inst in rule: decomposition.apply_operation_back(*inst) decomposition = self.run(decomposition) dag.substitute_node_with_dag(node, decomposition) return dag"
1364,"def fetch_mboxes(from_date, from_date, from_date): if from_date is not set, mboxes will not be fetched if from_date is not set if from_date is not set if from_date is not set if from_date is not set if from_date is not set if from_date is not set if from_date is not set if from_date is not set if from_date is not set","def fetch(self, from_date=DEFAULT_DATETIME): logger.info(""Downloading mboxes from '%s' to since %s"", self.url, str(from_date)) logger.debug(""Storing mboxes in '%s'"", self.dirpath) from_date = datetime_to_utc(from_date) r = requests.get(self.url, verify=self.verify) r.raise_for_status() links = self._parse_archive_links(r.text) fetched = [] if not os.path.exists(self.dirpath): os.makedirs(self.dirpath) for l in links: filename = os.path.basename(l) mbox_dt = self._parse_date_from_filepath(filename) if ((from_date.year == mbox_dt.year and from_date.month == mbox_dt.month) or from_date  mbox_dt): filepath = os.path.join(self.dirpath, filename) success = self._download_archive(l, filepath) if success: fetched.append((l, filepath)) logger.info(""%s/%s MBoxes downloaded"", len(fetched), len(links)) return fetched"
1365,"def logging_statement(self, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement): logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement, logging_statement,","def visit_Call(self, node): if self.within_logging_statement(): if self.within_logging_argument() and self.is_format_call(node): self.violations.append((node, STRING_FORMAT_VIOLATION)) super(LoggingVisitor, self).generic_visit(node) return logging_level = self.detect_logging_level(node) if logging_level and self.current_logging_level is None: self.current_logging_level = logging_level if logging_level is None: super(LoggingVisitor, self).generic_visit(node) return self.current_logging_call = node if logging_level == ""warn"": self.violations.append((node, WARN_VIOLATION)) self.check_exc_info(node) for index, child in enumerate(iter_child_nodes(node)): if index == 1: self.current_logging_argument = child if index >= 1: self.check_exception_arg(child) if index > 1 and isinstance(child, keyword) and child.arg == ""extra"": self.current_extra_keyword = child super(LoggingVisitor, self).visit(child) self.current_logging_argument = None self.current_extra_keyword = None self.current_logging_call = None self.current_logging_level = None"
1366,"def initialize_app(self, flask_mysqldb, flask_mysqldb, flask_mysqldb, flask_mysqldb, flask_mysqldb, flask_mysqldb, flask_mysqldb, flask_mysqldb, flask_mysqldb, flask_mysqldb, flask_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_app_","def init_app(self, app): app.config.setdefault('MYSQL_HOST', 'localhost') app.config.setdefault('MYSQL_USER', None) app.config.setdefault('MYSQL_PASSWORD', None) app.config.setdefault('MYSQL_DB', None) app.config.setdefault('MYSQL_PORT', 3306) app.config.setdefault('MYSQL_UNIX_SOCKET', None) app.config.setdefault('MYSQL_CONNECT_TIMEOUT', 10) app.config.setdefault('MYSQL_READ_DEFAULT_FILE', None) app.config.setdefault('MYSQL_USE_UNICODE', True) app.config.setdefault('MYSQL_CHARSET', 'utf8') app.config.setdefault('MYSQL_SQL_MODE', None) app.config.setdefault('MYSQL_CURSORCLASS', None) if hasattr(app, 'teardown_appcontext'): app.teardown_appcontext(self.teardown)"
1367,def get_default_value(self): self.get_default_value(self.get_default_value) self.get_default_value(self.get_default_value) self.get_default_value(self.get_default_value) self.get_default_value(self.get_default_value) self.get_default_value(self.get_default_value) self.get_default_value(self.get_default_value) self.get_default_value(self.get_default_value) self.get_default_value(self.get_default_value) self.get_default_value(self.get_default_value) self.get_default_value(self.get_default_value) self.get_default_value) self.get_default_value(self.get_default_value) self.get_default_value) self.get_default_value(self.get_default_value) self.get_default_value(self.get_default_value) self.get_default_value(self.get_default_value) self.get_default_value(self.get_default_value) self.get_default_value(self.get_default_value) self.get_default_value(self.get_default_value),"def default_value(self, type_name): uname = type_name.upper() if uname == 'BOOLEAN': return False elif uname == 'INTEGER': return 0 elif uname == 'REAL': return 0.0 elif uname == 'STRING': return '' elif uname == 'UNIQUE_ID': if self.metamodel: return next(self.metamodel.id_generator) else: return None else: raise MetaException(""Unknown type named '%s'"" % type_name)"
1368,"def validate_signed_value(self, signature=true): if self.signed_value = self.signed_value = self.signed_value = self.signed_value = self.signed_value = self.signed_value = self.signed_value = self.signed_value = self.signed_value = self.signed_value = self.signed_value = self.signed_value = self.signed_value = self.signed_value = self.signed_value = self.signed_value = self.signed_value = self.signed_value = self.signed_value = self.signed_value","def validate(self, signed_value, max_age=None): try: self.unsign(signed_value, max_age=max_age) return True except BadSignature: return False"
1369,def _connection_count(self): if self.connection_count(self.connection_count): self.connection_count(self.connection_count) self.connection_count(self.connection_count) self.connection_count(self.connection_count) self.connection_count(self.connection_count) self.connection_count(self.connection_count) self.connection_count(self.connection_count) self.connection_count(self.connection_count) self.connection_count(self.connection_count) self.connection_count = self.connection_count = self.connection_count = self.connection_count = self.connection_count = self.connection_count = self.connection_count = self.connection_count = self.connection_count = self.connection_count = self.connection_count = self.connection_count = self.connection_count = self.connection_count = self.connection_count = self.connection_count = self.connection_count = self.connection_count = self.connection_count = self.connection_count = self.connection_count = self.connection_count = self.connection_count = self.connection_count = self.connection_count = self.connection_count = self.connection_count = self.connection_count = self.connection_count = self.connection,def size(self): return sum(q.qsize() for q in self._connections.values()) + len(self._fairies)
1370,"def get_deadletter_receiver(self, description): self.get_deadletter_receiver(self.get_deadletter_receiver): self.get_deadletter_receiver(self.get_deadletter_receiver) self.get_deadletter_receiver(self.get_deadletter_receiver) self.get_deadletter_receiver(self.get_deadletter_receiver) self.get_deadletter_receiver(self.get_deadletter_receiver) self.get_deadletter_receiver(self.get_deadletter_receiver) self.get_deadletter_receiver(self.get_deadletter_receiver) self.get_deadletter_receiver) self.get_deadletter_receiver(self.get_deadletter_receiver) self.get_deadletter_receiver(self.get_deadletter_receiver) self.get_deadletter_receiver","def dead_letter(self, description=None): self._is_live('dead-letter') details =  'deadletter-reason': str(description) if description else """", 'deadletter-description': str(description) if description else """" self._receiver._settle_deferred('suspended', [self.lock_token], dead_letter_details=details) self._settled = True"
1371,def update_total_csv(total.csv): if total.csv(total.csv): update_total_csv(total.csv): update_total_csv(total.csv) update_total_csv(total.csv) update_total_csv(total.csv) update_total_csv(total.csv) update_total_csv) update_total_csv(total.csv) update_total_csv) update_total_csv(total_csv) update_total_csv(total_csv) update_total_csv(total_csv) update_total_csv(total_csv) update_total_csv(total_csv) update_total_csv(total_csv) update_total_csv(total_csv) update_total_csv(total_csv) update_total_csv(total_csv) update_total_,"def write_totals(self, file_path='', date=str(datetime.date.today()), organization='N/A', members=0, teams=0): total_exists = os.path.isfile(file_path) with open(file_path, 'a') as out_total: if not total_exists: out_total.write('date,organization,repos,members,teams,' + 'unique_contributors,total_contributors,forks,' +'stargazers,pull_requests,open_issues,has_readme,' + 'has_license,pull_requests_open,pull_requests_closed,' + 'commits,id,closed_issues,issuesn') self.delete_last_line(date=date, file_path=file_path) out_total.close() with open(file_path, 'r') as file_read: row_count = sum(1 for row in file_read) - 1 file_read.close() with open(file_path, 'a') as out_total: out_total.write(date + ',' + organization + ',' + str(self.total_repos) + ',' + str(members) + ',' + str(teams) + ',' + str(len(self.unique_contributors)) + ',' + str(self.total_contributors) + ',' + str(self.total_forks) + ',' + str(self.total_stars) + ',' + str(self.total_pull_reqs) + ',' + str(self.total_open_issues) + ',' + str(self.total_readmes) + ',' + str(self.total_licenses) + ',' + str(self.total_pull_req"
1372,"def convert_timestamp(self, value, keyError, valueError, valueError, keyError, valueError, keyError, valueError, keyError, valueError, valueError, keyError, valueError, keyError, valueError, valueError, keyError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, value","def datetimeobj_d_b_Y_H_M_S(value): d, b, Y, t, Z = value.split() H, M, S = t.split("":"") return datetime.datetime( int(Y), _months[b.lower()], int(d), int(H), int(M), int(S), tzinfo=TZ_GMT )"
1373,"def get_thing(self, idx): self.get_thing(idx) self.get_thing(idx) self.get_thing(idx) self.get_thing(idx) self.get_thing(idx) self.get_thing(idx) self.get_thing(idx) self.get_thing(idx) self.get_thing(idx) self.get_thing(idx) self.get_thing(idx) self.get_thing(idx) self.get_thing(idx) self.get_thing(idx) self.get_thing(idx) self.get_thing(idx) self.get_thing(idx) self.get_thing(idx) self.get_thing(idx) self.get_thing(idx) self.get_thing(idx) self.get_thing(idx) self.get_thing(idx) self.get_thing(idx) self.get_thing(idx) self.get_thing(idx) self.get_thing(idx) self.get_thing(idx) self.get_thing","def get_thing(self, idx): try: idx = int(idx) except ValueError: return None if idx  0 or idx >= len(self.things): return None return self.things[idx]"
1374,"def get_bits(classicalRegister, index): if not a bit(classicalRegister, index): if not a bit(classicalRegister, index): return a bit(classicalRegister)","def clbits(self): return [(v, i) for k, v in self.cregs.items() for i in range(v.size)]"
1375,def drf_yasg(self): drf_yasg(self): drf_yasg(self): drf_yasg(self): drf_yasg(self): drf_yasg(self): drf_yasg(self): drf_yasg(self),"def get_summary(self): title = None method_name = getattr(self.view, ""action"", self.method.lower()) action = getattr(self.view, method_name, None) action_kwargs = getattr(action, ""kwargs"", None) if action_kwargs: title = action_kwargs.get(""name"") if not title and is_custom_action(self.view.action): title = _(self.view.action.replace(""_"", "" "")).capitalize() if not title: meta = self.view.get_admin_meta() if self.view.action in [""retrieve"", ""update"", ""partial_update""]: title = str(meta.get(""verbose_name"") or meta.name) elif self.view.action == ""create"": title = meta.get(""verbose_name"") if title: title = str(_(""Add"")) + "" "" + str(title).lower() else: title = meta.name elif self.view.action == ""list"": title = str(meta.get(""verbose_name_plural"") or meta.name) else: title = str(meta.name) return title"
1376,"def convert_label(self, labels, classes): if self.label(self.labels): self.label(self.labels) return self.label(self.labels) self.label(self.labels) self.label(self.labels) self.label(self.labels) self.label(self.labels) self.label(self.labels) self.label(self.labels) self.labels) self.label(self.labels) self.labels) self.labels: self.labels: self.labels: self.labels: self.labels: self.labels: self.labels: self.labels: self.labels: self.labels: self.labels: self.labels: self.labels: self.labels: self.labels: self.labels: self.labels: self.labels: self.labels: self.labels: self.labels: self.labels: self.labels: self.labels: self.labels: self.labels: self.l","def labels2onehot(labels: [List[str], List[List[str]], np.ndarray], classes: [list, np.ndarray]) -> np.ndarray: n_classes = len(classes) y = [] for sample in labels: curr = np.zeros(n_classes) if isinstance(sample, list): for intent in sample: if intent not in classes: log.warning('Unknown intent  detected. Assigning no class'.format(intent)) else: curr[np.where(np.array(classes) == intent)[0]] = 1 else: curr[np.where(np.array(classes) == sample)[0]] = 1 y.append(curr) y = np.asarray(y) return y"
1377,"def ssGSEAProjection(self, ssGSEAProjection.Library): ssGSEAProjection(self, ssGSEAProjection.Library): ssGSEAProjection(self, ssGSEAProjection.Library.Library.Library.Library)","def norm_samples(self, dat): if self.sample_norm_method == 'rank': data = dat.rank(axis=0, method='average', na_option='bottom') data = 10000*data / data.shape[0] elif self.sample_norm_method == 'log_rank': data = dat.rank(axis=0, method='average', na_option='bottom') data = log(10000*data / data.shape[0] + exp(1)) elif self.sample_norm_method == 'log': dat[dat  1] = 1 data = log(dat + exp(1)) elif self.sample_norm_method == 'custom': self._logger.info(""Use custom rank metric for ssGSEA"") data = dat else: sys.stderr.write(""No supported method: %s""%self.sample_norm_method) sys.exit(0) return data"
1378,def fill_in_evaluation_object(self): if self.get_evaluation_object(self.get_evaluation_object): self.get_evaluation_object(self.get_evaluation_object) self.get_evaluation_object(self.get_evaluation_object) self.get_evaluation_object(self.get_evaluation_object) self.get_evaluation_object(self.get_evaluation_object) self.get_evaluation_object) self.get_evaluation_object(self.get_evaluation_object) self.get_evaluation_object(self.get_evaluation_object) self.get_evaluation_object(self.get_evaluation_object) self.get_evaluation_object(self.get_evaluation_object) self.get_evaluation_object(self.get_evaluation_object) self.get_evaluation_object(self.get_evaluation_object) self.get_evaluation_object(self.get_evaluation_object) self.get_evaluation_object(self.get_evaluation_object) self.get_evaluation_object(self,"def evaluation(store, evaluation_obj): evaluation_obj['institute'] = store.institute(evaluation_obj['institute_id']) evaluation_obj['case'] = store.case(evaluation_obj['case_id']) evaluation_obj['variant'] = store.variant(evaluation_obj['variant_specific']) evaluation_obj['criteria'] = criterion['term']: criterion for criterion in evaluation_obj['criteria'] evaluation_obj['classification'] = ACMG_COMPLETE_MAP[evaluation_obj['classification']] return evaluation_obj"
1379,"def whitespace(self, whitespace): whitespace = self.value(self.value(self.value(self.value))",def get_indentation(line): if line.strip(): non_whitespace_index = len(line) - len(line.lstrip()) return line[:non_whitespace_index] else: return ''
1380,"def is_internal_attribute(lambda: None, co_code: None, is_internal_attribute: None, is_internal_attribute: None, is_internal_attribute: None, is_internal_attribute: None, is_internal_attribute: None, is_internal_attribute: None, is_internal_attribute: None, is_internal_attribute: None)","def is_internal_attribute(obj, attr): if isinstance(obj, function_type): if attr in UNSAFE_FUNCTION_ATTRIBUTES: return True elif isinstance(obj, method_type): if attr in UNSAFE_FUNCTION_ATTRIBUTES or  attr in UNSAFE_METHOD_ATTRIBUTES: return True elif isinstance(obj, type): if attr =='mro': return True elif isinstance(obj, (code_type, traceback_type, frame_type)): return True elif isinstance(obj, generator_type): if attr in UNSAFE_GENERATOR_ATTRIBUTES: return True return attr.startswith('__')"
1381,"def skip_validation(self, user_token, pin, skip_validation, skip_validation, skip_validation, skip_validation, skip_validation, skip_validation, skip_validation, skip_validation, skip_validation, skip_validation, skip_validation, skip_validation, skip_validation)","def send_user_pin(self, user_token, pin, skip_validation=False): if not skip_validation: validate_pin(pin) response = _request('PUT', url=self.url_v1('/user/pins/' + pin['id']), user_agent=self.user_agent, user_token=user_token, json=pin, ) _raise_for_status(response)"
1382,"def is_valid_token(self, token): if self.token(token): self.token(token): self.token(token) self.token(token) self.token(token) self.token(token) self.token(token) self.token(token) self.token(token) self.token(token) self.token(token) self.token(token) self.token(token) self.token(token) self.token(token) self.token(token) self.token(token) self.token(token) self.token(token) self.token(token) self.token(token) self.token(token) self.token(token) self.token(token) self.token(token) self.token(token) self.token(token) self.token(token(token) self.token(token) self.token(token) self.token(token) self.token(token) self.token(token) self.token(token) self.token(to","def valid(self, token): now = time.time() if 'Bearer'in token: token = token[7:] data = None for secret in self.secrets: try: data = jwt.decode(token, secret) break except jwt.DecodeError: continue except jwt.ExpiredSignatureError: raise JwtFailed(""Jwt expired"") if not data: raise JwtFailed(""Jwt cannot be decoded"") exp = data.get('exp') if not exp: raise JwtFailed(""Jwt missing expiration (exp)"") if now - exp > self.age: raise JwtFailed(""Jwt bad expiration - greater than I want to accept"") jti = data.get('jti') if not jti: raise JwtFailed(""Jwt missing one-time id (jti)"") if self.already_used(jti): raise JwtFailed(""Jwt re-use disallowed (jti=)"".format(jti)) return data"
1383,"def direct_upload(self, video_id, only_data): if 'video_id': '124weg': if 'video_id': '124weg': if 'video_id': '124weg': if 'video_id': '124weg': if 'only_data': if 'only_data': if 'only_data': if 'only_data': 'only_data': 'only_data': 'only_data': 'only_data': 'only_data': 'only_data': 'only_data': 'only_data': 'only_data': 'only_data': 'only_data': 'only_data': 'only_data': 'only_data': 'only_data': 'only_data': 'only_data': 'only_data': 'only_data': 'only_data': 'only_data': 'only_data': 'only_data': 'only_data': 'only_data': 'only_data","def direct_upload(request): if request.method == ""POST"": try: form = YoutubeDirectUploadForm(request.POST, request.FILES) if form.is_valid(): uploaded_video = form.save() api = Api() api.authenticate() video_entry = api.upload_direct(uploaded_video.file_on_server.path, ""Uploaded video from zuqqa"") swf_url = video_entry.GetSwfUrl() youtube_url = video_entry.id.text url_parts = youtube_url.split(""/"") url_parts.reverse() video_id = url_parts[0] video = Video() video.user = request.user video.video_id = video_id video.title = 'tmp video' video.youtube_url = youtube_url video.swf_url = swf_url video.save() video_created.send(sender=video, video=video) uploaded_video.delete() return_only_data = request.GET.get('only_data') if return_only_data: return HttpResponse(json.dumps(""video_id"": video_id), content_type=""application/json"") else: try: next_url = settings.YOUTUBE_UPLOAD_REDIRECT_URL except AttributeError: next_url = reverse( ""django_youtube.views.video"", kwargs=""video_id"": video_id) return HttpResponseRedirect(next_url) except: import sys logger.error(""Unexpected error: %s - %s"" % (sys.exc_info()[ 0], sys.exc_info()[1])) return HttpResponse(""error happened"") form = YoutubeDirectUploadForm() if"
1384,"def calculate_tx_gas(self, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas, tx_gas,","def estimate_tx_gas_with_web3(self, safe_address: str, to: str, value: int, data: bytes) -> int: return self.ethereum_client.estimate_gas(safe_address, to, value, data, block_identifier='pending')"
1385,"def deactivate_security_context(self, token): self.get_security_context(context): self.get_security_context(context) self.get_security_context(context) self.get_security_context(context) self.get_security_context(context) self.get_security_context(context) self.get_security_context(context) self.get_security_context(context) self.get_context(context) self.get_context(context) self.get_context) self.get_context(context) self.get_context(context) self.get_context(context) self.get_context) self.get_context(context) self.get_context(context) self.get_context(context) self.get_context) self.get_context(context) self.get_context(context) self.get_context(context) self.get_context(context) self.get_context(context) self.get_context(context) self.get_context(context) self.get_context(context) self.get_con","def export(self): if not (self.flags & C.GSS_C_TRANS_FLAG): raise GSSException(""Context is not transferable."") if not self._ctx: raise GSSException(""Can't export empty/invalid context."") minor_status = ffi.new('OM_uint32[1]') output_token_buffer = ffi.new('gss_buffer_desc[1]') retval = C.gss_export_sec_context( minor_status, self._ctx, output_token_buffer ) try: if GSS_ERROR(retval): if minor_status[0] and self.mech_type: raise _exception_for_status(retval, minor_status[0], self.mech_type) else: raise _exception_for_status(retval, minor_status[0]) exported_token = _buf_to_str(output_token_buffer[0]) self._ctx = ffi.new('gss_ctx_id_t[1]') return exported_token finally: if output_token_buffer[0].length!= 0: C.gss_release_buffer(minor_status, output_token_buffer)"
1386,"def redirect_uri(self, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri, redirect_uri","def validate_redirect_uri(self, client_key, redirect_uri, request): log.debug('Validate redirect_uri %r for %r', redirect_uri, client_key) if not request.client: request.client = self._clientgetter(client_key=client_key) if not request.client: return False if not request.client.redirect_uris and redirect_uri is None: return True request.redirect_uri = redirect_uri return redirect_uri in request.client.redirect_uris"
1387,"def pulse_amplitude(times, amp, phase): pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude = pulse_amplitude","def triangle(times: np.ndarray, amp: complex, period: float, phase: float = 0) -> np.ndarray: return amp*(-2*np.abs(sawtooth(times, 1, period, (phase-np.pi/2)/2)) + 1).astype(np.complex_)"
1388,"def get_title(self, object): self.get_title(object) self.get_title(object) self.get_title(object) self.get_title(object) self.get_title(object) self.get_title(object) self.get_title(object) self.get_title(object) self.get_title(object) self.get_title(object) self.get_title(object) self.get_title(object) self.get_title(object) self.get_title(object) self.get_title(object) self.get_title(object) self.get_title(object) self.get_title(object) self.get_title(object) self.get_title(object) self.get_title(object) self.get_title(object) self.get_title(object) self.get_title(object) self.get_title(object) self.get_title(object) self.get_title(object) self.get_title(object) self.get_title(object) self.get_title(object) self.get_title(object) self.get_title(object) self.get_title(object) self.get_title(object) self.get_title(object) self.get_title(object)","def get_title(self, node): title = node.name if self.module_names: title = ""%s.%s"" % (node.root().name, title) return title"
1389,"def parser_formatter(self, parser_formatter): parser_formatter(self, parser_formatter): parser_formatter(self, parser_formatter): parser_formatter(self, parser_formatter): parser_formatter(self, parser_formatter): parser_formatter(self, parser_formatter): parser_formatter_formatter_formatter_formatter_formatter_formatter_formatter_formatter_formatter_formatter_formatter_formatter_formatter_formatter_formattr_formattr_formattr_formattr_formattr_formattr_formattr_formattr_formattr_formattr_formattr_formattr_formattr_formattr_formattr_formattr_formattr_formattr_formattr_formattr_formattr_formattr_formattr_formattr_formatt","def dispatch_command(function, *args, **kwargs): parser = argparse.ArgumentParser(formatter_class=PARSER_FORMATTER) set_default_command(parser, function) dispatch(parser, *args, **kwargs)"
1390,"def reencrypt_all_users(self, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, logger, reencrypt_user, reencrypt_user, reencrypt_user, reencrypt_user, reencrypt_user, reencrypt_user, reencrypt_user, reencrypt_user, reencrypt","def reencrypt_all_users(engine, old_crypto_factory, new_crypto_factory, logger): logger.info(""Beginning re-encryption for all users."") for user_id in all_user_ids(engine): reencrypt_single_user( engine, user_id, old_crypto=old_crypto_factory(user_id), new_crypto=new_crypto_factory(user_id), logger=logger, ) logger.info(""Finished re-encryption for all users."")"
1391,"def clean_up_after_run(self, runfunc, runkwargs, clean_up_after_run, clean_up_after_run, clean_up_after_run, clean_up_after_run, clean_up_after_run, runfunc, runkwargs, clean_up_after_run, clean_up_after_run, runkwargs, clean_up_after_run, runkwargs, clean_up_after_run, runkwargs, runkwargs, runkwargs, runkwargs, runkwargs, runkwargs.runkwargs.runkwargs.runkwargs.runkwargs.runkwargs.runkwargs.runkwargs.runkwargs.runkwargs.runkwargs.runkwargs.runkwargs.runkwargs.runkwargs.runkwargs.runkwargs.runkwargs.runkwargs.runkwargs.runkwargs.runkwargs.runkwargs.runkwargs.runkwargs.runkwargs","def _single_run(kwargs): pypet_root_logger = logging.getLogger('pypet') traj = kwargs['traj'] runfunc = kwargs['runfunc'] runargs = kwargs['runargs'] kwrunparams = kwargs['runkwargs'] clean_up_after_run = kwargs['clean_up_runs'] automatic_storing = kwargs['automatic_storing'] wrap_mode = kwargs['wrap_mode'] idx = traj.v_idx total_runs = len(traj) pypet_root_logger.info('n=========================================n''Starting single run #%d of %d''n=========================================n' % (idx, total_runs)) traj.f_start_run(turn_into_run=True) result = runfunc(traj, *runargs, **kwrunparams) if automatic_storing: traj.f_store() if wrap_mode == pypetconstants.WRAP_MODE_LOCAL: result = ((traj.v_idx, result), traj.f_get_run_information(traj.v_idx, copy=False), traj.v_storage_service.references) traj.v_storage_service.free_references() else: result = ((traj.v_idx, result), traj.f_get_run_information(traj.v_id"
1392,def delete_thumbnail(thumbnail): delete_thumbnail(thumbnail) delete_thumbnail(thumbnail) delete_thumbnail(thumbnail) delete_thumbnail(thumbnail) delete_thumbnail(thumbnail) delete_thumbnail(thumbnail) delete_thumbnail(thumbnail) delete_thumbnail(thumbnail_thumbnail_thumbnail_thumbnail_thumbnail_thumbnail_thumbnail_thumbnail_thumbnail_thumbnail_thumbnail_thumbnail_thumbnail_thumbnail) delete_thumbnail_thumbnail_thumbnail_thumbnail_thumbnail_thumbnail_thumbnail_thumbnail_thumbnail_thumbnail_thumbnail_thumbnail_thumbnail_thumbnail_thumbnail_,"def delete(source_name, size, metadata_backend=None, storage_backend=None): if storage_backend is None: storage_backend = backends.storage.get_backend() if metadata_backend is None: metadata_backend = backends.metadata.get_backend() storage_backend.delete(get_thumbnail_name(source_name, size)) metadata_backend.delete_thumbnail(source_name, size)"
1393,"def convert_six_text_type(self, six.text_type, six.text_type, six.text_type, six.text_type, six.text_type, six.text_type, six.text_type, six.text_type, six.text_type, six.text_type, six.text_type, six.text_type, six.text_type, six.text_type, six.text_type, six.text_type, six.text_type, six.text_type, six.text_type, six.text_type, six.text_type)","def to_text(s, blank_if_none=True): if s is None: if blank_if_none: return """" else: return None elif isinstance(s, text_type): return s else: return text_type(s)"
1394,"def select_item(self, item): if item is not in list(self, item): if item is not in list(self, item): if item is not in list(self, item): if item is not in list(self, item): if item is not in list(self, item): if item is not in list(self, item): if item is not in list(self, item): if item is not in list(self, item)","def _onSelectItem(self, selection, previousSelection): self._acceptButton.setEnabled(True) del self._selected[:] item = self._filesystemWidget.model().item(selection) self._selected.append(item.path)"
1395,"def get_social_login(self, get_social_login, complete_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get_social_login, get","def get_social_login(self, *args, **kwargs): social_login = super(SocialConnectMixin, self).get_social_login(*args, **kwargs) social_login.state['process'] = AuthProcess.CONNECT return social_login"
1396,def xs_conjoined(xs_conjoined): if xs_conjoined(xs_conjoined): return xs_conjoined(xs_conjoined) if xs_conjoined(xs_conjoined): return xs_conjoined(xs_conjoined),"def conj(coll, *xs): if coll is None: l = llist.List.empty() return l.cons(*xs) if isinstance(coll, IPersistentCollection): return coll.cons(*xs) raise TypeError( f""Object of type type(coll) does not implement Collection interface"" )"
1397,"def get_annual_rushing/receiving_stats(kind, 'R', 'P', 'B'): if 'R', 'P', 'B', 'R', 'R', 'P', 'B', 'R', 'R', 'P', 'B', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'P', 'R', 'R', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P","def rushing_and_receiving(self, kind='R'): doc = self.get_doc() table = (doc('table#rushing_and_receiving') if kind == 'R' else doc('table#rushing_and_receiving_playoffs')) if not table: table = (doc('table#receiving_and_rushing') if kind == 'R' else doc('table#receiving_and_rushing_playoffs')) df = sportsref.utils.parse_table(table) return df"
1398,"def pretrained(self, pretrained, pretrained, pretrained, pretrained, pretrained, pretrained, pretrained, pretrained, pretrained, pretrained, pretrained, pretrained, pretrained, pretrained, pretrained, pretrained, pretrained, pretrained, pretrained, pretrained, pretrained, pretrained, pretrained, pretrained, pretrained, pretrained, pretrained, pretrained, pretrained)","def squeezenet1_1(pretrained=False, **kwargs): rmodel = SqueezeNet(version=1.1, **kwargs) if pretrained: model.load_state_dict(model_zoo.load_url(model_urls['squeezenet1_1'])) return model"
1399,"def xy_report(self, xy_report, xy_report, xy_report, xy_report, xy_report, xy_report, xy_report, xy_report, xy_report, xy_report, xy_report, xy_report, xy_report)","def write_xy_report(odb, path, tags, columns, steps): xyData = [session.XYDataFromHistory(name = columns[i], odb = odb, outputVariableName = tags[i], steps = steps) for i in xrange(len(tags))] session.xyReportOptions.setValues(numDigits=8, numberFormat=SCIENTIFIC) session.writeXYReport(fileName=path, appendMode=OFF, xyData=xyData)"
1400,def remove_key(key): clean_dict(key): clean_dict(key): clean_dict(key): clean_dict(key): clean_dict(key): clean_dict(key): clean_dict(key): clean_dict(key): clean_dict(key): clean_dict(key): clean_dict(key): clean_dict(key): clean_dict(key): clean_dict(key): clean_dict(key),"def clean_dict(dict): if sys.version_info[0]  3: return k: v for k, v in dict.iteritems() if v is not None else: return k: v for k, v in dict.items() if v is not None"
1401,"def create_toolkit_widget(self, toolkit_widget): self.create_toolkit_widget(toolkit_widget) self.create_toolkit_widget(toolkit_widget) self.create_toolkit_widget(toolkit_widget) self.create_toolkit_widget(toolkit_widget) self.create_toolkit_widget(toolkit_widget) self.create_toolkit_widget) self.create_toolkit_widget(toolkit_widget) self.create_toolkit_widget(toolkit_widget) self.create_toolkit_widget(toolkit_widget) self.create_toolkit_widget(toolkit_widget) self.create_toolkit_widget(toolkit_widget) self.create_toolkit_widget(toolkit_widget) self.create_toolkit_widget(toolkit_widget) self.create_toolkit_widget(toolkit_widget) self.create_","def create_widget(self): self.widget = SubElement(self.parent_widget(), self.declaration.tag)"
1402,"def sparse_matrices(self, sparse_matrices, sparse_matrices, sparse_matrices, sparse_matrices, sparse_matrices, sparse_matrices, sparse_matrices, sparse_matrices, sparse_matrices, sparse_matrices, sparse_matrices, sparse_matrices, sparse_matrices, sparse_matrices, sparse_matrices, sparse_matrices, sparse_matrices)","def _values_of_same_type(self, val1, val2): if self._is_supported_matrix(val1) and self._is_supported_matrix(val2): return True else: return super(SparseParameter, self)._values_of_same_type(val1, val2)"
1403,def get_chart(self): if self.chart(self.chart(self.chart): self.chart(self.chart),"def url(self): self.render() return self._apiurl + '&'.join(self._parts()).replace(' ','+')"
1404,"def callback(self, callback): self.callback(self.callback) self.callback(self.callback) self.callback(self.callback) self.callback(self.callback) self.callback(self.callback) self.callback(self.callback) self.callback(self.callback) self.callback(self.callback) self.callback(self.callback) self.callback(self.callback) self.callback(self.callback) self.callback","def set_info_callback(self, callback): @wraps(callback) def wrapper(ssl, where, return_code): callback(Connection._reverse_mapping[ssl], where, return_code) self._info_callback = _ffi.callback( ""void (*)(const SSL *, int, int)"", wrapper) _lib.SSL_CTX_set_info_callback(self._context, self._info_callback)"
1405,"def initialize_local_cache(self, cache_name): self.cache(cache_name) self.cache(cache_name) self.cache(cache_name) self.cache(cache_name) self.cache(cache_name) self.cache(cache_name) self.cache(cache_name) self.cache(cache_name) self.cache(cache_name) self.cache(cache_name) self.cache(cache_name) self.cache_name) self.cache_name) self.cache_name) self.cache_name) self.cache_name(cache_name) self.cache_name) self.cache_name) self.cache_name(cache_name) self.cache_name) self.cache_name(cache_name) self.cache_name) self.cache_name(cache_name) self.cache_name) self.cache_name(cache_name) self.cache_name(cache_name) self.cache_name(cache_name) self.cache_name(cache_name) self.cache_name(cache_name) self","def init(self): cache.get('sitetrees_reset') and self.empty(init=False) self.cache = cache.get('sitetrees','sitetrees':, 'parents':, 'items_by_ids':, 'tree_aliases': )"
1406,"def raiseVersionNotFound('VersionNotFound', 'VersionNotFound', 'VersionNotFound', 'VersionNotFound', 'VersionNotFound', 'VersionNotFound', 'VersionNotFound', 'VersionNotFound', 'VersionNotFound', 'VersionNotFound', 'VersionNotFound', 'VersionNotFound', 'VersionNotFound', 'VersionNotFound', 'VersionNotFound', 'VersionNotFound', 'VersionNotFound', 'VersionNotFound', 'VersionNotFound', 'VersionNotFound', 'VersionNotFound', 'VersionNotFound', 'VersionNotFound', 'VersionNotFound', 'VersionNotFound', 'VersionNotFound', 'VersionNotFound', 'VersionNotFound', 'VersionNotFound',","def version_from_wsdl(wsdl_tree): prefix = _default_ns_prefix(wsdl_tree.nsmap) safe_map = wsdl_tree.nsmap.copy() try: del safe_map[None] except KeyError: pass try: version_el = wsdl_tree.xpath( 'wsdl:service/wsdl:documentation/' '*[self::p:version or self::p:Version]'.format(p=prefix), namespaces=safe_map )[0] except IndexError: raise VersionNotFound( 'Version not found in WSDL service documentation' ) else: return version_el.text"
1407,"def debug_info(self, debug_info): self.debug_info(self.debug_info) self.debug_info(self.debug_info) self.debug_info(self.debug_info) self.debug_info(self.debug_info) self.debug_info(self.debug_info) self.debug_info(self.debug_info) self.debug_info(self.debug_info) self.debug_info(self.debug_info) self.debug_info(self.debug_info) self.debug_info(self.debug_info) self.debug_info(self.debug_info) self.debug_info(self.debug_info) self.debug_info(self.debug_info) self.debug_info(self.debug_info) self.debug_info(self.debug_info) self.debug_info(self.debug_info) self.debug_info(self.debug_info) self.debug_info(self.debug_info) self.debug_info(self.debug_info) self.debug_info(self.debug_info) self.debug","def debug_storage(storage, base_info=False, chars=True, runs=False): ""Display debug information for the storage"" import codecs import locale import sys if six.PY2: stderr = codecs.getwriter(locale.getpreferredencoding())(sys.stderr) else: stderr = sys.stderr caller = inspect.stack()[1][3] stderr.write('in %sn' % caller) if base_info: stderr.write(u' base level : %dn' % storage['base_level']) stderr.write(u' base dir : %sn' % storage['base_dir']) if runs: stderr.write(u' runs : %sn' % list(storage['runs'])) if chars: output = u' Chars :'for _ch in storage['chars']: if _ch!= 'n': output += _ch['ch'] else: output += 'C' stderr.write(output + u'n') output = u' Res. levels : %sn' % u''.join( [six.text_type(_ch['level']) for _ch in storage['chars']]) stderr.write(output) _types = [_ch['type'].ljust(3) for _ch in storage['chars']] for i in range(3): if i: output = u' %sn' else: output = u' Res. types : %sn' stderr.write(output % u''.join([_t[i] for _t in _types]))"
1408,"def parse_frequency(self, variant, info_key(str, variant, variant, info_key(str, variant, variant, info_key(str, variant, variant, variant, variant, variant, variant, variant, info_key(str, variant, variant, variant, variant, variant, variant, variant, info_key, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, info_key(str, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant, variant,","def parse_frequency(variant, info_key): raw_annotation = variant.INFO.get(info_key) raw_annotation = None if raw_annotation == '.' else raw_annotation frequency = float(raw_annotation) if raw_annotation else None return frequency"
1409,"def _bottom_line(self, bottom line, bottom line, bottom line, bottom line, bottom line, bottom line, bottom line, bottom line, bottom line, bottom line, bottom line, bottom line, bottom line, bottom line, bottom line, bottom line, bottom line, bottom line, bottom line, bottom line, bottom line, bottom line)","def bot(self): ret = self.bot_format % self.bot_connect.center( self.width, self.bot_pad) if self.right_fill: ret = ret.ljust(self.right_fill, self.bot_pad) if self.left_fill: ret = ret.rjust(self.left_fill, self.bot_pad) ret = ret.center(self.layer_width, self.bot_bck) return ret"
1410,"def list_overview_fmt(self, list_overview_fmt, list_overview_fmt, list_overview_fmt, list_overview_fmt, list_overview_fmt, list_overview_fmt, list_overview_fmt, list_overview_fmt, list_overview_fmt, list_overview_fmt, list_overview_fmt, list_overview_fmt, list_overview_fmt)","def list_overview_fmt_gen(self): code, message = self.command(""LIST OVERVIEW.FMT"") if code!= 215: raise NNTPReplyError(code, message) for line in self.info_gen(code, message): try: name, suffix = line.rstrip().split("":"") except ValueError: raise NNTPDataError(""Invalid LIST OVERVIEW.FMT"") if suffix and not name: name, suffix = suffix, name if suffix and suffix!= ""full"": raise NNTPDataError(""Invalid LIST OVERVIEW.FMT"") yield (name, suffix == ""full"")"
1411,"def create_new_node(self, full_name, kwargs, args, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwar","def _construct_instance(self, constructor, full_name, *args, **kwargs): if getattr(constructor, 'KNOWS_TRAJECTORY', False): return constructor(full_name, self, *args, **kwargs) else: return constructor(full_name, *args, **kwargs)"
1412,"def find_IPython_module(self,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,","def get_ipython_module_path(module_str): if module_str == 'IPython': return os.path.join(get_ipython_package_dir(), '__init__.py') mod = import_item(module_str) the_path = mod.__file__.replace('.pyc', '.py') the_path = the_path.replace('.pyo', '.py') return py3compat.cast_unicode(the_path, fs_encoding)"
1413,"def _persist(self, file_like): self.persist(file_like) self.persist(file_like) self.persist(file_like) self.persist(file_like) self.persist(file_like) self.persist(file_like) self.persist(file_like) self.persist(file_like) self.persist(file_like) self.persist(file_like) self.persist(file_like) self.persist(file_like)","def save(self, output): assert self._mph, ""There is no MPH?"" if isinstance(output, six.string_types): with open(abspath(output), 'w') as out: _cmph.cmph_dump(self._mph, out) else: _cmph.cmph_dump(self._mph, output)"
1414,"def get_optparse(self, command_options): self.get_optparse(command_options): self.get_optparse(command_options) self.get_optparse(command_options) self.get_optparse(command_options) self.get_optparse(command_options) self.get_optparse(command_options) self.get_optparse(command_options) self.get_optparse(command_options) self.get_optparse(command_options) self.get_optparse(command_options) self.get_optparse(command_options) self.get_optparse(command_options) self.get_optparse(command_options) self.get_optparse(command_options) self.get_optparse(command_options) self.get_optparse(command_options) self.get_optparse(command_options) self.get_optparse(command_options) self.get_optparse(command_options) self.get_optparse(command_options) self","def options_getter(command_options): def get_options(option_func=None): from optparse import make_option from django.core.management.base import BaseCommand func = option_func or make_option options = tuple([func(*option.args, **option.kwargs) for option in command_options]) if option_func is None: if VERSION  (1, 8): result = BaseCommand.option_list + options else: result = [] else: result = options return result return get_options"
1415,"def convert_seconds(self, hh, mm, ss): self.convert(self.convert(self.convert(self.convert(self.convert(self.convert(self.convert(self.convert(self.convert(self.convert(self.convert(self.convert(self.convert(self.convert(self.convert(self.convert(self.convert(self.convert))) self.convert(self.convert(self.convert(self.convert(self.convert(self.convert(self.convert(self.convert(self.convert(self.convert(self.convert))))))))))))))","def s2h(ss): mm, ss = divmod(ss, 60) hh, mm = divmod(mm, 60) dd, hh = divmod(hh, 24) tstr = ""%02i:%04.1f"" % (mm, ss) if hh > 0: tstr = (""%02i:"" % hh) + tstr if dd > 0: tstr = (""%id "" % dd) + tstr return tstr"
1416,"def request_token_handler_decorator(self, dictionary): self.decorator(self.decorator): self.decorator(self.decorator) self.decorator(self.decorator) self.decorator(self.decorator) self.decorator(self.decorator) self.decorator(self.decorator) self.decorator(self.decorator) self.decorator(self.decorator) self.decorator(self.decorator) self.decorator(self.decorator) self.decorator(self.decorator) self.decorator) self.decorator(self.decorator) self.decorator) self.decorator(self.decorator) self.decorator) self.decorator(self.decorator) self.decorator) self.decorator(self.decorator) self.decorator(self.decorator) self.decorator(self.decorator) self.decorator) self.decorator(self.decorator) self.decorator(self.decorator) self.decorator(self.decorator) self.decorator(self.de","def request_token_handler(self, f): @wraps(f) def decorated(*args, **kwargs): server = self.server uri, http_method, body, headers = extract_params() credentials = f(*args, **kwargs) try: ret = server.create_request_token_response( uri, http_method, body, headers, credentials) return create_response(*ret) except errors.OAuth1Error as e: return _error_response(e) return decorated"
1417,def ________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________,"def admin_footer(parser, token): tag_name = token.split_contents() if len(tag_name) > 1: raise base.TemplateSyntaxError('tag does not accept any argument(s): '.format( token.contents.split()[0], ', '.join(token.contents.split()[1:]) )) return AdminFooterNode()"
1418,def ________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________,"def _prepare_static_info(self): pipeline_files =  with open(join(self.workdir, self.pipeline_name)) as fh: pipeline_files[""pipelineFile""] = fh.readlines() nf_config = join(self.workdir, ""nextflow.config"") if os.path.exists(nf_config): with open(nf_config) as fh: pipeline_files[""configFile""] = fh.readlines() configs =  ""params.config"": ""paramsFile"", ""resources.config"": ""resourcesFile"", ""containers.config"": ""containersFile"", ""user.config"": ""userFile"",  for config, key in configs.items(): cfile = join(self.workdir, config) if os.path.exists(cfile): with open(cfile) as fh: pipeline_files[key] = fh.readlines() return pipeline_files"
1419,"def add_subparsers(args, kwargs, args, kwargs, args, kwargs, args, kwargs, args, kwargs, args, kwargs, args, kwargs, args, kwargs, args, kwargs, args, kwargs, args, kwargs, args, args, args, args, args, kwargs, args, kwargs, args, kwargs, kwargs)","def set_subparsers_args(self, *args, **kwargs): self.subparsers_args = args self.subparsers_kwargs = kwargs"
1420,"def convert_args(args, name, dtype): args = args, dtype = dtype, dtype = dtype, dtype = dtype, dtype = dtype, dtype = dtype, dtype = dtype, dtype = dtype, dtype = dtype, dtype = dtype, dtype = dtype, dtype = dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, d","def convert_args_to_tensor(args, dtype=None, name=None): if dtype is None: if expand_as_args(args) or _expand_as_kwargs(args): shallow_args = _get_shallow_structure(args) return nest.map_structure_up_to( shallow_args, lambda s: _nested_convert_to_tensor(s, name=name), args) else: return _nested_convert_to_tensor(args, name=name) else: return nest.map_structure_up_to( dtype, lambda s, dtype: _nested_convert_to_tensor(s, dtype, name), args, dtype)"
1421,"def build_file_path(self, path): if self.path(path): self.path(path): self.path(path): self.path(path)","def read(*paths): filename = os.path.join(*paths) with codecs.open(filename, mode='r', encoding='utf-8') as handle: return handle.read()"
1422,"def get_wave_functions(indep_min, indep_max, indep_min): if indep_min = indep_max, wave = indep_min, wave = indep_max, wave = indep_min, wave = indep_max, wave = indep_min, wave = indep_max, wave = indep_min, wave = indep_max, wave = indep_min, wave = indep_max, wave = indep_max, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave","def nmin(wave, indep_min=None, indep_max=None): rret = copy.copy(wave) _bound_waveform(ret, indep_min, indep_max) return np.min(ret._dep_vector)"
1423,"def override_configuration(self, configuration): if self.configure(configure): self.configure(configure): self.configure(configure) self.configure(configure) self.configure(configure) self.configure(configure) self.configure(configure) self.configure(configure) self.configure(configure) self.configure(configure) self.configure(configure) self.configure(configure) self.configure(configure) self.configure) self.configure(configure) self.configure) self.configure) self.configure(configure) self.configure) self.configure) self.configure(configure) self.configure) self.configure) self.configure(configure) self.configure) self.configure(configure) self.configure) self.configure) self.configure(configure) self.configure) self.configure(configure) self.configure) self.configure(configure) self.configure) self.configure(configure) self.configure(configure) self.configure(configure) self.configure) self.configure(configure) self.configure(configure) self.config","def load_command_line_configuration(self, args=None): with _patch_optparse(): if args is None: args = sys.argv[1:] else: args = list(args) (options, args) = self.cmdline_parser.parse_args(args=args) for provider in self._nocallback_options: config = provider.config for attr in config.__dict__.keys(): value = getattr(options, attr, None) if value is None: continue setattr(config, attr, value) return args"
1424,"def _valid_audio(y, np.ndarray): if y is not np.ndarray: if y is not np.ndarray: if y is not np.ndarray: if y is not np.ndarray: if y is not np.ndarray: if y is not np.ndarray: if y is not np.ndarray: librosa.librosa.librosa.librosa.librosa.librosa.librosa.librosa.librosa.librosa.librosa.librosa.librosa.librosa.librosa.librosa.librosa.librosa.librosa.librosa.librosa.librosa.librosa.librosa.librosa.librosa.liblibrosa.liblibrosa.liblib.lib.lib.lib.lib.lib.lib.lib.lib.lib.lib.lib.lib.lib.lib.lib.lib.lib.lib.lib.lib.lib.lib.lib.lib.lib.lib.lib.lib.lib.lib.lib.lib.","def valid_audio(y, mono=True): if not isinstance(y, np.ndarray): raise ParameterError('data must be of type numpy.ndarray') if not np.issubdtype(y.dtype, np.floating): raise ParameterError('data must be floating-point') if mono and y.ndim!= 1: raise ParameterError('Invalid shape for monophonic audio:''ndim=:d, shape='.format(y.ndim, y.shape)) elif y.ndim > 2 or y.ndim == 0: raise ParameterError('Audio must have shape (samples,) or (channels, samples).''Received shape='.format(y.shape)) if not np.isfinite(y).all(): raise ParameterError('Audio buffer is not finite everywhere') return True"
1425,"def hidden_for_sketch(self, sketch_hidden_size, attention_depth, csoftmax, attention_depth, csoftmax, attention_depth, csoftmax, attention_depth, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax, csoftmax","def attention_gen_block(hidden_for_sketch, hidden_for_attn_alignment, key, attention_depth): with tf.name_scope('attention_block'): sketch_dims = tf.shape(hidden_for_sketch) batch_size = sketch_dims[0] num_tokens = sketch_dims[1] hidden_size = sketch_dims[2] attn_alignment_dims = tf.shape(hidden_for_attn_alignment) attn_alignment_hidden_size = attn_alignment_dims[2] sketches = [tf.zeros(shape=[batch_size, hidden_size], dtype=tf.float32)] aligned_hiddens = [] cum_att = tf.zeros(shape=[batch_size, num_tokens]) for i in range(attention_depth): sketch, cum_att_, aligned_hidden = attention_gen_step(hidden_for_sketch, hidden_for_attn_alignment, sketches[-1], key, cum_att) sketches.append(sketch) #sketch aligned_hiddens.append(aligned_hidden) #sketch cum_att += cum_att_ final_aligned_hiddens = tf.reshape(tf.transpose(tf.stack(aligned_hiddens), [1, 0, 2]),[1, attention_depth, attn_alignment_hidden_size]) return final_aligned_hiddens"
1426,"def reconstruct(self, v_protocol, v_protocol, v_protocol, v_protocol, v_protocol, v_protocol, v_protocol, v_protocol, v_protocol, v_protocol, v_protocol, v_protocol, v_protocol, v_protocol, v_protocol, v_protocol, v_protocol, v_protocol)","def _load(self, load_dict): try: self.v_protocol = load_dict.pop(PickleParameter.PROTOCOL) except KeyError: dump = next(load_dict.values()) self.v_protocol = PickleParameter._get_protocol(dump) for key in load_dict: val = load_dict[key] self._data[key] = pickle.loads(val)"
1427,"def generate_id(self, id): if self.get_id(id): self.get_id(id): self.get_id(id): self.get_id(id): self.get_id(id): self.get_id(id)","def get_pk_value_on_save(self, instance): value = super(AleaIdField, self).get_pk_value_on_save(instance) if not value: value = self.get_seeded_value(instance) return value"
1428,"def get_bots(self, limit=50, offset=0, offset=0, offset=0, offset=0, offset=0, offset=0, limit=50, offset=0, offset=0, offset=0, offset=0, offset=0, offset=0, offset=0, offset=0, offset=0, offset=0, offset=0, offset=0, offset=0, offset=0, offset=0, offset=0, offset=0, offset=0, offset=0, offset=====================================================================================================================================================================================","async def get_bots(self, limit: int = 50, offset: int = 0): return await self.http.get_bots(limit, offset)"
1429,def defer(self): if self.get_element(self.get_element(self.get_element(self.get_element(self.get_element(self.get_element(self.get_element(self.get_element(self.get_element(self.get_element(self.get_element(self.get_element(self.get_element(self.get_element(self.get_element(self.get_element(self.get_element(self.get_element(self.get_element(self.get_element)))))))))),"def fromStringProto(self, inString, proto): value, = amp.AmpList.fromStringProto(self, inString, proto) return value"
1430,def remove_current_test_element(self): self.mining_data(self.mining_data) self.mining_data(self.mining_data) self.mining_data(self.mining_data) self.mining_data(self.mining_data) self.mining_data(self.mining_data) self.mining_data(self.mining_data) self.mining_data(self.mining_data) self.mining_data(self.mining_data) self.mining_data(self.mining_data) self.mining_data(self.mining_data) self.mining_data(self.mining_data) self.mining_data(self.mining_data) self.mining_data(self.mining_data) self.mining_data(self.mining_data) self.mining_data(self.mining_data) self.mining_data(self.mining_data) self.mining_data(self.mining_data) self.mining_data(self.mining_data) self.mining_data(self.mining_data) self.mining_data(self.mining_data) self.mining_,"def remove(self): if PyFunceble.CONFIGURATION[""mining""]: if PyFunceble.INTERN[""file_to_test""] in PyFunceble.INTERN[""mined""]: for element in PyFunceble.INTERN[""mined""][ PyFunceble.INTERN[""file_to_test""] ]: if ( self.to_get_bare in PyFunceble.INTERN[""mined""][ PyFunceble.INTERN[""file_to_test""] ][element] ): PyFunceble.INTERN[""mined""][PyFunceble.INTERN[""file_to_test""]][ element ].remove(self.to_get_bare) self._backup()"
1431,"def check_node(self, node, node, node, node, node, node, node, node, node, node, node, node, node, node, node, node, node, node, node)","def _check_position(self, node): if self._first_non_import_node: self.add_message(""wrong-import-position"", node=node, args=node.as_string())"
1432,def update_variants(variants): variants(variants): variants(variants): variants(variants): variants(variants): variants(variants): variants(variants): variants(variants): variants(variants),"def update_compounds(self, variants): LOG.debug(""Updating compound objects"") for var_id in variants: variant_obj = variants[var_id] if not variant_obj.get('compounds'): continue updated_compounds = self.update_variant_compounds(variant_obj, variants) variant_obj['compounds'] = updated_compounds LOG.debug(""Compounds updated"") return variants"
1433,Transform Stinespring representation to Choi representation. Transform Stinespring representation to Choi representation. Transform Stinespring representation.,"def _stinespring_to_choi(data, input_dim, output_dim): trace_dim = data[0].shape[0] // output_dim stine_l = np.reshape(data[0], (output_dim, trace_dim, input_dim)) if data[1] is None: stine_r = stine_l else: stine_r = np.reshape(data[1], (output_dim, trace_dim, input_dim)) return np.reshape( np.einsum('iAj,kAl->jilk', stine_l, stine_r.conj()), 2 * [input_dim * output_dim])"
1434,"def parse_encoding_cookie(self, url, errors): url = self.__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________","def read_py_url(url, errors='replace', skip_encoding_cookie=True): response = urllib.urlopen(url) buffer = io.BytesIO(response.read()) encoding, lines = detect_encoding(buffer.readline) buffer.seek(0) text = TextIOWrapper(buffer, encoding, errors=errors, line_buffering=True) text.mode = 'r' if skip_encoding_cookie: return """".join(strip_encoding_cookie(text)) else: return text.read()"
1435,def enaml_event(enaml_event): if enaml_event(enaml_event): if enaml_event(enaml_event): if enaml_event(enaml_event): enaml_event(enaml_event): enaml_event(enaml_event): enaml_event(enaml_event): enaml_event): enaml_event(enaml_event): enaml_event(enaml_event): enaml_event): enaml_event(enaml_event): enaml_event(enaml_event): enaml_event(enaml_event): enaml_event): enaml_event(enaml_event): enaml_event(enaml_event): enaml_event): enaml_event(enaml_event): enaml_event(enaml_event),"def on_dom_modified(self, change): log.debug(f'Update from enaml: change') self.write_message(json.dumps(change['value']))"
1436,"def transition_uniform(n_states, n_states, n_states, n_states, n_states, n_states, n_states, n_states, n_states, n_states, n_states, n_states, n_states, n_states, n_states, n_states, n_states, transition_uniform, transition_uniform, transition_uniform, transition_uniform, transition_uniform, transition_uniform, transition_uniform, transition_uniform, transition_uniform, transition_uniform, transition_uniform, transition_uniform, transition_uniform, transition_uniform, transition_uniform, transition_uniform, transition_uniform, transition_uniform, transition_uniform, transition_uniform, transition_uniform, transition_uniform, transition_uniform, transition_uniform, transition_uniform, transition_uniform, transition_uniform, transition_uniform, transition_uniform, transition_uniform, transition_uniform, transition_uni","def transition_uniform(n_states): if not isinstance(n_states, int) or n_states = 0: raise ParameterError('n_states= must be a positive integer') transition = np.empty((n_states, n_states), dtype=np.float) transition.fill(1./n_states) return transition"
1437,"def xpath(self, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath) xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath, xpath","def xml_find(xpath): def xpath_find(value): validate(ET.iselement, value) value = value.find(xpath) if value is None: raise ValueError(""XPath '0' did not return an element"".format(xpath)) return validate(ET.iselement, value) return transform(xpath_find)"
1438,"def parse_html(self, base_url, docinfo.URL, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url): base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_url, base_","def fromstring(html, guess_charset=True, parser=None): if not isinstance(html, _strings): raise TypeError('string required') doc = document_fromstring(html, parser=parser, guess_charset=guess_charset) start = html[:50].lstrip().lower() if start.startswith('html') or start.startswith('!doctype'): return doc head = _find_tag(doc, 'head') if len(head): return doc body = _find_tag(doc, 'body') if (len(body) == 1 and (not body.text or not body.text.strip()) and (not body[-1].tail or not body[-1].tail.strip())): return body[0] if _contains_block_level_tag(body): body.tag = 'div' else: body.tag ='span' return body"
1439,"def create_job(self, cloud_service_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id, job_collection_id","def create_job(self, cloud_service_id, job_collection_id, job_id, job): _validate_not_none('cloud_service_id', cloud_service_id) _validate_not_none('job_collection_id', job_collection_id) _validate_not_none('job_id', job_id) _validate_not_none('job', job) path = self._get_job_collection_path( cloud_service_id, job_collection_id, job_id) self.content_type = ""application/json"" return self._perform_put(path, JSONEncoder().encode(job), as_async=True)"
1440,"def sympy_access(self, source, destination): if self.sympy_access(source, destination): self.sympy_access(source, destination): self.sympy_access(source, destination)","def compile_sympy_accesses(self, sources=True, destinations=True): sympy_accesses = defaultdict(list) for var_name in self.variables: if sources: for r in self.sources.get(var_name, []): if r is None: continue sympy_accesses[var_name].append(self.access_to_sympy(var_name, r)) if destinations: for w in self.destinations.get(var_name, []): if w is None: continue sympy_accesses[var_name].append(self.access_to_sympy(var_name, w)) return sympy_accesses"
1441,"def get_genmodel_jar(path, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name, genmodel_name,","def download_mojo(self, path=""."", get_genmodel_jar=False, genmodel_name=""""): assert_is_type(path, str) assert_is_type(get_genmodel_jar, bool) if not self.have_mojo: raise H2OValueError(""Export to MOJO not supported"") if get_genmodel_jar: if genmodel_name == """": h2o.api(""GET /3/h2o-genmodel.jar"", save_to=os.path.join(path, ""h2o-genmodel.jar"")) else: h2o.api(""GET /3/h2o-genmodel.jar"", save_to=os.path.join(path, genmodel_name)) return h2o.api(""GET /3/Models/%s/mojo"" % self.model_id, save_to=path)"
1442,"def accept_visit_name(self, visit_name): if 'accept' is not self.get('visit_name'): return self.get('visit_name') self.get('visit_name') self.get('visit_name') self.get('visit_name') self.get('visit_name') self.get('visit_name') self.get('visit_name') self.get('visit_name') self.get('visit_name') self.get('visit_name') self.get('visit_name') self.get('visit_name') self.get('visit_name') self.get('visit_name') self.get('visit_name') self.get('visit_name') self.get('visit_name') self.get('visit_name') self.get('visit_name') self.get('visit_name') self.get('visit_name') self.get('visit_name') self.get('visit_name') self.get('visit_name","def _get_visit_name(self): try: return self.TYPE.replace(""-"", ""_"") except Exception: return self.__class__.__name__.lower()"
1443,"def tuple(latest release, latest stable): return tuple(latest release, latest stable)","def get_relevant_versions(self, package_name: str): versions = self.get_ordered_versions(package_name) pre_releases = [version for version in versions if not version.is_prerelease] return ( versions[-1], pre_releases[-1] )"
1444,"def copy_image(self, name, 'name'): if self.copy(self.copy, 'name'): self.copy(self.copy, 'name') self.copy(self.copy, 'name') self.copy(self.copy, 'name') self.copy(self.copy, 'name') self.copy(self.copy, 'name') self.copy(self.copy, 'name') self.copy(self.copy(self.copy, 'name') self.copy, 'name') self.copy(self.copy, 'name') self.copy, 'name' self.copy, 'name' self.copy, 'name' self.copy, 'name' self.copy, 'name' self.copy, 'name' self.copy, 'name' self.copy, 'name' self.copy, 'name' self.copy, 'name' self.copy, 'name' self.copy, 'name' self.copy, 'name' self.copy, 'name' self.copy, 'name' self.copy, 'name' self.copy, 'name' self.copy, 'name' self.copy, '","def _copy_image(self, name): image = self._get_image(name) QtGui.QApplication.clipboard().setImage(image)"
1445,"def list_rooms(self, teamId, sortBy, max, request_parameters, request_parameters): self.get_rooms(self.get_rooms) self.get_rooms(self.get_rooms) self.get_rooms(self.get_rooms) self.get_rooms(self.get_rooms) self.get_rooms(self.get_rooms) self.get_rooms(self.get_rooms) self.get_rooms) self.get_rooms(self.get_rooms) self.get_rooms) self.get_rooms(self.get_rooms) self.get_rooms(self.get_rooms) self.get_rooms(self.get_rooms) self.get_rooms(self.get_rooms) self.get_rooms(self.get_rooms) self.get_rooms) self.get_rooms(self.get_rooms) self.get_rooms(self.get_rooms) self.get_rooms(self.get_rooms) self.get_rooms(self.get_rooms) self.get_rooms(self.get_rooms) self.get_rooms(self","def list(self, teamId=None, type=None, sortBy=None, max=None, **request_parameters): check_type(teamId, basestring) check_type(type, basestring) check_type(sortBy, basestring) check_type(max, int) params = dict_from_items_with_values( request_parameters, teamId=teamId, type=type, sortBy=sortBy, max=max, ) items = self._session.get_items(API_ENDPOINT, params=params) for item in items: yield self._object_factory(OBJECT_TYPE, item)"
1446,"def return_system_cpu_times(self, system_cpu_times): self.cpu_times(system_cpu_times) self.cpu_times(system_cpu_times) self.cpu_times(system_cpu_times) self.cpu_times(system_cpu_times) self.cpu_times(system_cpu_times) self.cpu_times(system_cpu_times) self.cpu_times) self.cpu_times(system_cpu_times) self.cpu_times) self.cpu_times(system_cpu_times) self.cpu_times(system_cpu_times) self.cpu_times) self.cpu_times(system_cpu_times) self.cpu_times) self.cpu_times_cpu_times_cpu_times_cpu_times_cpu_times_cpu_times_cpu_times_cpu_times_cpu_times_cpu_times_cpu_times_cpu_times_cpu_times_cpu_time","def get_system_per_cpu_times(): ret = [] for cpu_t in _psutil_bsd.get_system_per_cpu_times(): user, nice, system, idle, irq = cpu_t item = _cputimes_ntuple(user, nice, system, idle, irq) ret.append(item) return ret"
1447,"def _encode(self, SECRET_KEY): self.encode(self.encode) self.encode(self.encode) self.encode(self.encode) self.encode(self.encode) self.encode(self.encode) self.encode(self.encode) self.encode(self.encode) self.encode(self.encode) self.encode(self.encode) self.encode(self.encode) self.encode(self.encode) self.encode(self.encode) self.encode(self.encode) self.encode(self.encode) self.encode(self.encode) self.encode(self.encode) self.encode(self.encode) self.encode(self.encode) self.encode(self.encode) self.encode(self.encode) self.encode(self.encode) self.encode(self.encode) self.encode(self.encode) self.encode(self.encode) self.encode(self.encode) self.encode(self.encode) self.encode(self.encode) self.encode(self.encode) self.",def encode(something): secret_key = current_app.config.get('SECRET_KEY') s = URLSafeSerializer(secret_key) return s.dumps(something)
1448,def ________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________,"def permissions(self): api_version = self._get_api_version('permissions') if api_version == '2015-07-01': from.v2015_07_01.operations import PermissionsOperations as OperationClass elif api_version == '2018-01-01-preview': from.v2018_01_01_preview.operations import PermissionsOperations as OperationClass else: raise NotImplementedError(""APIVersion  is not available"".format(api_version)) return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))"
1449,"def label(self, axes, args): args = self.args(args) self.args(args) self.args(args) self.args(args) self.args(args) self.args(args) self.args(args) self.args(args) self.args(args) self.args(args) self.args(args) self.args(args) self.args) self.args) self.args(args) self.args(args) self.args) self.args(args) self.args) self.args) self.args","def label(self, index, *args): self.data['labels'].append( str('%s:|%s'%(index, '|'.join(map(str,args)) )).replace('None','') ) return self.parent"
1450,"def write(ostream, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_","def write(self, ostream, kmip_version=enums.KMIPVersion.KMIP_1_0): tstream = BytearrayStream() if self.unique_identifier is not None: self.unique_identifier.write(tstream, kmip_version=kmip_version) self.length = tstream.length() super(ActivateRequestPayload, self).write( ostream, kmip_version=kmip_version ) ostream.write(tstream.buffer)"
1451,def rotate(self): if self.rotate(self.rotate): self.rotate(self.rotate) self.rotate(self.rotate) self.rotate(self.rotate) self.rotate(self.rotate) self.rotate(self.rotate) self.rotate(self.rotate) self.rotate(self.rotate) self.rotate(self.rotate) self.rotate(self.rotate) self.rotate(self.rotate) self.rotate(self.rotate) self.rotate(self.rotate) self.rotate(self.rotate) self.rotate) self.rotate(self.rotate) self.rotate(self.rotate) self.rotate) self.rotate(self.rotate) self.rotate(self.rotate) self.rotate) self.rotate(self.rotate) self.rotate(self.rotate) self.rotate) self.rotate(self.rotate) self.rotate(self.rotate) self.rotate(self.rotate) self.rotate(self.rotate) self.rotate) self.rotate(self.rotate) self.rotate(self.rotate) self.rotate,"def rotate(script, axis='z', angle=0.0): angle = math.radians(angle) if axis.lower() == 'x': vert_function(script, x_func='x', y_func='y*cos(angle)-z*sin(angle)'.format(angle=angle), z_func='y*sin(angle)+z*cos(angle)'.format(angle=angle)) elif axis.lower() == 'y': vert_function(script, x_func='z*sin(angle)+x*cos(angle)'.format(angle=angle), y_func='y', z_func='z*cos(angle)-x*sin(angle)'.format(angle=angle)) elif axis.lower() == 'z': vert_function(script, x_func='x*cos(angle)-y*sin(angle)'.format(angle=angle), y_func='x*sin(angle)+y*cos(angle)'.format(angle=angle), z_func='z') else: print('Axis name is not valid; exiting...') sys.exit(1) return None"
1452,"def build_masked_autoregressive_density_estimator(self, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_dense, masked_","def masked_autoregressive_default_template(hidden_layers, shift_only=False, activation=tf.nn.relu, log_scale_min_clip=-5., log_scale_max_clip=3., log_scale_clip_gradient=False, name=None, *args, **kwargs): input_depth = tf.compat.dimension_value( tensorshape_util.with_rank_at_least(x.shape, 1)[-1]) if input_depth is None: raise NotImplementedError( ""Rightmost dimension must be known prior to graph execution."") input_shape = ( np.int32(tensorshape_util.as_list(x.shape)) if tensorshape_util.is_fully_defined(x.shape) else tf.shape(input=x)) if tensorshape_util.rank(x.shape) == 1: x = x[tf.newaxis,...] for i, units in enumerate(hidden_layers): x = masked_dense( inputs=x, units=units, num_blocks=input_depth, exclusive=True if i == 0 else False, activation=activation, *args, **kwargs) x = masked_dense( inputs=x, units=(1 if shift_only else 2) * input_depth, num_blocks=input_depth, activation=None, *args, **kwargs) if shift_only: x = tf.reshape(x, shape=input_shape) return x, None x = tf.reshape(x, shape=tf.concat([input_shape, [2]], axis=0)) shift, log_scale = tf.un"
1453,"def make_distribution_fn(make_distribution_fn, make_distribution_fn, make_distribution_fn, make_distribution_fn, make_distribution_fn, make_distribution_fn, make_distribution_fn, make_distribution_fn, make_distribution_fn, make_distribution_fn, get_distribution_fn, get_distribution_fn)","def get_config(self): config = 'make_distribution_fn': _serialize_function(self._make_distribution_fn), 'convert_to_tensor_fn': _serialize(self._convert_to_tensor_fn),  base_config = super(DistributionLambda, self).get_config() return dict(list(base_config.items()) + list(config.items()))"
1454,"def train_linear_regression_algorithm(f(x) = WX, h(x) = WX, h(x) = WX, h(x) = WX, h(x) = WX, h(x) = WX, h(x) = WX, h(x) = WX, h(x) = WX, h(x) = WX, h(x) = WX, h(x) = WX, h(x) = WX, h(x) = WX, h(x) = WX, h(x) = WX, h(x) = WX, h(x) = WX, h(x) = WX, h(x) = WX, h(x) = WX, h(x) = WX, h(x) = WX, h(x) = WX, h(x) = WX, h(x) = WX, h(x) = WX, h(x) = WX, h(x) = WX, h(x) = WX, h(x) = WX) = WX) = W","def train(self): if (self.status!= 'init'): print(""Please load train data and init W first."") return self.W self.status = 'train' self.xpsedo = self.calculate_psedo_X(self.train_X) self.W = np.dot(self.xpsedo, self.train_Y) return self.W"
1455,"def deserialize_dict(self, schema): schema = self.bind_schema(schema) self.bind_dict(schema) self.bind_dict(schema) self.bind_dict(schema) self.bind_dict(schema) self.bind_dict(schema) self.bind_dict(schema) self.bind_dict(schema) self.bind_dict(schema) self.bind_dict(schema) self.bind_dict(schema) self.bind_dict(schema) self.bind_dict(schema) self.bind_dict(schema) self.bind_dict(schema) self.bind_dict(schema) self.bind_dict(schema) self.bind_dict(schema) self.bind_dict(schema) self.bind_dict(schema) self.bind_dict(schema) self.bind_dict(schema) self.bind_dict(schema) self.bind_dict(schema) self.bind_dict(schema) self.bind_dict(schema) self.bind_dict(schema) self.bind_dict","def from_dict(cls, dict_): try: data, _ = cls.schema.load(dict_) except ValidationError as ex: raise ModelValidationError( ex.messages, ex.field_names, ex.fields, ex.data, **ex.kwargs) from None return data"
1456,"def print_annotation(self, key, value): if self.get_annotation(key, value): self.get_annotation(key, value): self.get_annotation(key, value)","def print_annotation(self): for path, ann in self.annotation.items(): print("": "".format(path, ann['value']))"
1457,"def create_github3.py_session(self, token): if token is not provided, will try to use github_api_token(token) if token is not provided, will try to use github_api_token(token) if token is not provided, will try to use github_api_token(token) if token is not provided, will try to use github_api_token(token)","def create_enterprise_session(url, token=None): gh_session = github3.enterprise_login(url=url, token=token) if gh_session is None: msg = 'Unable to connect to GitHub Enterprise (%s) with provided token.' raise RuntimeError(msg, url) return gh_session"
1458,def ________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________,def readable(path): try: st = os.stat(path) return 0!= st.st_mode & READABLE_MASK except os.error: return None return True
1459,"def ping_timer(self, ping_timer): return self.png_timer(ping_timer) return self.png_timer(ping_timer)","def send_ping(self): self.log.debug(""send_ping(): Sending ping to API.."") self.socket.send(json.dumps('event': 'ping')) self.pong_timer = Timer(self.pong_timeout, self._check_pong) self.pong_timer.start()"
1460,def _status_of_all_servers(server_status): server_status = server_status(server_status) return server_status(server_status) return server_status(server_status),"def server_status(request): data =  'cache_stats': _get_cache_stats(), 'can_get_slabs': hasattr(mc_client, 'get_slabs'),  return render_to_response('memcache_admin/server_status.html', data, RequestContext(request))"
1461,"def valueError(self, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError, valueError)","def validate_string_list(lst): if not isinstance(lst, list): raise ValueError('input %r must be a list' % lst) for x in lst: if not isinstance(x, basestring): raise ValueError('element %r in list must be a string' % x)"
1462,"def extract(href_string, first_byte, byte_length, anchor_text, byte_length, anchor_text, byte_length, anchor_text, byte_length, anchor_text, byte_length, anchor_text, byte_length, anchor_text, byte_length, byte_length, anchor_text, byte_length, byte_length, byte_length, byte_length, byte_length, byte_length): byte_length, first_byte, byte_length, byte_length, byte_length, byte_length, byte_length, byte_length, byte_length, byte_length, byte_length, byte_length, byte_length, byte_length, byte_length, byte_length, byte_length, byte_length, byte_length, byte_length)","def byte_href_anchors(self, chars=False): input_buffer = self.clean_html if chars: input_buffer = input_buffer.decode('utf8') idx = 0 # parts = input_buffer.split('/a>') assert len('/a>'.join(parts) ) == len(input_buffer) for part in parts: # m = anchors_re.match(part) if not m: idx += len(part) + 4 continue before = m.group('before') ahref = m.group('ahref') # idx += len(before) + len(ahref) first = idx length = len(m.group('anchor')) # idx += length + 4 if chars: yield m.group('href').encode('utf8'), first, length, m.group('anchor').encode('utf8') else: yield m.group('href'), first, length, m.group('anchor') assert idx - 4 == len(input_buffer)"
1463,"def remove_all_components(self, canvas): self.remove_components(components) self.remove_components(components) self.remove_components(components) self.remove_components(components) self.remove_components(components) self.remove_components(components) self.remove_components(components) self.removable_components(components) self.removable_components(components) self.removable_components(components) self.removable_components) self.removable_components(components) self.removable_components(components) self.removable_components) self.removable_components_components_components_components_components_components_components_components_components_components_components_components_components_components_components_components_components_components_components_components_components_components_component","def clear_canvas(self): logger.debug(""Clearing the diagram canvas!"") old_canvas = self.diagram_canvas new_canvas = Canvas() new_canvas.copy_traits(old_canvas, [""bgcolor"", ""draw_axes""]) self.diagram_canvas = new_canvas self.viewport.component=new_canvas self.viewport.request_redraw() return"
1464,"def prepare_response(self, Content-Type): self.response(Response): self.response(Response) self.response(Response) self.response(Response) self.response(Response) self.response(Response) self.response(Response) self.response(Response) self.response(Response) self.response) self.response(Response) self.response) self.response) self.response) self.response) self.response) self.response) self.response) self.response(Response) self.response) self.response(Response) self.response) self.response(Response) self.response(Response) self.respon","def _process_response(response, save_to): status_code = response.status_code if status_code == 200 and save_to: if save_to.startswith(""""): save_to = os.path.expanduser(save_to) if os.path.isdir(save_to) or save_to.endswith(os.path.sep): dirname = os.path.abspath(save_to) filename = H2OConnection._find_file_name(response) else: dirname, filename = os.path.split(os.path.abspath(save_to)) fullname = os.path.join(dirname, filename) try: if not os.path.exists(dirname): os.makedirs(dirname) with open(fullname, ""wb"") as f: for chunk in response.iter_content(chunk_size=65536): if chunk: f.write(chunk) except OSError as e: raise H2OValueError(""Cannot write to file %s: %s"" % (fullname, e)) return fullname content_type = response.headers.get(""Content-Type"", """") if "";"" in content_type: content_type = content_type[:content_type.index("";"")] if content_type == ""application/json"": try: data = response.json(object_pairs_hook=H2OResponse) except (JSONDecodeError, requests.exceptions.ContentDecodingError) as e: raise H2OServerError(""Malformed JSON from server (%s):n%s"" % (str(e), response.text)) else: data = response.text if status_code in 200, 201, 202"
1465,"def new_session(self, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session) new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session, new_session.new_session.new_session.new_session.new_session.new_session.new_session.new_session.new_session.new_session.new_session.new_session.new_session.new_session.new_session.new_session.new_session.new_session.new_session","def reset(self, new_session=True): self.history_manager.reset(new_session) if new_session: self.execution_count = 1 if self.displayhook.do_full_cache: self.displayhook.flush() if self.user_ns is not self.user_global_ns: self.user_ns.clear() ns = self.user_global_ns drop_keys = set(ns.keys()) drop_keys.discard('__builtin__') drop_keys.discard('__builtins__') drop_keys.discard('__name__') for k in drop_keys: del ns[k] self.user_ns_hidden.clear() self.init_user_ns() self.alias_manager.clear_aliases() self.alias_manager.init_aliases() self.clear_main_mod_cache() self.new_main_mod()"
1466,"def open_config_scope(self, name_or_scope): if self.config_scope(self.config_scope): self.config_scope(self.config_scope) self.config_scope(self.config_scope) self.config_scope(self.config_scope) self.config_scope(self.config_scope) self.config_scope(self.config_scope) self.config_scope(self.config_scope) self.config_scope(self.config_scope) self.config_scope) self.config_scope(self.config_scope) self.config_scope) self.config_scope(self.config_scope) self.config_scope(self.config_scope) self.config_scope) self.config_scope) self.config_scope(self.config_scope) self.config_scope) self.config_scope(self.config_scope) self.config_scope) self.config_scope(self.config_scope) self.config_scope) self.config_scope(self.config_scope) self.config_scope) self.config_scope(self.config_scope) self.config_scope(self.config_scope) self.config_scope(self.config_scope) self.config_scope) self.config_scope(self.config","def config_scope(name_or_scope): try: valid_value = True if isinstance(name_or_scope, list): new_scope = name_or_scope elif name_or_scope and isinstance(name_or_scope, six.string_types): new_scope = current_scope() new_scope.extend(name_or_scope.split('/')) else: valid_value = name_or_scope in (None, '') new_scope = [] _ACTIVE_SCOPES.append(new_scope) scopes_are_valid = map(config_parser.MODULE_RE.match, new_scope) if not valid_value or not all(scopes_are_valid): err_str = 'Invalid value for name_or_scope:.' raise ValueError(err_str.format(name_or_scope)) yield new_scope finally: _ACTIVE_SCOPES.pop()"
1467,"def find_element(self, WebElement): self.get_element(self.get_element): self.get_element(self.get_element) self.get_element(self.get_element) self.get_element(self.get_element) self.get_element(self.get_element) self.get_element(self.get_element) self.get_element(self.get_element) self.get_element(self.get_element) self.get_element(self.get_element) self.get_element(self.get_element) self.get_element(self.get_element) self.get_element(self.get_element) self.get_element(self.get_element) self.get_element(self.get_element) self.get_element(self.get_element) self.get_element(self.get_element) self.get_element(self.get_element) self.get_element(self.get_element) self.get_element(self.get_element(self.get_element) self.get_element(self.get_","def from_web_element(self, web_element): if isinstance(web_element, WebElement) is not True: raise TypeError(""web_element parameter is not of type WebElement."") self._web_element = web_element return self"
1468,"def _size_in_bytes(self, data_frame): if self.size_in_bytes(DataFrame): self.size_in_bytes(DataFrame) self.size_in_bytes(DataFrame) self.size_in_bytes(DataFrame) self.size_in_bytes(DataFrame) self.size_in_bytes(DataFrame) self.size_in_bytes(DataFrame) self.size_in_bytes(DataFrame) self.size_in_bytes(DataFrame) self.size_in_bytes(DataFrame) self.size_in_bytes(DataFrame) self.size_in_bytes(DataFrame) self.size_in_bytes(DataFrame) self.size_in_bytes(DataFrame) self.size_in_bytes(DataFrame) self.size_in_bytes(DataFrame) self.size_in_bytes(DataFrame) self.size_in_bytes(DataFrame) self.size_in_bytes(DataFrame) self.size_in_bytes(DataFram","def byte_size(self, selection=False, virtual=False): bytes_per_row = 0 N = self.count(selection=selection) extra = 0 for column in list(self.get_column_names(virtual=virtual)): dtype = self.dtype(column) dtype_internal = self.dtype(column, internal=True) #if dtype in [str_type, str] and dtype_internal.kind == 'O': if isinstance(self.columns[column], ColumnString): extra += self.columns[column].nbytes else: bytes_per_row += dtype_internal.itemsize if np.ma.isMaskedArray(self.columns[column]): bytes_per_row += 1 return bytes_per_row * self.count(selection=selection) + extra"
1469,"def enable_event_loop(self, app, app): app = self.get_event_loop(app, app, app)","def enable_gtk3(self, app=None): from IPython.lib.inputhookgtk3 import inputhook_gtk3 self.set_inputhook(inputhook_gtk3) self._current_gui = GUI_GTK"
1470,"def extend_request(req, receiver, aud, aud): req = req(req, receiver, aud, receiver, aud, receiver, aud, receiver) req = req(req, receiver, aud, receiver) req = req(req, receiver) req = req(req, receiver) req = req(req, receiver) req = req(req) req(req) req(req) req) req(req) req(req) req(req) req) req(req) req(req) req(req) req(req) req(req) req(req) req(req) req(req) req(req) req(req) req(req) req(req) req(req) req(req) req(req) req(req) req(req) req(req) req(req) req(req)","def self_sign(self, req, receiver='', aud=None): if self.entity_id: _iss = self.entity_id else: _iss = self.iss creq = req.copy() if not'metadata_statement_uris' in creq and not 'metadata_statements' in creq: _copy = creq.copy() _jws = self.self_signer.sign(_copy, receiver=receiver, iss=_iss, aud=aud) sms_spec ='metadata_statements': self.iss: _jws else: for ref in ['metadata_statement_uris','metadata_statements']: try: del creq[ref] except KeyError: pass sms_spec ='metadata_statements': Message() for ref in ['metadata_statement_uris','metadata_statements']: if ref not in req: continue for foid, value in req[ref].items(): _copy = creq.copy() _copy[ref] = Message() _copy[ref][foid] = value _jws = self.self_signer.sign(_copy, receiver=receiver, iss=_iss, aud=aud) sms_spec['metadata_statements'][foid] = _jws creq.update(sms_spec) return creq"
1471,"def create_context(self, context, indent, indent): self.create_context = self.create_context = self.create_context = self.create_context = self.create_context = self.create_context = self.create_context = self.create_context","def withIndent(self, indent=1): ctx = copy(self) ctx.indent += indent return ctx"
1472,"def Shutdown(self, prompt, prompt): if self.shutdown(self.shutdown): self.shutdown(self.shutdown) self.shutdown(self.shutdown) self.shutdown(self.shutdown) self.shutdown(self.shutdown) self.shutdown(self.shutdown) self.shutdown(self.shutdown) self.shutdown(self.shutdown) self.shutdown(self.shutdown) self.shutdown(self.shutdown) self.shutdown(self.shutdown) self.shutdown(self.shutdown) self.shutdown(self.shutdown) self.shutdown) self.shutdown(self.shutdown) self.shutdown) self.shutdown(self.shutdown) self.shutdown(self.shutdown) self.shutdown) self.shutdown(self.shutdown) self.shutdown) self.shutdown(self.shutdown) self.shutdown(self.shutdown) self.shutdown(self.shutdown) self.shutdown) self.shutdown(self.shutdown) self","def shutdown(self, prompt=False): if not self.is_running(): return assert_is_type(prompt, bool) if prompt: question = ""Are you sure you want to shutdown the H2O instance running at %s (Y/N)? ""  % h2o.connection().base_url response = input(question) else: response = ""Y"" if response.lower() in ""y"", ""yes"": h2o.api(""POST /3/Shutdown"") h2o.connection().close()"
1473,"def add_weight_math(name, nin, nout, std, sparsity, diagonal, n_i, n_o, sparsity, diagonal, diagonal, diagonal, diagonal, diagonal, diagonal, diagonal, diagonal, diagonal, diagonal, diagonal, diagonal, diagonal, diagonal, diagonal, diagonal, diagonal, diagonal, diagonal, diagonal, diagonal, diagonal, diagonal, diagonal, diagonal, diagonal, diagonal, diagonal, diagonal, diagonal, diagonal, diagonal, diagonal, diagonal, diagonal, diagonal, diagonal, diagonal, diagonal","def add_weights(self, name, nin, nout, mean=0, std=0, sparsity=0, diagonal=0): glorot = 1 / np.sqrt(nin + nout) m = self.kwargs.get('mean_'.format(name), self.kwargs.get('mean', mean)) s = self.kwargs.get('std_'.format(name), self.kwargs.get('std', std or glorot)) p = self.kwargs.get('sparsity_'.format(name), self.kwargs.get('sparsity', sparsity)) d = self.kwargs.get( 'diagonal_'.format(name), self.kwargs.get('diagonal', diagonal)) self._params.append(theano.shared( util.random_matrix(nin, nout, mean=m, std=s, sparsity=p, diagonal=d, rng=self.rng), name=self._fmt(name)))"
1474,"def _convert_size(self, vector, matrix): if self.convert_size = self.convert_size = self.convert_size = self.convert_size = self.convert_size = self.convert_size = self.convert_size = self.convert_size = self.convert_size = self.convert_size = self.convert_size = self.convert_size = self.convert_size = self.convert_size = self.convert_size = self.convert_size = self.convert_size = self.convert_size = self.convert_size","def vector_size_to_square_matrix_size(d, validate_args, name=None): if isinstance(d, (float, int, np.generic, np.ndarray)): n = (-1 + np.sqrt(1 + 8 * d)) / 2. if float(int(n))!= n: raise ValueError(""Vector length is not a triangular number."") return int(n) else: with tf.name_scope(name or ""vector_size_to_square_matrix_size"") as name: n = (-1. + tf.sqrt(1 + 8. * tf.cast(d, dtype=tf.float32))) / 2. if validate_args: with tf.control_dependencies([ assert_util.assert_equal( tf.cast(tf.cast(n, dtype=tf.int32), dtype=tf.float32), n, message=""Vector length is not a triangular number"") ]): n = tf.identity(n) return tf.cast(n, d.dtype)"
1475,"def get_key_value(self, key): self.get_key_value(key) self.get_key_value(key) self.get_key_value(key) self.get_key_value(key) self.get_key_value(key) self.get_key_value(key) self.get_key_value(key) self.get_key_value(key) self.get_key_value(key) self.get_key_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.get_value(key) self.","def get(self, k, wait=False, wait_index=False, timeout='5m'): k = k.lstrip('/') url = '/'.format(self.endpoint, k) params =  if wait: params['index'] = wait_index params['wait'] = timeout r = requests.get(url, params=params) if r.status_code == 404: raise KeyDoesNotExist(""Key "" + k + "" does not exist"") if r.status_code!= 200: raise KVStoreError('GET returned '.format(r.status_code)) try: return base64.b64decode(r.json()[0]['Value']) except TypeError as e: return """""
1476,"def apply_stashed_values(self, signals): self.get_stashed_values(signals) self.get_stashed_values(signals) self.get_stashed_values(signals) self.get_stashed_values(signals) self.get_stashed_values(signals) self.get_stashed_values(signals) self.get_stashed_values(signals) self.get_stashed_values(signals) self.get_stashed_values(signals) self.get_stashed_values(signals) self.get_stashed_values(signals) self.get_stashed_values(signals) self.get_stashed_values(signals) self.get_stashed_values(signals) self.get_stashed_values(signals) self.get_stashed_values) self.get_stashed_values(signals) self.get_stashed_values(signals) self.get_stashed_values(signals) self.get_stashed_values(signals) self.get_stashed_values(signals) self.get_stashed_values(signals) self","def _scheduleApplyValues(self) -> None: assert not self._applyValPlaned, self.now self._add_process(self._applyValues(), PRIORITY_APPLY_COMB) self._applyValPlaned = True if self._runSeqProcessesPlaned: return assert not self._seqProcsToRun and not self._runSeqProcessesPlaned, self.now self._add_process(self._runSeqProcesses(), PRIORITY_APPLY_SEQ) self._runSeqProcessesPlaned = True"
1477,"def add_missing_row(self, id_cols, reference_col): if self.add_missing_row = self.add_missing_row = self.add_missing_row = self.add_missing_row = self.add_missing_row = self.add_missing_row = self.add_missing_row = self.add_missing_row = self.add_missing_row = self.add_missing_row = self.add_missing_row = self.add_missing_row = self.add_missing_row = self.add_missing_row = self.add_missing_row = self.add_missing_row","def add_missing_row( df: pd.DataFrame, id_cols: List[str], reference_col: str, complete_index: Union[Dict[str, str], List[str]] = None, method: str = None, cols_to_keep: List[str] = None ) -> pd.DataFrame: if cols_to_keep is None: cols_for_index = [reference_col] else: cols_for_index = [reference_col] + cols_to_keep check_params_columns_duplicate(id_cols + cols_for_index) if method == 'between' or method == 'between_and_after': df['start'] = df.groupby(id_cols)[reference_col].transform(min) id_cols += ['start'] if method == 'between' or method == 'between_and_before': df['end'] = df.groupby(id_cols)[reference_col].transform(max) id_cols += ['end'] names = id_cols + cols_for_index new_df = df.set_index(names) index_values = df.groupby(id_cols).sum().index.values if complete_index is None: complete_index = df.groupby(cols_for_index).sum().index.values elif isinstance(complete_index, dict): if complete_index['type'] == 'date': freq = complete_index['freq'] date_format = complete_index['format'] start = complete_index['start'] end = complete_index['end'] if isinstance(freq, dict): freq = pd.DateOffset(**k: in"
1478,def load_hpo_gene_lines(hpo_gene_lines): if hpo_gene_lines(hpo_gene_lines): if hpo_gene_lines(hpo_gene_lines): if hpo_gene_lines(hpo_gene_lines): if hpo_gene_lines(hpo_gene_lines): if hpo_gene_lines(hpo_gene_lines): if hpo_gene_lines(hpo_gene_lines): if hpo_gene_lines(hpo_gene_lines): if hpo_gene_lines(hpo_gene_lines): if hpo_gene_lines(hpo_gene_lines): if hpo_gene_lines(hpo_gene_lines): if hpo_gene_lines(hpo_gene_lines): if hpo_gene_lines(hpo_gene_lines): if hpo_gene_lines),"def load_hpo(adapter, disease_lines, hpo_disease_lines=None, hpo_lines=None, hpo_gene_lines=None): alias_genes = adapter.genes_by_alias() if not hpo_lines: hpo_lines = fetch_hpo_terms() if not hpo_gene_lines: hpo_gene_lines = fetch_hpo_to_genes() if not hpo_disease_lines: hpo_disease_lines = fetch_hpo_phenotype_to_terms() load_hpo_terms(adapter, hpo_lines, hpo_gene_lines, alias_genes) load_disease_terms(adapter, disease_lines, alias_genes, hpo_disease_lines)"
1479,"def create_transform_job(self, config, wait_for_completion, wait_for_completion, wait_for_completion, wait_for_completion, wait_for_completion, wait_for_completion, check_interval, max_ingestion_time, max_ingestion_time, max_ingestion_time, max_ingestion_time, max_ingestion_time, max_ingestion_time, max_ingestion_time, max_ingestion_time, max_ingestion_time, max_ingestion_time, max_ingestion_time, max_ingestion_time, max_ingestion_time, max_ingestion_time, max_ingestion_time, max_ingestion_time) max_ingestion_time, max_ingestion_time, max_ingestion_time, max_ingestion_time, max_ingestion_time, max_ingestion_time, max_ingestion_time, max_ingestion_time, max_ingestion_time, max_ingestion_time, max_ingestion_time, max_ingestion_time, max_ingestion_time, max_ingestion_time, max_ingestion_time, max_ingestion_time, max_ingestion_time, max","def create_transform_job(self, config, wait_for_completion=True, check_interval=30, max_ingestion_time=None): self.check_s3_url(config['TransformInput']['DataSource']['S3DataSource']['S3Uri']) response = self.get_conn().create_transform_job(**config) if wait_for_completion: self.check_status(config['TransformJobName'], 'TransformJobStatus', self.describe_transform_job, check_interval, max_ingestion_time ) return response"
1480,"def post_notification(ntype, sender, *args, **kwargs): post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification = post_notification =","def add_observer(self, callback, ntype, sender): assert(callback!= None) self.registered_types.add(ntype) self.registered_senders.add(sender) self.observers.setdefault((ntype,sender), set()).add(callback)"
1481,"def parse_argument(self, argument, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser, parser)","def setup_cmd_parser(cls): parser = BackendCommandArgumentParser(cls.BACKEND.CATEGORIES, archive=True) parser.parser.add_argument('url', help=""URL of the RSS feed"") return parser"
1482,"def stream.write(self, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode) unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode, unicode","def format(self, layout, stream=None, encoding=None): if stream is None: stream = sys.stdout if not encoding: encoding = getattr(stream, ""encoding"", ""UTF-8"") self.encoding = encoding or ""UTF-8"" self.out = stream self.begin_format() layout.accept(self) self.end_format()"
1483,"def render_tag(self, tag): if tag is self.get_value(tag): self.get_value(tag): self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.get_value(tag) self.","def render_tag(self, context, *tag_args, **tag_kwargs): if self.as_var: context[self.as_var] = self.get_value(context, *tag_args, **tag_kwargs) return u''"
1484,def rename_key(key_to_rename): if key_to_rename(key_to_rename): if key_to_rename(key_to_rename): if key_to_rename(key_to_rename): if key_to_rename(key_to_rename): if key_to_rename(key_to_rename): if key_to_rename: if key_to_rename(key_to_rename): if key_to_rename(key_to_rename): if key_to_rename(key_to_rename): if key_to_rename(key_to_rename): if key_to_rename(key_to_rename): if key_to_rename(key_to_rename),"def rename_key(self, key_to_rename, strict=True): if isinstance(self.main_dictionnary, dict) and isinstance(key_to_rename, dict): for old, new in key_to_rename.items(): if strict: if old in self.main_dictionnary: self.main_dictionnary[new] = self.main_dictionnary.pop(old) else: to_rename =  for index in self.main_dictionnary: if old in index: to_rename.update(index: new[:-1] + index.split(old)[-1]) self.main_dictionnary = Dict(self.main_dictionnary).rename_key( to_rename, True ) return self.main_dictionnary return None"
1485,"def list_of_sections_including_students(self, section_id, section_id, section_id, section_id, section_id, section_id, section_id, section_id, section_id, section_id, section_id, section_id, section_id, section_id, section_id, section_id, section_id, section_id, section_id, section_id, section_id, section_id, section_id, section_id, section_id, section_id)","def get_sections_with_students_in_course(self, course_id, params=): include = params.get(""include"", []) if ""students"" not in include: include.append(""students"") params[""include""] = include return self.get_sections_in_course(course_id, params)"
1486,"def decode_control(self, PUBREL): self.decode_control(PUBREL) self.decode_control(PUBREL) self.decode_control(PUBREL) self.decode_control(PUBREL) self.decode_control(PUBREL) self.decode_control(PUBREL) self.decode_control(PUBREL) self.decode_control(PUBREL) self.decode_control(PUBREL) self.decode_control(PUBREL) self.decode_control(PUBREL) self.decode_control(PUBREL) self.decode_control(PUBREL) self.decode_control(PUBREL) self.decode_control(PUBREL) self.decode_control(PUBREL) self.decode_control(PUBREL) self.decode_control(PUBREL) self.decode_control(PUBREL) self.decode_control(PUBREL) self.decode_control(PUBREL) self.decode_control(PUBREL) self.decode_control(PUBREL) self.decode_control(PUBREL) self.decode_control(PUBREL) self.decode_control(PUBRE","def decode(self, packet): self.encoded = packet lenLen = 1 while packet[lenLen] & 0x80: lenLen += 1 packet_remaining = packet[lenLen+1:] self.msgId = decode16Int(packet_remaining) self.dup = (packet[0] & 0x08) == 0x08"
1487,def kl(a || b): if kl(a || b) = kl(a || b) = kl(a || b) = kl(a || b) = kl(a || b) = kl(a || b) = kl(a || b) = kl(a || b)) = kl(a || b) = kl(a) = kl(a) = kl(a) = kl(a) = kl(a) = kl(a) = kl(a) = kl(a) = kl(a)) = kl(a) = kl(a) = kl(a) = kl(a) = kl(a) = kl(a) = kl(a) = kl(b) = kl(a) = kl(a) = kl(b) = kl(a) = kl(a) = kl(b) = kl(a) = kl(b) = kl(a) =,"def _kl_independent(a, b, name=""kl_independent""): p = a.distribution q = b.distribution if (tensorshape_util.is_fully_defined(a.event_shape) and tensorshape_util.is_fully_defined(b.event_shape)): if a.event_shape == b.event_shape: if p.event_shape == q.event_shape: num_reduce_dims = (tensorshape_util.rank(a.event_shape) - tensorshape_util.rank(p.event_shape)) reduce_dims = [-i - 1 for i in range(0, num_reduce_dims)] return tf.reduce_sum( input_tensor=kullback_leibler.kl_divergence(p, q, name=name), axis=reduce_dims) else: raise NotImplementedError(""KL between Independents with different "" ""event shapes not supported."") else: raise ValueError(""Event shapes do not match."") else: with tf.control_dependencies( [ assert_util.assert_equal(a.event_shape_tensor(), b.event_shape_tensor()), assert_util.assert_equal(p.event_shape_tensor(), q.event_shape_tensor()) ]): num_reduce_dims = ( prefer_static.rank_from_shape( a.event_shape_tensor, a.event_shape) - prefer_static.rank_from_shape( p.event_shape_tensor, a.event_shape))"
1488,"def create_default_policies(self, name, description): self.create_default_policies(self.create_default_policies): self.create_default_policies(self.create_default_policies) self.create_default_policies(self.create_default_policies) self.create_default_policies(self.create_default_policies) self.create_default_policies(self.create_default_policies) self.create_default_policies) self.create_default_policies(self.create_default_policies) self.create_default_policies(self.create_default_policies) self.create_default_policies) self.create_default_policies(self.create_default_policies) self.create_default_policies) self.create_default_policies(self.create_default_policies) self.create_default_policies(self.create_default_policies) self.create_default_policies) self.create_default_policies(self.create_default_policies) self.create_default_policies(self.create_policies) self.create","def create_default_policies(self): res = requests.post(self.url + '/api/policies/createDefault', headers=self.hdrs, verify=self.ssl_verify) return self._request_result(res)"
1489,def load_callable(self): if self.load_callable(self.load_callable): self.load_callable(self.load_callable) self.load_callable(self.load_callable) self.load_callable(self.load_callable) self.load_callable(self.load_callable) self.load_callable(self.load_callable) self.load_callable(self.load_callable) self.load_callable(self.load_callable) self.load_callable(self.load_callable) self.load_callable(self.load_callable) self.load_callable(self.load_callable) self.load_callable(self.load_callable) self.load_callable(self.load_callable) self.load_callable(self.load_callable) self.load_callable(self.load_callable) self.load_callable(self.load_callable) self.load_callable(self.load_callable) self.load_callable(self.load_callable) self.load_callable(self.load_callable) self.load_callable(self.load_callable) self.load_callable,"def _deferred_add(cls, add_action): module, _, obj = add_action.partition(',') mod = importlib.import_module(module) if obj == """": _, con = annotate.context_from_module(mod) return con if hasattr(mod, obj): return getattr(mod, obj) raise ArgumentError(""Attempted to import nonexistent object from module"", module=module, object=obj)"
1490,"def send_first_metadata_statement(req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req): req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, req, re","def create(self, req, **kwargs): response = requests.post(self.url, json=req, **self.req_args()) return self.parse_response(response)"
1491,"def sparse_x(x, ignore_value, ignore_value, ignore_value, ignore_value, ignore_value, ignore_value, ignore_value, ignore_value, ignore_value, ignore_value, ignore_value, ignore_value): return sparse_x if sparse_x is None: return sparse_x if sparse_x is None: return sparse_x if sparse_x is None: return sparse_x: return sparse_x: return sparse_x: return sparse_x: return sparse_x: return sparse_x: return sparse_x: return sparse_x: return sparse_x: return sparse_x: return sparse_x: return sparse_x: return sparse_x: return sparse_x: return sparse_x: return sparse_x: return sparse_x: return sparse_x: return sparse_x: return sparse_x: return sparse_x: return sparse_x: return sparse_x: return sparse_x: return sparse_x: return sparse_x: return sparse_x: return spars","def dense_to_sparse(x, ignore_value=None, name=None): with tf.compat.v1.name_scope(name, 'dense_to_sparse', [x, ignore_value]): x = tf.convert_to_tensor(value=x, name='x') if ignore_value is None: if x.dtype.base_dtype == tf.string: ignore_value = '' else: ignore_value = x.dtype.as_numpy_dtype(0) ignore_value = tf.cast(ignore_value, x.dtype, name='ignore_value') indices = tf.where(tf.not_equal(x, ignore_value), name='indices') return tf.SparseTensor( indices=indices, values=tf.gather_nd(x, indices, name='values'), dense_shape=tf.shape(input=x, out_type=tf.int64, name='dense_shape'))"
1492,"def create_map(country_codes, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chld, chl","def map(self, geo, country_codes): assert geo in GEO, 'Geograpic area %s not recognized'%geo self._geo = geo self._ld = country_codes return self"
1493,"def format_format(self, format_format): format_format(format_format): format_format(format_format) str(format_format) str(format_format) str(format_format) str(format_format) str(format_format) str(format_format) str(format_format) str(format_format) str(format_format) str(format_format) str(format_format) str(format_format) str(format_format) str(format_format) str(format_format) str(format_format) str(format_format) str(format_format) str(format_format) str(format_format) str(format_format) str(format_format) str(format_format) str(format_format) str(format_format) str(format) str(format) str(format) str(format) str(format) str(format) str(format) str(format) str(format) str(format) str(format) str(format) str(format) str(format) str(format) str(format) str(format) str(format) str(format) str(format) str(format) str(format) str(format) str(format) str(format) str(format) str(format) str(format) str(format) str(format","def get_level_str(self): if self.is_relative: level_str = str(self.level) + ""%"" else: level_str = self.level return level_str"
1494,"def input_args(self, args): args = self.args(args) self.args(args) self.args(args) self.args(args) self.args(args) self.args(args) self.args(args) self.args) self.args(args) self.args(args) self.args(args) self.args(args) self.args) self.args) self.args","def _prepare_args(log_likelihood_fn, state, log_likelihood=None, description='log_likelihood'): state_parts = list(state) if mcmc_util.is_list_like(state) else [state] state_parts = [tf.convert_to_tensor(s, name='current_state') for s in state_parts] log_likelihood = _maybe_call_fn( log_likelihood_fn, state_parts, log_likelihood, description) return [state_parts, log_likelihood]"
1495,"def _is_rendered(self, context, is_rendered, is_rendered, context, is_rendered): context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context = context =","def get_context_data(self, **kwargs): ctx = super(RenderWidgetMixin, self).get_context_data(**kwargs) ctx.update( 'is_rendered': True, 'widget': self.widget, ) ctx.update(self.widget.get_context_data()) return ctx"
1496,"def split(self, posix=False, strict=False, posix=False, posix=False, strict=False, posix=False, posix=False, posix=False, posix=False, posix=False, posix=False, posix=False, posix=False, posix=False, posix=False, posix=False)","def arg_split(s, posix=False, strict=True): is_unicode = False if (not py3compat.PY3) and isinstance(s, unicode): is_unicode = True s = s.encode('utf-8') lex = shlex.shlex(s, posix=posix) lex.whitespace_split = True lex.commenters='' #fix for GH-1269 tokens = [] while True: try: tokens.append(lex.next()) except StopIteration: break except ValueError: if strict: raise tokens.append(lex.token) break if is_unicode: tokens = [x.decode('utf-8') for x in tokens] return tokens"
1497,"def list_endpoint(self, endpoint_id, query_id, query_id, query_id, query_id, query_id, query_id, query_id, query_id, query_id, query_id, query_id, query_id, query_id, query_id, query_id, query_id, query_id, query_id, query_id, query_id, query_id, query_id, query_id, query_id, query_id, query_id, query_id, query_id, query_id, query_id, query_id, query_id, query_id, query_id, query_id, query_id, query_id, query_id, query_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id","def list_endpoints(self, query=None): bot.info('Please select an endpoint id to query from') endpoints = self._get_endpoints(query) bot.custom(prefix=""Globus"", message=""Endpoints"", color=""CYAN"") rows = [] for kind,eps in endpoints.items(): for epid,epmeta in eps.items(): rows.append([epid, '[%s]' %kind, epmeta['name']]) bot.table(rows) return rows"
1498,"def describefeaturetype_request(self, descriptionfeaturetype): self.get_url(self.get_url) self.get_url(self.get_url) self.get_url(self.get_url) self.get_url(self.get_url) self.get_url(self.get_url) self.get_url(self.get_url) self.get_url(self.get_url) self.get_url(self.get_url) self.get_url) self.get_url) self.get_url(self.get_url) self.get_url(self.get_url) self.get_url) self.get_url) self.get_url(self.get_url) self.get_url) self.get_url(self.get_url) self.get_url) self.get_url(self.get_url) self.get_url) self.get_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url_url","def _get_describefeaturetype_url(url, version, typename): query_string = [] if url.find('?')!= -1: query_string = cgi.parse_qsl(url.split('?')[1]) params = [x[0] for x in query_string] if'service' not in params: query_string.append(('service', 'WFS')) if'request' not in params: query_string.append(('request', 'DescribeFeatureType')) if'version' not in params: query_string.append(('version', version)) query_string.append(('typeName', typename)) urlqs = urlencode(tuple(query_string)) return url.split('?')[0] + '?' + urlqs"
1499,def calculate_great_circle_distance(great circle distance):                                                                                                                    .,"def gcd(self, lon1, lat1, lon2, lat2): lon1, lat1, lon2, lat2 = map(math.radians, [lon1, lat1, lon2, lat2]) dlon = lon2 - lon1 dlat = lat2 - lat1 a = math.sin(dlat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2) ** 2 c = 2 * math.asin(math.sqrt(a)) dis = E.R * c return dis"
1500,def parse(self[index:index]): if parse(self[index:index]) is False: if parse(self[index:index]) is False: if parse(self[index:index]) is False: if parse(self[index:index]) is False: if parse(self[index:index]) is False: if parse(self[index]) is False(self[index] = self[index] = self[index] = self[index] = self[index] = self[index] = self[index] = self[index] = self[index] = self[index] = self[index] = self[index] = self[index] = self[index] = self[index] = self[index] = self[index] = self[index] = self[index] = self[index] = self[index] = self[index] = self[index] = self[index] = self[index] = self[index] = self[index] = self[index] = self[index] = self[index] = self[index] = self[index] = self[index] = self[index] = self[index] = self[index],"def insert(self, index: int, string: str) -> None: ss, se = self._span lststr = self._lststr lststr0 = lststr[0] if index  0: index += se - ss if index  0: index = 0 elif index > se - ss: index = se - ss index += ss lststr[0] = lststr0[:index] + string + lststr0[index:] string_len = len(string) self._insert_update( index=index, length=string_len) type_to_spans = self._type_to_spans for type_, spans in parse_to_spans( bytearray(string, 'ascii','replace') ).items(): for s, e in spans: insort(type_to_spans[type_], [index + s, index + e])"
1501,"def read_phantom_tools(self, phantom_tools, phantom_tools, phantom_tools, phantom_tools, phantom_tools, phantom_tools, phantom_tools, phantom_tools, phantom_tools, phantom_tools, phantom_tools, phantom_tools, phantom_tools, phantom_tools) phantom_tools, phantom_tools, phantom_tools, phantom_tools, phantom_tools, phantom_tools, phantom_tools, phantom_tools, phantom_tools, phantom_tools, phantomtools, phantomtools, phantomtools, phantomtools, phantomtools, phantomtools, phantomtools, phantomtools, phantomtools, phantomtools, phantomtools, phantomtools, phantomtools, phanto","def read_config(self): self.threads = self.cfg[""threads""] or str(int(multiprocessing.cpu_count() / 2) + 1) self.phantom_modules_path = self.cfg[""phantom_modules_path""] self.additional_libs =''.join(self.cfg[""additional_libs""]) self.answ_log_level = self.cfg[""writelog""] if self.answ_log_level.lower() in ['0', 'false']: self.answ_log_level = 'none' elif self.answ_log_level.lower() in ['1', 'true']: self.answ_log_level = 'all' self.timeout = parse_duration(self.cfg[""timeout""]) if self.timeout > 120000: logger.warning( ""You've set timeout over 2 minutes."" "" Are you a functional tester?"") self.answ_log = self.core.mkstemp("".log"", ""answ_"") self.core.add_artifact_file(self.answ_log) self.core.add_artifact_file(self.phout_file) self.core.add_artifact_file(self.stat_log) self.phantom_log = self.core.mkstemp("".log"", ""phantom_"") self.core.add_artifact_file(self.phantom_log) main_stream = StreamConfig( self.core, len(self.streams), self.phout_file, self.answ_log, self.answ_log_level, self.timeout, self.cfg, True) self.streams.append(main_stream) for section in self.multi(): self.streams.append( StreamConfig( self.core, len(self.streams), self.ph"
1502,"def seg1(self, seg2): if self.seg(self.seg1), self.seg(self.seg2), self.seg(self.seg1), self.seg(self.seg2), self.seg(self.seg2), self.seg(self.seg1), self.seg(self.seg2), self.seg(self.seg1))","def _segments_are_adjacent(seg1, seg2): lsf1, lss1 = seg1 lsf2, lss2 = seg2 for i, f1 in enumerate(lsf1): for j, f2 in enumerate(lsf2): if f1 = f2 + 1 and f1 >= f2 - 1: if lss1[i] = lss2[j] + 1 and lss1[i] >= lss2[j] - 1: return True return False"
1503,"def translate_date_string(self, date_string, date_string, keep_formatting, keep_formatting, keep_formatting, keep_formatting, keep_formatting, keep_formatting, keep_formatting, keep_formatting, keep_formatting, keep_formatting, keep_formatting, keep_formatting, keep_formatting, keep_formatting, keep_formatting, translate_date_string, keep_formatting, translate_date_string, translate_date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string, date_string","def translate(self, date_string, keep_formatting=False, settings=None): date_string = self._translate_numerals(date_string) if settings.NORMALIZE: date_string = normalize_unicode(date_string) date_string = self._simplify(date_string, settings=settings) dictionary = self._get_dictionary(settings) date_string_tokens = dictionary.split(date_string, keep_formatting) relative_translations = self._get_relative_translations(settings=settings) for i, word in enumerate(date_string_tokens): word = word.lower() for pattern, replacement in relative_translations.items(): if pattern.match(word): date_string_tokens[i] = pattern.sub(replacement, word) else: if word in dictionary: date_string_tokens[i] = dictionary[word] or '' if ""in"" in date_string_tokens: date_string_tokens = self._clear_future_words(date_string_tokens) return self._join(list(filter(bool, date_string_tokens)), separator="""" if keep_formatting else "" "", settings=settings)"
1504,"def initialize_log(filename, filename, loglevel): if filename is None: if filename is None: if filename is None: if filename is None: if filename is None: if filename is None: if filename is None: if filename is None: if filename is None: if filename is None: if filename is None: if filename is None: if filename is None: if filename is None: loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = loglevel = log","def init_log(logger, filename=None, loglevel=None): template = '[%(asctime)s] %(levelname)-8s: %(name)-25s: %(message)s' formatter = logging.Formatter(template) if loglevel: logger.setLevel(getattr(logging, loglevel)) console = logging.StreamHandler() console.setLevel('WARNING') console.setFormatter(formatter) if filename: file_handler = logging.FileHandler(filename, encoding='utf-8') if loglevel: file_handler.setLevel(getattr(logging, loglevel)) file_handler.setFormatter(formatter) logger.addHandler(file_handler) else: if loglevel: console.setLevel(getattr(logging, loglevel)) logger.addHandler(console)"
1505,"def _basename(self, 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'foo', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar","def get_basename(self, path): if path[-1] == PATH_SEP: path = path[0:-1] return os.path.basename(path)"
1506,"def close_remote_worker(self, remote_worker): self.close_worker(self.close_worker) self.close_worker(self.close_worker) self.close_worker(self.close_worker) self.close_worker(self.close_worker) self.close_worker(self.close_worker) self.close_worker(self.close_worker) self.close_worker(self.close_worker) self.close_worker","def connection_lost(self, exc): logger.debug(""worker connection lost"") self._worker.close() self._workers.remove(self._worker)"
1507,"def hpo_genes(self, hpo_genes, hpo_genes, hpo_genes, hpo_genes, hpo_genes, hpo_genes, hpo_genes, hpo_genes, hpo_genes, hpo_genes, hpo_genes): hpo_genes, hpo_genes, hpo_genes, hpo_genes, hpo_genes, hpo_genes, hpo_genes, hpo_genes, hpo_genes, hpo_genes, hpo_genes, hpo_genes, hpo_genes, hpo_genes, hpo_genes, hpo_genes, hpo_genes, hpo_genes, hpo_genes, hpo_genes, hpo_gen","def hpo_genes(context, hpo_term): LOG.info(""Running scout export hpo_genes"") adapter = context.obj['adapter'] header = [""#Gene_idtCount""] if not hpo_term: LOG.warning(""Please use at least one hpo term"") context.abort() for line in header: click.echo(line) for term in adapter.generate_hpo_gene_list(*hpo_term): click.echo(""0t1"".format(term[0], term[1]))"
1508,def add_result(result(result(result(result(result(result(result(result(result(result(result(result(result(result(result(result(result(result(result(result(result(result(result(result(result(result))))))))))))(result(result(result(result(result(result(result(result(result(result(result(result(result(result(result(result(result(result(result(result(result(result(result))))))))))))))))))))),"def add(self, result): assert not self._complete self._results.append(result) self._change()"
1509,"def phred(self, phred, phred, phred, phred, phred, phred, phred, phred, phred, phred, phred, phred, phred, phred, phred, phred, phred, phred, phred, phred, phred, phred, phred, phred, phred, phred)","def parse_cadd(variant, transcripts): cadd = 0 cadd_keys = ['CADD', 'CADD_PHRED'] for key in cadd_keys: cadd = variant.INFO.get(key, 0) if cadd: return float(cadd) for transcript in transcripts: cadd_entry = transcript.get('cadd') if (cadd_entry and cadd_entry > cadd): cadd = cadd_entry return cadd"
1510,"def Generate_user+password(user, password): if user is None: if password is None: if password is None: if password is None: if password is None: if password is None: if password is None: if password is None: if password is None: if password is None: if password is None: if password is None: if password is None: if password is None: if password is None: if password is None: if password is None: if password is None: if password is None: if password is None: if password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password = password =","def _create_url_identifier(user, password): if user is not None: user = parse.quote(user.encode('utf-8'), safe=USERINFO_SAFE_CHARS) if password: password = parse.quote(password.encode('utf-8'), safe=USERINFO_SAFE_CHARS) return '0:1'.format(user, password) return user return None"
1511,"def split_vertices(vert_displacement_ratio, vert_displacement_ratio, vert_displacement_ratio, vert_displacement_ratio, vert_displacement_ratio, vert_displacement_ratio, vert_displacement_ratio, vert_displacement_ratio, vert_displacement_ratio, vert_displacement_ratio, vert_displacement_ratio, vert_displacement_ratio, vert_displacement_ratio, vert_displacement_ratio, vert_displacement_ratio)","def split_vert_on_nonmanifold_face(script, vert_displacement_ratio=0.0): filter_xml = ''.join(['filter name=""Split Vertexes Incident on Non Manifold Faces"">n','Param name=""VertDispRatio"" ', 'value="""" '.format(vert_displacement_ratio), 'description=""Vertex Displacement Ratio"" ', 'type=""RichFloat"" ', '/>n','/filter>n']) util.write_filter(script, filter_xml) return None"
1512,"def parse_streams(self, url, name_key, name_prefix, name_prefix, check_streams, force_restart, force_restart, check_streams, force_restart, name_fmt, name_fmt, name_fmt, name_fmt, name_fmt, name_fmt, name_fmt, name_fmt, name_fmt, name_fmt, name_fmt, name_fmt, name_fmt, name_fmt, name_fmt, name_fmt, name_fmt, name_fmt, name_fmt, name_fmt)","def parse_variant_playlist(cls, session_, url, name_key=""name"", name_prefix="""", check_streams=False, force_restart=False, name_fmt=None, start_offset=0, duration=None, **request_params): locale = session_.localization name_key = request_params.pop(""namekey"", name_key) name_prefix = request_params.pop(""nameprefix"", name_prefix) audio_select = session_.options.get(""hls-audio-select"") or [] res = session_.http.get(url, exception=IOError, **request_params) try: parser = hls_playlist.load(res.text, base_uri=res.url) except ValueError as err: raise IOError(""Failed to parse playlist: 0"".format(err)) streams =  for playlist in filter(lambda p: not p.is_iframe, parser.playlists): names = dict(name=None, pixels=None, bitrate=None) audio_streams = [] fallback_audio = [] default_audio = [] preferred_audio = [] for media in playlist.media: if media.type == ""VIDEO"" and media.name: names[""name""] = media.name elif media.type == ""AUDIO"": audio_streams.append(media) for media in audio_streams: if not media.uri: continue if not fallback_audio and media.default: fallback_audio = [media] if not default_audio and (media.autoselect and locale.equivalent(language=media.language)): default_audio = [media] if (('*' in audio_select or media.language in audio_select or media.name in audio_select) or ((not preferred_audio or media."
1513,def get_item(self): if self.get_item(self.get_item): self.get_item(self.get_item): self.get_item(self.get_item): self.get_item(self.get_item): self.get_item(self.get_item): self.get_item(self.get_item),"def normal_right_down(self, event): x = event.x y = event.y component = self.component for tool in component.tools: component.active_tool = self event.handled = True component.active_tool = None component.request_redraw() return"
1514,"def list_contents_of_dir(self, directory): if self.dir(dir): self.dir(dir): self.dir(dir): self.dir(dir): self.dir(dir): self.dir(dir): self.dir(dir): self.dir(dir)","def list(self): return [File(f, parent=self) for f in os.listdir(self.path)]"
1515,"def delete_image_registry(self, registry): if self.repo(self.repo): self.repo(self.repo) self.repo(self.repo) self.repo(self.repo) self.repo(self.repo) self.repo(self.repo) self.repo(self.repo) self.repo(self.repo) self.repo(self.repo) self.repo_repo_repo_repo_repo_repo_repo_repo_repo_repo_repo_repo_repo_repo_repo_repo_repo_repo_repo_repo_repo_repo_repo_repo_repo_repo_repo_repo_repo_repo_repo_repo_repo_repo_repo_repo_repo_repo_repo_repo_repo_repo_repo_","def delete_registry(self, registry): if re.match("".*/.*"", registry): return [False, ""input registry name cannot contain '/' characters - valid registry names are of the form host>:port> where :port> is optional""] url = self.url + ""/api/scanning/v1/anchore/registries/"" + registry res = requests.delete(url, headers=self.hdrs, verify=self.ssl_verify) if not self._checkResponse(res): return [False, self.lasterr] return [True, res.json()]"
1516,"def convert_bytes_to_bytes(self, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s,","def path_string(s): if isinstance(s, binary_type): return s elif isinstance(s, text_type): return s.encode(sys.getfilesystemencoding()) else: raise TypeError(""Path must be represented as bytes or unicode string"")"
1517,"def flush_none_control_response(self, flush_none, flush_none, flush_none, flush_none, flush_none, flush_none, flush_none, flush_none, flush_none, flush_none, flush_none, flush_none, flush_none, flush_none, flush_none, flush_none, flush_none, flush_none, flush_none, flush_none, flush_none, flush_none, flush_none, flush_none, flush_none, flush_none, flush_none, flush_none, flush_none, flush_none, control replies, control replies, control replies, control replies, control replies, control replies, control replies, control replies, control replies, control replies, control replies, control replies, control replies, control replies, control replies, control replies, control replies, control replies, control replies, control replies, control replies, control replies, control replies, control replies, control replies",def _flush_ignored_control(self): while self._ignored_control_replies > 0: self.session.recv(self._control_socket) self._ignored_control_replies -= 1
1518,"def create_x-axis(self, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis) axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis,","def create_x_axis(self, name, label=None, format=None, date=False, custom_format=False): axis =  if custom_format and format: axis['tickFormat'] = format elif format: if format == 'AM_PM': axis['tickFormat'] = ""function(d)  return get_am_pm(parseInt(d)); "" else: axis['tickFormat'] = ""d3.format(',%s')"" % format if label: axis['axisLabel'] = ""'"" + label + ""'"" if date: self.dateformat = format axis['tickFormat'] = (""function(d)  return d3.time.format('%s')"" ""(new Date(parseInt(d))) n"" """" % self.dateformat) if name[0] == 'x': self.x_axis_date = True self.axislist[name] = axis if name == ""xAxis"" and self.focus_enable: self.axislist['x2Axis'] = axis"
1519,def compute_estimator(self): self.get_estimator(self.get_estimator) self.get_estimator(self.estimator) self.estimator(self.estimator) self.estimator(self.estimator) self.estimator(self.estimator) self.estimator(self.estimator) self.estimator(self.estimator) self.estimator(self.estimator) self.estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_estimator_,"def _score(estimator, Z_test, scorer): score = scorer(estimator, Z_test) if not isinstance(score, numbers.Number): raise ValueError(""scoring must return a number, got %s (%s) instead."" % (str(score), type(score))) return score"
1520,"def get_method(self, doc): if self.get_method(self.get_method(self.get_method(self.get_method(self.get_method(self.get_method(self.get_method(self.get_method(self.get_method(self.get_method(self.get_method(self.get_method(self.get_method(self.get_method(self.get_method(self.get_method(self.get_method(self.get_method(self.get_method(self.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method)","def getObjDebugName(self, obj: Union[Interface, Unit, Param]) -> str: return obj._getFullName()"
1521,def block_proposal(block_proposal): block_proposal(block_proposal): block_proposal(block_proposal): block_proposal(block_proposal): block_proposal(block_proposal): block_proposal(block_proposal): block_proposal(block_proposal): block_proposal(block_proposal): block_proposal(block_proposal(block_proposal): block_proposal(block_proposal): block_proposal(block_proposal): block_proposal(block_proposal): block_proposal(block_proposal): block_proposal(block_proposal): block_proposal(block_proposal): block_proposal(block_proposal): block_proposal): block_proposal(block_proposal): block_proposal(block_proposal): block_proposal(block_proposal): block_proposal(block_proposal): block_proposal(block_proposal): block_proposal(block_proposal,"def last_voted_blockproposal(self): ""the last block proposal node voted on"" for r in self.rounds: if isinstance(self.rounds[r].proposal, BlockProposal): assert isinstance(self.rounds[r].lock, Vote) if self.rounds[r].proposal.blockhash == self.rounds[r].lock.blockhash: return self.rounds[r].proposal"
1522,"def _leapfrog_integrator_one_step(self, tf, dtype, tf, dtype, tf, dtype, tf, dtype, tf, dtype, tf, dtype, tf, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype, dtype,","def _leapfrog_integrator_one_step( target_log_prob_fn, independent_chain_ndims, step_sizes, current_momentum_parts, current_state_parts, current_target_log_prob, current_target_log_prob_grad_parts, state_gradients_are_stopped=False, name=None): with tf.compat.v1.name_scope(name, 'hmc_leapfrog_integrator_one_step', [ independent_chain_ndims, step_sizes, current_momentum_parts, current_state_parts, current_target_log_prob, current_target_log_prob_grad_parts ]): proposed_momentum_parts = [ v + 0.5 * tf.cast(eps, v.dtype) * g for v, eps, g in zip(current_momentum_parts, step_sizes, current_target_log_prob_grad_parts)] proposed_state_parts = [ x + tf.cast(eps, v.dtype) * v for x, eps, v in zip(current_state_parts, step_sizes, proposed_momentum_parts)] if state_gradients_are_stopped: proposed_state_parts = [tf.stop_gradient(x) for x in proposed_state_parts] [ proposed_target_log_prob, proposed_target_log_prob_grad_parts, ] = mcmc_util.maybe_call_fn_and_grads( target_log_prob_fn, proposed_state_parts) if not proposed_target_log_prob.dtype.is_floating: raise TypeError('target_log_prob_fn must produce a Tensor''with float dtype."
1523,"def upload_bundles(self, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr, jstr,","def loads(self, jstr): if isinstance(jstr, dict): _info = jstr else: _info = json.loads(jstr) for iss, jwks in _info.items(): kj = KeyJar() if isinstance(jwks, dict): kj.import_jwks(jwks, issuer=iss) else: kj.import_jwks_as_json(jwks, issuer=iss) self.bundle[iss] = kj return self"
1524,"def load_multi_images(filenames, masker, nan_to_num, nan_to_num, nan_to_num, nan_to_num, nan_to_num, nan_to_num, nan_to_num, nan_to_num, nan_to_num, nan_to_num, nan_to_num, nan_to_num): nan_to_num, nan_to_num, nan_to_num, nan_to_num, nan_to_num, nan_to_num, nan_to_num, nan_to_num, nan_to_num, nan_to_num, nan_to_num, nan_to_num, nan_to_num, nan_to_num, nan_to_num, nan_to_num, nan_to_num, nan_to_num, nan_to_num, nan_to_num, nan_to_num, nan_to_num, nan_to_num, n","def load_imgs(filenames, masker, nan_to_num=True): if isinstance(filenames, string_types): filenames = [filenames] data = np.zeros((masker.n_vox_in_mask, len(filenames))) for i, f in enumerate(filenames): data[:, i] = masker.mask(f, nan_to_num) return data"
1525,"def get_link_tag(self, link): self.get_link_tag(link) self.get_link_tag(link) self.get_link_tag(link) self.get_link_tag(link) self.get_link_tag(link) self.get_link_tag(link) self.get_link_tag(link) self.get_link_tag(link) self.get_link_tag(link) self.get_link_tag(link) self.get_link_tag(link) self.get_link_tag(link) self.get_link_tag(link) self.get_link_tag(link) self.get_link_tag(link) self.get_link_tag(link) self.get_tag(link) self.get_tag(link) self.get_tag(link) self.get_tag(link) self.get_tag(link) self.get_tag(link) self.get_tag(link) self.get_tag(link) self.get_tag(link) self.get_tag(link) self.get_tag(link) self.get_tag(link) self.get_tag(link) self.get_tag(link) self.get_tag(link) self.get_tag(","def cssfile(url): if not url.startswith('http://') and not url[:1] == '/': #add media_url for relative paths url = settings.STATIC_URL + url return 'link href=""src"" rel=""stylesheet"">'.format(src=url)"
1526,"def uncharge(mol, mol, mol, mol, mol, mol, mol, mol, mol, mol, mol, mol, mol, mol, mol, mol, mol, mol, mol, mol, mol, mol, mol, mol, mol, mol, mol, mol, mol, mol, mol, mol, mol, mol, mol, mol, mol, mol, mol, mol, mol, mol)","def uncharge(self, mol): log.debug('Running Uncharger') mol = copy.deepcopy(mol) pos_remainder = 0 neg_count = 0 for atom in mol.GetAtoms(): while atom.GetFormalCharge() > 0 and atom.GetNumExplicitHs() > 0: atom.SetNumExplicitHs(atom.GetNumExplicitHs() - 1) atom.SetFormalCharge(atom.GetFormalCharge() - 1) log.info('Removed positive charge') chg = atom.GetFormalCharge() if chg > 0: pos_remainder += chg elif chg  0: neg_count += -chg neg_skip = self._get_neg_skip(mol, pos_remainder) for atom in mol.GetAtoms(): log.info(atom.GetIdx()) if atom.GetIdx() in neg_skip: continue while atom.GetFormalCharge()  0 and neg_count > pos_remainder: atom.SetNumExplicitHs(atom.GetNumExplicitHs() + 1) atom.SetFormalCharge(atom.GetFormalCharge() + 1) neg_count -= 1 log.info('Removed negative charge') return mol"
1527,"def authenticate_user(self, username, password, request_token, request_token, request_token, request_token, request_token, request_token, request_token, request_token, request_token, request_token, request_token, request_token, request_token, request_token, request_token, request_token, request_token, request_token, request_token, request_token, request_token, request_token, request_token, request_token, request_token)","def token_validate_with_login(self, **kwargs): path = self._get_path('token_validate_with_login') response = self._GET(path, kwargs) self._set_attrs_to_values(response) return response"
1528,def validate_pypebbleapi.schemas(pypebbleapi.schema): if pypebbleapi.schema(pypebbleapi.schema): if pypebbleapi.schema(pypebbleapi.schema): if pypebbleapi.schema(pypebbleapi.schema): if pypebbleapi.schema(pypebbleapi.schema): if pypebbleapi.schema): if pypebbleapi.schema = pypebbleapi.schema = pypebbleapi.schema = pypebbleapi.schema = pypebbleapi.schema = pypebbleapi.schema = pypebbleapi.schema = pypebbleapi.schema = pypebbleapi.schema = pypebbleapi.schema = pypebbleapi.schema = pypebbleapi.schema = pypebbleapi.schema = pypebbleapi.schema = pypebbleapi.schem,def validate_pin(pin): v = _Validator(schemas.pin) if v.validate(pin): return else: raise schemas.DocumentError(errors=v.errors)
1529,"def plot_job_history(list, interval, interval, interval, interval, interval, interval, interval, interval, interval, interval, interval, interval, interval, interval, interval, interval, interval, interval, interval, interval, interval, interval, interval, interval, interval, interval, interval, interval, interval, interval, interval, interval, interval, interval, interval, interval, interval, interval, interval, interval, interval)","def plot_job_history(jobs, interval='year'): return datetime.datetime.strptime(job.creation_date(), '%Y-%m-%dT%H:%M:%S.%fZ') current_time = datetime.datetime.now() if interval == 'year': bins = [(current_time - datetime.timedelta(days=k*365/12)) for k in range(12)] elif interval =='month': bins = [(current_time - datetime.timedelta(days=k)) for k in range(30)] elif interval == 'week': bins = [(current_time - datetime.timedelta(days=k)) for k in range(7)] binned_jobs = [0]*len(bins) if interval == 'year': for job in jobs: for ind, dat in enumerate(bins): date = get_date(job) if date.month == dat.month: binned_jobs[ind] += 1 break else: continue else: for job in jobs: for ind, dat in enumerate(bins): date = get_date(job) if date.day == dat.day and date.month == dat.month: binned_jobs[ind] += 1 break else: continue nz_bins = [] nz_idx = [] for ind, val in enumerate(binned_jobs): if val!= 0: nz_idx.append(ind) nz_bins.append(val) total_jobs = sum(binned_jobs) colors = ['#003f5c', '#ffa600', '#374c80', '#ff764a', '#7a5195', '#ef5675',"
1530,"def edit_panel_gene(self, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene): panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_gene, panel_","def gene_edit(panel_id, hgnc_id): panel_obj = store.panel(panel_id) hgnc_gene = store.hgnc_gene(hgnc_id) panel_gene = controllers.existing_gene(store, panel_obj, hgnc_id) form = PanelGeneForm() transcript_choices = [] for transcript in hgnc_gene['transcripts']: if transcript.get('refseq_id'): refseq_id = transcript.get('refseq_id') transcript_choices.append((refseq_id, refseq_id)) form.disease_associated_transcripts.choices = transcript_choices if form.validate_on_submit(): action = 'edit' if panel_gene else 'add' info_data = form.data.copy() if 'csrf_token' in info_data: del info_data['csrf_token'] store.add_pending(panel_obj, hgnc_gene, action=action, info=info_data) return redirect(url_for('.panel', panel_id=panel_id)) if panel_gene: for field_key in ['disease_associated_transcripts','reduced_penetrance','mosaicism', 'inheritance_models', 'database_entry_version', 'comment']: form_field = getattr(form, field_key) if not form_field.data: panel_value = panel_gene.get(field_key) if panel_value is not None: form_field.process_data(panel_value) return dict(panel=panel_obj, form=form, gene=hgnc_gene, panel_gene=panel_"
1531,"def check_file(self, byte, byte, byte): self.get_file(self.get_file) self.get_file(self.get_file) self.get_file(self.get_file) self.get_file(self.get_file) self.get_file(self.get_file) self.get_file(self.get_file) self.get_file(self.get_file) self.get_file(self.get_file) self.get_file) self.get_file) self.get_file(self.get_file) self.get_file(self.get_file) self.get_file(self.get_file) self.get_file(self.get_file) self.get_file) self.get_file) self.get_file(self.get_file) self.get_file(self.get_file) self.get_file(self.get_file) self.get_file(self.get_file) self.get_file(self.get_file) self.get_file(self.get_file) self.get_file(self.get_file) self.get_file(self.get_file) self.get_file(self.get_file) self.get_file",def is_empty(self): something = self.read(1) if something: if self.buf: self.buf = something + self.buf else: self.buf = something return False else: return True
1532,"def sampling_dist_q(log_f, sampling_dist_q, sampling_dist_q, sampling_dist_q, sampling_dist_q, sampling_dist_q, sampling_dist_q, sampling_dist_q, sampling_dist_q, sampling_dist_q, sampling_dist_q, sampling_dist_q, sampling_dist_q, sampling_dist_q, sampling_dist_q, sampling_dist_q, sampling_dist_q, sampling_dist_q)","def expectation_importance_sampler_logspace( log_f, log_p, sampling_dist_q, z=None, n=None, seed=None, name='expectation_importance_sampler_logspace'): rq = sampling_dist_q with tf.name_scope(name): z = _get_samples(q, z, n, seed) log_values = log_f(z) + log_p(z) - q.log_prob(z) return _logspace_mean(log_values)"
1533,"def match_nodes(self, mme_base_url(str)): mme_base_url(str)): mme_base_url(str): mme_base_url(str)): mme_base_url(str): mme_base_url(str)): mme_base_url(str)): mme_base_url(str): mme_base_url(str): mme_base_url(str): mme_base_url(str): mme_base_url(str)): mme_base_url(str): mme_base_url(str): mme_base_url(str): mme_base_url(str): mme_base_url(str): mme_base_url(str): mme_base_url(str): mme_base_url(str): mme_base_url(str): mme_base_url(st","def mme_nodes(mme_base_url, token): nodes = [] if not mme_base_url or not token: return nodes url = ''.join([mme_base_url, '/nodes']) nodes = matchmaker_request(url=url, token=token, method='GET') LOG.info('Matchmaker has the following connected nodes:'.format(nodes)) return nodes"
1534,"def create_distribution(self, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params, params)","def params_size(num_components, event_shape=(), name=None): return MixtureSameFamily.params_size( num_components, IndependentNormal.params_size(event_shape, name=name), name=name)"
1535,def maintain_current_time(self): if self.maintained_time(): self.maintained_time(): self.maintained_time(): self.maintained_time(): self.maintained_time(): self.maintained_time(): self.maintained_time(): self.maintained_time(): self.maintained_time(): self.maintained_time(): self.maintained_time(): self.maintained_time(): self.maintained_time(),"def maintained_time(): ct = cmds.currentTime(query=True) try: yield finally: cmds.currentTime(ct, edit=True)"
1536,"def get_statistics(self, force=True): self.get_statistics(self, force=True) self.get_statistics(self, force=True) self.get_statistics(self.statistics) self.get_statistics(self.statistics) self.get_statistics(self.statistics) self.get_statistics(self.statistics) self.get_statistics(self.statistics) self.get_statistics) self.get_statistics(self.get_statistics) self.get_statistics(self.get_statistics) self.get_statistics) self.get_statistics(self.get_statistics) self.get_statistics(self.get_statistics) self.get_statistics) self.get_statistics(self.get_statistics) self.get_statistics(self.get_statistics) self.get_statistics(self.get_statistics) self.get_statistics) self.get_statistics(self.get_statistics) self.get_statistics(self.get_statistics) self.get_statistics","def get_stats(self, username='', password='', organization='llnl', force=True, repo_type='public'): date = str(datetime.date.today()) file_path = ('../github_stats_output/' + date[:4] + '/' + date[:7] + '/' + date + '.csv') if force or not os.path.isfile(file_path): my_github.login(username, password) calls_beginning = self.logged_in_gh.ratelimit_remaining + 1 print 'Rate Limit:'+ str(calls_beginning) my_github.get_org(organization) count_members = my_github.get_mems_of_org() count_teams = my_github.get_teams_of_org() my_github.repos(repo_type=repo_type, organization=organization) #Write JSON my_github.write_org_json(dict_to_write=self.members_json, path_ending_type='members', is_list=True) my_github.write_org_json(dict_to_write='singleton': self.org_retrieved.to_json(), path_ending_type='organization') my_github.write_org_json(dict_to_write=self.teams_json, path_ending_type='teams', is_list=True) my_github.write_repo_json(dict_to_write=self.repos_json, path_ending_type='repo') my_github.write_repo_json(dict_to_write=self.contributors_json, path_ending_type='contributors', is_list=True) my_github.write_repo_json(dict_to_write=self.pull_"
1537,"def stream_decode(stream, validator, snippet, snippet, snippet, snippet, snippet, validator, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet, snippet,","def decode_stream(stream, validator=None, snippet=False): tree = etree.parse(stream) if validator is not None: validator.assertValid(tree) root = tree.getroot() v = root.get(""version"") if not snippet and (not v or v!= ""2.0""): raise ValueError(""Only OpenMath 2.0 is supported"") return decode_xml(root)"
1538,"def set_command_line_options(self, command line): def set_command_line_options(command line): def set_command_line_options(command line): def set_command_line_options(command line): def set_command_line_options(command line): def set_command_line_options(command line): def set_command_line_options(command line): def set_command_line_options(command line_options(command line_options): def set_command line_options(command line_options): def set_command line_options): def set_command line_options(command line_options): def set_command line_options(command line_options): def set_options(command line_options): def set_options(command line_options): def set_options): def set_options(command line_options): def set_options(command line_options): def set_options(command line_options)","def options(self, parser, env=os.environ): super(NoseExclude, self).options(parser, env) env_dirs = [] if 'NOSE_EXCLUDE_DIRS' in env: exclude_dirs = env.get('NOSE_EXCLUDE_DIRS','') env_dirs.extend(exclude_dirs.split(';')) parser.add_option( ""--exclude-dir"", action=""append"", dest=""exclude_dirs"", default=env_dirs, help=""Directory to exclude from test discovery.  Path can be relative to current working directory  or an absolute path. May be specified multiple  times. [NOSE_EXCLUDE_DIRS]"") parser.add_option( ""--exclude-dir-file"", type=""string"", dest=""exclude_dir_file"", default=env.get('NOSE_EXCLUDE_DIRS_FILE', False), help=""A file containing a list of directories to exclude  from test discovery. Paths can be relative to current  working directory or an absolute path.  [NOSE_EXCLUDE_DIRS_FILE]"")"
1539,"def extract_percussive(y_percussive, kwargs, y_percussive, y_percussive, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, y_percussive, y_percussive, y_percussive, y_percussive, y_percussive, y_percussive, y_percussive, y_percussive, y_percussive, y_percussive, y_percussive, y_percussive, y_percussive, y_percussive, y_percussive, y_percussive, y_percussive, y_percussive, y_percussive, y_percussive","def percussive(y, **kwargs): stft = core.stft(y) stft_perc = decompose.hpss(stft, **kwargs)[1] y_perc = util.fix_length(core.istft(stft_perc, dtype=y.dtype), len(y)) return y_perc"
1540,"def add_label(self, dictionary): self.label(self.label): self.label(self.label) self.label(self.label) self.label(self.label) self.label(self.label) self.label(self.label) self.label(self.label) self.label(self.label) self.label(self.label) self.label(self.label) self.label(self.label) self.label) self.label) self.label(self.label) self.label(self.label) self.label) self.label) self.label(self.label) self.label) self.label(self.label) self.label) self.label(self.label) self.label) self.label) self.label(self.label) self.label) self.label(self.label) self.label) self.label(self.label) self.label) self.label(self.label) self.label) self.label(self.label) self.label) self.label(self.label) self","def _add_label_from_dict(self, query_params=None): return self.fetch_json( uri_path=self.base_uri + '/labels', http_method='POST', query_params=query_params or  )"
1541,"def get_max_statement_id(self, max_statement_id, max_statement_id, max_statement_id, max_statement_id, max_statement_id, max_statement_id, max_statement_id, max_statement_id, max_statement_id, max_statement_id, max_statement_id, max_statement_id, max_statement_id, max_statement_id): max_statement_id, max_statement_id, max_statement_id, max_statement_id, max_statement_id, max_statement_id, max_statement_id, max_statement_id, max_statement_id, max_statement_id, max_statement_id, max_statement_id, max_statement_id, max_statement_id, max_statement_id, max_statement_id, max_statement_id, max_statement_id, max_statement_id, max_statement_id, max_statement_id, max_statement","def maxStmId(proc): maxId = 0 for stm in proc.statements: maxId = max(maxId, getMaxStmIdForStm(stm)) return maxId"
1542,"def command(self, command): if command(command): if command(command): if command(command): if command(command): if command(command): if command(command): if command(command): if command(command): if command(command): command(command): command(command): command(command): command(command): command(command): command(command): command(command): command(command): command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command) command(command","def columnize_commands(self, commands): commands.sort() width = self.debugger.settings['width'] return columnize.columnize(commands, displaywidth=width, lineprefix=' ')"
1543,"def autorange(self, sig, kde_x, yd, trans, thresh, trans, trans, trans, trans, trans, trans, trans, trans, trans, trans, trans, trans, trans, trans, trans, thresh, thresh, thresh, thresh, trans, sig, sig, tsig, sig, sig, sig, sig, sig, sig, kde_x, kde_x, kde_x, trans, trans, trans, trans, trans, trans, trans, trans, trans, trans, trans, trans, trans, trans, trans, trans, trans, trans, trans, trans, trans, trans, trans, trans, trans, trans, trans, trans, trans, tsig, tsig, tsig, tsig, tsig, tsig, tsig, tsig, tsig, tsig, tsig, tsig, tsig, tsig, tsig, tsig, tsig, tsig, tsig, tsig, tsig, tsig, tsig, tsig, tsig, tsig, tsig, tsig","def autorange_components(t, sig, transform='log', gwin=7, swin=None, win=30, on_mult=(1.5, 1.), off_mult=(1., 1.5), thresh=None): failed = [] if swin is not None: sigs = fastsmooth(sig, swin) else: sigs = sig if transform == 'log': tsigs = np.log10(sigs) tsig = np.log10(sig) else: tsigs = sigs tsig = sig if thresh is None: bins = 50 kde_x = np.linspace(tsigs.min(), tsigs.max(), bins) kde = gaussian_kde(tsigs) yd = kde.pdf(kde_x) mins = findmins(kde_x, yd) if len(mins) > 0: bkg = tsigs  (mins[0]) thresh = mins[0] else: bkg = np.ones(tsigs.size, dtype=bool) else: bkg = tsigs  thresh fbkg = bkg fsig = bkg zeros = bool_2_indices(fsig) g = abs(fastgrad(sigs, gwin)) if zeros is not None: zeros = zeros.flatten() trans = dict(zeros=zeros.flatten(), lohi=[], pgs=[], excl=[], tps=[], failed=[], xs=[], ys=[]) for z in zeros: if z - win  0: lo = gwin // 2 hi = int(z + win) elif z + win > (len(sig) - gwin //"
1544,"def time_str(self, time_str, time_str, time_str, time_str, time_str, time_str, time_str, time_str, time_str, time_str, time_str, time_str, time_str, time_str, time_str, time_str, time_str, time_str, time_str, time_str, time_str, time_str, time_str, time_str, time_str, time_str, time_str, time_str, time_str, time_str, time_str, time_str, time_str, time_str, date_str, date_str, date_str, date_str, date_str, date_str, date_str, date_str, date_str, date_str, date_str, date_str, date_str, date_str, date","def parse_iso_8601_time_str(time_str): if re.search('.d3Z$', time_str): submitted_at = datetime.datetime.strptime(time_str,  '%Y-%m-%dT%H:%M:%S.%fZ') else: submitted_at = datetime.datetime.strptime(time_str,  '%Y-%m-%dT%H:%M:%SZ') return submitted_at.replace(tzinfo=UTC_TIMEZONE)"
1545,"def QasmQobjInstruction(self, QasmQobjInstruction, QasmQobjInstruction, QasmQobjInstruction, QasmQobjInstruction, QasmQobjInstruction, QasmQobjInstruction, QasmQobjInstruction, QasmQobjInstruction, QasmQobjInstruction, QasmQobjInstruction, QasmQobjInstruction, QasmQobjInstruction, QasmQobjInstruction, QasmQobjInstruction, QasmQobjInstruction, QasmQobjInstruction, QasmQobjInstruction, QasmQobjInstruction, QasmQobjInstruction, QasmQobjInstruction, QasmQobjInstruction, QasmQobjInstruction, QasmQobjInstruction, QasmQob","def assemble(self): instruction = QasmQobjInstruction(name=self.name) if self.params: params = [ x.evalf() if hasattr(x, 'evalf') else x for x in self.params ] params = [ sympy.matrix2numpy(x, dtype=complex) if isinstance( x, sympy.Matrix) else x for x in params ] instruction.params = params if self.num_qubits: instruction.qubits = list(range(self.num_qubits)) if self.num_clbits: instruction.memory = list(range(self.num_clbits)) if self.control: instruction._control = self.control return instruction"
1546,"def assign_task_number(self, task_number, task_number, task_number, task_number, task_number, task_number, task_number, task_number, task_number, task_number, task_number, task_number, task_number, task_number, task_number, task_number, task_number, task_number, task_number, task_number, task_number, task_number, task_number, task_number, task_number, task_number, task_number, task_number, task_number, task_number, task_number, task_number, task_number, task_number).","def apply_tasks_to_issue(self, issue, tasks, issue_body=None): issue_body = issue_body or issue.body task_numbers = transport.format_task_numbers_with_links(tasks) if task_numbers: new_body = transport.ASANA_SECTION_RE.sub('', issue_body) new_body = new_body + ""n# transport.issue_edit(issue, body=new_body) return new_body return issue_body"
1547,"def autodoc.Documenter(self, obj, parent, obj, obj, obj, parent, obj, obj, parent, obj, obj, parent, obj, obj, parent, obj, obj, parent, obj, obj, parent, obj, parent, obj): if self.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.doc.","def get_documenter(obj, parent): from sphinx.ext.autodoc import AutoDirective, DataDocumenter,  ModuleDocumenter if inspect.ismodule(obj): return ModuleDocumenter if parent is not None: parent_doc_cls = get_documenter(parent, None) else: parent_doc_cls = ModuleDocumenter if hasattr(parent, '__name__'): parent_doc = parent_doc_cls(FakeDirective(), parent.__name__) else: parent_doc = parent_doc_cls(FakeDirective(), """") classes = [cls for cls in AutoDirective._registry.values() if cls.can_document_member(obj, '', False, parent_doc)] if classes: classes.sort(key=lambda cls: cls.priority) return classes[-1] else: return DataDocumenter"
1548,"def munging_operations_on_frame(self, fr): munging_operations_on_frame(self, fr): munging_operations_on_frame(self, fr): munging_operations_on_frame(self, fr)","def fit(self, fr): assert_is_type(fr, H2OFrame) steps = ""[%s]"" % "","".join(quoted(step[1].to_rest(step[0]).replace('""', ""'"")) for step in self.steps) j = h2o.api(""POST /99/Assembly"", data=""steps"": steps, ""frame"": fr.frame_id) self.id = j[""assembly""][""name""] return H2OFrame.get_frame(j[""result""][""name""])"
1549,"def get_phase(self, wave, unwrap, unwrap, unwrap, rad, unwrap, wave, unwrap, unwrap, unwrap, unwrap, unwrap, unwrap, unwrap, unwrap, unwrap, unwrap, unwrap, unwrap, unwrap, unwrap, unwrap, unwrap, unwrap, unwrap, wave, unwrap, unwrap, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave, wave,","def phase(wave, unwrap=True, rad=True): rret = copy.copy(wave) ret.dep_units = ""rad"" if rad else ""deg"" ret.dep_name = ""phase(0)"".format(ret.dep_name) ret._dep_vector = ( np.unwrap(np.angle(ret._dep_vector)) if unwrap else np.angle(ret._dep_vector) ) if not rad: ret._dep_vector = np.rad2deg(ret._dep_vector) return ret"
1550,"def get_mboxes(self, mboxes, mboxes, mboxes, mboxes, mboxes, mboxes, mboxes, mboxes, mboxes, mboxes, mboxes, mboxes, mboxes, mboxes, mboxes, mboxes, mboxes, mboxes, mboxes, mboxes, mboxes, mboxes, mboxes)","def mboxes(self): archives = [] if os.path.isfile(self.dirpath): try: archives.append(MBoxArchive(self.dirpath)) except OSError as e: logger.warning(""Ignoring %s mbox due to: %s"", self.dirpath, str(e)) else: for root, _, files in os.walk(self.dirpath): for filename in sorted(files): try: location = os.path.join(root, filename) archives.append(MBoxArchive(location)) except OSError as e: logger.warning(""Ignoring %s mbox due to: %s"", filename, str(e)) return archives"
1551,"def update_internal_counters(self, internal_counters, internal_counters, internal_counters, internal_counters, internal_counters, internal_counters, internal_counters, internal_counters, internal_counters, internal_counters, internal_counters, internal_counters, internal_counters)",def __update_count(self): self._ntypes = self.count_types() self._nvars = self.count_vars() self._nfuns = self.count_funs()
1552,,"def requirements(work_dir, hive_root, with_requirements, with_dockerfile, active_module, active_module_file): modules_in_hive = map( lambda x: x.replace(hive_root, '').lstrip('/'), filter(lambda x: x.startswith(hive_root), active_module_paths)) modules_path =''.join(modules_in_hive) docker_file = os.path.join( os.path.dirname(requirements_root), 'Dockerfile' ) if os.path.exists(docker_file): click.echo(click.style(""Found Dockerfile,try update..."", fg=""yellow"")) with open(docker_file, 'r') as fp: buffer = fp.read() pass import re replaced = re.sub('ARG HIVE_PACKAGES="".*""', 'ARG HIVE_PACKAGES=""%s""' % modules_path, buffer) with open(docker_file, 'w') as fp: fp.write(replaced) pass pass pass def build_migrations(): models_pairs = filter( lambda pair: os.path.exists(pair[0]), map(lambda x: (os.path.join(x[0],'models.py'), x[1]), [(v, active_module_list[i]) for i, v in enumerate(active_module_paths)])) try: _, models = zip(*models_pairs) except ValueError: click.echo(click.style(""No models found,"" ""is it include in "" ""your PYTHONPATH?n"" ""Modules: %s"" % ','.join(active_"
1553,def convert_annotations(dict): annotations = protobuf(dict) if annotations = protobuf(dict) if annotations = protobuf(dict) if annotations = protobuf(dict) if annotations = protobuf(dict) if annotations = protobuf(dict),"def _convert_annotations(annotations): pb_annotations = [] for value, ts in annotations.items(): pb_annotations.append(zipkin_pb2.Annotation( timestamp=int(ts * 1000 * 1000), value=value, )) return pb_annotations"
1554,"def get_bucket_allocation_counts(self, keys): if keys are not self.get_bucket_allocation_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts(self.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts.get_counts","def get_subset_counts(self, *keys): if self.prepickle: key_set = [pickle.dumps(key) for key in set(keys)] else: key_set = list(set(keys)) hashtables = [unordered_storage('type': 'dict') for _ in range(self.b)] Hss = self.keys.getmany(*key_set) for key, Hs in zip(key_set, Hss): for H, hashtable in zip(Hs, hashtables): hashtable.insert(H, key) return [hashtable.itemcounts() for hashtable in hashtables]"
1555,"def calculate_heatmap(self, heatmap): self.get_heatmap(heatmap) self.get_heatmap(heatmap) self.get_heatmap(heatmap) self.get_heatmap(heatmap) self.get_heatmap(heatmap) self.get_heatmap(heatmap) self.get_heatmap(heatmap) self.get_heatmap(heatmap) self.get_heatmap(heatmap) self.get_heatmap(heatmap) self.get_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_heatmap_he","def profile_function(self): with _CodeHeatmapCalculator() as prof: result = self._run_object(*self._run_args, **self._run_kwargs) code_lines, start_line = inspect.getsourcelines(self._run_object) source_lines = [] for line in code_lines: source_lines.append(('line', start_line, line)) start_line += 1 filename = os.path.abspath(inspect.getsourcefile(self._run_object)) heatmap = prof.heatmap[filename] run_time = sum(time for time in heatmap.values()) return  'objectName': self._object_name, 'runTime': run_time,'result': result, 'timestamp': int(time.time()), 'heatmaps': [ 'name': self._object_name, 'heatmap': heatmap, 'executionCount': prof.execution_count[filename],'srcCode': source_lines, 'runTime': run_time ]"
1556,"def get_file_to_stream(stream, share_name, directory_name, file_name, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs","def get_file_to_stream(self, stream, share_name, directory_name, file_name, **kwargs): self.connection.get_file_to_stream(share_name, directory_name, file_name, stream, **kwargs)"
1557,"def compute_monitoring(self, monitors, name, expression): if self.monitoring(self.monitoring): self.monitoring = self.monitoring(monitoring)","def monitors(self, **kwargs): monitors = super(Classifier, self).monitors(**kwargs) regs = regularizers.from_kwargs(self, **kwargs) outputs, _ = self.build_graph(regs) return monitors + [('acc', self.losses[0].accuracy(outputs))]"
1558,"def ensembl_lines(ensembl_id, exac_lines, genemap_lines, hpo_lines, exac_lines, hgnc_id, hgnc_id, hgnc_id, exac_lines, hgnc_id, hgnc_id, hgnc_id, hgnc_id, hgnc_id, hgnc_id, hgnc_id, hgnc_id, hgnc_id, hgnc_id, hgnc_id, hgnc_id, hgnc_id, hgnc_id, hgnc_id, hgnc_id, hgnc_id, hgnc_id, hgnc_id, hgnc_id, hgnc_id, hgnc_id, hgnc_id, hgnc_id, hgnc_id,","def link_genes(ensembl_lines, hgnc_lines, exac_lines, mim2gene_lines, genemap_lines, hpo_lines): genes =  LOG.info(""Linking genes"") for hgnc_gene in parse_hgnc_genes(hgnc_lines): hgnc_id = hgnc_gene['hgnc_id'] genes[hgnc_id] = hgnc_gene add_ensembl_info(genes, ensembl_lines) symbol_to_id = genes_by_alias(genes) add_exac_info(genes, symbol_to_id, exac_lines) add_omim_info(genes, symbol_to_id, genemap_lines, mim2gene_lines) add_incomplete_penetrance(genes, symbol_to_id, hpo_lines) return genes"
1559,"def calculate_total_scattering(self, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction, scattering_fraction,","def build_b(self, scattering_fraction=0.01833): lg.info('Building b with scattering fraction of ::'+ str(scattering_fraction)) self.b = (self.b_b + self.b_water / 2.0) / scattering_fraction"
1560,"def named_nodes(self, named_nodes(), named_nodes(), named_nodes(), named_nodes(), named_nodes(), named_nodes(), named_nodes(), named_nodes(), named_nodes(), named_nodes())","def get_named_nodes(self, *names): warnings.warn('The method get_named_nodes() is being replaced by named_nodes()', 'Returning a list of node_ids is also deprecated, named_nodes()''returns a list of DAGNodes ', DeprecationWarning, 2) named_nodes = [] for node in self._multi_graph.nodes(): if node.type == 'op' and node.op.name in names: named_nodes.append(node._node_id) return named_nodes"
1561,"def populate_harmonic_tensor(self, harmonic_out, shape=x.shape, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic_out, harmonic","def harmonics_2d(harmonic_out, x, freqs, h_range, kind='linear', fill_value=0, axis=0): idx_in = [slice(None)] * x.ndim idx_freq = [slice(None)] * x.ndim idx_out = [slice(None)] * harmonic_out.ndim ni_axis = (1 + axis) % x.ndim for i in range(x.shape[ni_axis]): idx_in[ni_axis] = slice(i, i + 1) idx_freq[ni_axis] = i idx_out[1 + ni_axis] = idx_in[ni_axis] harmonics_1d(harmonic_out[tuple(idx_out)], x[tuple(idx_in)], freqs[tuple(idx_freq)], h_range, kind=kind, fill_value=fill_value, axis=axis)"
1562,"def safe_execfile(fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname,","def safe_execfile_ipy(self, fname): fname = os.path.abspath(os.path.expanduser(fname)) try: with open(fname) as thefile: pass except: warn('Could not open file %s> for safe execution.' % fname) return dname = os.path.dirname(fname) with prepended_to_syspath(dname): try: with open(fname) as thefile: self.run_cell(thefile.read(), store_history=False) except: self.showtraceback() warn('Unknown failure executing file: %s>' % fname)"
1563,"def write_optical_flow(flow, ndarray, filename, quantize, quantize, quantize, concat_axis, concat_axis, concat_axis, concat_axis, concat_axis, concat_axis, concat_axis, concat_axis, concat_axis, concat_axis, concat_axis, concat_axis, concat_axis): concat_axis = concat_axis = concat_axis = concat_axis = concat_axis = concat_axis = concat_axis = concat_axis = concat_axis = concat_axis = concat_axis = concat_axis = concat_axis = concat_axis = concat_axis = concat_axis = concat_axis = concat_axis = concat_axis = concat_axis = concat_axis = concat_axis = concat_axis = concat_axis = concat_axis = concat_axis = concat_axis = concat_axis = concat_axis = concat_axis = concat_axis = concat_axis =","def flowwrite(flow, filename, quantize=False, concat_axis=0, *args, **kwargs): if not quantize: with open(filename, 'wb') as f: f.write('PIEH'.encode('utf-8')) np.array([flow.shape[1], flow.shape[0]], dtype=np.int32).tofile(f) flow = flow.astype(np.float32) flow.tofile(f) f.flush() else: assert concat_axis in [0, 1] dx, dy = quantize_flow(flow, *args, **kwargs) dxdy = np.concatenate((dx, dy), axis=concat_axis) imwrite(dxdy, filename)"
1564,"def construct_oAuth2(self, spotify.Client, spotify.Client, spotify.Client, spotify.Client, spotify.Client, spotify.Client, spotify.Client, spotify.Client, spotify.Client, spotify.Client, spotify.Client, spotify.Client)","def from_client(cls, client, *args, **kwargs): return cls(client.http.client_id, *args, **kwargs)"
1565,"def match_selector(self, node.assert_matches_selector, xpath, visible=True, kwargs, args, kwargs, args, kwargs, args, kwargs, args, kwargs, args, kwargs, args, kwargs, args, kwargs, args, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs","def assert_matches_selector(self, *args, **kwargs): query = SelectorQuery(*args, **kwargs) @self.synchronize(wait=query.wait) def assert_matches_selector(): result = query.resolve_for(self.find_first(""xpath"", ""./parent::*"", minimum=0) or self.query_scope) if self not in result: raise ExpectationNotMet(""Item does not match the provided selector"") return True return assert_matches_selector()"
1566,"def url_preprocessor(self, url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor) url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor, url_preprocessor,","def url_value_preprocessor(self, f): self.record_once(lambda s: s.app.url_value_preprocessors.setdefault(self.name, []).append(f)) return f"
1567,def octs_to_octs(frequency): octs_to_octs(frequency): octs_to_octs(frequency): octs_to_octs(frequency): octs_to_octs(frequency): octs_to_octs(frequency): octs_to_octs_to_octs_to_octs(frequency): octs_to_octs_to_octs_to_octs_to_octs_to_octs_to_octs_to_octs_to_octs_to_octs_to_octs_to_octs_to_octs_to_octs_to_octs_to_octs_to_octs_to_octs_to_octs_to_octs_to_octs_to_oc,"def hz_to_octs(frequencies, A440=440.0): return np.log2(np.asanyarray(frequencies) / (float(A440) / 16))"
1568,"def fast_only(self, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only): if fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only, fast_only.fast_only.fast_only.fast_only.fast_only.fast_only.fast_only.fast_only.fast_only.fast_only.fast_only.fast_only.fast_only.fast_only.fast_only.fast_only.fast_only.fast_only.","def hcompress(self, hashroot): hfiles = self.keys(hashroot + ""/*"") all =  for f in hfiles: all.update(self[f]) self.uncache(f) self[hashroot + '/xx'] = all for f in hfiles: p = self.root / f if p.basename() == 'xx': continue p.remove()"
1569,def set_precision(self): self.precision(self.precision): self.precision(self.precision) self.precision(self.precision) self.precision(self.precision) self.precision(self.precision) self.precision(self.precision) self.precision(self.precision) self.precision(self.precision) self.precision(self.precision) self.precision(self.precision) self.precision(self.precision) self.precision(self.precision) self.precision(self.precision) self.precision(self.precision) self.precision(self.precision) self.precision) self.precision) self.precision(self.precision) self.precision) self.precision) self.precision(self.precision) self.precision) self.precision) self.precision(self.precision) self.precision) self.precision) self.precision(self.precision) self.precision) self.precision(self.precision) self.precision) self.precision) self.precision(self.precision) self.precision) self.precision) self.precision(self.precision) self.pre,"def _set_precision(self, precision): if self.one_mutation: self.min_width = 10*self.one_mutation else: self.min_width = 0.001 if precision in [0,1,2,3]: self.precision=precision if self.one_mutation and self.one_mutation1e-4 and precision2: self.logger(""ClockTree._set_precision: FOR LONG SEQUENCES (>1e4) precision>=2 IS RECOMMENDED."" "" nt **** precision %d was specified by the user""%precision, level=0) else: if self.one_mutation: if self.one_mutation>1e-4: self.precision=1 else: self.precision=2 else: self.precision=1 self.logger(""ClockTree: Setting precision to level %s""%self.precision, 2) if self.precision==0: self.node_grid_points = ttconf.NODE_GRID_SIZE_ROUGH self.branch_grid_points = ttconf.BRANCH_GRID_SIZE_ROUGH self.n_integral = ttconf.N_INTEGRAL_ROUGH elif self.precision==2: self.node_grid_points = ttconf.NODE_GRID_SIZE_FINE self.branch_grid_points = ttconf.BRANCH_GRID_SIZE_FINE self.n_integral = ttconf.N_INTEGRAL_FINE elif self.precision==3: self.node_grid_points = ttconf.NODE_GRID_SIZE_ULTRA self.branch_grid_points = ttconf.BRANCH_GRID_SIZE_ULTRA self.n_integral = ttconf.N_INTEGRAL_ULTRA else: self.node_gri"
1570,"def open_las(f, closefd=False): if source is a filename if source is a stream if source is a stream if source is a stream if source is a stream if source is a stream if source is a stream if source is a stream if source is a stream if source is a stream if source is a stream if source is a stream if source is a stream if source is","def open_las(source, closefd=True): if isinstance(source, str): stream = open(source, mode=""rb"") if not closefd: raise ValueError(""Cannot use closefd with filename"") elif isinstance(source, bytes): stream = io.BytesIO(source) else: stream = source return LasReader(stream, closefd=closefd)"
1571,"def _node_vars_name(self, vars): if vars = self.vars.name, vars = self.vars.name, vars = self.vars.name, vars = self.vars.name, vars = self.vars.name, vars = self.vars.name, vars = self.vars.name, vars = self.vars.name, vars = self.vars.name, vars = self.vars.name, vars.name, vars.name, vars.name, vars.name, vars.name, vars.name, vars.name, vars.name, vars.name, vars.name, vars.name, vars.name, vars.name, vars.name, vars.name, vars.name, vars.name, vars.name, vars.name, vars.name, vars.name, vars.name, vars.name, vars.name, vars.name, vars.name, vars.name, vars.name",def vars(self): if self._vars is None: self._vars = NNTreeNodeVars(self) return self._vars
1572,"def GET_Params(self, GET_Params): self.get_params(get_params): self.get_params(get_params) self.get_params(get_params) self.get_params(get_params) self.get_params(get_params) self.get_params(get_params) self.get_params(get_params) self.get_params) self.get_params(get_params) self.get_params) self.get_params) self.get_params(get_params) self.get_params) self.get_params(get_params) self.get_params) self.get_params(get_params) self.get_params) self.get_params(get_params) self.get_params(get_params) self.get_params(get_params) self.get_params(get_params) self.get_params) self.get_params(get_params","def get_gecko_params(request, uid=None, days_back=0, cumulative=True, frequency=settings.STATISTIC_FREQUENCY_DAILY, min_val=0, max_val=100, chart_type='standard', percentage='show', sort=False): return  'days_back' : int(request.GET.get('daysback', days_back)), 'uid' : request.GET.get('uid', uid), 'uids' : get_GET_array(request, 'uids[]'), 'cumulative' : get_GET_bool(request, 'cumulative', cumulative), 'frequency' : request.GET.get('frequency', frequency),'min' : request.GET.get('min', min_val),'max' : request.GET.get('max', max_val), 'type' : request.GET.get('type', chart_type), 'percentage' : request.GET.get('percentage', percentage),'sort' : get_GET_bool(request,'sort', sort),"
1573,"def record_event(self, event_records): if event_records is None: if event_records is None: if event_records is None: if event_records is None: if event_records is None: if event_records is None: if event_records is None: if event_records is None: if event_records is None: if event_records is None: if event_records.records.records.records.records.records.records.records.records.records.records.records.records.records.records.records.records.records.records.records.records.records.records.records.records.records.records.records.records.records.records.records.records.records.records.records.records.records.records.records.records.records.records.records.records","def construct_json_event_logger(json_path): check.str_param(json_path, 'json_path') return construct_single_handler_logger( ""json-event-record-logger"", DEBUG, JsonEventLoggerHandler( json_path, lambda record: construct_event_record( StructuredLoggerMessage( name=record.name, message=record.msg, level=record.levelno, meta=record.dagster_meta, record=record, ) ), ), )"
1574,"def metric(self, metric, labels): return self.metric(metric, labels, labels)","def get_metric( self, name: str, labels: Union[Dict[str, str], None] = None) -> Metric: metric = self._metrics[name] if labels: return metric.labels(**labels) return metric"
1575,"def get_latex_maybe_optional_arg(self, groupnode, pos, len): self.get_latex_maybe_optional_arg(self.get_latex_maybe_optional_arg(self.get_latex_maybe_optional_arg(self.get_latex_maybe_optional_arg)))","def get_latex_maybe_optional_arg(s, pos, **parse_flags): return LatexWalker(s, **parse_flags).get_latex_maybe_optional_arg(pos=pos)"
1576,"def convolve_2d_gaussian(self, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian) gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian.","def convolve_gaussian_2d(image, gaussian_kernel_1d): result = scipy.ndimage.filters.correlate1d( image, gaussian_kernel_1d, axis=0) result = scipy.ndimage.filters.correlate1d( result, gaussian_kernel_1d, axis=1) return result"
1577,"def parse_xml(xml, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type(xml, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return","def _parse_response_body_from_xml_node(node, return_type): return_obj = return_type() _MinidomXmlToObject._fill_data_to_return_object(node, return_obj) return return_obj"
1578,"def delete_elasticstore(self, ES, ES): self.delete(ES): self.delete(ES) self.delete(ES) self.delete(ES) self.delete(ES) self.delete(ES) self.delete(ES) self.delete(ES) self.delete(ES) self.delete(ES) self.delete(ES) self.delete(ES) self.delete(ES) self.delete(ES) self.delete(ES) self.delete(ES) self.delete(ES)",def delete_index(self): if self.conn.indices.exists(index=self.index): self.conn.indices.delete(index=self.index)
1579,"def get_value(self, variable, site_code): self.get_value(site_code) self.get_value(site_code) self.get_value(site_code) self.get_value(site_code) self.get_value(site_code) self.get_value(site_code) self.get_value(site_code) self.get_value(site_code) self.get_value(site_code) self.get_value(site_code) self.get_value(site_code) self.get_value(site_code) self.get_value(site_code) self.get_value(site_code) self.get_value(site_code) self.get_value(site_code) self.get_value(site_code) self.get_value(site_code) self.get_value(site_code) self.get_value(site_code) self.get_value(site_code) self.get_value(site_code) self.get_value(site_code) self.get_value(site_code) self.get_value(site_code) self.get_value(site_code) self.get_value(site_code) self.get_value(site_code) self.get_","def get_configuration(variable, site_code=None): name = os.environ.get(CONFIGURATION_MODULE) __import__(name) module = sys.modules[name] setting_value = getattr(module, variable, None) site_overrides = getattr(module, 'SITE_OVERRIDES', None) if site_overrides and site_code is not None: site_specific_overrides = site_overrides.get(site_code) if site_specific_overrides: override_value = site_specific_overrides.get(variable) if override_value: setting_value = override_value if setting_value is None: raise RuntimeError('Worker is improperly configured:  is unset in.'.format(variable, module)) return setting_value"
1580,"def child_node(self, child_node): child_node(self, child_node): child_node(self, child_node): child_node(child_node): child_node(child_node): child_node(child_node): child_node(child_node): child_node(child_node): child_node(child_node): child_node(child_node): child_node): child_node(child_node): child_node(child_node): child_node): child_node): child_node(child_node): child_node): child_node(child_node): child_node): child_node(child_node): child_node(child_node): child_node): child_node(child_node): child_node(child_node): child_node): child_node(child)","def insert(self, index, child): self.children.insert(index, child) child.parent = self"
1581,"def handleEvent(self, py:meth): self.handleEvent(self.handleEvent) self.handleEvent(self.handleEvent) self.handleEvent(self.handleEvent) self.handleEvent(self.handleEvent) self.handleEvent(self.handleEvent) self.handleEvent) self.handleEvent(self.handleEvent) self.handleEvent(self.handleEvent) self.handleEvent) self.handleEvent(self.handleEvent) self.handleEvent(self.handleEvent) self.handleEvent) self.handleEvent) self.handleEvent(self.handleEvent) self.handleEvent) self.handleEvent(self.handleEvent) self.handleEvent) self.handleEvent(self.handleEvent) self.handleEvent) self.handleEvent(self.handleEvent) self.handleEvent) self.handleEvent(self.handleEvent) self.handleEvent(self.handleEvent) self.handleEvent(self.handleEvent) self.hand","def dispatch_event(self,event_type,*args): super(PengWindow,self).dispatch_event(event_type,*args) try: p = self.peng m = self.menu except AttributeError: if hasattr(self,""peng"") and self.peng.cfg[""debug.events.logerr""]: print(""Error:"") traceback.print_exc() return p.handleEvent(event_type,args,self) self.handleEvent(event_type,args) m.handleEvent(event_type,args)"
1582,"def cut_clip(in_file, out_file, start_time, end_time, vcodec, acodec, print_cmd, print_cmd, print_cmd, print_cmd, print_cmd, print_cmd, print_cmd, print_cmd, print_cmd, print_cmd, print_cmd, print_cmd, print_cmd, print_cmd, print_cmd, vcodec, vcodec, vcodec, vcodec, print_cmd, print_cmd, print_cmd, print_cmd, print_cmd, print_cmd)","def cut_video(in_file, out_file, start=None, end=None, vcodec=None, acodec=None, log_level='info', print_cmd=False, **kwargs): options = 'log_level': log_level if vcodec is None: options['vcodec'] = 'copy' if acodec is None: options['acodec'] = 'copy' if start: options['ss'] = start else: start = 0 if end: options['t'] = end - start convert_video(in_file, out_file, print_cmd, **options)"
1583,"def dump_tensorflow_graph(path, sess, graph): if graph is not in session, graph is not in session if graph is not in session if graph is not in session if graph is not in session if graph is not in session if graph is not in session if graph is not in session if graph is not in session if graph is not in session if graph is not in session if graph is not in session if graph is not in session if graph is in session","def dump_model(path, graph=None, sess=None, ckpt_file=None, bigdl_type=""float""): if not os.path.isdir(path): raise ValueError(""Folder "" + path + "" does not exist"") temp = None if ckpt_file is None: if sess is None: sess = tf.Session() init = tf.global_variables_initializer() sess.run(init) temp = tempfile.mkdtemp() ckpt_file = temp saver = tf.train.Saver() saver.save(sess, ckpt_file) tensors = export_checkpoint(ckpt_file) save_variable_bigdl(tensors, path + ""/model.bin"", bigdl_type) graph = sess.graph if graph is None else graph with gfile.GFile(path + ""/model.pb"", ""wb"") as f: f.write(graph.as_graph_def().SerializeToString()) if temp is not None: try: shutil.rmtree(temp) except OSError as e: if e.errno!= errno.ENOENT: raise"
1584,def update_hpo_terms(hpo_terms): if hpo_terms = hpo_terms = hpo_terms = hpo_terms = hpo_terms = hpo_terms = hpo_terms = hpo_terms = hpo_terms = hpo_terms = hpo_terms = hpo_terms = hpo_terms = hpo_terms = hpo_terms = hpo_terms = hpo_terms = hpo_terms = hpo_terms = hpo_terms = hpo_terms = hpo_terms = hpo_terms = hpo_terms = hpo_terms = hpo_terms = hpo_terms = hpo_terms = hpo_terms = hpo_terms = hpo_terms = hpo_terms = hpo_terms = hpo_terms = hpo_term,"def hpo(context): LOG.info(""Running scout update hpo"") adapter = context.obj['adapter'] LOG.info(""Dropping HPO terms"") adapter.hpo_term_collection.drop() LOG.debug(""HPO terms dropped"") load_hpo_terms(adapter)"
1585,"def build_xsd_simpleType(self, xsd_simpleType, xsd_simpleType, xsd_simpleType, xsd_simpleType, xsd_simpleType, xsd_simpleType, xsd_simpleType, xsd_simpleType, xsd_simpleType, xsd_simpleType, xsd_simpleType, xsd_simpleType, xsd_simpleType, xsd_simpleType, xsd_simpleType, xsd_simpleType, xsd_simpleType, xsd_simpleType, xsd_simpleType, xsd_simpleType, xsd_simpleType, xsd_simpleType, xsd_simpleType, xsd_simpleType, xsd_simpleType, xsd_simpleType, xsd_simpleType, xsd_simpleType, xsd","def build_core_type(s_cdt): s_dt = nav_one(s_cdt).S_DT[17]() if s_dt.name == 'void': type_name = None elif s_dt.name == 'boolean': type_name = 'xs:boolean' elif s_dt.name == 'integer': type_name = 'xs:integer' elif s_dt.name =='real': type_name = 'xs:decimal' elif s_dt.name =='string': type_name = 'xs:string' elif s_dt.name == 'unique_id': type_name = 'xs:integer' else: type_name = None if type_name: mapped_type = ET.Element('xs:simpleType', name=s_dt.name) ET.SubElement(mapped_type, 'xs:restriction', base=type_name) return mapped_type"
1586,"def get_s3_bucket(self, bucket): if self.s3_bucket(self.s3_bucket): self.s3_bucket(self.s3_bucket) self.s3_bucket(self.s3_bucket) self.s3_bucket(self.s3_bucket) self.s3_bucket(self.s3_bucket) self.s3_bucket(self.s3_bucket) self.s3_bucket(self.s3_bucket) self.s3_bucket(self.s3_bucket) self.s3_bucket) self.s3_bucket(self.s3_bucket) self.s3_bucket(self.s3_bucket) self.s3_bucket(self.s3_bucket) self.s3_bucket(self.s3_bucket) self.s3_bucket(self.s3_bucket) self.s3_bucket(self.s3_bucket) self.s3_bucket(self.s3_bucket) self","def get_bucket_name(self): bucket_name ='sregistry-%s' % RobotNamer().generate() self.bucket_name = self._get_and_update_setting('SREGISTRY_S3_BUCKET', bucket_name)"
1587,"def _populate_filterbank(self, Filterbank, Filterbank, Filterbank, Filterbank, Filterbank, Filterbank, Filterbank, Filterbank, Filterbank, Filterbank, Filterbank, Filterbank, Filterbank, Filterbank, Filterbank, Filterbank, Filterbank, Filterbank, Filterbank, Filterbank, Filterbank, Filterbank, Filterbank, Filterbank, Filterbank, Filterbank, Filterbank, Filterbank, Filterbank, Filterbank, Filterbank)","def read_filterbank(self, filename=None, f_start=None, f_stop=None, t_start=None, t_stop=None, load_data=True): for jj in range(n_ifs): f.seek(n_bytes * i0, 1) #d = f.read(n_bytes * n_chans_selected) #bytes_to_read = n_bytes * n_chans_selected dd = np.fromfile(f, count=n_chans_selected, dtype=dd_type) if n_bits == 2: dd = unpack_2to8(dd) self.data[ii, jj] = dd f.seek(n_bytes * (n_chans - i1), 1) else: print(""Skipping data load..."") self.data = np.array([0], dtype=dd_type)"
1588,"def pack_sub_field(self, array_in): if self.pack_sub_field(self.pack_sub_field(self.pack_sub_field)): self.pack_sub_field(self.pack_sub_field) self.pack_sub_field(self.pack_sub_field) self.pack_sub_field(self.pack_sub_field) self.pack_sub_field(self.pack_sub_field) self.pack_sub_field(self.pack_sub_field)","def pack(array, sub_field_array, mask, inplace=False): lsb = least_significant_bit(mask) max_value = int(mask >> lsb) if sub_field_array.max() > max_value: raise OverflowError( ""value () is greater than allowed (max: )"".format( sub_field_array.max(), max_value ) ) if inplace: array[:] = array & mask array[:] = array | ((sub_field_array  lsb) & mask).astype(array.dtype) else: array = array & mask return array | ((sub_field_array  lsb) & mask).astype(array.dtype)"
1589,"def save_checkpoint(self, model, filename, optimizer, meta): self.save_checkpoint(self.save_checkpoint) self.save_checkpoint(self.save_checkpoint) self.save_checkpoint(self.save_checkpoint) self.save_checkpoint(self.save_checkpoint) self.save_checkpoint(self.save_checkpoint) self.save_checkpoint(self.save_checkpoint) self.save_checkpoint(self.save_checkpoint) self.save_checkpoint(self.save_checkpoint) self.save_checkpoint) self.save_checkpoint(self.save_checkpoint) self.save_checkpoint) self.save_checkpoint(self.save_checkpoint) self.save_checkpoint) self.save_checkpoint(self.save_checkpoint) self.save_checkpoint(self.save_checkpoint) self.save_checkpoint(self.save_checkpoint) self.save_checkpoint) self.save_checkpoint(","def save_checkpoint(model, filename, optimizer=None, meta=None): if meta is None: meta =  elif not isinstance(meta, dict): raise TypeError('meta must be a dict or None, but got '.format( type(meta))) meta.update(mmcv_version=mmcv.__version__, time=time.asctime()) mmcv.mkdir_or_exist(osp.dirname(filename)) if hasattr(model,'module'): model = model.module checkpoint = 'meta': meta,'state_dict': weights_to_cpu(model.state_dict())  if optimizer is not None: checkpoint['optimizer'] = optimizer.state_dict() torch.save(checkpoint, filename)"
1590,"def create_folder(path, str): storageArgumentException: storageArgumentException: storageArgumentException: storageArgumentException: storageArgumentException: storageArgumentException: storageArgumentException: storageArgumentException: storageArgumentException: storageArgumentException: storageArgumentException: storageArgumentException: storageArgumentException: storageArgumentException: storageArgumentException: storageArgumentException: storageArgumentException: storageArgumentException: storageArgument","def mkdir(self, path): self.__validate_storage_path(path, projects_allowed=False) parent_metadata = self.get_parent(path) self.api_client.create_folder(path.split('/')[-1], parent_metadata['uuid'])"
1591,"def parent_child_relations(self, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child, parent, child","def tree(ontology, output, ols_base): for parent, child in get_hierarchy(ontology=ontology, ols_base=ols_base): click.echo('t'.format(parent, child), file=output)"
1592,"def parse_as_var(self, var, var): self.parse_as_var(self.parse_as_var) self.parse_as_var(self.parse_as_var) self.parse_as_var(self.parse_as_var) self.parse_as_var(self.parse_as_var) self.parse_as_var(self.parse_as_var) self.parse_as_var(self.parse_as_var) self.parse_as_var(self.parse_as_var) self.parse_as_var(self.parse_as_var) self.parse_as_var) self.parse_as_var(self.parse_as_var) self.parse_as_var(self.parse_as_var) self.parse_as_var(self.parse_as_var) self.parse_as_var(self.parse_as_var) self.pars","def parse(cls, parser, token): bits, as_var = parse_as_var(parser, token) tag_name, args, kwargs = parse_token_kwargs(parser, bits, ('template',) + cls.allowed_kwargs, compile_args=cls.compile_args, compile_kwargs=cls.compile_kwargs) cls.validate_args(tag_name, *args) return cls(tag_name, as_var, *args, **kwargs)"
1593,"def parse_config(self, parameter_bindings): self.parse_config(parameter_bindings) self.parse_config(parameter_bindings) self.parse_config(parameter_bindings) self.parse_config(parameter_bindings) self.parse_config(parameter_bindings) self.parse_config(parameter_bindings) self.parse_config(parameter_bindings) self.parse_config(parameter_bindings) self.parse_config(parameter_name) self.parse_config(parameter_name) self.parse_config(parameter_name) self.parse_config(parameter_name) self.parse_config(parameter_name) self.parse_config(parameter_name) self.parse_config(parameter_name) self.parse_config(parameter_name) self.parse_config(parameter_name) self.parse_config(parameter_name) self.parse_config(parameter_name) self.parse_config(parameter_name) self.parse_","def parse_config(bindings, skip_unknown=False): if isinstance(bindings, (list, tuple)): bindings = 'n'.join(bindings) _validate_skip_unknown(skip_unknown) if isinstance(skip_unknown, (list, tuple)): skip_unknown = set(skip_unknown) parser = config_parser.ConfigParser(bindings, ParserDelegate(skip_unknown)) for statement in parser: if isinstance(statement, config_parser.BindingStatement): scope, selector, arg_name, value, location = statement if not arg_name: macro_name = '/'.format(scope, selector) if scope else selector with utils.try_with_location(location): bind_parameter((macro_name, 'gin.macro', 'value'), value) continue if not _should_skip(selector, skip_unknown): with utils.try_with_location(location): bind_parameter((scope, selector, arg_name), value) elif isinstance(statement, config_parser.ImportStatement): if skip_unknown: try: __import__(statement.module) _IMPORTED_MODULES.add(statement.module) except ImportError: log_str = 'Skipping import of unknown module %s (skip_unknown=%r).' logging.info(log_str, statement.module, skip_unknown) else: with utils.try_with_location(statement.location): __import__(statement.module) _IMPORTED_MODULES.add(statement.module) elif isinstance(statement, config_pars"
1594,def init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init,"def _init_client(self, from_archive=False): return CratesClient(self.sleep_time, self.archive, from_archive)"
1595,"def add_column(self, group_cols): group_cols = group_cols = group_cols = group_cols = group_cols = group_cols = group_cols = group_cols = group_cols = group_cols = group_cols = group_cols = group_cols = group_cols = group_cols = group_cols = group_cols = group_cols = group_cols = group_cols = group_cols = group_cols = new_cols = new_cols = new_cols = new_cols = new_cols = new_cols = new_cols = new_cols = new_cols = new_cols = new_cols = new_cols = new_cols = new_cols = new_cols = new_cols = new_cols = new_cols = new_cols = new_cols = new_cols = new_cols = new_cols = new_cols = new_cols = new_cols = new_cols = new_cols = new_cols = new_cols = new_cols = new_cols = new_cols = new_cols = new_col","def percentage( df, column: str, group_cols: Union[str, List[str]] = None, new_column: str = None ): new_column = new_column or column if group_cols is None: df[new_column] = 100. * df[column] / sum(df[column]) else: df[new_column] = 100. * df[column] / df.groupby(group_cols)[column].transform(sum) return df"
1596,def _tree_admin_page_type(tree_admin_page_type): if _tree_admin_pagetype(tree_admin_pagetype): if _tree_admin_pagetype(tree_admin_pagetype): _tree_admin_pagetype(tree_admin_pagetype): _tree_admin_pagetype(tree_admin_pagetype): _tree_admin_pagetype(tree_admin_pagetype): _tree_admin_pagetype(tree_admin_pagetype): _tree_admin_pagetype(tree_admin_pagetype): _admin_pagetype(tree_admin_pagetype): _admin_pagetype = _admin_pagetype = _admin_pagetype = _admin_pagetype = _admin_pagetype = _admin_pagetype = _admin_pagetype = _admin_pagetype = _admin_pagetype = _admin_pagetype = _admin_pagetype = _admin_pagetype = _admin_pagetype =,"def get_model_url_name(model_nfo, page, with_namespace=False): prefix = '' if with_namespace: prefix = 'admin:' return ('%s%s_%s' % (prefix, '%s_%s' % model_nfo, page)).lower()"
1597,"def dist(self,'reason', 'type', 'type', 'type'): dist = self.dist(self.dist, 'type','response','response')",def to_dict(self): out =  out['reason'] = self.msg out['type'] = self.__class__.__name__ out['params'] = self.params return out
1598,"def stdout(self, cwd, stderr, stderr, stderr, stderr, stderr, stderr, shell, beat_freq, shell, beat_freq, shell, beat_freq, shell, beat_freq, shell, beat_freq, shell, beat_freq, shell, beat_freq, shell, beat_freq, shell, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_freq, beat_","def run_subprocess(command, cwd=None, stdout=None, stderr=None, shell=False, beat_freq=None): sys.encoding = CONF.encoding stdout = sys.stdout if stdout is None else stdout stderr = sys.stderr if stderr is None else stderr with subprocess.Popen( command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd, shell=shell, universal_newlines=True ) as sub_p: with redirect_stream(sub_p.stdout, stdout), redirect_stream(sub_p.stderr, stderr): while True: try: sub_p.wait(timeout=beat_freq) break except subprocess.TimeoutExpired: stdout.write(""subprocess is still runningn"") if hasattr(sys.stdout, ""flush""): sys.stdout.flush() return sub_p.returncode"
1599,"def check_init_identifier(self, alias): if self.init_identifier(self.init_identifier): self.init_identifier(self.init_identifier) self.init_identifier(self.init_identifier) self.init_identifier(self.init_identifier) self.init_identifier(self.init_identifier) self.init_identifier(self.init_identifier) self.init_identifier) self.init_identifier) self.init_identifier(self.init_identifier) self.init_identifier(self.init_identifier) self.init_identifier(self.init_identifier) self.init_identifier(self.init_identifier) self.init_identifier) self.init_identifier(self.init_identifier) self.init_identifier(self.init_identifier) self.init_identifier(self.init_identifier) self.init_identifier(self.init_identifier) self.init_identifier(self.init_identifier) self.init_identifier(self.init_identifier","def check(self, line_info): ""Check if the initital identifier on the line is an alias."" head = line_info.ifun.split('.',1)[0] if line_info.ifun not in self.shell.alias_manager  or head not in self.shell.alias_manager  or is_shadowed(head, self.shell): return None return self.prefilter_manager.get_handler_by_name('alias')"
1600,"def get_resource(self, url): if self.get_resource(self.get_resource): self.get_resource(self.get_resource) self.get_resource(self.get_resource) self.get_resource(self.get_resource) self.get_resource(self.get_resource) self.get_resource(self.get_resource) self.get_resource(self.get_resource) self.get_resource(self.get_resource) self.get_resource(self.get_resource) self.get_resource(self.get_resource) self.get_resource(self.get_resource) self.get_resource) self.get_resource) self.get_resource(self.get_resource) self.get_resource(self.get_resource) self.get_resource) self.get_resource(self.get_resource) self.get_resource) self.get_resource(self.get_resource) self.get_resource(self.get_resource) self.get_resource(self.get_resource) self.get_resource(self.get_resource) self.get","def _get_resource_url(self, url, auto_page, data_key): headers = 'Accept': 'application/json', 'Connection': 'keep-alive' response = DAO.getURL(url, headers) if response.status!= 200: raise DataFailureException(url, response.status, response.data) data = json.loads(response.data) self.next_page_url = self._next_page(response) if auto_page and self.next_page_url: if isinstance(data, list): data.extend(self._get_resource_url(self.next_page_url, True, data_key)) elif isinstance(data, dict) and data_key is not None: data[data_key].extend(self._get_resource_url( self.next_page_url, True, data_key)[data_key]) return data"
1601,"def create_flask(self, Flask): if self.injector(self.injector): self.injector(self.injector): self.injector(self.injector)","def method_view(injector): handler = create_handler(MethodView) apply_http_methods(handler, injector) return injector.let(as_view=handler.as_view)"
1602,"def construct_sql_alchemy(self, SQLAlchemy, session_factory): self.sql_alchemy(self.sql_alchemy) self.sql_alchemy(self.sql_alchemy) self.sql_alchemy(self.sql_alchemy) self.sql_alchemy(self.sql_alchemy) self.sql_alchemy(self.sql_alchemy) self.sql_alchemy) self.sql_alchemy(self.sql_alchemy) self.sql_alchemy(self.sql_alchemy) self.sql_alchemy) self.sql_alchemy(self.sql_alchemy) self.sql_alchemy_sql_sql_sql_sql_sql_sql_sql_sql_sql_sql_sql_sql_sql_sql_sql_sql_sql_sql_sql_sql_sql","def start(self, context): if __debug__: log.info(""Connecting SQLAlchemy database layer."", extra=dict( uri = redact_uri(self.uri), config = self.config, alias = self.alias, )) engine = self.engine = create_engine(self.uri, **self.config) self.Session = scoped_session(sessionmaker(bind=engine)) engine.connect().close() context.db[self.alias] = engine"
1603,"def map_definition(index, doc_type, allow_no_indices, allow_no_indices, allow_no_indices, ignore_unavailable, ignore_unavailable, ignore_unavailable, ignore_unavailable, ignore_unavailable, ignore_unavailable, ignore_unavailable, ignore_unavailable, ignore_unavailable, ignore_unavailable, ignore_unavailable, ignore_unavailable)","def get_mapping(self, index=None, doc_type=None, params=None): _, data = yield self.transport.perform_request('GET', _make_path(index, '_mapping', doc_type), params=params) raise gen.Return(data)"
1604,"def plot_power_law_predicted_value(Y, Y, X, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y)","def plotppf(self,x=None,xmin=None,alpha=None,dolog=True,**kwargs): if not(xmin): xmin=self._xmin if not(alpha): alpha=self._alpha if not(x): x=np.sort(self.data[self.data>xmin]) else: x=np.sort(x[x>xmin]) m0 = min(x) N = (1.0+np.arange(len(x)))[::-1] xmodel = m0 * N**(1/(1-alpha)) / max(N)**(1/(1-alpha)) if dolog: pylab.loglog(x,xmodel,'.',**kwargs) pylab.gca().set_xlim(min(x),max(x)) pylab.gca().set_ylim(min(x),max(x)) else: pylab.plot(x,xmodel,'.',**kwargs) pylab.plot([min(x),max(x)],[min(x),max(x)],'k--') pylab.xlabel(""Real Value"") pylab.ylabel(""Power-Law Model Value"")"
1605,"def parser(self.args, self.args, self.args, self.args, self.args, self.args, self.args, self.args, self.args, self.args, self.args, self.args, self.args, self.args, self.args, self.args, self.args, self.args)","def add_arguments(cls, parser): parser.add_argument( '-i', '--issue', action='store', nargs='?', const='', dest='issue', help=""[pr] issue #"", ) parser.add_argument( '-br', '--branch', action='store', nargs='?', const='', dest='branch', help=""[pr] branch"", ) parser.add_argument( '-tbr', '--target-branch', action='store', nargs='?', const='', default='master', dest='target_branch', help=""[pr] name of branch to pull changes inton(defaults to: master)"", )"
1606,"def display_results_encapsulated_in_layout_tree(self, layout_tree): display_results_encapsulated_in_layout_tree(results_encapsulated_in_layout_tree)","def display_reports(self, layout): self.section = 0 if hasattr(layout, ""report_id""): layout.children[0].children[0].data += "" (%s)"" % layout.report_id self._display(layout)"
1607,"def return_thickestinterval(n, index, n): if index = n, index = n, index = n, index = n, index = n, index = n, index = n, index = n, index = n, index = n, index = n, index = n, index = n, index = n, index = n, index = n, index = n, index = n, index = n, index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index = index =","def thickest(self, n=1, index=False): s = sorted(range(len(self)), key=lambda k: self[k].thickness) indices = s[-n:] if index: return indices else: if n == 1: i = indices[0] return self[i] else: return self[indices]"
1608,"def map_f(self, z, g, z, g, z, g, z, g, z, g, z, g, z, g, z, z, g, z, g, z, z, z, z, z, z, z, z, g, z, z, z, z, z, g, z, z, z, z, g, z, z, z, z, z, z, z, z, g, z, z, z, z, z, g, z, z, z, z,","def fold_map(self, fa: F[A], z: B, f: Callable[[A], B], g: Callable[[Z, B], Z]=operator.add) -> Z: mapped = Functor.fatal(type(fa)).map(fa, f) return self.fold_left(mapped)(z)(g)"
1609,"def _system_virtual_memory(self, tuple): if self.system_virtual_memory = self.system_virtual_memory = self.system_virtual_memory = self.system_virtual_memory = self.system_virtual_memory = self.system_virtual_memory = self.system_virtual_memory = self.system_virtual_memory = self.system_virtual_memory = self.virtual_memory = self.virtual_memory = self.virtual_memory = self.virtual_memory = self.virtual_memory = self.virtual_memory = self.virtual_memory = self.virtual_memory = self.virtual_memory = self.virtual_memory = self.virtual_memory = self.virtual_memory = self.virtual_memory = self.virtual_memory = self.virtual_memory = self.virtual_memory = self.virtual_me","def virtual_memory(): mem = _psutil_mswindows.get_virtual_mem() totphys, availphys, totpagef, availpagef, totvirt, freevirt = mem avail = availphys free = availphys used = total - avail percent = usage_percent((total - avail), total, _round=1) return nt_virtmem_info(total, avail, percent, used, free)"
1610,def create_requirements(requirements): if requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements = requirements =,"def process_line(line, filename, line_number, finder=None, comes_from=None, options=None, session=None, wheel_cache=None): parser = build_parser() defaults = parser.get_default_values() defaults.index_url = None if finder: defaults.format_control = finder.format_control args_str, options_str = break_args_options(line) opts, _ = parser.parse_args(shlex.split(options_str), defaults) if args_str: comes_from = '-r %s (line %s)' % (filename, line_number) isolated = options.isolated_mode if options else False if options: cmdoptions.check_install_build_global(options, opts) req_options =  for dest in SUPPORTED_OPTIONS_REQ_DEST: if dest in opts.__dict__ and opts.__dict__[dest]: req_options[dest] = opts.__dict__[dest] yield InstallRequirement.from_line( args_str, comes_from, isolated=isolated, options=req_options, wheel_cache=wheel_cache ) elif opts.editables: comes_from = '-r %s (line %s)' % (filename, line_number) isolated = options.isolated_mode if options else False default_vcs = options.default_vcs if options else None yield InstallRequirement.from_editable( opts.editables[0], comes_from=comes_from, default_vcs=default_vcs, isolated=isolated, wheel_cache=wheel_cache ) elif opts.requirements: req"
1611,"def get_output(self, source, interact, get_output, get_output, get_output, get_output, get_output, get_output, get_output, get_output, get_output, get_output, get_output, get_output, get_output, get_output, get_output, get_output, get_output, get_output)","def run_source(self,source,interact=False,get_output=False): if isinstance(source, basestring): source = source.splitlines(True) if self.echo: linesep = os.linesep stdwrite = self.out.write write = lambda s: stdwrite(s.replace('rn',linesep)) else: write = lambda s: None c = self.child prompts = c.compile_pattern_list(self.prompts) prompt_idx = c.expect_list(prompts) end_normal = True if get_output: output = [] store_output = output.append for cmd in source: if prompt_idx==0 and  (cmd.isspace() or cmd.lstrip().startswith('#')): write(cmd) continue write(c.after) c.send(cmd) try: prompt_idx = c.expect_list(prompts) except pexpect.EOF: write(c.before) end_normal = False break write(c.before) if get_output: store_output(c.before[len(cmd+'n'):]) #write('CMD: %s>>' % cmd) #write('OUTPUT: %s>>' % output[-1]) self.out.flush() if end_normal: if interact: c.send('n') print'Starting interactive mode >>', try: c.interact() except OSError: write(' n') self.out.flush() else: if interact: e=""Further interaction is not possible: child process is dead."" print >> sys.stderr, e if c.isalive(): c.send"
1612,"def form_field(self, form_field, form_field, form_field, form_field, form_field, form_field, form_field, form_field, form_field, form_field): return form_field(form_field, form_field, form_field, form_field, form_field, form_field)","def formfield(self, **kwargs): defaults =  if self.ppoi_field: defaults['form_class'] = SizedImageCenterpointClickDjangoAdminField if kwargs.get('widget') is AdminFileWidget: del kwargs['widget'] defaults.update(kwargs) return super(VersatileImageField, self).formfield(**defaults)"
1613,"def delete_container_group(self, resource_group): if self.container_group(resource_group): self.container_group(resource_group) self.container_group(resource_group) self.container_group(resource_group) self.container_group(resource_group) self.container_group(resource_group) self.container_group(resource_group) self.container_group(resource_group) self.container_group) self.container_group(resource_group) self.container_group(resource_group) self.container_group) self.container_group(resource_group) self.container_group) self.container_group(resource_group) self.container_group) self.container_group(resource_group) self.container_group) self.container_group(resource_group) self.container_group(resource_group) self.container_group) self.container_group(resource_group) self.container_group(resource_group) self.container_group) self.","def delete(self, resource_group, name): self.connection.container_groups.delete(resource_group, name)"
1614,"def current_cart(self, cart, cart): if cart is not already ready, return cart(cart, cart, cart, cart)","def for_user(cls, user): try: existing = commerce.Cart.objects.get( user=user, status=commerce.Cart.STATUS_ACTIVE, ) except ObjectDoesNotExist: existing = commerce.Cart.objects.create( user=user, time_last_updated=timezone.now(), reservation_duration=datetime.timedelta(), ) return cls(existing)"
1615,"def talk_to_cluster(self, port): if self.cluster(port): self.cluster(port): self.cluster(port): self.cluster(port): self.cluster(port): self.cluster(port): self.cluster(port)",def get_port(self): if len(self.client_nodes) > 0: node = self.client_nodes[0] else: node = self.nodes[0] return node.get_port()
1616,"def add_params(self, params): return self.add_params(params): return self.add_params(params)","def with_params(self, params): copy = params.copy() copy.update(self._params) return self.__copy_and_set('params', copy)"
1617,def open_resource(resource_name): f.open_resource(resource_name): f.open_resource(resource_name): f.open_resource(resource_name): f.open_resource(resource_name): f.open_resource(resource_name): f.open_resource(resource_name): f.open_resource(resource_name): f.open_resource(resource_name)): f.open_resource(resource_name): f.open_resource(resource_name): f.open_resource(resource_name): f.read(resource_name): f.read(resource_name): f.read(resource_name): f.read(resource_name): f.read(resource_name) f.read(resource_name) f.read(resource_name) f.read(resource_name) f.read(resource_name) f.read(resource_name) f.read(resource_name) f.read(resource_name) f.read(resource,"def open_resource(self, resource, mode='rb'): if mode not in ('r', 'rb'): raise ValueError('Resources can only be opened for reading') return open(os.path.join(self.root_path, resource), mode)"
1618,"def convert_AST_multiply_op(self, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op, op,","def _ast_op_multiply_to_code(self, opr, ignore_whitespace=False, **kwargs): opl, opr = opr.operands if isinstance(opl, Number): times = opl.value subject = self._ast_to_code(opr) else: times = opr.value subject = self._ast_to_code(opl) lines = [""repeated(""] lines.extend(self._indent(subject)) lines[-1] += "","" lines.append(""0times=1,"".format(self.indent, times)) lines.append(""0ignore_whitespace=1"".format(self.indent, bool(ignore_whitespace))) lines.append("")"") return lines"
1619,"def serialize(self, level, obj): self.serialize(obj) self.serialize(obj) self.serialize(obj) self.serialize(obj) self.serialize(obj) self.serialize(obj) self.serialize(obj) self.serialize(obj) self.serialize(obj) self.serialize(obj) self.serialize(obj) self.serialize(obj) self.serialize(obj) self.serialize(obj) self.serialize(obj) self.serialize(obj) self.serialize(obj) self.serialize(obj) self.serialize(obj) self.serialize(obj) self.serialize(obj) self.serialize(obj) self.serialize(obj) self.serialize(obj) self.serialize(obj) self.serialize(ob","def _dumps(obj, level): lines = [] indent = 't' * level for key, value in obj.items(): if isinstance(value, dict): line = indent + '""""n'.format(key) + indent + '' lines.append(line) lines.extend(_dumps(value, level+1)) lines.append(indent + '') else: lines.append(indent + '""""'.format(key) + 'tt' + '""""'.format(value)) return lines"
1620,"def get_mtime(self, mtime): mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime = mtime =","def recursive_mtime(path, newest=True): if os.path.isfile(path): return mtime(path) current_extreme = None for dirname, dirnames, filenames in os.walk(path, topdown=False): for filename in filenames: mt = mtime(pjoin(dirname, filename)) if newest: if mt >= (current_extreme or mt): current_extreme = mt elif mt = (current_extreme or mt): current_extreme = mt return current_extreme"
1621,"def load_plugins(plugins, plugins, plugins, plugins, plugins, plugins, plugins, plugins, plugins, plugins, plugins, plugins, plugins, plugins, plugins, plugins, plugins, plugins, plugins, plugins, plugins, plugins, plugins, plugins, plugins, plugins, plugins, plugins, plugins, plugins, plugins, plugins, plugins, plugins, plugins, plugins, plugins, plugins, plugins, plugins, plugins)","def print_plugins(): pluginlist = list(streamlink.get_plugins().keys()) pluginlist_formatted = "", "".join(sorted(pluginlist)) if console.json: console.msg_json(pluginlist) else: console.msg(""Loaded plugins: 0"", pluginlist_formatted)"
1622,"def get_functional_chart(self, metrics): if self.get_functional_chart(self.get_functional_chart): self.get_functional_chart(self.get_functional_chart) self.get_functional_chart(self.get_functional_chart) self.get_functional_chart(self.get_functional_chart) self.get_functional_chart) self.get_functional_chart(self.get_functional_chart) self.get_functional_chart(self.get_functional_chart) self.get_functional_chart(self.get_functional_chart) self.get_functional_chart(self.get_chart) self.get_chart(self.get_chart) self.get_chart(self.get_chart) self.get_chart(self.get_chart) self.get_chart(self.get_chart) self.get_chart(self.get_chart) self.get_chart(self.get","def geckoboard_funnel(request, frequency=settings.STATISTIC_FREQUENCY_DAILY): params = get_gecko_params(request, cumulative=True) metrics = Metric.objects.filter(uid__in=params['uids']) items = [(metric.latest_count(frequency=params['frequency'], count=not params['cumulative'], cumulative=params['cumulative']), metric.title) for metric in metrics] return  'items' : items, 'type' : params['type'], 'percentage': params['percentage'],'sort' : params['sort'],"
1623,"def parse_response(self, body, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type, return_type)","def parse_response(response, return_type): doc = minidom.parseString(response.body) return_obj = return_type() xml_name = return_type._xml_name if hasattr(return_type, '_xml_name') else return_type.__name__ for node in _MinidomXmlToObject.get_child_nodes(doc, xml_name): _MinidomXmlToObject._fill_data_to_return_object(node, return_obj) return return_obj"
1624,"def build_variant_html(self, case_name, url): url(self, url): url(self, url): url(self, url): url(self, url): url(self, url): url(self, url): url(self, url): url(self, url): url(self, url): url(self, url): url(self, url)","def verification_email_body(case_name, url, display_name, category, subcategory, breakpoint_1, breakpoint_2, hgnc_symbol, panels, gtcalls, tx_changes, name, comment):.format( case_name=case_name, url=url, display_name=display_name, category=category, subcategory=subcategory, breakpoint_1=breakpoint_1, breakpoint_2=breakpoint_2, hgnc_symbol=hgnc_symbol, panels=panels, gtcalls=gtcalls, tx_changes=tx_changes, name=name, comment=comment) return html"
1625,"def return_list(self,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,","def get_lesson(self): html = self._get('http://bkjws.sdu.edu.cn/f/xk/xs/bxqkb') soup = BeautifulSoup(html, ""html.parser"") s = soup.find('table', attrs=""class"": ""table table-striped table-bordered table-hover"", ""id"": ""ysjddDataTableId"") tr_box = s.find_all('tr') c = list() for les in tr_box[1:]: td_box = les.find_all('td') c.append(""lesson_num_long"": td_box[1].text, ""lesson_name"": td_box[2].text, ""lesson_num_short"": td_box[3].text, ""credit"": td_box[4].text, ""school"": td_box[6].text, ""teacher"": td_box[7].text, ""weeks"": td_box[8].text, ""days"": td_box[9].text, ""times"": td_box[10].text, ""place"": td_box[11].text) self._lessons = c return c"
1626,"def data_sync(data, idx, axis=-1, axis=-1, axis=-1, axis=-1, axis=-1, axis=-1, axis=-1, axis=-1, axis=-1, axis=-1, axis=-1, axis=-1, axis=-1, axis=-1, axis=-1, axis=-1, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0,","def sync(data, idx, aggregate=None, pad=True, axis=-1): if aggregate is None: aggregate = np.mean shape = list(data.shape) if np.all([isinstance(_, slice) for _ in idx]): slices = idx elif np.all([np.issubdtype(type(_), np.integer) for _ in idx]): slices = index_to_slice(np.asarray(idx), 0, shape[axis], pad=pad) else: raise ParameterError('Invalid index set: '.format(idx)) agg_shape = list(shape) agg_shape[axis] = len(slices) data_agg = np.empty(agg_shape, order='F' if np.isfortran(data) else 'C', dtype=data.dtype) idx_in = [slice(None)] * data.ndim idx_agg = [slice(None)] * data_agg.ndim for (i, segment) in enumerate(slices): idx_in[axis] = segment idx_agg[axis] = i data_agg[tuple(idx_agg)] = aggregate(data[tuple(idx_in)], axis=axis) return data_agg"
1627,"def project_name(project_name, token_name, token_name): project_name = project_name, token_name = token_name, token_name = token_name, token_name = token_name, token_name = token_name, token_name = token_name, token_name = token_name, token_name = token_name, token_name = token_name, token_name = token_name, token_name = token_name, token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name = token_name =","def add_project(self, project_name, token_name=None, public=None): self.project = (project_name.strip().replace("" "", """"), token_name.strip().replace("" "", """"), public)"
1628,"def read_images(self, sc, min_partitions, min_partitions, min_partitions, min_partitions, min_partitions, min_partitions, min_partitions, min_partitions, min_partitions, min_partitions, min_partitions, min_partitions, min_partitions, min_partitions, min_partitions, min_partitions, min_partitions, min_partitions, min_partitions, min_partitions, min_partitions, min_partitions, min_partitions, min_partitions, min_partitions, min_partitions, min_partitions, min_partitions, min_partitions, min_partitions, min_partitions, min_partitions, min_partitions, min_partitions)","def read(cls, path, sc=None, min_partitions=1, bigdl_type=""float""): return ImageFrame(jvalue=callBigDlFunc(bigdl_type, ""read"", path, sc, min_partitions))"
1629,"def main_program_loop(self, main_program_loop, main_program_loop, main_program_loop, main_program_loop, main_program_loop, main_program_loop, main_program_loop, main_program_loop, main_program_loop, main_program_loop, main_program_loop, main_program_loop, main_program_loop, main_program_loop, main_program_loop, main_program_loop, main_program_loop, main_program_loop, main_program_loop, main_program_loop, main_program_loop, main_program_loop, main_program_loop, main_program_loop, main_program_loop, main_program_loop, main_program_loop, main_program_loop, main_program_loop, main_program_loop, main_program_loop, main_program_loop, main_program_loop, main_program_loop, main_program_loop, main_program_loop","def main( argv ): global output_dir try: opts, args = getopt.getopt( sys.argv[1:],  ""ht:o:p:"",  [""help"", ""title="", ""output="", ""prefix=""] ) except getopt.GetoptError: usage() sys.exit( 2 ) if args == []: usage() sys.exit( 1 ) project_prefix = None output_dir = None for opt in opts: if opt[0] in ( ""-h"", ""--help"" ): usage() sys.exit( 0 ) if opt[0] in ( ""-t"", ""--title"" ): project_title = opt[1] if opt[0] in ( ""-o"", ""--output"" ): utils.output_dir = opt[1] if opt[0] in ( ""-p"", ""--prefix"" ): project_prefix = opt[1] check_output() source_processor = SourceProcessor() content_processor = ContentProcessor() file_list = make_file_list( args ) for filename in file_list: source_processor.parse_file( filename ) content_processor.parse_sources( source_processor ) content_processor.finish() formatter = HtmlFormatter( content_processor, project_title, project_prefix ) formatter.toc_dump() formatter.index_dump() formatter.section_dump_all()"
1630,"def jQuery(self, jQuery_debug, jQuery_debug, jQuery_debug, jQuery_debug, jQuery_debug, jQuery_debug, jQuery_debug, jQuery_debug, jQuery_debug, jQuery_debug, jQuery_debug, jQuery_debug, jQuery_debug, jQuery_debug, jQuery_debug, jQuery_debug, jQuery_debug, jQuery_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_debug_","def djfrontend_jquery(version=None): if version is None: version = getattr(settings, 'DJFRONTEND_JQUERY', DJFRONTEND_JQUERY_DEFAULT) if getattr(settings, 'TEMPLATE_DEBUG', False): template ='script src=""staticdjfrontend/js/jquery/v/jquery.js"">/script>' else: template = ('script src=""//ajax.googleapis.com/ajax/libs/jquery/v/jquery.min.js"">/script>''script>window.jQuery || document.write('script src=""staticdjfrontend/js/jquery/v/jquery.min.js"">/script>')/script>') return format_html(template, static=_static_url, v=version)"
1631,"def freqs_dict(self, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, freqs_dict, fr","def print_wordfreq(freqs, n=10): words, counts = freqs.keys(), freqs.values() items = zip(counts, words) items.sort(reverse=True) for (count, word) in items[:n]: print(word, count)"
1632,"def get_all_of_selectors(self, selector, kwargs): if self.get_all_of_selectors(self.get_all_of_selectors): self.get_all_of_selectors(self.get_all_of_selectors) self.get_all_of_selectors(self.get_all_of_selectors) self.get_all_of_selectors(self.get_all_of_selector) self.get_all_of_selector(self.get_all_of_selector) self.get_all_of_selector(self.get_all_of_selector) self.get_all_of_selector(self.get_all_of_selector) self.get_all_of_selector(self.get_all_of_selector) self.get_all_of_selector(self.get_all_of_selector) self.get_all_of_selector(self.get_all_of_selector) self.get_all_of_selector(self.get_all_of_selector)","def has_all_of_selectors(self, selector, *locators, **kwargs): return self.assert_all_of_selectors(selector, *locators, **kwargs)"
1633,def renew_lock(self): if self.renew_lock(self.renew_lock): self.renew_lock(self.renew_lock) self.renew_lock(self.renew_lock) self.renew_lock(self.renew_lock) self.renew_lock(self.renew_lock) self.renew_lock(self.renew_lock) self.renew_lock(self.renew_lock) self.renew_lock(self.renew_lock) self.renew_lock(self.renew_lock) self.renew_lock) self.renew_lock(self.renew_lock) self.renew_lock) self.renew_lock(self.renew_lock) self.renew_lock) self.renew_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock_lock,"def renew_lock(self): self._can_run() expiry = self._mgmt_request_response( REQUEST_RESPONSE_RENEW_SESSION_LOCK_OPERATION,'session-id': self.session_id, mgmt_handlers.default) self.locked_until = datetime.datetime.fromtimestamp(expiry[b'expiration']/1000.0)"
1634,"def convert_ramons(self, ramons, flatten, dict, ramons, ramons, ramons, ramons, ramons, ramons, ramons, ramons, ramons, dict, ramons, ramons, ramons, ramons, ramons, ramons, ramons, ramons, ramons, ramons, ramons, dict, ramons, dict, ramons, dict, ramons, dict, ramons, dict, ramons, dict, ramons, dict, ramons, dict, ramons, dict, ramons, dict, ramons, dict, ramons, dict, ramons, ramons, ramons, ramons, ramons, ramons, ramons, ramons, ramons, ramons, ramons, ramons, ramons, ramons, ramons, ramons, ramons","def to_dict(ramons, flatten=False): if type(ramons) is not list: ramons = [ramons] out_ramons =  for r in ramons: out_ramons[r.id] =  ""id"": r.id, ""type"": _reverse_ramon_types[type(r)], ""metadata"": vars(r)  return out_ramons"
1635,"def add_page(self, page): if self.add_page(page): self.add_page(page): self.add_page(page): self.add_page(page) self.add_page(page) self.add_page(page) self.add_page(page) self.add_page(page) self.add_page(page) self.add_page(page) self.add_page(page) self.add_page(page) self.add_page(page) self.add_page) self.add_page(page) self.add_page(page) self.add_page(page) self.add_page(page) self.add_page(page) self.add_page(page) self.add_page(page) self.add_page(page) self.add_page(page) self.add_page(page) self.add_page(page) self.add_page(page) self.add_page(page) self.add_page(page) self.add_page(page) self.add_page(page) self.add_page(page) self.add_page(page) self.add_page(page) self.add_page(page) self.add_page(page) self.add_page","def _add_page(self, text): save_cursor = self.parent.document.page.cursor.copy() save_cursor.x_reset() save_cursor.y_reset() self.parent.document.add_page() self.parent.document.set_cursor(save_cursor) self.parent.document.add_text(text)"
1636,"def validate_inv_hessian(inv_hessian, inv_hessian, inv_hessian, inv_hessian, inv_hessian, inv_hessian, inv_hessian, inv_hessian, inv_hessian, inv_hessian, inv_hessian, inv_hessian, inv_hessian, inv_hessian, inv_hessian, inv_hessian, inv_hessian, inv_hessian, inv_hessian)","def _inv_hessian_control_inputs(inv_hessian): is_positive_definite = tf.reduce_all( input_tensor=tf.math.is_finite(tf.linalg.cholesky(inv_hessian)), axis=[-1, -2]) is_symmetric = tf.equal(bfgs_utils.norm( inv_hessian - _batch_transpose(inv_hessian), dims=2), 0) return [tf.Assert(is_positive_definite, ['Initial inverse Hessian is not positive definite.', inv_hessian]), tf.Assert(is_symmetric, ['Initial inverse Hessian is not symmetric', inv_hessian])]"
1637,"def create_grading_standards(self, grading_standards): if self.grading_standards(self.grading_standards): self.grading_standards(self.grading_standards) self.grading_standards(self.grading_standards) self.grading_standards) self.grading_standards(self.grading_standards) self.grading_standards(self.grading_standards) self.grading_standards(self.grading_standards)","def create_grading_standard_for_course(self, course_id, name, grading_scheme, creator): url = COURSES_API.format(course_id) + ""/grading_standards"" body =  ""title"": name, ""grading_scheme_entry"": grading_scheme, ""as_user_id"": creator  return GradingStandard(data=self._post_resource(url, body))"
1638,"def fetch_task_record(self, keys, msg_id, msg_id, keys, msg_id, keys, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, msg_id, m","def db_query(self, query, keys=None): if isinstance(keys, basestring): keys = [keys] content = dict(query=query, keys=keys) self.session.send(self._query_socket, ""db_request"", content=content) idents, msg = self.session.recv(self._query_socket, 0) if self.debug: pprint(msg) content = msg['content'] if content['status']!= 'ok': raise self._unwrap_exception(content) records = content['records'] buffer_lens = content['buffer_lens'] result_buffer_lens = content['result_buffer_lens'] buffers = msg['buffers'] has_bufs = buffer_lens is not None has_rbufs = result_buffer_lens is not None for i,rec in enumerate(records): if has_bufs: blen = buffer_lens[i] rec['buffers'], buffers = buffers[:blen],buffers[blen:] if has_rbufs: blen = result_buffer_lens[i] rec['result_buffers'], buffers = buffers[:blen],buffers[blen:] return records"
1639,"def add_subvolume(self, token, channel, token, token, token): if self.subvolume(subvolume, token, channel, token, token, token, token, token, token, token, token, token, token, token, token, token, token, token, token, token, token, token, token, token, token, token, token, token, token, token, token, token, token, token, token, token, token, token, token, token, token, token): if self.subvolume(subvolume): if self.subvolume(subvolume, notes, notes, notes, notes, notes, notes, notes, notes, notes, notes, notes, notes, notes, notes, notes, notes)","def add_subvolume(self, token, channel, secret, x_start, x_stop, y_start, y_stop, z_start, z_stop, resolution, title, notes): md = self.get_metadata(token)['metadata'] if'subvolumes' in md: subvols = md['subvolumes'] else: subvols = [] subvols.append( 'token': token, 'channel': channel, 'x_start': x_start, 'x_stop': x_stop, 'y_start': y_start, 'y_stop': y_stop, 'z_start': z_start, 'z_stop': z_stop,'resolution': resolution, 'title': title, 'notes': notes ) return self.set_metadata(token, 'secret': secret,'subvolumes': subvols )"
1640,"def parse_file_like(self, xml, xml, xml, xml, xml, xml, xml, xml, xml, xml, xml, xml, xml, xml, xml, xml, xml, xml, xml, xml, xml, xml, xml, xml, xml, xml, xml, xml)","def parse_osm_file(f, parse_timestamps=True): nodes = [] ways = [] relations = [] for p in iter_osm_file(f, parse_timestamps): if type(p) == model.Node: nodes.append(p) elif type(p) == model.Way: ways.append(p) elif type(p) == model.Relation: relations.append(p) return (nodes, ways, relations)"
1641,"def add_texture_category(self, texture): if self.category(texture): self.category(texture): self.category(texture): self.category(texture)","def addCategory(self,name): self.categories[name]= self.categoriesTexCache[name]= self.categoriesTexBin[name]=pyglet.image.atlas.TextureBin(self.texsize,self.texsize) self.peng.sendEvent(""peng3d:rsrc.category.add"",""peng"":self.peng,""category"":name)"
1642,"def accept('fields', 'x', 'y', '%d'): if 'key' is not present, api_accepts will return '%d' if 'key' is not present, api_accepts will return '%d' if 'key' is not present, api_accepts will return '%d' if 'key' is not present, api_accepts will return '%d', '%d', '%d', '%d', '%d', '%d', '%d', '%d', '%d', '%d', '%d', '%d', '%d', '%d', '%d', '%d', '%d', '%d', '%d', '%d', '%d', '%d', '%d', '%d', '%d', '%d', '%d', '%d', '%d', '%d', '%d', '%d","def api_accepts(fields): def decorator(func): @wraps(func) def wrapped_func(request, *args, **kwargs): if request.method not in ['GET', 'POST']: return func(request, *args, **kwargs) form_class = type('ApiForm', (forms.Form,), fields.copy()) form = form_class(getattr(request, request.method)) if not form.is_valid(): if settings.DEBUG: return JsonResponseBadRequest( 'failed to validate: %s' % dict(form.errors) ) else: logger.warn( 'input to '%s' failed to validate: %s', request.path, dict(form.errors) ) return func(request, *args, **kwargs) for (field_name, field_instance) in fields.items(): if isinstance(field_instance, models.Model): field_type = type(field_instance) field_id = '%s-id' % field_name if field_id not in request.REQUEST: return JsonResponseBadRequest( 'field %s not present' % field_name ) field_pk = int(request.REQUEST[field_id]) try: field_value = field_type.objects.get(pk=field_pk) except field_type.DoesNotExist: return JsonResponseNotFound( '%s with pk=%d does not exist' % ( field_type, field_pk ) ) form.cleaned_data[field_name] = field_value validated_request = ValidatedRequest(request, form) return func(validated_request, *args, **kwargs) return wrapped_func return decorator"
1643,"def attach_method(self, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method, method)","def add_method(cls): def wrapper(f): #if hasattr(cls, f.__name__): setattr(cls, f.__name__, f) return f return wrapper"
1644,"def include_if_exists_head.html(self, include_if_exists_head.html, include_if_exists_head.html, include_if_exists_head.html, include_if_exists_head.html, include_if_exists_head.html, include_if_exists_head.html, include_if_exists_head.html, include_if_exists_head.html, include_if_exists_head.html)","def include_if_exists(parser, token): try: tag_name, template_name = token.split_contents() except ValueError: raise template.TemplateSyntaxError,  ""%r tag requires a single argument"" % token.contents.split()[0] return IncludeNode(template_name)"
1645,"def checksum(self.ean, self.ean, self.ean, self.ean, self.ean, self.ean, self.ean, self.ean, self.ean, self.ean, self.ean, self.ean, self.ean, self.ean, self.ean)","def calculate_checksum(self): def sum_(x, y): return int(x) + int(y) evensum = reduce(sum_, self.ean[::2]) oddsum = reduce(sum_, self.ean[1::2]) return (10 - ((evensum + oddsum * 3) % 10)) % 10"
1646,def ________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________,"def _write_header(po_path, lang, header): po_file = open(po_path, 'w') po_file.write(header + 'n') po_file.write('msgid """"' + 'nmsgstr """"' + 'n""MIME-Version:'+ settings.METADATA['MIME-Version'] + r'n""' 'n""Content-Type:'+ settings.METADATA['Content-Type'] + r'n""' 'n""Content-Transfer-Encoding:'+ settings.METADATA['Content-Transfer-Encoding'] + r'n""' 'n""Language:'+ lang + r'n""' + 'n') po_file.close()"
1647,"def config_conf_section(self, config_conf_section, config_conf_section, config_conf_section, config_conf_section, config_conf_section, config_conf_section, config_conf_section, config_conf_section, config_conf_section, config_conf_section, config_conf_section, config_conf_section, config_conf_section, config_conf_section, config_conf_section, config_conf_section, config_conf_section, config_conf_section, config_conf_section, config_conf_section, config_conf_section)","def config_cmd_handler(conf, config='config'): if conf[config].create or conf[config].update: conf.create_config_(update=conf[config].update) if conf[config].create_local: conf.create_config_(index=-1, update=conf[config].update) if conf[config].edit: if not conf.config_files_[0].is_file(): conf.create_config_(update=conf[config].update) subprocess.call(shlex.split(' '.format(conf[config].editor, conf.config_files_[0])))"
1648,"def format_return(self, value, str): if self.get_value(self.get_value): self.get_value(self.get_value): self.get_value(self.get_value) self.get_value(self.get_value) self.get_value(self.get_value) self.get_value(self.get_value) self.get_value(self.get_value) self.get_value(self.get_value) self.get_value(self.get_value) self.get_value(self.get_value) self.get_value(self.get_value) self.get_value(self.get_value) self.get_value) self.get_value(self.get_value) self.get_value) self.get_value(self.get_value) self.get_value(self.get_value) self.get_value(self.get_value) self.get_value) self.get_value(self.get_value) self.get_value(self.get_value) self.get_value(self.get_value) self.get_value(self.get_value) self.get_value(self.get_value) self.get_value(self.get_value","def format_returnvalue(self, value): self._ensure_loaded() if not self.return_info.is_data: return None if self.return_info.type_name is not None: return typeinfo.type_system.format_value(value, self.return_info.type_name, self.return_info.formatter) return self.return_info.formatter(value)"
1649,"def sendConnECT(self, clientId, keepalive, sessionFlag): return self.return(self.returnCode, sessionFlag) self.return(self.returnCode, sessionFlag) self.return(self.returnCode, sessionFlag) self.return(self.returnCode, sessionFlag) self.return(self.returnCode, sessionFlag) self.return(self.returnCode, sessionFlag) self.return(self.returnCode, self.returnCode, self.returnCode, self.returnCode, self.returnCode, self.returnCode, self.returnCode, self.returnCode, self.returnCode, self.returnCode, self.returnCode, self.returnCode, self.returnCode, self.returnCode, self.returnCode, self.returnCode, self.returnCode, self.returnCode, self.returnCode, self.returnCode, self.returnCode, self.returnCode, self.returnCode, self.returnCode, self.returnCode)","def connect(clientId, keepalive=0, willTopic=None, willMessage=None, willQoS=0, willRetain=False, username=None, password=None, cleanStart=True, version=mqtt.v311):"
1650,def set_axis_scaling(axis scaling): set_axis_scaling(axis scaling): set_axis_scaling(axis scaling): set_axis_scaling(axis scaling): set_axis_scaling(axis scaling),"def __scale_axes(axes, ax_type, which): kwargs = dict() if which == 'x': thresh = 'linthreshx' base = 'basex' scale = 'linscalex' scaler = axes.set_xscale limit = axes.set_xlim else: thresh = 'linthreshy' base = 'basey' scale = 'linscaley' scaler = axes.set_yscale limit = axes.set_ylim if ax_type =='mel': mode ='symlog' kwargs[thresh] = 1000.0 kwargs[base] = 2 elif ax_type == 'log': mode ='symlog' kwargs[base] = 2 kwargs[thresh] = core.note_to_hz('C2') kwargs[scale] = 0.5 elif ax_type in ['cqt', 'cqt_hz', 'cqt_note']: mode = 'log' kwargs[base] = 2 elif ax_type == 'tempo': mode = 'log' kwargs[base] = 2 limit(16, 480) else: return scaler(mode, **kwargs)"
1651,"def get(self, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get, get","def get(url,headers=None,token=None,data=None,return_json=True): bot.debug(""GET %s"" %url) return call(url, headers=headers, func=requests.get, data=data, return_json=return_json)"
1652,"def fpointer(self, mdir, nindent, fpointer, fpointer, mdir, fpointer, fpointer, mdir, fpointer, mdir, fpointer, mdir, fpointer, mdir, fpointer, mdir, fpointer, mdir, fpointer, mdir, fpointer, mdir, fpointer, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir, mdir,","def ste(command, nindent, mdir, fpointer): rterm_echo( ""$PMISC_DIRseppypkgsepcmd"".format(sep=os.path.sep, cmd=command), nindent, ""PMISC_DIR"": mdir, fpointer, )"
1653,"def create_icosahedron(self, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv, subdiv","def icosphere(script, radius=1.0, diameter=None, subdivisions=3, color=None): """"""create an icosphere mesh radius Radius of the sphere if diameter is not None: radius = diameter / 2 filter_xml = ''.join(['filter name=""Sphere"">n','Param name=""radius"" ', 'value=""%s""'% radius, 'description=""Radius"" ', 'type=""RichFloat"" ', '/>n','Param name=""subdiv"" ', 'value=""%d""'% subdivisions, 'description=""Subdiv. Level"" ', 'type=""RichInt"" ', '/>n','/filter>n']) util.write_filter(script, filter_xml) if isinstance(script, FilterScript): script.add_layer('Sphere', change_layer=True) if color is not None: vert_color.function(script, color=color) return None"
1654,"def output_with_deps(self, dependencies): if self.output_with_deps(self.output_with_deps) self.output_with_deps(self.output_with_deps) self.output_with_deps(self.output_with_deps) self.output_with_deps(self.output_with_deps) self.output_with_deps","def with_dependencies(dependencies, output_tensor, name=None): if tf.executing_eagerly(): return output_tensor with tf.name_scope(name or ""control_dependency"") as name: with tf.control_dependencies(d for d in dependencies if d is not None): output_tensor = tf.convert_to_tensor(value=output_tensor) if isinstance(output_tensor, tf.Tensor): return tf.identity(output_tensor, name=name) else: return tf.IndexedSlices( tf.identity(output_tensor.values, name=name), output_tensor.indices, output_tensor.dense_shape)"
1655,"def close_implement_interpreter(self, readfunc): if self.__init__(self._init__): self._init__(self._init__) self._init__(self._init__) self._init__(self._init__) self._init__(self._init__) self._init__(self._init__init__(self._init__)","def interact(banner=None, readfunc=None, my_locals=None, my_globals=None): console = code.InteractiveConsole(my_locals, filename='trepan>') console.runcode = lambda code_obj: runcode(console, code_obj) setattr(console, 'globals', my_globals) if readfunc is not None: console.raw_input = readfunc else: try: import readline except ImportError: pass console.interact(banner) pass"
1656,"def list_of_databases(self, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database) database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database, database","def db_list(username=None, password=None, host=None, port=None, maintain_db='postgres'): ""returns a list of all databases on this server"" conn = _connection(username=username, password=password, host=host, port=port, db=maintain_db) cur = conn.cursor() cur.execute('SELECT DATNAME from pg_database') rows = cur.fetchall() conn.close() result = [] for row in rows: result.append(row[0]) return result"
1657,"def remove_dup_args(self, args): if self.get_dup_args(args): self.get_dup_args(args): self.get_dup_args(args): self.get_dup_args(args): self.get_dup_args(args): self.get_dup_args(args): self.get_dup_args(args)","def rm_dup_args_safe(self, tag: str = None) -> None: name_to_lastarg_vals =   for arg in reversed(self.arguments): name = arg.name.strip(WS) if arg.positional: val = arg.value else: val = arg.value.strip(WS) if name in name_to_lastarg_vals: if not val: del arg[0:len(arg.string)] else: lastarg, dup_vals = name_to_lastarg_vals[name] if val in dup_vals: del arg[0:len(arg.string)] elif '' in dup_vals: del lastarg[0:len(lastarg.string)] dup_vals.pop(0) else: dup_vals.append(val) if tag: arg.value += tag else: name_to_lastarg_vals[name] = (arg, [val])"
1658,"def compute_dataframe(dataframe, dataframe): dataframe = dataframe = dataframe = dataframe = dataframe = dataframe = dataframe = dataframe = dataframe = dataframe = dataframe = dataframe = dataframe = dataframe = dataframe = dataframe = dataframe = dataframe = dataframe = dataframe = dataframe = dataframe = dataframe = dataframe = dataframe = dataframe = dataframe = dataframe = dataframe = dataframe = dataframe = dataframe = dataframe","def _compute_start_end(df, start, end): result =  time_dict ='start': start, 'end': end totals = df.groupby('date').agg('value': sum).reset_index() for time_name, time in time_dict.items(): if not totals[totals['date'] == time['id']].empty: value = totals.loc[ totals['date'] == time['id'], 'value' ].values[0] else: value = 0 result[time_name] = pd.DataFrame([ 'value': value, 'label': time['label'], 'groups': time['label'] ]) return result['start'], result['end']"
1659,"def new_node(self, node_type, node_type, node_type, node_type, node_type, node_type, node_type, node_type, node_type, node_type, node_type, node_type, node_type, node_type, node_type, node_type, node_type, node_type, node_type, node_type, node_type, node_type, node_type, node_type, node_type, node_type, node_type)","def retyped(self, new_type): return ParseNode(new_type, children=list(self.children), consumed=self.consumed, position=self.position, ignored=self.ignored)"
1660,"def createEndpoint(self, port, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, service_name, ipv4, ipv6, ipv6, ipv6, ipv6, ipv6, ipv6, ipv6, ipv6, ipv6, ipv6, ipv6, ipv6, ipv6, ipv6, ipv6, ipv6, ipv6, ipv6, ipv6, ipv4, ipv6, ipv6,","def create_endpoint(port=0, service_name='unknown', ipv4=None, ipv6=None): ipv4_int = 0 ipv6_binary = None if ipv4: ipv4_int = struct.unpack('!i', socket.inet_pton(socket.AF_INET, ipv4))[0] if ipv6: ipv6_binary = socket.inet_pton(socket.AF_INET6, ipv6) port = struct.unpack('h', struct.pack('H', port))[0] return zipkin_core.Endpoint( ipv4=ipv4_int, ipv6=ipv6_binary, port=port, service_name=service_name, )"
1661,"def fetch_resource(self, resource, params, params): self.get_resource(resource, params) self.get_resource(resource) self.get_resource(resource) self.get_resource(resource) self.get_resource(resource) self.get_resource(resource) self.get_resource(resource) self.get_resource(resource) self.get_resource(resource) self.get_resource) self.get_resource) self.get_resource(resource) self.get_resource) self.get_resource(resource) self.get_resource(resource) self.get_resource) self.get_resource(resource) self.get_resource) self.get_resource(resource) self.get_resource) self.get_resource(resource) self.get_resource(resource) self.get_resource(resource) self.get_resource) self.get_resource(resource) self.get_resource(resource) self.get_resource) self.get_resource(resource) self.get_resource(resource) self.get_resource(resource","def _fetch(self, resource, params): url = urijoin(self.base_url, resource) params[self.PKEY] = self.api_key params[self.PSIGN] = 'true', do_fetch = True while do_fetch: logger.debug(""Meetup client calls resource: %s params: %s"", resource, str(params)) if not self.from_archive: self.sleep_for_rate_limit() r = self.fetch(url, payload=params) if not self.from_archive: self.update_rate_limit(r) yield r.text if r.links and 'next' in r.links: url = r.links['next']['url'] params =  self.PKEY: self.api_key, self.PSIGN: 'true'  else: do_fetch = False"
1662,"def start_new_line(self, number): if number is not specified, start_new_line(self, number): if number is not specified, start_new_line(self, number): start_new_line(self, number)","def add_newline(self, number=1): if isinstance(number, int): try: self.page._add_newline(self.font, number, self.double_spacing) except ValueError: self.add_page() else: raise TypeError(""Number of newlines must be an integer."")"
1663,"def _report_summary(self, group): if self.report_summary(group): self.report_summary(group) return self.report_summary(group)","def _group_report(self,group,name): if group: print '%s jobs:' % name for job in group: print '%s : %s' % (job.num,job) print return True"
1664,"def respawn_workers(self, task): self.respawn_workers(task) self.respawn_worker(task) self.respawn_worker(task) self.respawn_worker(task) self.respawn_worker(task) self.respawn_worker(task) self.respawn_worker(task) self.respawn_worker(task) self.respawn_worker(task) self.respawn_worker(task) self.respawn_worker(task) self.respawn_worker(task) self.respawn_worker(task) self.respawn_worker(task) self.respawn_worker(task) self.respawn_worker(task) self.respawn_worker(task) self.respawn_worker(task) self.respawn_worker(task) self.respawn_worker(task) self.respawn_worker(task) self.respawn_worker(task) self.respawn_worker(task) self.respawn_worker(task) self.respawn_worker(task) self.respawn_worker(task) self.respawn_worker(task) self.respawn_worker(task) self.respawn_","def respawn_dead_workers(self): for w_id, p in self.workers.items(): if not p.is_alive(): if self.log_level >= 2: self.log.info(""Worker w_id 0 died"".format(w_id)) task = self.worker_assignments.get(w_id, ) if self.log_level >= 2 and task!= : self.log.info( ""Dead worker w_id 0 was assigned task - 1"".format( w_id, task ) ) error_suffix = """" if task!= : del self.worker_assignments[w_id] if self.resubmit_on_error or self.hot_loop: self.work_todo.append(task) self.queue_task(task) if self.log_level >= 2: self.log.info(""Resubmitting task - 0"".format(task)) error_suffix = "" with task=1"".format(task) if self.log_level >= 1: self.log.debug( ""TaskMgr.work_todo: 0 tasks left"".format(len(self.work_todo)) ) if self.log_level >= 2: self.log.info( ""Respawning worker - w_id=01"".format(w_id, error_suffix) ) self.workers[w_id] = Process( target=Worker, args=(w_id, self.t_q, self.r_q, self.worker_cycle_sleep), ) self.workers[w_id].daemon = True self.workers[w_id].start()"
1665,"def image_name(self, image_name, tag, base, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, image_name, image_name, tag, image_name, image_name, image_name, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase, lowercase,","def parse_image_name(image_name, tag=None, version=None, defaults=True, ext=""sif"", default_collection=""library"", default_tag=""latest"", base=None, lowercase=True): original = image_name if base is not None: image_name = image_name.replace(base,'').strip('/') image_name = re.sub('[.](img|simg|sif)','', image_name) uri_regexes = [ _reduced_uri, _default_uri, _docker_uri ] for r in uri_regexes: match = r.match(image_name) if match: break if not match: bot.exit('Could not parse image ""%s""! Exiting.' % image) registry = match.group('registry') collection = match.group('collection') repo_name = match.group('repo') repo_tag = match.group('tag') version = match.group('version') assert(repo_name) collection = set_default(collection, default_collection, defaults) repo_tag = set_default(repo_tag, default_tag, defaults) if lowercase: collection = collection.lower().rstrip('/') repo_name = repo_name.lower() repo_tag = repo_tag.lower() else: collection = collection.rstrip('/') if version!= None: version = version.lower() if registry == None: uri = ""%s/%s"" % (collection, repo_name) else: uri = ""%s/%s/%s"" % (registry, collection, repo_name) url = uri if repo_tag!= None: uri = ""%s-%s"" % (uri, repo_tag) tag_uri = ""%s:%s"" % ("
1666,def main_program(self): if self.program(self.program): self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program) self.program(self.program) self.program(self.program) self.program) self.program(self.program) self.program(self.program) self.program) self.program(self.program) self.program(self.program) self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program(self.program) self.program) self.program(self.program) self.program(self.program) self,"def main(argv): global g_script_name global g_tmp_dir g_script_name = os.path.basename(argv[0]) parse_args(argv) g_tmp_dir = tempfile.mkdtemp(suffix="".tmp_minicran"") print ""Created tmp directory: "" + g_tmp_dir atexit.register(remove_tmp_dir) try: b = MinicranBuilder(g_print_only, g_output_dir, g_tmp_dir, g_platform, g_rversion, g_branch, g_buildnum) b.build() except KeyboardInterrupt: print("""") pass"
1667,def get_ground_truth_match(self): if self.get_ground_truth_match(self.get_ground_truth_match): self.get_ground_truth_match(self.get_ground_truth_match) self.get_ground_truth_match(self.get_ground_truth_match) self.get_ground_truth_match(self.get_ground_truth_match) self.get_ground_truth_match(self.get_ground_truth_match) self.get_ground_truth_match(self.get_ground_truth_match) self.get_ground_truth_match(self.get_ground_truth_match) self.get_ground_truth_match(self.get_ground_truth_match) self.get_truth_match(self.get_truth_match) self.get_truth_match(self.get_truth_match) self.get_truth_match(self.get_truth_match) self.get_truth_match(self.get_truth_match) self.get_truth_match(self.get_truth_match) self.get_truth_match(self.get_truth_,"def read_matches_files(data_dir, matches_file): matches = [] with open(os.path.join(data_dir, matches_file), 'r') as f: for line in f: line_split = line.split() matches.append([int(line_split[0]), int(line_split[3]), int(line_split[1] == line_split[4])]) return torch.LongTensor(matches)"
1668,"def lock(self, parameters): self.get_params(params) self.get_params(params) self.get_params(params) self.get_params(params) self.get_params(params) self.get_params(params) self.get_params(params) self.get_params(params) self.get_params(params) self.get_params) self.get_params(params) self.get_params) self.get_params(params) self.get_params(params) self.get_params) self.get_params(params) self.get_params(params) self.get_params(params) self.get_params) self.get_params(params) self.get_params(params) self.get_params(params) self.get_params(params) self.get_params(params) self.get_params(params) self.get_param",def f_lock_derived_parameters(self): for par in self._derived_parameters.values(): if not par.f_is_empty(): par.f_lock()
1669,"def parse_AST(self, root, AST): if self.get_root(root, root, root): self.get_root(root): self.get_root(root): self.get_root(root): self.get_root(root, root, root)","def t_parse(self, s): with self.lock: try: return self.parser.parse(s, lexer=self.lexer, debug=False) except CannotParse as e: e.s = s raise e"
1670,"def add_an_item(self, item, variant, quantity, quantity): return self.add_an_item(self.add_an_item)","def add_item_to_basket(self, item, variant=VARIANT.MEDIUM, quantity=1): item_type = item.type if item_type == 'Pizza': return self.add_pizza_to_basket(item, variant, quantity) elif item_type == 'Side': return self.add_side_to_basket(item, quantity) return None"
1671,"def get_ref_annotation(self, time, id_tier): if self.ref_annotation(self.ref_annotation) self.ref_annotation(self.ref_annotation) self.ref_annotation(self.ref_annotation) self.ref_annotation(self.ref_annotation) self.ref_annotation(self.ref_annotation) self.ref_annotation(self.ref_annotation) self.ref_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annotation_annot","def get_ref_annotation_data_before_time(self, id_tier, time): befores = self.get_ref_annotation_data_between_times(id_tier, 0, time) if befores: return [max(befores, key=lambda x: x[0])] else: return []"
1672,"def calculate_change_hash(self, item): if item is not self.get_change_hash(self.get_change_hash) return self.get_change_hash(self.get_change_hash) return self.get_change_hash(self.get_change_hash)",def calculate_change_hash(item: StoreItem) -> str: cpy = copy(item) if cpy.e_tag is not None: del cpy.e_tag return str(cpy)
1673,"def add_layer(z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index, z_index","def addLayer(self,layer,z_index=None): if z_index is None: z_index = layer.z_index i = 0 for l,z in self.layers: if z>z_index: break i+=1 self._layers[layer.name]=layer self.layers.insert(i,[layer,z_index])"
1674,"def turn_on_device(self, gateway): self.set_device(device): self.set_device(device) self.set_device(device) self.set_device(device) self.set_device(device) self.set_device(device) self.set_device(device) self.set_device(device) self.set_device(device) self.set_device(device) self.set_device(device) self.set_device) self.set_device(device) self.set_device(device) self.set_device(device) self.set_device(device) self.set_device(device) self.set_device) self.set_device(device) self.set_device(device) self.set_device(device) self.set_device(device) self.set_device(device) self.set_device) self.set_device(device) self.set_device(device) self.set_device(device) self.set_device(device) self.set_device(device) self.set","def write_to_hub(self, sid, **kwargs): if self.key is None: _LOGGER.error('Gateway Key is not provided. Can not send commands to the gateway.') return False data =  for key in kwargs: data[key] = kwargs[key] if not self.token: _LOGGER.debug('Gateway Token was not obtained yet. Cannot send commands to the gateway.') return False cmd = dict() cmd['cmd'] = 'write' cmd['sid'] = sid if int(self.proto[0:1]) == 1: data['key'] = self._get_key() cmd['data'] = data else: cmd['key'] = self._get_key() cmd['params'] = [data] resp = self._send_cmd(json.dumps(cmd), ""write_ack"") if int(self.proto[0:1]) == 1  else self._send_cmd(json.dumps(cmd), ""write_rsp"") _LOGGER.debug(""write_ack  %s"", resp) if _validate_data(resp): return True if not _validate_keyerror(resp): return False resp = self._send_cmd('""cmd"" : ""get_id_list""', ""get_id_list_ack"") if int(self.proto[0:1]) == 1  else self._send_cmd('""cmd"" : ""discovery""', ""discovery_rsp"") _LOGGER.debug(""get_id_list  %s"", resp) if resp is None or ""token"" not in resp: _LOGGER.error('No new token from gateway. Can"
1675,"def get_last(n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n,","def _get_log_lines(self, n=300): with open(self.log_file) as fh: last_lines = fh.readlines()[-n:] return last_lines"
1676,"def open_files(self, os-es): os_es = self.open_files(self.open_files) self.open_files(self.open_files) self.open_files(self.open_files) self.open_files(self.open_files) self.open_files(self.open_files) self.open_files(self.open_files) self.open_files(self.open_files) self.open_files) self.open_files) self.open_files(self.open_files) self.open_files(self.open_files) self.open_files(self.open_files) self.open_files) self.open_files(self.open_files) self.open_files) self.open_files(self.open_files) self.open_files(self.open_files) self.open_files(self.open_files) self.open_files(self.open_files) self.open_files) self.open_files(self.open_files) self.open_files(self.open_files) self.","def get_open_fds(verbose=False): pid = os.getpid() procs = subprocess.check_output( [ ""lsof"", '-w', '-Ff', ""-p"", str( pid ) ] ) if verbose: oprocs = subprocess.check_output( [ ""lsof"", '-w', ""-p"", str( pid ) ] ) logger.info(oprocs) open_files = filter( lambda s: s and s[ 0 ] == 'f' and s[1: ].isdigit(), procs.split( 'n' ) ) return open_files"
1677,"def is_empty(observation_index_points, observation_index_points, observation_index_points, observation_index_points, observation_index_points, observation_index_points, observation_index_points, observation_index_points, observation_index_points, observation_index_points, observation_index_points, observation_index_points, observation_index_points, observation_index_points, observation_index_points, observation_index_points)","def _is_empty_observation_data( feature_ndims, observation_index_points, observations): if observation_index_points is None and observations is None: return True num_obs = tf.compat.dimension_value( observation_index_points.shape[-(feature_ndims + 1)]) if num_obs is not None and num_obs == 0: return True return False"
1678,"def start_slackapi(self, RTM): self.slackapi(self.slackapi) self.slackapi(self.slackapi) self.slackapi(self.slackapi) self.slackapi(self.slackapi) self.slackapi(self.slackapi) self.slackapi(self.slackapi) self.slackapi(self.slackapi) self.slackapi(self.slackapi) self.slackapi(self.slackapi) self.slackapi(self.slackapi) self.slackapi(self.slackapi) self.slackapi(self.slackapi) self.slackapi(self.slackapi) self.slackapi(self.slackapi) self.slackapi(self.slackapi) self.slackapi(self.slackapi) self.slackapi(self.slackapi) self.slackapi(self.slackapi) self.slackapi(self.slackapi) self.slackapi(self.slackapi) self.sl",def run(self): slack = SlackAPI(token=self.token) rtm = slack.rtm_start() factory = SlackClientFactory(rtm['url']) factory.protocol = SlackClientProtocol factory.protocol.slack = slack factory.protocol.channel_layer = self.channel_layer factory.channel_name = self.channel_name factory.run()
1679,"def delete_pool(self, pool_name, pool_name, pool_name, pool_name, pool_name, pool_name, pool_name, pool_name, pool_name, pool_name, pool_name, pool_name, pool_name, pool_name, pool_name, pool_name, pool_name, pool_name, pool_name, pool_name, pool_name)","def delete_pool(name, session=None): if not (name and name.strip()): raise AirflowBadRequest(""Pool name shouldn't be empty"") pool = session.query(Pool).filter_by(pool=name).first() if pool is None: raise PoolNotFound(""Pool '%s' doesn't exist"" % name) session.delete(pool) session.commit() return pool"
1680,"def getOrganisation(self, list(Organisation)): self.getOrganisation(Organisation)) self.getOrganisation(Organisation) self.getOrganisation(Organisation) self.getOrganisation(Organisation) self.getOrganisation(Organisation) self.getOrganisation(Organisation) self.getOrganisation(Organisation) self.getOrganisation(Organisation) self.getOrganisation(Organisation) self.getOrganisation) self.getOrganisation(Organisation) self.getOrganisation) self.getOrganisation(Organisation) self.getOrganisation) self.getOrganisation(Organisation) self.getOrganisation) self.getOrganisation(Organisation) self.getOrganisation) self.getOrganisation(Organisation) self.getOrganisation(Organisation) self.getOrganisation(Organisation) self.getOrganisation) self.getOrganisation(Organisation) self.getOrganisation(Organisation) self.getOrganisation(Organisation) self.getOrganisation(Organisation) self.getOrganisation(Organisation) self.getOrganisation(Or","def get_organisations(self, **query_params): organisations = self.get_organisations_json(self.base_uri, query_params=query_params) organisations_list = [] for organisation_json in organisations: organisations_list.append( self.create_organisation(organisation_json)) return organisations_list"
1681,"def set_state(self, target): self.set_state(target): self.set_state(target) self.set_state(target) self.set_state(target) self.set_state(target) self.set_state(target) self.set_state(target) self.set_state(target) self.set_state(target) self.set_state(target) self.set_state(target) self.set_state(target) self.set_state(target) self.set_state(target) self.set_state(target) self.set_state(target) self.set_state(target) self.set_state(target) self.set_state(target) self.set_state(target) self.set_state(target) self.set_state(target) self.set_state(target) self.set_state(target) self.set_state(target) self.set_state(target) self.set_state(target) self.set_state(target) self.set_state(target) self.set_state(target) self.set_state(target) self.set_state(target) self.set_state(target) self.set_state(target) self.set_state(target) self.set_state(target",def unset_state(self): glDisable(self.region.material.target) self.region.bone.unsetRotate(self.data)
1682,def malformed_msg_handler(self): if self.msg_handler(self.msg_handler): self.msg_handler(self.msg_handler): self.msg_handler(self.msg_handler): self.msg_handler(self.msg_handler),"def default_malformed_message_handler(worker, exc_info, message_parts): exc_type, exc, tb = exc_info exc_strs = traceback.format_exception_only(exc_type, exc) exc_str = exc_strs[0].strip() if len(exc_strs) > 1: exc_str += '...' warn('%s> occurred by %r' % (exc_str, message_parts), MalformedMessage)"
1683,"def pprint(self, class, type): self.pprint(self.pprint) self.pprint(self.pprint) self.pprint(self.pprint) self.pprint(self.pprint) self.pprint(self.pprint) self.pprint(self.pprint) self.pprint(self.pprint) self.pprint(self.pprint) self.pprint(self.pprint) self.pprint(self.pprint) self.pprint(self.pprint) self.pprint(self.pprint) self.pprint(self.pprint) self.pprint(self.pprint) self.pprint(self.pprint) self.pprint(self.pprint) self.pprint(self.pprint) self.pprint(self.pprint) self.pprint(self.pprint) self.pprint) self.pprint(self.pprint) self.pprint(self.pprint) self.pprint(self.pprint) self.pprint(self.pprint) self.pprint(self.pprint) self.pprint(self.pprint) self.pprint(self.pprint) self.pprint(self.pprint) self.pprint(self.pprint) self","def _type_pprint(obj, p, cycle): if obj.__module__ in ('__builtin__', 'exceptions'): name = obj.__name__ else: name = obj.__module__ + '.' + obj.__name__ p.text(name)"
1684,"def opentack_glance_mq(self, mq, mq): if self.mq(self.mq) self.mq(self.mq) self.mq(self.mq) self.mq(self.mq) self.mq(self.mq) self.mq(self.mq) self.mq(self.mq) self.mq(self.mq) self.mq(self.mq) self.mq(self.mq) self.mq) self.mq) self.mq: self.mq: self.mq: self.mq: self.mq: self.mq: self.mq: self.mq","def init_glance_consumer(self, mq): if not self.enable_component_notification(Openstack.Glance): log.debug(""disable listening glance notification"") return for i in range(self.config.glance_mq_consumer_count): mq.create_consumer(self.config.glance_mq_exchange, self.config.glance_mq_queue, ProcessFactory.process(Openstack.Glance)) log.debug(""enable listening openstack glance notification."")"
1685,def add_builtin(self): if self.add_builtin(self.add_builtin): self.add_builtin(self.add_builtin) self.add_builtin(self.add_builtin) self.add_builtin(self.add_builtin) self.add_builtin(self.add_builtin) self.add_builtin(self.add_builtin) self.add_builtin(self.add_builtin) self.add_builtin(self.add_builtin) self.add_builtin(self.add_builtin) self.add_builtin(self.add_builtin) self.add_builtin(self.add_builtin) self.add_builtin(self.add_builtin) self.add_builtin(self.add_builtin) self.add_builtin(self.add_builtin) self.add_builtin(self.add_builtin) self.add_builtin(self.add_builtin) self.add_builtin(self.add_builtin) self.add_builtin(self.add_builtin) self.add_builtin(self.add_builtin) self.add_builtin(self.add_builtin) self.add_builtin,"def remove_builtin(self, key, orig): if orig is BuiltinUndefined: del __builtin__.__dict__[key] else: __builtin__.__dict__[key] = orig"
1686,"def broadcast_mask(time_series_tensor, time_series_tensor): time_series_tensor = time_series_tensor, broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask = broadcast_mask.","def moments_of_masked_time_series(time_series_tensor, broadcast_mask): num_unmasked_entries = tf.cast( tf.reduce_sum(input_tensor=tf.cast(broadcast_mask, tf.int32), axis=-1), time_series_tensor.dtype) mean = (tf.reduce_sum(input_tensor=tf.where( broadcast_mask, tf.zeros_like(time_series_tensor), time_series_tensor), axis=-1) / num_unmasked_entries) variance = (tf.reduce_sum(input_tensor=tf.where( broadcast_mask, tf.zeros_like(time_series_tensor), (time_series_tensor - mean[..., tf.newaxis]) ** 2), axis=-1) / num_unmasked_entries) return mean, variance"
1687,"def draw_component(self, component, component): if component is not self.component(component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component, component)","def _draw_mainlayer(self, gc, view_bounds=None, mode=""default""): gc.save_state() try: font = str_to_font(str(self.pen.font)) gc.set_font(font) gc.set_fill_color(self.pen.color_) x = self.text_x - ( self.text_w / 2 ) y = self.text_y - ( font.size / 2 ) ctm = gc.get_ctm() if hasattr(ctm, ""__len__"") and len(ctm) == 6: scale = sqrt( (ctm[0] + ctm[1]) * (ctm[0] + ctm[1]) / 2.0 +  (ctm[2] + ctm[3]) * (ctm[2] + ctm[3]) / 2.0 ) elif hasattr(gc, ""get_ctm_scale""): scale = gc.get_ctm_scale() else: raise RuntimeError(""Unable to get scale from GC."") x *= scale y *= scale gc.show_text_at_point(self.text, x, y) finally: gc.restore_state()"
1688,"def REBINDING_State(self, REBINDING_State, REBINDING_State, REBINDING_State, REBINDING_State, REBINDING_State, REBINDING_State, REBINDING_State, REBINDING_State, REBINDING_State, REBINDING_State)","def REBINDING(self): logger.debug('In state: REBINDING') self.current_state = STATE_REBINDING if self.script is not None: self.script.script_init(self.client.lease, self.current_state) self.script.script_go() else: set_net(self.client.lease)"
1689,"def href(self, domain_substrings,'require_abs_url','require_abs_url','require_abs_url', 'domain_substrings','require_abs_url','require_abs_url','require_abs_url','require_abs_url','require_abs_url','require_abs_url','require_abs_url','require_abs_url','require_abs_url','require_abs_url','require_abs_url','require_abs_url','require_abs_url','require_abs_url', 'domain_substrings', 'domain_substrings', 'domain_substrings', 'domain_substrings', 'domain_substrings', 'domain_substrings', 'domain_substrings', 'domain_substrings', 'domain_substrings', 'domain_substrings","def href_filter(self, href): if self.config['require_abs_url']: if not href.lower().startswith(('http://', 'https://')): return False if self.config['all_domains']: # return True if self.config['domain_substrings']: parts = href.split('/') if len(parts)  3: return False domain = parts[2].lower() for substring in self.config['domain_substrings']: try: if substring in domain: return True except Exception, exc: logger.warn('%r in %r raised', substring, domain, exc_info=True) return False"
1690,"def opsgenie_payload(self, payload): opsgenie_payload(payload): opsgenie_payload(payload): opsgenie_payload(payload): opsgenie_payload(payload): opsgenie_payload(payload): opsgenie_payload(payload): opsgenie_payload(payload): opsgenie_payload(payload): opsgenie_payload): opsgenie_payload(payload): opsgenie_payload): opsgenie_payload(payload): opsgenie_payload(payload): opsgenie_payload(payload): opsgenie_payload): opsgenie_payload(payload): opsgenie_payload(payload): opsgenie_payload(payload): opsgenie_payload): opsgenie_payload(payload): opsgenie_payload(payload): opsgenie_payload(payload): opsgenie_payload","def _build_opsgenie_payload(self): payload =  for key in [ ""message"", ""alias"", ""description"", ""responders"", ""visibleTo"", ""actions"", ""tags"", ""details"", ""entity"", ""source"", ""priority"", ""user"", ""note"" ]: val = getattr(self, key) if val: payload[key] = val return payload"
1691,"def dict_merge(self, lft, rgt): if lft['key'] = rgt['key'] = rgt['key'] = rgt['key'] = rgt['key'] = rgt['key'] = rgt['key'] = rgt['key'] = rgt['key'] = rgt['key'] = rgt['key'] = rgt['key'] = rgt['key'] = rgt['key'] = rgt['key'] = rgt['key'] = rgt['key'] = rgt['key'] = rgt['key'] = rgt['key'] = rgt['key'] = rgt['key'] = rgt['key'] = rgt['key'] = rgt['key'] = rgt['key'] = rgt['key'] = rgt['","def dict_merge(lft, rgt): if not isinstance(rgt, dict): return rgt result = deepcopy(lft) for key, val in rgt.iteritems(): if key in result and isinstance(result[key], dict): result[key] = dict_merge(result[key], val) else: result[key] = deepcopy(val) return result"
1692,"def get_page_token(self, page_token): self.get_page_token(page_token) self.get_page_token(page_token) self.get_page_token(page_token) self.get_page_token(page_token) self.get_page_token(page_token) self.get_page_token(page_token) self.get_page_token(page_token) self.get_token(page_token) self.get_token(page_token) self.get_token(page_token) self.get_token(page_token) self.get_token(page_token) self.get_token(page_token) self.get_token(page_token) self.get_token(page_token) self.get_token(page_token) self.get_token(page_token) self.get_token(page_token) self.get_token(page_token) self.get_token(page_token) self.get_token(page_token) self.get_token(page_token) self.get_","def get_page_api(client_access_token, page_id): graph = GraphAPI(client_access_token) resp = graph.get('me/accounts') page_access_token = None for page in resp['data']: if page['id'] == page_id: page_access_token = page['access_token'] break return GraphAPI(page_access_token)"
1693,"def get_upload_sets(app, upload_sets): if app.upload_sets(app.upload_sets): if app.upload_sets(app.upload_sets): if app.upload_sets(app.upload_sets): if app.upload_sets(app.upload_sets): if app.upload_sets(app.upload_sets): if app.upload_sets(app.upload_sets): if app.upload_sets): if app.upload_sets(app.upload_sets): app.upload_sets(app.upload_sets): app.upload_sets): app.upload_sets(app.upload_sets): app.upload_sets(app.upload_sets): app.upload_sets): app.upload_sets(app.upload_sets): app.upload_sets(app.upload_sets): app.upload_sets): app.upload_sets(app.upload_sets): app.upload_sets)","def configure_uploads(app, upload_sets): if isinstance(upload_sets, UploadSet): upload_sets = (upload_sets,) if not hasattr(app, 'upload_set_config'): app.upload_set_config =  set_config = app.upload_set_config defaults = dict(dest=app.config.get('UPLOADS_DEFAULT_DEST'), url=app.config.get('UPLOADS_DEFAULT_URL')) for uset in upload_sets: config = config_for_set(uset, app, defaults) set_config[uset.name] = config should_serve = any(s.base_url is None for s in set_config.values()) if '_uploads' not in app.blueprints and should_serve: app.register_blueprint(uploads_mod)"
1694,def ancestor_nodes(nodes): return ancestor_nodes(nodes): return ancestor_nodes(nodes): return ancestor_nodes(nodes),"def get_ancestors(self, node, level): if level == 0: return for ancestor in node.ancestors(recurs=False): if not self.show_node(ancestor): continue yield ancestor"
1695,"def list_of_team_ids_for_year(self, team_ids_for_year, year_ids_for_year, year_ids_for_year, year_ids_for_year, year_ids_for_year, year_ids_for_year, year_ids_for_year, year_ids_for_year, year_ids_for_year, year_ids_for_year_ids_for_year_ids_for_year_ids_for_year_ids_for_year_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id_id",def get_team_ids(self): df = self.team_stats_per_game() if not df.empty: return df.index.tolist() else: print('ERROR: no teams found') return []
1696,"def change_content(self, content, tip): if not a tip, if not a tip, if not a tip, if not a tip, if not a tip, if not a tip, if not a tip, if not a tip, if not a tip, if not a tip, if not a tip, if not a tip, if not a tip, if not a tip, if not a tip is a tip is a tip is a tip is not","def _document_contents_change(self, position, removed, added): position += added document = self._control.document() if position == self._get_cursor().position(): self._call_tip()"
1697,"def crop(image, width, height, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi, ppoi","def crop_on_centerpoint(self, image, width, height, ppoi=(0.5, 0.5)): ppoi_x_axis = int(image.size[0] * ppoi[0]) ppoi_y_axis = int(image.size[1] * ppoi[1]) center_pixel_coord = (ppoi_x_axis, ppoi_y_axis) orig_aspect_ratio = float( image.size[0] ) / float( image.size[1] ) crop_aspect_ratio = float(width) / float(height) if orig_aspect_ratio >= crop_aspect_ratio: orig_crop_width = int( (crop_aspect_ratio * float(image.size[1])) + 0.5 ) orig_crop_height = image.size[1] crop_boundary_top = 0 crop_boundary_bottom = orig_crop_height crop_boundary_left = center_pixel_coord[0] - (orig_crop_width // 2) crop_boundary_right = crop_boundary_left + orig_crop_width if crop_boundary_left  0: crop_boundary_left = 0 crop_boundary_right = crop_boundary_left + orig_crop_width elif crop_boundary_right > image.size[0]: crop_boundary_right = image.size[0] crop_boundary_left = image.size[0] - orig_crop_width else: orig_crop_width = image.size[0] orig_crop_height = int( (float(image.size[0]) / crop_aspect_ratio) + 0.5 ) crop_boundary_left = 0 crop_boundary_right = orig_crop_width crop_boundary_top = center_pixel_coord[1]"
1698,"def add_inputs(multisigAddresses, multisigAddresses, multisigAddresses, multisigAddresses, multisigAddresses, multisigAddresses, multisigAddresses, multisigAddresses, multisigAddresses, multisigAddresses, multisigAddresses, multisigAddresses, multisigAddresses, multisigAddresses, multisigAddresses, multisigAddresses, multisigAddresses, multisigAddresses, multisigAddresses, multisigAddresses)","def add_inputs(self, inputs): if self.hash: raise RuntimeError('Bundle is already finalized.') count = 0 for addy in inputs: if count > 0: raise ValueError( 'cls only supports 1 input.'.format(cls=type(self).__name__), ) if not isinstance(addy, MultisigAddress): raise with_context( exc = TypeError( 'Incorrect input type for cls''(expected expected, actual actual).'.format( actual = type(addy).__name__, cls = type(self).__name__, expected = MultisigAddress.__name__, ), ), context =  'actual_input': addy,, ) security_level = addy.security_level if security_level  1: raise with_context( exc = ValueError( 'Unable to determine security level for type''(is digests populated correctly?).'.format( type = type(addy).__name__, ), ), context =  'actual_input': addy,'security_level': security_level,, ) if not addy.balance: raise with_context( exc = ValueError( 'Cannot add input with empty/unknown balance to type''(use Iota.get_balances to get balance first).'.format( type = type(self).__name__, ), ), context =  'actual_input': addy,, ) self._create_input_transactions(addy) count += 1"
1699,"def cgroup_name(self, cgroup_name, cgroup_name, cgroup_name, cgroup_name, cgroup_name, cgroup_name, cgroup_name, cgroup_name, cgroup_name, cgroup_name, cgroup_name, cgroup_name, cgroup_name, cgroup_name, cgroup_name, cgroup_name, cgroup_name): cgroup_name, cgroup_name, cgroup_name, cgroup_name, cgroup_name, cgroup_name, cgroup_name, cgroup_name, cgroup_name, cgroup_name, cgroup_name, cgroup_name, cgroup_name, dict[str, str, dict[str, str, dict[str, str, dict[str, str, dict[str, str, dict[str, str, dict[str, str, dict[str, str, cgroupname, cgroupname, cgroupname, cgroupname, cgroupname, cgroupname, dict[str","def _get_cgroup_names(): with open(""/proc/self/cgroup"") as f: lines = f.readlines() d =  for line in lines: line_split = line.rstrip().split("":"") subsystem = line_split[1] group_name = line_split[2] d[subsystem] = group_name return d"
1700,"def update_job(self, job_name, body): if self.transfer(self.transfer_job): self.transfer(self.transfer_job): self.transfer(self.transferJob) self.transfer(self.transferJob) self.transfer(self.transferJob) self.transfer(self.transferJob) self.transfer(self.transferJob) self.transfer(self.transferJob) self.transferJob) self.transfer(self.transferJob) self.transfer(self.transferJob) self.transfer(self.transferJob) self.transferJob) self.transfer(self.transferJob) self.transfer(self.transferJob) self.transferJob) self.transferJob) self.transfer(self.transfer(self.transfer(self.transferJob) self.transferJob) self.transferJob) self.transferJob) self.transfer(self.transfer(self.transfer(self.transfer(self.transfer(self.transfer(self.transfer","def update_transfer_job(self, job_name, body): body = self._inject_project_id(body, BODY, PROJECT_ID) return ( self.get_conn().transferJobs().patch(jobName=job_name, body=body).execute(num_retries=self.num_retries) )"
1701,"def build_name_attribute(self, name): self.name_attribute(self.name) self.name_attribute(self.name) self.name_attribute(self.name) self.name_attribute(self.name) self.name_attribute(self.name) self.name_attribute(self.name) self.name_attribute(self.name) self.name_attribute(self.name_attribute) self.name_attribute) self.name_attribute) self.name_attribute) self.name_attribute) self.name_attribute) self.name_attribute) self.name_attribute) self.name_attribute) self.name_attribute) self.name_attribute) self.name_attribute) self.name_attribute) self.name_attribute) self.name_attribute) self.name_attribute","def _build_name_attribute(self, name=None): name_list = [] if name: name_list.append(self.attribute_factory.create_attribute( enums.AttributeType.NAME, name) ) return name_list"
1702,def clean_all_timeslots(self): if self.clean_timeslots(self.clean_timeslots): self.clean_timeslots(self.clean_timeslots) self.clean_timeslots(self.clean_timeslots) self.clean_timeslots(self.clean_timeslots) self.clean_timeslots(self.clean_timeslots) self.clean_timeslots(self.clean_timeslots) self.clean_timeslots) self.clean_timeslots) self.clean_timeslots_timeslots_timeslots_timeslots_timeslots_timeslots_timeslots_timeslots_timeslots_timeslots_timeslots_timeslots_timeslots_timeslot_timeslot_timeslot_timeslot_timeslot_timeslot_timeslot_timeslot_timeslot_timeslot_timeslot_timeslot_timeslot_timeslot_timeslot_timeslot_timeslot_timeslot_timeslot_timeslot_timeslot_timeslot,"def clean_time_slots(self): ts = ((a[0], a[1]) for t in self.tiers.values() for a in t[0].values()) for a in a for b in ts for a in b  set(self.timeslots): del(self.timeslots[a])"
1703,"def send_event(self, data): if self.event(event): self.event(event): self.event(event) self.event(event) self.event(event) self.event(event) self.event(event) self.event(event) self.event(event) self.event(event) self.event(event) self.event(event) self.event(event) self.event) self.event(event) self.event(event) self.event) self.event(event) self.event) self.event(event) self.event) self.event(event) self.event) self.event(event) self.event) self.event) self.event(event) self.event) self.event(event) self.event(event) self.event) self.event(event) self.event(event) self.event) self.event(event) self.event(event) self.event) self.event(event) self.event(event) self.event","def sendEvent(self,event,data=None): if self.cfg[""debug.events.dumpfile""]!="""" and event not in self.event_list: self.event_list.add(event) if event not in self.eventHandlers: if event not in self.events_ignored or self.events_ignored[event]=self.cfg[""events.maxignore""]: self.events_ignored[event] = self.events_ignored.get(event,0)+1 return for handler in self.eventHandlers[event]: f = handler[0] try: f(event,data) except Exception: if not handler[1]: raise else: if self.cfg[""events.removeonerror""]: self.delEventListener(event,f)"
1704,"def hide_size(key, context, hidden_size, hidden_size, hidden_size, hidden_size, hidden_size, hidden_size, hidden_size, hidden_size, hidden_size, hidden_size, hidden_size, hidden_size, hidden_size, hidden_size, hidden_size, hidden_size, hidden_size, hidden_size, hidden_size, hidden_size, hidden_size, hidden_size, hidden_size, hidden_size, hidden_size, hidden_size)","def general_attention(key, context, hidden_size, projected_align=False): if hidden_size % 2!= 0: raise ValueError(""hidden size must be dividable by two"") batch_size = tf.shape(context)[0] max_num_tokens, token_size = context.get_shape().as_list()[-2:] r_context = tf.reshape(context, shape=[-1, max_num_tokens, token_size]) projected_key =  tf.layers.dense(key, hidden_size, kernel_initializer=xav()) r_projected_key = tf.reshape(projected_key, shape=[-1, hidden_size, 1]) lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size//2) lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size//2) (output_fw, output_bw), states =  tf.nn.bidirectional_dynamic_rnn(cell_fw=lstm_fw_cell, cell_bw=lstm_bw_cell, inputs=r_context, dtype=tf.float32) bilstm_output = tf.concat([output_fw, output_bw], -1) attn = tf.nn.softmax(tf.matmul(bilstm_output, r_projected_key), dim=1) if projected_align: log.info(""Using projected attention alignment"") t_context = tf.transpose(bilstm_output, [0, 2, 1]) output = tf.reshape(tf.matmul(t_context, attn), shape=[batch"
1705,"def beat_tracker(y, onset_envelope, onset_envelope, onset_envelope, onset_envelope, onset_envelope, onset_envelope, onset_envelope, onset_envelope, onset_envelope, onset_envelope, onset_envelope, onset_envelope, onset_envelope, onset_envelope, onset_envelope, onset_envelope, onset_envelope, onset_envelope, onset_envelope, onset_envelope, onset_envelope, onset_envelope, onset_envelope, onset_envelope, onset_envelope) onset_envelope, onset_envelope, onset_envelope, onset_envelope, onset_envelope, onset_envelope, onset_envelope, onset_envelope, onset_envelope, onset_envelope, onset_envelope, onset_envelope, onset_envelope, onset_envelope, onset_envelope, onset_envelope, onset_","def beat_track(y=None, sr=22050, onset_envelope=None, hop_length=512, start_bpm=120.0, tightness=100, trim=True, bpm=None, units='frames'): rif onset_envelope is None: if y is None: raise ParameterError('y or onset_envelope must be provided') onset_envelope = onset.onset_strength(y=y, sr=sr, hop_length=hop_length, aggregate=np.median) if not onset_envelope.any(): return (0, np.array([], dtype=int)) if bpm is None: bpm = tempo(onset_envelope=onset_envelope, sr=sr, hop_length=hop_length, start_bpm=start_bpm)[0] beats = __beat_tracker(onset_envelope, bpm, float(sr) / hop_length, tightness, trim) if units == 'frames': pass elif units =='samples': beats = core.frames_to_samples(beats, hop_length=hop_length) elif units == 'time': beats = core.frames_to_time(beats, hop_length=hop_length, sr=sr) else: raise ParameterError('Invalid unit type: '.format(units)) return (bpm, beats)"
1706,"def createURLAdapter(self, versionadded, versionchanged, versionchanged, versionchanged, versionchanged, versionchanged, versionchanged, versionadded, versionchanged, versionchanged, versionchanged, versionchanged, versionchanged, versionadded, versionchanged, versionchanged, versionchanged, versionchanged, versionchanged, versionchanged, versionadded, versionadded, versionchanged, versionchanged, versionchanged, versionchanged, versionadded)","def create_url_adapter(self, request): if request is not None: return self.url_map.bind_to_environ(request.environ, server_name=self.config['SERVER_NAME']) if self.config['SERVER_NAME'] is not None: return self.url_map.bind( self.config['SERVER_NAME'], script_name=self.config['APPLICATION_ROOT'] or '/', url_scheme=self.config['PREFERRED_URL_SCHEME'])"
1707,"def events_backtracked(self, onset_envelope=oenv, onset_bt_rms=0, onset_raw=0, onset_bt=0, onset_bt=0, onset_raw=0, onset_bt=0, onset_backtrack=0, onset_backtrack=0, onset_backtrack=0, onset_backtrack=0, onset_backtrack=0, onset_backtrack=0, onset_backtrack=0, onset_backtrack=0, onset_backtrack=0, onset_backtrack=0, onset_backtrack=0, onset_backtrack=0, onset_backtrack=0, onset_backtrack=0, onset_backtrack=0, onset_backtrack=0, onset_backtrack=0, onset_backtrack=0, onset_backtrack=0, onset_backtrack=0, onset_backtrack=0, onset_backtrack=0, onset_backtrack=0, onset_backtrack=0, onset_backtrack=0, onset_backtrack=0, onset_backtrack=0, onset_backtrack=0, onset_","def onset_backtrack(events, energy): minima = np.flatnonzero((energy[1:-1] = energy[:-2]) & (energy[1:-1]  energy[2:])) minima = util.fix_frames(1 + minima, x_min=0) return minima[util.match_events(events, minima, right=False)]"
1708,def send_payload(kwargs): kwargs = kwargs(kwargs): kwargs = kwargs(kwargs) return kwargs(kwargs),"def send(self, api_key=None, secret=None, list_data=None, auth=False, **kwargs): if auth: nonce = str(int(time.time() * 10000000)) auth_string = 'AUTH' + nonce auth_sig = hmac.new(secret.encode(), auth_string.encode(), hashlib.sha384).hexdigest() payload = 'event': 'auth', 'apiKey': api_key, 'authSig': auth_sig, 'authPayload': auth_string, 'authNonce': nonce payload = json.dumps(payload) elif list_data: payload = json.dumps(list_data) else: payload = json.dumps(kwargs) self.log.debug(""send(): Sending payload to API: %s"", payload) try: self.socket.send(payload) except websocket.WebSocketConnectionClosedException: self.log.error(""send(): Did not send out payload %s - client not connected. "", kwargs)"
1709,"def unitary_op(self, unitary_op, unitary_op, unitary_op, unitary_op, unitary_op, unitary_op, unitary_op, unitary_op, unitary_op, unitary_op, unitary_op, unitary_op, unitary_op, unitary_op, unitary_op, unitary_op, unitary_op, unitary_op, unitary_op) unitary_op, unitary_op, unitary_op, unitary_op, unitary_op, unitary_op, unitary_op, unitary_op, unitary_op, unitary_op, unitary_op, unitary_op, unitary_op, unitary_op, unitary_list, unitary_list, unitary_list, unitary_list, unitary_list, unitary_list, unitary_list, unitary_list, unitary_list, unitary_list, unitary_list, unitary_list, unitary_list, unitary_list, unitary_list, unitary_list, unitary_list, unitary_list, unit","def p_unitary_op_2(self, program): program[0] = node.CustomUnitary([program[1], program[2]]) self.verify_as_gate(program[1], program[2]) self.verify_reg_list(program[2], 'qreg') self.verify_distinct([program[2]])"
1710,"def read_log(ti, try_number, metadata): if ti = ti, try_number = ti, try_number = ti.read_log(ti, try_number) if ti = ti.read_log(ti, try_number) if ti = ti.read_log(ti, try_number) if ti = ti.read_log(ti) read_log(ti, try_number) read_log(ti, try_number) read_log(ti, try_number) read_log(ti, try_number) read_log(ti, try_number) read_log(ti, try_number) read_log(ti, try_number) read_log(ti, try_number) read_log(ti, try_number) read_log(ti, try_number) read_log(ti, try_number) read_log(ti, try_number) read_log(ti, try_number) read_log(ti, try_number) read_log(ti, try_number) read","def _read(self, ti, try_number, metadata=None): log_relative_path = self._render_filename(ti, try_number) remote_loc = os.path.join(self.remote_base, log_relative_path) if self.wasb_log_exists(remote_loc): remote_log = self.wasb_read(remote_loc, return_error=True) log = '*** Reading remote log from.nn'.format( remote_loc, remote_log) return log, 'end_of_log': True else: return super()._read(ti, try_number)"
1711,"def remove_task_instances(self, task_instances, task_instances, task_instances, task_instances, task_instances, task_instances, task_instances, task_instances, task_instances, task_instances, task_instances, task_instances, task_instances, task_instances)",def clear_dag_task_instances(): session = settings.Session() TI = TaskInstance tis = ( session.query(TI).filter(TI.dag_id.in_(DAG_IDS)).all() ) for ti in tis: logging.info('Deleting TaskInstance :: '.format(ti)) session.delete(ti) session.commit()
1712,"def convert_camel_case(self, name): if self.camel_case = self.camel_case = self.camel_case = self.camel_case = self.camel_case = self.camel_case","def convert_camel_case(name): s1 = re.sub('(.)([A-Z][a-z]+)', r'1_2', name) return re.sub('([a-z0-9])([A-Z])', r'1_2', s1).lower()"
1713,"def insert_raw_image(self, jpg, png, jpeg, png, jpeg, png, jpeg, png, jpeg, png, jpeg, png, jpeg, png, jpeg, png, jpeg, png, jpeg, png, jpeg, png, jpeg, png, jpeg, jpeg, jpeg, jpeg, jpeg, jpeg, jpeg, jpeg, jpeg, jpeg, jpeg, jpeg, jpeg, jpeg, jpeg, jpeg, jpeg, jpeg, jpeg, jpeg, jpeg, jpeg, jpeg, jpeg, jpeg, jpeg, jpeg, jpeg, jpeg, jpeg, jpeg, jpeg, jpeg, jpeg, jpeg, jpeg, jpe","def _insert_img(self, cursor, img, fmt): try: image = QtGui.QImage() image.loadFromData(img, fmt.upper()) except ValueError: self._insert_plain_text(cursor, 'Received invalid %s data.'%fmt) else: format = self._add_image(image) cursor.insertBlock() cursor.insertImage(format) cursor.insertBlock()"
1714,"def remove_nested_brackets(self, text): if self.remove_brackets(self.remove_brackets): self.remove_brackets(self.remove_brackets) self.remove_brackets(self.remove_brackets) self.remove_brackets(brackets) self.remove_brackets(brackets) self.remove_brackets) self.remove_brackets(brackets) self.remove_brackets(brackets) self.remove_brackets) self.remove_brackets(brackets) self.remove_brackets(brackets) self.remove_brackets) self.remove_brackets(brackets) self.remove_brackets(brackets) self.remove_brackets(brackets) self.remove_brackets(brackets) self.remove_brackets(brackets) self.remov","def remove_inner_forks(text): n = 1 while n: text, n = re.subn(r'([()]*)', '', text) return text"
1715,"def parse_dictionary(self, dictionary): self.parse_dictionary(dictionary): self.parsed_dictionary(dictionary): self.parsed_dictionary(dictionary)","def row_to_dict(self, row, allele, alternate_alleles): return 'build-chromosome-start-stop-allele'.format(**kwargs).upper() if allele == '.': allele = row.REF or allele genomic_coordinates =  'build': self.genome_build, 'chromosome': row.CHROM,'start': row.POS,'stop': row.POS + len(row.REF) - 1  variant_sbid = _variant_sbid(allele=allele, **genomic_coordinates) return  'genomic_coordinates': genomic_coordinates, 'variant': variant_sbid, 'allele': allele, 'row_id': row.ID,'reference_allele': row.REF, 'alternate_alleles': alternate_alleles, 'info': self._parse_info(row.INFO), 'qual': row.QUAL, 'filter': row.FILTER"
1716,"def manifest_attendee(self, attendee, attendee): return self.get_attendee(attendee, attendee, attendee)","def attendee(request, form, user_id=None): if user_id is None and form.cleaned_data[""user""] is not None: user_id = form.cleaned_data[""user""] if user_id is None: return attendee_list(request) attendee = people.Attendee.objects.get(user__id=user_id) name = attendee.attendeeprofilebase.attendee_name() reports = [] profile_data = [] try: profile = people.AttendeeProfileBase.objects.get_subclass( attendee=attendee ) fields = profile._meta.get_fields() except people.AttendeeProfileBase.DoesNotExist: fields = [] exclude = set([""attendeeprofilebase_ptr"", ""id""]) for field in fields: if field.name in exclude: continue if not hasattr(field, ""verbose_name""): continue value = getattr(profile, field.name) if isinstance(field, models.ManyToManyField): value = "", "".join(str(i) for i in value.all()) profile_data.append((field.verbose_name, value)) cart = CartController.for_user(attendee.user) reservation = cart.cart.reservation_duration + cart.cart.time_last_updated profile_data.append((""Current cart reserved until"", reservation)) reports.append(ListReport(""Profile"", ["""", """"], profile_data)) links = [] links.append(( reverse(views.badge, args=[user_id]), ""View badge"", )) links.append(( reverse(views.amend_registration, args=[user_id]), ""Amend current cart"", )) links.append(( reverse(views.extend_reservation"
1717,"def broadcast_event(self, event, sample): if event(self, sample): if event(self, sample): if event(self, sample): if event(self, sample): if event(self, sample): if event(self, sample): if event(self, sample): if event(self, sample): if event(self, sample): if event(self, sample): if event(self, sample): if event(self): if event(self)","def _broadcast_event_and_samples(event, samples, event_ndims): samples_shape = tf.concat( [tf.shape(input=samples)[:-event_ndims - 1], tf.shape(input=samples)[tf.rank(samples) - event_ndims:]], axis=0) event *= tf.ones(samples_shape, dtype=event.dtype) event = tf.expand_dims(event, axis=-event_ndims - 1) samples *= tf.ones_like(event, dtype=samples.dtype) return event, samples"
1718,"def logging.log(level, format, *format_args): if logging.log(level, format, *format_args): if logging.log(level, format, *format_args): if logging.log(level, format, *format_args): if logging.log(level, format, *format_args): if logging.log(level, format, *format_args): if logging.log(level, *format_args): if logging.log(level, *format_args): if logging.log(level, *format_args): if logging.log(level, *format_args): if logging.log(level, *format_args): if logging.log(level, *format_args): logging.log(level, *format_args): logging.log(level, *format_args): logging.log(level, *format_args): logging.log(level, *format_args): logging.log(level, *format_args): logging.log(level, *format_args): logging.log(level, *format_arg","def _check_log_method(self, node, name): if name == ""log"": if node.starargs or node.kwargs or len(node.args)  2: return format_pos = 1 elif name in CHECKED_CONVENIENCE_FUNCTIONS: if node.starargs or node.kwargs or not node.args: return format_pos = 0 else: return if isinstance(node.args[format_pos], astroid.BinOp): binop = node.args[format_pos] emit = binop.op == ""%"" if binop.op == ""+"": total_number_of_strings = sum( 1 for operand in (binop.left, binop.right) if self._is_operand_literal_str(utils.safe_infer(operand)) ) emit = total_number_of_strings > 0 if emit: self.add_message(""logging-not-lazy"", node=node) elif isinstance(node.args[format_pos], astroid.Call): self._check_call_func(node.args[format_pos]) elif isinstance(node.args[format_pos], astroid.Const): self._check_format_string(node, format_pos) elif isinstance( node.args[format_pos], (astroid.FormattedValue, astroid.JoinedStr) ): self.add_message(""logging-fstring-interpolation"", node=node)"
1719,"def _input_flags(self, flags): if flags are None: if flags are None: if flags are None: if flags are None: if flags are None: if flags are None: if flags are None: if flags are None: if flags are None: if flags are None: if flags are None: if flags are None: if flags are None: if flags are None","def usage(): global g_script_name print("""") print(""Usage: "" + g_script_name + "" [...options...]"") print("""") print("" --help print out this help menu and show all the valid flags and inputs."") print("""") print("" --inputfileadd filename where the new java messages to ignore are stored in."") print("""") print("" --inputfilerm filename where the java messages are removed from the ignored list."") print("""") print("" --loadjavamessage filename pickle file that stores the dict structure containing java messages to include."") print("""") print("" --savejavamessage filename pickle file that saves the final dict structure after update."") print("""") print("" --printjavamessage filename print java ignored java messages stored in pickle file filenam onto console and save into a text file."") print("""") sys.exit(1)"
1720,"def png_filename_base(self, png_filename_base): png_filename_base = png_filename_base = png_filename_base = png_filename_base = png_filename_base = png_filename_base = png_filename_base = png_filename_base = png_filename_base = png_filename_base = png_filename_base = png_filename_base = png_filename_base = png_filename_base = png_filename_base = png_filename_base = png_filename_base = png_filename_base = png_filename_base = png_filename_base = png_filename_base = png_filename_base = png_filename_base = png_filename_base = png_filename_base = png_filename_base = png_filename_base = png_filename_base = png_filename_base = png_filename_base = png_filename",def load_collection(png_filename_base): files = glob.glob(png_filename_base) files.sort() numpy_data = [] for f in files: numpy_data.append(load(f)) return numpy.concatenate(numpy_data)
1721,def return_artilce_page_content(artilce/page): return artilce_page_content(artilce/page) return artilce_page_content(artilce/page),"def content_to_html(content, article_id): if node['type'] == 'paragraph': return html + 'p>%s/p>' % node['data'] else: if node['type'] == 'ad': id = 'div-gpt-ad-1443288719995-' + str(10 + index) + '-' + str(article_id) dfp_type = 'Intra_Article_' + str(index + 1) size = 'banner' if node['data'] =='mobile': size = 'box' newString = 'div class=""o-article-embed__advertisement"">div class=""o-advertisement o-advertisment--banner o-advertisement--center"">div class=""adslot"" id=""' + id + '"" data-size=""' + size + '"" data-dfp=""' + dfp_type + '"">/div>/div>/div>' return html + 'div class=""o-article-embed o-article-embed--advertisement"">%s/div>n' % newString try: if node['type'] == 'poll': node['type'] = 'widget' node['data']['data'] = node['data'] return html + embeds.render(node['type'], node['data']) except EmbedException: return html html = '' index = 0 for node in content: html = render_node(html, node, index) if (node['type'] == 'ad'): index += 1 return mark_safe(html)"
1722,"def NDArray(self, envelope): if envelope is not self.get(envelope): if envelope is not self.get(envelope): return self.get(envelope)","def array(self, envelope=()): args = () if envelope: args = self.get_offset(envelope) return self.ds.ReadAsArray(*args)"
1723,"def predict_on_a_data(test_data, test_data): test_data = test_data.predict(test_data, test_data, test_data, test_data, test_data, test_data, test_data): test_data = test_data.predict(test_data, test_data, test_data, test_data, test_data)","def predict(self, test_data): if self._fetch(): self._model = h2o.get_model(self._leader_id) return self._model.predict(test_data) print(""No model built yet..."")"
1724,"def dataWidth(dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWidth, dataWi","def packData(self, data): typeOfWord = simBitsT(self.wordWidth, None) fieldToVal = self._fieldToTPart if fieldToVal is None: fieldToVal = self._fieldToTPart = self.fieldToDataDict( self.origin.dtype, data, ) for _, transParts in self.walkWords(showPadding=True): actualVldMask = 0 actualVal = 0 for tPart in transParts: high, low = tPart.getBusWordBitRange() fhigh, flow = tPart.getFieldBitRange() if not tPart.isPadding: val = fieldToVal.get(tPart.tmpl.origin, None) else: val = None if val is None: newBits = 0 vld = 0 else: newBits = selectBitRange(val, flow, fhigh - flow) vld = mask(high - low)  low actualVal = setBitRange(actualVal, low, high - low, newBits) actualVldMask = setBitRange(actualVal, low, high - low, vld) yield typeOfWord.getValueCls()(actualVal, typeOfWord, actualVldMask, -1)"
1725,"def load_model(path, dir=my_path, h2o.save_model, dir=my_path, h2o.load_model, h2o.load_model, h2o.load_model, h2o.load_model, h2o.load_model, h2o.load_model, h2o.load_model, h2o.load_model, h2o.load_model, h2o.load_model, h2o.load_model, h2o.load_model, h2o.load_model, h2o.load_model, h2o.load_model, h2o.load_model, h2o.load_model, h2o.load_model, h2o.load_model, h2o.load_model, h2o.load_model, h2o.load_model, h2o.load_model, h2o.load_model, h2o.load_model, h2o.load_model, h2o.load_model, h2o.load_model, h2o.load_model, h2o.load_","def load_model(path): assert_is_type(path, str) res = api(""POST /99/Models.bin/%s"" % """", data=""dir"": path) return get_model(res[""models""][0][""model_id""][""name""])"
1726,"def template_fields(self, template_fields): if template_fields = self.___________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________","def template_field_role(app, typ, rawtext, text, lineno, inliner, options=, content=[]): text = utils.unescape(text) try: template_fields = get_template_field(app.env, text) except RoleException as e: msg = inliner.reporter.error(""invalid class name %s n%s"" % (text, e, ), line=lineno) prb = inliner.problematic(rawtext, rawtext, msg) return [prb], [msg] node = nodes.inline(rawtext=rawtext) for i, field in enumerate(template_fields): if i!= 0: node += nodes.Text("", "") node += nodes.literal(field, """", nodes.Text(field)) return [node], []"
1727,"def delete_token(project_name, dataset_name, channel_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name, token_name,","def delete_token(self, token_name, project_name, dataset_name): url = self.url() + ""/nd/resource/dataset/"".format(dataset_name) + ""/project/"".format(project_name) + ""/token//"".format(token_name) req = self.remote_utils.delete_url(url) if req.status_code is not 204: raise RemoteDataUploadError(""Could not delete "".format(req.text)) if req.content == """" or req.content == b'': return True else: return False"
1728,"def get_help_info(self, context, function): self.get_help_info(self.get_help_info) self.get_help_info(self.get_help_info) self.get_help_info(self.get_help_info) self.get_help_info(self.get_help_info) self.get_help_info(self.get_help_info) self.get_help_info(self.get_help_info) self.get_help_info(self.get_help_info) self.get_help_info(self.get_help_info) self.get_help_info(self.get_help_info) self.get_help_info(self.get_help_info) self.get_help_info(self.get_help_info) self.get_help_info(self.get_help_info) self.get_help_info(self.get_help_info) self.get_help_info(self.get_help_info(self.get_help_info) self.get_help_info(self.get_help_info(self.get_help_info) self.get_help_info(self.get_help_info) self.get_help_info(self.get_","def _builtin_help(self, args): if len(args) == 0: return self.list_dir(self.contexts[-1]) if len(args) == 1: func = self.find_function(self.contexts[-1], args[0]) return annotate.get_help(func) help_text = ""Too many arguments: "" + str(args) + ""n"" help_text += ""Usage: help [function]"" return help_text"
1729,"def est_idx(self, est_idx, est_labels, est_idx, est_labels, est_idx, est_labels, est_idx, est_labels, est_idx, est_labels, est_idx, est_labels, est_idx, est_labels, est_idx, est_labels, est_idx, est_labels, est_labels, est_labels, est_labels, est_labels, est_labels, est_labels, est_labels, est_labels, est_labels, est_labels, est_idx, est_labels, est_labels, est_labels, est_labels, est_idx, est_labels, est_labels, est_labels, est_labels, est_labels, est_idx, est_labels, est_labels, est_labels, est_labels, est_idx, est_idx, est_idx, est_labels, est","def processFlat(self): F = self._preprocess() F = U.normalize(F, norm_type=self.config[""label_norm_feats""], floor=self.config[""label_norm_floor""], min_db=self.config[""label_norm_min_db""]) est_labels = compute_similarity(F, self.in_bound_idxs, dirichlet=self.config[""dirichlet""], xmeans=self.config[""xmeans""], k=self.config[""k""], offset=self.config[""2dfmc_offset""]) self.in_bound_idxs, est_labels = self._postprocess(self.in_bound_idxs, est_labels) return self.in_bound_idxs, est_labels"
1730,"def onset_force_function(self, onset_force_function): if onset_force_function(self, onset_force_function): if onset_force_function(self, onset_force_function): if onset_force_function(self, onset_force_function): if onset_force_function(self, onset_force_function): if onset_force_function(self, onset_force_function): if onset_force_function(self, onset_force_function): onset_force_function(self, onset_force_function): onset_force_function(self, onset_force_function): onset_force_function(self, onset_force_function): onset_force_function(self, onset_force_function): onset_force_function(self): onset_force_function(self): onset_force_function(self): onset_force_function(self): onset = onset = onset = onset = onset = onset = onset = onset = onset = onset = onset = onset = onset = onset = onset = onset = onset = onset = onset =",def __normalize_onsets(onsets): norm = onsets.std(ddof=1) if norm > 0: onsets = onsets / norm return onsets
1731,"def subscribe_user_token(user_token, topic, user_token, topic, user_token, topic, user_token, user_token, topic, user_token, topic, user_token, topic, user_token, topic, user_token, topic, user_token, topic, user_token, topic, user_token, topic, user_token, user_token, topic, user_token, topic, user_token, topic, user_token, user_token, user_token, user_token, user_token, user_token, user_token, user_token, user_token, user_token, user_token, user_token, user_token, user_token, user_token, user_token, user_token, user_token, user_token, user_token, user_token, user_token, user_token, user_token, user_token, user_token, user_token, user_token, user_token, user_token, user_token, user_token, user_token, user_token, user_token, user_token, topic,","def subscribe(self, user_token, topic): response = _request('POST', url=self.url_v1('/user/subscriptions/' + topic), user_agent=self.user_agent, user_token=user_token, ) _raise_for_status(response)"
1732,"def force_put(self, force_put, force_put, force_put, force_put, force_put, force_put, force_put, force_put, force_put, force_put, force_put, force_put, force_put, force_put, force_put, force_put, force_put, force_put, force_put, force_put, force_put, force_put, force_put, force_put, force_put, force_put)","def forceupdate(self, *args, **kw): self._update(False, self._ON_DUP_OVERWRITE, *args, **kw)"
1733,"def create_gaussian_kernel(self, gaussian_kernel, gaussian_kernel, gaussian_kernel, gaussian_kernel, gaussian_kernel, gaussian_kernel, gaussian_kernel, gaussian_kernel, gaussian_kernel, gaussian_kernel, gaussian_kernel, gaussian_kernel, gaussian_kernel, gaussian_kernel, gaussian_kernel, gaussian_kernel)","def compute_gaussian_krnl(M): g = signal.gaussian(M, M // 3., sym=True) G = np.dot(g.reshape(-1, 1), g.reshape(1, -1)) G[M // 2:, :M // 2] = -G[M // 2:, :M // 2] G[:M // 2, M // 2:] = -G[:M // 2, M // 2:] return G"
1734,"def tuple(code, translation): return tuple(code, translation) if tuple(code, translation): return tuple(code, translation) return tuple(code, translation)","def get_translations_sorted(codes): codes = codes or self.codes return self._get_priority_translations(priority, codes)"
1735,def chromosome(str): chromosome(str): chromosome(str): chromosome(str): chromosome(str): chromosome(str): chromosome(str): chromosome(str): chromosome(str): chromosome(str): chromosome(str): chromosome(str): chromosome(str): chromosome(str): chromosome(str): chromosome(str): chromosome(str): chromosome(str) coordinates(dict)) coordinates(dict)) coordinates(dict) coordinates(dict) coordinates(dict)) coordinates(dict) coordinates(dict) coordinates) coordinates(dict) coordinates) coordinates = coordinates = coordinates = coordinates = coordinates = coordinates = coordinates = coordinates = coordinates = coordinates = coordinates = coordinates = coordinates = coordinates = coordinates = coordinates = coordinates = coordinates = coordinates = coordinates = coordinates = coordinates = coordinates =,"def check_coordinates(chromosome, pos, coordinates): chrom_match = CHR_PATTERN.match(chromosome) chrom = chrom_match.group(2) if chrom!= coordinates['chrom']: return False if (pos >= coordinates['start'] and pos = coordinates['end']): return True return False"
1736,"def stdout_txt(self, stdout_txt, stdout_txt, stdout_txt, stdout_txt, stdout_txt, stdout_txt, stdout_txt, stdout_txt, stdout_txt, stdout_txt, stdout_txt, stdout_txt, stdout_txt, stdout_txt, stdout_txt, stdout_txt, stdout_txt, stdout_txt, stdout_txt, stdout_txt, stdout_txt, stdout_txt, stdout_txt, stdout_txt, stdout_txt, stdout_txt, stdout_txt, stdout_txt, stdout","def execute_get_text(command): try: completed = subprocess.run( command, check=True, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE ) except subprocess.CalledProcessError as err: raise else: print(completed.stdout.decode('utf-8') + str("":"") + completed.stderr.decode(""utf-8"")) return completed.stdout.decode('utf-8') + completed.stderr.decode(""utf-8"")"
1737,"def initialize_lists_path(self, mailing lists): if self.lists_path(lists_path): self.lists_path(lists_path) self.lists_path(lists_path) self.lists_path(lists_path) self.lists_path(lists_path) self.lists_path(lists_path) self.lists_path(lists_path) self.lists_path(lists_path) self.lists_path(lists_path) self.lists_path(lists_path) self.lists_path) self.lists_path(lists_path) self.lists_path) self.lists_path(lists_path) self.lists_path(lists_path) self.lists_path) self.lists_path(lists_path) self.lists_path) self.lists_path(lists_path) self.lists_path) self.lists_path(lists_path) self.lists_path) self.lists_path(lists_path) self.lists_path(lists_path) self.lists_path) self.lists_path(lists_path) self.list","def _pre_init(self): if not self.parsed_args.mboxes_path: base_path = os.path.expanduser('/.perceval/mailinglists/') dirpath = os.path.join(base_path, GROUPSIO_URL, 'g', self.parsed_args.group_name) else: dirpath = self.parsed_args.mboxes_path setattr(self.parsed_args, 'dirpath', dirpath)"
1738,"def get_device_management_request(device_management_request, requestId, requestId, requestId, requestId, requestId, requestId, requestId, requestId, requestId, requestId, requestId, requestId, requestId, requestId, requestId, requestId, requestId, requestId, requestId, requestId, requestId, requestId, requestId, requestId, requestId, requestId, requestId, requestId)","def get(self, requestId): url = MgmtRequests.mgmtSingleRequest % (requestId) r = self._apiClient.get(url) if r.status_code == 200: return r.json() else: raise ApiException(r)"
1739,"def marquee('A test',40,'-'): marquee('A test',40,'-'): marquee('A test',40,'-'): marquee('A test',40,'-'): marquee('A test',40,'-'): marquee('A test',40,'-')","def marquee(txt='',width=78,mark='*'): if not txt: return (mark*width)[:width] nmark = (width-len(txt)-2)//len(mark)//2 if nmark  0: nmark =0 marks = mark*nmark return '%s %s %s' % (marks,txt,marks)"
1740,"def tree_admin_urls(self, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items, tree_items","def get_urls(self): urls = super(TreeAdmin, self).get_urls() prefix_change = 'change/' if DJANGO_POST_19 else '' sitetree_urls = [ url(r'change/$', redirects_handler, name=get_tree_item_url_name('changelist')), url(r'((?Ptree_id>d+)/)?%sitem_add/$' % prefix_change, self.admin_site.admin_view(self.tree_admin.item_add), name=get_tree_item_url_name('add')), url(r'(?Ptree_id>d+)/%sitem_(?Pitem_id>d+)/$' % prefix_change, self.admin_site.admin_view(self.tree_admin.item_edit), name=get_tree_item_url_name('change')), url(r'%sitem_(?Pitem_id>d+)/$' % prefix_change, self.admin_site.admin_view(self.tree_admin.item_edit), name=get_tree_item_url_name('change')), url(r'((?Ptree_id>d+)/)?%sitem_(?Pitem_id>d+)/delete/$' % prefix_change, self.admin_site.admin_view(self.tree_admin.item_delete), name=get_tree_item_url_name('delete')), url(r'((?Ptree_id>d+)/)?%sitem_(?Pitem_id>d+)/history/$' % prefix_change, self"
1741,"def print_content(self, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table, table)","def show(self, header=True): if header and self._table_header: print(self._table_header + "":"", end=' ') if self._table_description: print(self._table_description) print() table = copy.deepcopy(self._cell_values) nr = 0 if _is_list_of_lists(table): nr = len( table) if nr > 20: trunc_table = [] trunc_table += [v for v in table[:5]] trunc_table.append([""---""] * len(table[0])) trunc_table += [v for v in table[(nr - 5):]] table = trunc_table H2ODisplay(table, self._col_header, numalign=""left"", stralign=""left"") if nr > 20 and can_use_pandas(): print('nSee the whole table with table.as_data_frame()')"
1742,"def add_parameter(self, new_parameter, data=42, comment='Example!'): self.add_parameter(self.add_parameter) self.add_parameter(self.add_parameter) self.add_parameter(self.add_parameter) self.add_parameter(self.add_parameter) self.add_parameter(self.add_parameter) self.add_parameter(self.add_parameter) self.add_parameter(self.add_parameter) self.add_parameter) self.add_parameter(self.add_parameter) self.add_parameter) self.add_parameter(self.add_parameter) self.add_parameter(self.add_parameter) self.add_parameter) self.add_parameter(self.add_parameter) self.add_parameter) self.add_parameter(self.add_parameter) self.add_parameter(self.add_parameter) self.add_parameter(self.add_parameter) self.add_parameter(self.add_parameter) self.add_parameter(self.add_parameter) self.add_parameter(self.add_parameter) self.add_para","def f_add_parameter(self, *args, **kwargs): return self._nn_interface._add_generic(self, type_name=PARAMETER, group_type_name=PARAMETER_GROUP, args=args, kwargs=kwargs)"
1743,"def _filterbank(self, filterbank): if self.filterbank(self.filterbank): self.filterbank(self.filterbank): self.filterbank(self.filterbank) self.filterbank(self.filterbank) self.filterbank(self.filterbank) self.filterbank(self.filterbank) self.filterbank(self.filterbank) self.filterbank(self.filterbank) self.filterbank(self.filterbank) self.filterbank(self.filterbank) self.filterbank) self.filterbank(self.filterbank)","def is_filterbank(filename): with open(filename, 'rb') as fh: is_fil = True try: keyword, value, idx = read_next_header_keyword(fh) try: assert keyword == b'HEADER_START' except AssertionError: is_fil = False except KeyError: is_fil = False return is_fil"
1744,"def filename_out(self, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out, filename_out)","def __write_to_hdf5_light(self, filename_out, *args, **kwargs): block_size = 0 with h5py.File(filename_out, 'w') as h5: h5.attrs[b'CLASS'] = b'FILTERBANK' h5.attrs[b'VERSION'] = b'1.0' if HAS_BITSHUFFLE: bs_compression = bitshuffle.h5.H5FILTER bs_compression_opts = (block_size, bitshuffle.h5.H5_COMPRESS_LZ4) else: bs_compression = None bs_compression_opts = None logger.warning(""Warning: bitshuffle not found. No compression applied."") dset = h5.create_dataset('data', data=self.data, compression=bs_compression, compression_opts=bs_compression_opts) dset_mask = h5.create_dataset('mask', shape=self.file_shape, compression=bs_compression, compression_opts=bs_compression_opts, dtype='uint8') dset.dims[0].label = b""frequency"" dset.dims[1].label = b""feed_id"" dset.dims[2].label = b""time"" dset_mask.dims[0].label = b""frequency"" dset_mask.dims[1].label = b""feed_id"" dset_mask.dims[2].label = b""time"" for key, value in self.header.items(): dset.attrs[key] = value"
1745,"def patch_properties(self, agent): if self.patch_properties(self.patch_properties): self.patch_properties(self.patch_properties) self.patch_properties(self.patch_properties) self.patch_properties(self.patch_properties) self.patch_properties(self.patch_properties) self.patch_properties(self.patch_properties) self.patch_properties(self.patch_properties) self.patch_properties(self.patch_properties) self.patch_properties) self.patch_properties(self.patch_properties) self.patch_properties(self.patch_properties) self.patch_properties) self.patch_properties(self.patch_properties) self.patch_properties) self.patch_properties(self.patch_properties) self.patch_properties(self.patch_properties) self.patch_properties(self.patch_properties) self.patch","def patch_transport_fake_push_producer(transport): patch_if_missing(transport, 'pauseProducing', lambda: None) patch_if_missing(transport,'resumeProducing', lambda: None) patch_if_missing(transport,'stopProducing', transport.loseConnection)"
1746,"def run_hook_func(self, hook_func): hook_func = self.crawl(self.crawl, hook_func)","async def _run_spider_hook(self, hook_func): if callable(hook_func): try: aws_hook_func = hook_func(weakref.proxy(self)) if isawaitable(aws_hook_func): await aws_hook_func except Exception as e: self.logger.error(f'Hook hook_func.__name__: e')"
1747,"r"">== r"">= r"">= r"">= r"">= r"">= r"">= r"">= r"">= r"">= r"">= r"">= r"">= r"">= r"">= r"">=","def t_GE(self, t): r"">="" t.endlexpos = t.lexpos + len(t.value) return t"
1748,"def get_legal_agreements(name, for_transfer, for_privacy, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, raw, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer, for_transfer","def list_agreements( self, name, include_privacy=None, for_transfer=None, custom_headers=None, raw=False, **operation_config): agreement_option = models.TopLevelDomainAgreementOption(include_privacy=include_privacy, for_transfer=for_transfer) def internal_paging(next_link=None, raw=False): if not next_link: url = self.list_agreements.metadata['url'] path_format_arguments =  'name': self._serialize.url(""name"", name,'str'),'subscriptionId': self._serialize.url(""self.config.subscription_id"", self.config.subscription_id,'str')  url = self._client.format_url(url, **path_format_arguments) query_parameters =  query_parameters['api-version'] = self._serialize.query(""self.api_version"", self.api_version,'str') else: url = next_link query_parameters =  header_parameters =  header_parameters['Accept'] = 'application/json' header_parameters['Content-Type'] = 'application/json; charset=utf-8' if self.config.generate_client_request_id: header_parameters['x-ms-client-request-id'] = str(uuid.uuid1()) if custom_headers: header_parameters.update(custom_headers) if self.config.accept_language is not None: header_parameters['accept-language'] = self._serialize.header(""self.config.accept_language"", self.config.accept_language,'str') body_content = self"
1749,"def pprint_factory(self, pprint_factory, pprint_factory, pprint_factory, pprint_factory, pprint_factory, pprint_factory, pprint_factory, pprint_factory, pprint_factory, pprint_factory, pprint_factory, pprint_factory, pprint_factory, pprint_factory, pprint_factory): pprint_factory, pprint_factory, pprint_factory, pprint_factory, pprint_factory, pprint_factory, pprint_factory, pprint_factory, pprint_factory, pprint_factory, pprint_factory, pprint_factory, pprint_factory, pprint_factory, pprint_factory, pprint_factory, pprint_factory, pprint_factory, pprint_factory, pprint_factory, pprint_factory, pprint_factory, pprint_factory, pprint_factory, pprint_factory, pprint_factory,","def _dict_pprinter_factory(start, end, basetype=None): def inner(obj, p, cycle): typ = type(obj) if basetype is not None and typ is not basetype and typ.__repr__!= basetype.__repr__: return p.text(typ.__repr__(obj)) if cycle: return p.text('...') p.begin_group(1, start) keys = obj.keys() try: keys.sort() except Exception, e: pass for idx, key in enumerate(keys): if idx: p.text(',') p.breakable() p.pretty(key) p.text(': ') p.pretty(obj[key]) p.end_group(1, end) return inner"
1750,"def self_attuned(self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_attuned, self_at","def multiplicative_self_attention(units, n_hidden=None, n_output_features=None, activation=None): n_input_features = units.get_shape().as_list()[2] if n_hidden is None: n_hidden = n_input_features if n_output_features is None: n_output_features = n_input_features queries = tf.layers.dense(expand_tile(units, 1), n_hidden, kernel_initializer=INITIALIZER()) keys = tf.layers.dense(expand_tile(units, 2), n_hidden, kernel_initializer=INITIALIZER()) scores = tf.reduce_sum(queries * keys, axis=3, keep_dims=True) attention = tf.nn.softmax(scores, dim=2) attended_units = tf.reduce_sum(attention * expand_tile(units, 1), axis=2) output = tf.layers.dense(attended_units, n_output_features, activation, kernel_initializer=INITIALIZER()) return output"
1751,"def read(input_buffer, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, versionNotSupported, versionNotSupported, versionNotSupported, versionNotSupported, versionNotSupported, versionNotSupported, versionNotSupported, versionNotSupported, versionNotSupported, versionNotSupported)","def read(self, input_buffer, kmip_version=enums.KMIPVersion.KMIP_2_0): if kmip_version  enums.KMIPVersion.KMIP_2_0: raise exceptions.VersionNotSupported( ""KMIP  does not support the DefaultsInformation "" ""object."".format( kmip_version.value ) ) super(DefaultsInformation, self).read( input_buffer, kmip_version=kmip_version ) local_buffer = utils.BytearrayStream(input_buffer.read(self.length)) object_defaults = [] while self.is_tag_next(enums.Tags.OBJECT_DEFAULTS, local_buffer): object_default = ObjectDefaults() object_default.read(local_buffer, kmip_version=kmip_version) object_defaults.append(object_default) if len(object_defaults) == 0: raise exceptions.InvalidKmipEncoding( ""The DefaultsInformation encoding is missing the object "" ""defaults structure."" ) else: self._object_defaults = object_defaults self.is_oversized(local_buffer)"
1752,"def add_new_email_address(self, selected_address_books, selected_address_books, selected_address_books, selected_address_books, selected_address_books, selected_address_books, selected_address_books, selected_address_books, selected_address_books, selected_address_books, selected_address_books, selected_address_books, selected_address_book.AddressBook.AddressBook.AddressBook.AddressBook.AddressBook.AddressBook.AddressBook.AddressBook.AddressBook.AddressBook)","def add_email_subcommand(input_from_stdin_or_file, selected_address_books): message = message_from_string(input_from_stdin_or_file, policy=SMTP_POLICY) print(""Khard: Add email address to contact"") if not message['From']  or not message['From'].addresses: print(""Found no email address"") sys.exit(1) email_address = message['From'].addresses[0].addr_spec name = message['From'].addresses[0].display_name print(""Email address: %s"" % email_address) if not name: name = input(""Contact's name: "") selected_vcard = choose_vcard_from_list( ""Select contact for the found e-mail address"", get_contact_list_by_user_selection(selected_address_books, name, True)) if selected_vcard is None: while True: input_string = input(""Contact %s does not exist. Do you want "" ""to create it (y/n)? "" % name) if input_string.lower() in ["""", ""n"", ""q""]: print(""Canceled"") sys.exit(0) if input_string.lower() == ""y"": break selected_address_book = choose_address_book_from_list( ""Select address book for new contact"", config.abooks) if selected_address_book is None: print(""Error: address book list is empty"") sys.exit(1) while True: first_name = input(""First name: "") last_name = input(""Last name: "") organisation = input(""Organisation: "") if not first_name and not last_name and not organisation: print(""Error: All fields are empty."") else: break selected_vcard = CarddavObject.from_user_input( selected_"
1753,"def connect(self, connection): return self.connect(connection)","def get_conn(self): db = self.get_connection(getattr(self, self.conn_name_attr)) return self.connector.connect( host=db.host, port=db.port, username=db.login, schema=db.schema)"
1754,"def change(self, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth) cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cmeth, cm","def class_method(self, cmeth: typing.Optional[typing.Callable[..., typing.Any]]) -> ""SeparateClassMethod"": self.__class_method = cmeth return self"
1755,"def read_list(self, file_items): self.read_list(file_items) self.read_list(file_items) self.read_list(file_items) self.read_list(file_items) self.read_list(file_items) self.read_list(file_items) self.read_list(file_items) self.read_list(file_items) self.read_list(file_items) self.read_list(file_items)","def read_list_from_file(file_path, quiet=False): try: if not check_if_file_exists(file_path, quiet=quiet): return [] with codecs.open(file_path, ""r"", encoding=""utf-8"") as f: content = f.readlines() if sys.version_info[0]  3: content = [str(item.encode('utf8')) for item in content] content = [item.strip() for item in content] return [i for i in content if i] except Exception as exception: print(str(exception)) return []"
1756,def ________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________,"def web_application_firewall_policies(self): api_version = self._get_api_version('web_application_firewall_policies') if api_version == '2018-12-01': from.v2018_12_01.operations import WebApplicationFirewallPoliciesOperations as OperationClass elif api_version == '2019-02-01': from.v2019_02_01.operations import WebApplicationFirewallPoliciesOperations as OperationClass else: raise NotImplementedError(""APIVersion  is not available"".format(api_version)) return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))"
1757,"def register_custom_template_filter(self, name): if self.filter(self.filter): self.filter(self.filter) self.filter(self.filter) self.filter(self.filter) self.filter(self.filter) self.filter(self.filter) self.filter(self.filter) self.filter(self.filter) self.filter(self.filter) self.filter(self.filter) self.filter(self.filter) self.filter(self.filter) self.filter(self.filter) self.filter(self.filter) self.filter(self.filter) self.filter(self.filter) self.filter(self.filter) self.filter(self.filter) self.filter) self.filter(self.filter) self.filter) self.filter(self.filter) self.filter(self.filter) self.filter) self.filter(self.filter) self.filter(self.filter) self.filter) self.filter(self.filter) self.filter(self.filter) self.filter) self.filter(self.filter) self.filter(self.filter) self.filter(self.filter) self.filter(self.filter) self.filter(self.filter) self.filter(self.filter) self.filter(self.filter) self.filter(","def template_filter(self, name=None): def decorator(f): self.add_template_filter(f, name=name) return f return decorator"
1758,"def extract_X.509_certificate(self, X.509_certificate, X.509_certificate, X.509_certificate, X.509_certificate, X.509_certificate, X.509_certificate, X.509_certificate, X.509_certificate)","def get_certificate_from_connection(connection): certificate = connection.getpeercert(binary_form=True) if certificate: return x509.load_der_x509_certificate( certificate, backends.default_backend() ) return None"
1759,"def greenlets(self, websockets, postgreSQL, greenlets): greenlets = greenlets.get_websocket(greenlets.get_websocket(greenlets))","def serve(listen, verbosity=1, debug_port=0, **ssl_args): launcher.logger.info( 'Received signal %s in frame %r', sigmap.get(signum, signum), frame, ) launcher.stop() for signum in [signal.SIGINT, signal.SIGQUIT]: gevent.signal(signum, sighandler) launcher.run()"
1760,"def read_all_patches(self, patch_name): if patch_name is None: if patch_name is None: if patch_name is None: if patch_name is None: if patch_name is None: if patch_name is None: if patch_name is None: if patch_name is None: if patch_name is None: if patch_name is None: if patch_name is None: if patch_name is None: if patch_name is None: if patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch_name = patch","def read(self): self.patchlines = [] self.patch2line = dict() if self.exists(): with open(self.series_file, ""r"") as f: for line in f: self.add_patch(line)"
1761,"def get_frequency(self, frequency): self.frequency(self.frequency): self.frequency(self.frequency) self.frequency(self.frequency) self.frequency(self.frequency) self.frequency(self.frequency) self.frequency(self.frequency) self.frequency(self.frequency) self.frequency(self.frequency) self.frequency) self.frequency) self.frequency = self.frequency = self.frequency = self.frequency = self.frequency = self.frequency = self.frequency","def __coord_mel_hz(n, fmin=0, fmax=11025.0, **_kwargs): if fmin is None: fmin = 0 if fmax is None: fmax = 11025.0 basis = core.mel_frequencies(n, fmin=fmin, fmax=fmax) basis[1:] -= 0.5 * np.diff(basis) basis = np.append(np.maximum(0, basis), [fmax]) return basis"
1762,"def use_batch_norm(n_tokens, n_hidden_list, n_hidden_list, n_hidden_list, n_hidden_list, n_hidden_list, n_hidden_list, n_hidden_list, n_hidden_list, n_hidden_list, n_hidden_list, n_hidden_list, n_hidden_list, n_hidden_list, n_hidden_list, n_hidden_list, n_hidden_list, n_hidden_list)","def u_shape(units: tf.Tensor, n_hidden_list: List, filter_width=7, use_batch_norm=False, training_ph=None): units_for_skip_conn = [] conv_net_params = 'filter_width': filter_width, 'use_batch_norm': use_batch_norm, 'training_ph': training_ph for n_hidden in n_hidden_list: units = stacked_cnn(units, [n_hidden], **conv_net_params) units_for_skip_conn.append(units) units = tf.layers.max_pooling1d(units, pool_size=2, strides=2, padding='same') units = stacked_cnn(units, [n_hidden], **conv_net_params) for down_step, n_hidden in enumerate(n_hidden_list[::-1]): units = tf.expand_dims(units, axis=2) units = tf.layers.conv2d_transpose(units, n_hidden, filter_width, strides=(2, 1), padding='same') units = tf.squeeze(units, axis=2) skip_units = units_for_skip_conn[-(down_step + 1)] if skip_units.get_shape().as_list()[-1]!= n_hidden: skip_units = tf.layers.dense(skip_units, n_hidden) units = skip_units + units units = stacked_cnn(units, [n_hidden], **conv_net_params) return units"
1763,def r'[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9]+[0-9],"def t_NUMBER(self, t): r'[0-9]+' t.endlexpos = t.lexpos + len(t.value) return t"
1764,"def sort_col(col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col, col,","def SortColumn(self, col): ""col -> sort the data based on the column indexed by col"" name = self.columns[col].name _data = [] for row in self.data: rowname, entry = row _data.append((entry.get(name, None), row)) _data.sort() self.data = [] for sortvalue, row in _data: self.data.append(row)"
1765,def _new_archive(self): if self.new_archive(self.new_archive): self.new_archive(self.new_archive) self.new_archive(self.new_archive) self.new_archive(self.new_archive) self.new_archive(self.new_archive) self.new_archive(self.new_archive) self.new_archive(self.new_archive) self.new_archive(self.new_archive) self.new_archive(self.new_archive) self.new_archive(self.new_archive) self.new_archive(self.new_archive) self.new_archive(self.new_archive) self.new_archive(self.new_archive) self.new_archive(self.new_archive) self.new_archive(self.new_archive) self.new_archive(self.new_archive) self.new_archive(self.new_archive) self.new_archive(self.new_archive) self.new_archive(self.new_archive) self.new_archive(self.new_archive) self.new_,"def _get_new_archive_fields(self): for i in xrange(3): records = self._dmpaft_cmd(self._archive_time) if records is not None: break time.sleep(1) if records is None: raise NoDeviceException('Can not access weather station') new_rec = None for r in records: new_time = (r['DateStamp'], r['TimeStamp']) if self._archive_time  new_time: self._archive_time = new_time new_rec = r return new_rec"
1766,"def create(self, application_definition_id, custom_headers, raw=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=True, polling=Tru","def create_or_update_by_id( self, application_definition_id, parameters, custom_headers=None, raw=False, polling=True, **operation_config): raw_result = self._create_or_update_by_id_initial( application_definition_id=application_definition_id, parameters=parameters, custom_headers=custom_headers, raw=True, **operation_config ) def get_long_running_output(response): deserialized = self._deserialize('ApplicationDefinition', response) if raw: client_raw_response = ClientRawResponse(deserialized, response) return client_raw_response return deserialized lro_delay = operation_config.get( 'long_running_operation_timeout', self.config.long_running_operation_timeout) if polling is True: polling_method = ARMPolling(lro_delay, **operation_config) elif polling is False: polling_method = NoPolling() else: polling_method = polling return LROPoller(self._client, raw_result, get_long_running_output, polling_method)"
1767,"def make_local_instances_of_static_files(self, static_files): if self.get_static_files(self.get_static_files): self.get_static_files(self.get_static_files) self.get_static_files(self.get_static_files) self.get_static_files(self.get_static_files) self.get_static_files(self.get_static_files) self.get_static_files(self.get_static_files)","def make_local_static_report_files(self): for static, pkgdir in self.STATIC_FILES: shutil.copyfile( data_filename(static, pkgdir), os.path.join(self.directory, static) ) if self.extra_css: shutil.copyfile( self.config.extra_css, os.path.join(self.directory, self.extra_css) )"
1768,"def transformImageFeature(self, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature, ImageFeature,","def transform(self, image_feature, bigdl_type=""float""): callBigDlFunc(bigdl_type, ""transformImageFeature"", self.value, image_feature) return image_feature"
1769,"def transfer_future(self, transfer_future, transfer_future, transfer_future, transfer_future, transfer_future, transfer_future, transfer_future, transfer_future, transfer_future, transfer_future, transfer_future, transfer_future, transfer_future, transfer_future, transfer_future, transfer_future, transfer_future, transfer_future, transfer_future, transfer_future)","def _get_upload_input_manager_cls(self, transfer_future): upload_manager_resolver_chain = [ UploadFilenameInputManager, UploadSeekableInputManager, UploadNonSeekableInputManager ] fileobj = transfer_future.meta.call_args.fileobj for upload_manager_cls in upload_manager_resolver_chain: if upload_manager_cls.is_compatible(fileobj): return upload_manager_cls raise RuntimeError( 'Input %s of type: %s is not supported.' % ( fileobj, type(fileobj)))"
1770,"def write_data(output_stream, output_stream, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmip_version, kmi","def write(self, output_stream, kmip_version=enums.KMIPVersion.KMIP_1_0): local_stream = utils.BytearrayStream() if self._asynchronous_correlation_value: self._asynchronous_correlation_value.write( local_stream, kmip_version=kmip_version ) self.length = local_stream.length() super(CancelRequestPayload, self).write( output_stream, kmip_version=kmip_version ) output_stream.write(local_stream.buffer)"
1771,"def app(self, app, app): app = self.qt(app, app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt(app) self.qt","def enable_qt4(self, app=None): from IPython.lib.inputhookqt4 import create_inputhook_qt4 app, inputhook_qt4 = create_inputhook_qt4(self, app) self.set_inputhook(inputhook_qt4) self._current_gui = GUI_QT4 app._in_event_loop = True self._apps[GUI_QT4] = app return app"
1772,"def update_secrets(self, srgistry_CLIENT_SECRETS, SREGISTRY_CLIENT_SECRETS, SREGISTRY_CLIENT_SECRETS, SREGISTRY_CLIENT_SECRETS, SREGISTRY_CLIENT_SECRETS, SREGISTRY_CLIENT_SECRETS, SREGISTRY_CLIENT_SECRETS, SREGISTRY_CLIENT_SECRETS)",def _update_secrets(self): bot.debug('Creating aws client...') try: from awscli.clidriver import create_clidriver except: bot.exit('Please install pip install sregistry[aws]') driver = create_clidriver() self.aws = driver.session.create_client('ecr')
1773,"def learn_idf_vector(self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self, self.","def fit(self, Z): X = Z[:, 'X'] if isinstance(Z, DictRDD) else Z check_rdd(X, (sp.spmatrix, np.ndarray)) def mapper(X, use_idf=self.use_idf): if not sp.issparse(X): X = sp.csc_matrix(X) if use_idf: return _document_frequency(X) if self.use_idf: n_samples, n_features = X.shape df = X.map(mapper).treeReduce(operator.add) df += int(self.smooth_idf) n_samples += int(self.smooth_idf) idf = np.log(float(n_samples) / df) + 1.0 self._idf_diag = sp.spdiags(idf, diags=0, m=n_features, n=n_features) return self"
1774,"def _fit_data(data, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n_clusters, n","def fit_kmeans(self, data, n_clusters, **kwargs): km = cl.KMeans(n_clusters=n_clusters, **kwargs) km.fit(data) return km"
1775,"def _scaled_y_positions(x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y,","def y(self): return scale_dimension(self.Y, self.header.y_scale, self.header.y_offset)"
1776,"def instantiate_index(self, index): if index is not self.init(index) if index is not self.init(index) if index is not self.init(index)","def _create_idx_from_stream(self, stream): try: p_id[0], coordinates, obj = next(stream_iter) except StopIteration: return -1 except Exception as exc: self._exception = exc return -1 if self.interleaved: coordinates = Index.deinterleave(coordinates) for i in range(dimension): mins[i] = coordinates[i*2] maxs[i] = coordinates[(i*2)+1] p_mins[0] = ctypes.cast(mins, ctypes.POINTER(ctypes.c_double)) p_maxs[0] = ctypes.cast(maxs, ctypes.POINTER(ctypes.c_double)) p_dimension[0] = dimension if obj is None: p_data[0] = no_data p_length[0] = 0 else: p_length[0], data, _ = self._serialize(obj) p_data[0] = ctypes.cast(data, ctypes.POINTER(ctypes.c_ubyte)) return 0 stream = core.NEXTFUNC(py_next_item) return IndexStreamHandle(self.properties.handle, stream)"
1777,"def remove_dup_args_safe(self, wikitext): if self.dup_args_safe(self.dup_args_safe): self.dup_args_safe(self.dup_args_safe)",def rm_first_of_dup_args(self) -> None: names = set() for a in reversed(self.arguments): name = a.name.strip(WS) if name in names: del a[:len(a.string)] else: names.add(name)
1778,"def getargspec(inspect.getargspec, argh.dispatch_command(cythonCompiledFunc)): argh.dispatch_command(argh.dispatch_command(argh.dispatch_command(argh.dispatch_command(cythonCompiledFunc)))): argh.dispatch_command(argh.dispatch_command(argh.dispatch_command(argh.dispatch_command(argh.dispatch_command(argh.dispatch_command(argh.dispatch_command(argh.dispatch_command(cythonCompiledFunc)))))))))))))))))))))))))))))))))))))))))))))))))","def getargspec_permissive(func): if inspect.ismethod(func): func = func.im_func if not (hasattr(func, ""func_code"") and hasattr(func, ""func_defaults"")): raise TypeError('!r missing func_code or func_defaults'.format(func)) args, varargs, varkw = inspect.getargs(func.func_code) return inspect.ArgSpec(args, varargs, varkw, func.func_defaults)"
1779,"def custom_patterns(custom_patterns, custom_patterns_dir, custom_patterns_dir, custom_patterns_dir, custom_patterns_dir, custom_patterns_dir, custom_patterns_dir, custom_patterns_dir, custom_patterns_dir, custom_patterns_dir, custom_patterns_dir, custom_patterns_dir, custom_patterns_dir, custom_patterns_dir, custom_patterns_dir, custom_patterns_dir, custom_patterns_dir, custom_patterns_dir, custom_patterns_dir)","def match(self, text): match_obj = None if self.fullmatch: match_obj = self.regex_obj.fullmatch(text) else: match_obj = self.regex_obj.search(text) if match_obj == None: return None matches = match_obj.groupdict() for key,match in matches.items(): try: if self.type_mapper[key] == 'int': matches[key] = int(match) if self.type_mapper[key] == 'float': matches[key] = float(match) except (TypeError, KeyError) as e: pass return matches"
1780,"def utterances_batch(self, utterances_ids): utterances_batch = self.batch(utterances_batch, utterances_ids)","def _call(self, utterances_batch: list, utterances_ids: Optional[list]=None) -> list: batch_size = len(utterances_batch) ids = utterances_ids or list(range(batch_size)) batch_history = [self.history[utt_id] for utt_id in ids] responses = [] filtered = self.skills_filter(utterances_batch, batch_history) for skill_i, (filtered_utterances, skill) in enumerate(zip(filtered, self.wrapped_skills)): skill_i_utt_indexes = [utt_index for utt_index, utt_filter in enumerate(filtered_utterances) if utt_filter] if skill_i_utt_indexes: skill_i_utt_batch = [utterances_batch[i] for i in skill_i_utt_indexes] skill_i_utt_ids = [ids[i] for i in skill_i_utt_indexes] res = [(None, 0.)] * batch_size predicted, confidence = skill(skill_i_utt_batch, skill_i_utt_ids) for i, predicted, confidence in zip(skill_i_utt_indexes, predicted, confidence): res[i] = (predicted, confidence) responses.append(res) responses = self.skills_processor(utterances_batch, batch_history, *responses) return responses"
1781,"def format_file(self, filename): self.format(filename) self.format(filename) self.format(filename) self.format(filename) self.format(filename) self.format(filename) self.format(filename) self.format(filename) self.format(filename) self.format(filename) self.format(filename) self.format(filename) self.format(filename) self.format(filename) self.format(filename) self.format(filename) self.format(filename) self.format(filename)","def format_filesize(size): for suffix in (""bytes"", ""KB"", ""MB"", ""GB"", ""TB""): if size  1024.0: if suffix in (""GB"", ""TB""): return ""0:3.2f 1"".format(size, suffix) else: return ""0:3.1f 1"".format(size, suffix) size /= 1024.0"
1782,"def wait_inprogress_transfer(self, KeyboardInterrupt): wait_inprogress_transfer(KeyboardInterrupt): wait_inprogress_transfer(KeyboardInterrupt): wait_inprogress_transfer(KeyboardInterrupt): wait_inprogress_transfer(KeyboardInterrupt): wait_inprogress_transfer(KeyboardInterrupt): wait_inprogress_transfer(KeyboardInterrupt): wait_inprogress_transfer(KeyboardInterrupt)","def wait(self): try: transfer_coordinator = None for transfer_coordinator in self.tracked_transfer_coordinators: transfer_coordinator.result() except KeyboardInterrupt: logger.debug('Received KeyboardInterrupt in wait()') if transfer_coordinator: logger.debug( 'On KeyboardInterrupt was waiting for %s', transfer_coordinator) raise except Exception: pass"
1783,def log_in_django_staff_user.login_django_staff_user.login_django_staff_user.login_django_staff_user.login_django_staff_user.login_django_staff_user.login_django_staff_user.login_django_staff_user.login_django_staff_user.login_django_staff_user.login_django_django_django_django_django_django_django_django_django_django_django_django_django_django_django_django_django_django_django_django_django_django_django_django_django_django_django_django_django_django_django_django_django_django_django_django_django_django_django_django_django_django_django_django_django_django,"def create(self, request): login_form = AuthenticationForm(request, data=request.data) if not login_form.is_valid(): raise serializers.ValidationError(login_form.errors) auth_login(request, login_form.get_user()) serializer = UserSerializer(request.user) return Response(serializer.data, status=status.HTTP_200_OK)"
1784,"def ppoi_data(instance, ppoi_data): if ppoi_data is None: if ppoi_data is None: if ppoi_data is None: if ppoi_data is None: if ppoi_data is None: if ppoi_data is None: if ppoi_data is None: if ppoi_data is None: if ppoi_data = ppoi_data = ppoi_data = ppoi_data = ppoi_data = ppoi_data = ppoi_data = ppoi_data = ppoi_data = ppoi_data = ppoi_data = ppoi_data = ppoi_data = ppoi_data = ppoi_data = ppoi_data = ppoi_data = ppoi_data = ppoi_data = ppoi_data = ppoi_data = ppoi_data = ppoi_data = ppoi_data = ppoi_data = ppoi_data = ppoi_data =","def save_form_data(self, instance, data): to_assign = data if data and isinstance(data, tuple): if data[0] is None: current_field = getattr(instance, self.name) if data[1]: current_field.ppoi = data[1] to_assign = current_field elif data[0] is False: to_assign = '' else: to_assign = data[0] super(VersatileImageField, self).save_form_data(instance, to_assign)"
1785,"def retry(self, config, timeout): self.retry(self.retry) self.retry(self.retry) self.retry(self.retry) self.retry(self.retry) self.retry(self.retry) self.retry(self.retry) self.retry(self.retry) self.retry(self.retry) self.retry(self.retry) self.retry(self.retry) self.retry","def recognize_speech(self, config, audio, retry=None, timeout=None): client = self.get_conn() response = client.recognize(config=config, audio=audio, retry=retry, timeout=timeout) self.log.info(""Recognised speech: %s"" % response) return response"
1786,"def load_job_iterator(self, job_iterator, job_iterator, job_iterator, job_iterator, job_iterator, job_iterator, job_iterator, job_iterator, job_iterator, job_iterator, job_iterator, job_iterator, job_iterator)","def _load_job(self): try: next_job = next(self._jobs) except StopIteration: self._on_deck = None else: if not isinstance(next_job, Job): next_job = DefaultJob(next_job) self._on_deck = next_job self._active_jobs += 1"
1787,"def get_conversion_method(self, instruction): if instruction is not self.get_conversion_method(self.get_conversion_method(self.get_conversion_method))","def get_bound_method(self, instruction): try: return self._bound_instructions[type(instruction)] except KeyError: raise PulseError('Qobj conversion method for %s is not found.' % instruction)"
1788,"def stdout(self.port, self.port, self.port): if self.port = self.port, self.port = self.port if self.port = self.port if self.port = self.port if self.port = self.port if self.port = self.port if self.port = self.port if self.port = self.port if self.port = self.port","def scrape_port_from_stdout(self): regex = re.compile(r""Open H2O Flow in your web browser: https?://([:]+):(d+)"") retries_left = 30 while retries_left and not self.terminated: with open(self.output_file_name, ""r"") as f: for line in f: mm = re.search(regex, line) if mm is not None: self.port = mm.group(2) print(""H2O cloud %d node %d listening on port %sn with output file %s"" % (self.cloud_num, self.node_num, self.port, self.output_file_name)) return if self.terminated: break retries_left -= 1 time.sleep(1) if self.terminated: return print(""nERROR: Too many retries starting cloud %d.nCheck the output log %s.n"" % (self.cloud_num, self.output_file_name)) sys.exit(1)"
1789,def _win_board(self): if self.win_board(self.win_board): self.win_board(self.win_board),def winner(self): 'The winner of this board if one exists.' for potential_win in self._potential_wins(): if potential_win == tuple('XXX'): return Outcome.win_for_crosses elif potential_win == tuple('OOO'): return Outcome.win_for_naughts if self._count(' ') == 0: return Outcome.draw return Outcome.ongoing
1790,"def make_python_file(self, src, ext, filename, open_filehandle, filename, open_filehandle, filename, open_filehandle, filename, open_filehandle, filename, open_filehandle, filename, open_filehandle, ext, ext, src, src, src, src, ext, ext, ext, ext, src, src, src, src, filename, open_filehandle, filename, open_filehandle, open_filehandle, open_filehandle, open_filehandle, open_filehandle, open_filehandle, src, src, src, src, src, src, src, src, src, src, src, open_filehandle, open_filehandle, open_filehandle, open_filehandle, open_filehandle, open_filehandle, open_filehandle, open_filehandle, open_filehandle, open_filehandle, sr","def temp_pyfile(src, ext='.py'): fname = tempfile.mkstemp(ext)[1] f = open(fname,'w') f.write(src) f.flush() return fname, f"
1791,"def run_object_type(self, run_object_type, run_object_type, run_object_type, run_object_type, run_object_type, run_object_type, run_object_type, run_object_type, run_object_type, run_object_type, run_object_type, run_object_type, run_object_type, run_object_type, run_object_type)","def get_run_object_type(run_object): if isinstance(run_object, tuple): return 'function' run_object, _, _ = run_object.partition(' ') if os.path.isdir(run_object): return 'package' return'module'"
1792,"def write_properties(self, key, value): fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh = fh =","def write_property(fh, key, value): if key is COMMENT: write_comment(fh, value) return _require_string(key, 'keys') _require_string(value, 'values') fh.write(_escape_key(key)) fh.write(b'=') fh.write(_escape_value(value)) fh.write(b'n')"
1793,def _persist_metamodel(self): if self.metamodel(self.metamodel): self.metamodel(self.metamodel) self.metamodel(self.metamodel) self.metamodel(self.metamodel) self.metamodel(self.metamodel) self.metamodel(self.metamodel) self.metamodel(self.metamodel) self.metamodel(self.metamodel) self.metamodel(self.metamodel) self.metamodel(self.metamodel) self.metamodel(self.metamodel) self.metamodel(self.metamodel) self.metamodel(self.metamodel) self.metamodel) self.metamodel(self.metamodel) self.metamodel) self.metamodel(self.metamodel) self.metamodel(self.metamodel) self.metamodel(self.metamodel) self.metamodel(self.metamodel) self.metamodel(self.metamodel) self.metamodel(self.metamodel) self.metamodel) self.metamodel(self.metamodel) self.metamodel(self.metamodel,"def persist_instances(metamodel, path, mode='w'): with open(path, mode) as f: for inst in metamodel.instances: s = serialize_instance(inst) f.write(s)"
1794,"def parse_status_info(self, status_info): self.status_info = self.status_info = self.status_info = self.status_info = self.status_info = self.status_info = self.status_info","def _update_pipeline_status(self): prev_status = self.status_info with open(self.log_file) as fh: for line in fh: if ""Session aborted"" in line: self.status_info = ""aborted"" self.send = True if prev_status!= self.status_info  else self.send return if ""Execution complete -- Goodbye"" in line: self.status_info = ""complete"" self.send = True if prev_status!= self.status_info  else self.send return self.status_info = ""running"" self.send = True if prev_status!= self.status_info  else self.send"
1795,"def interpret_expression_string(self, string, context): return self.get_expression_string(self, context)","def expression(self, string): if not isinstance(string, six.string_types): return lambda ctxt: string expr = self._jinja.compile_expression(string) return lambda ctxt: expr(ctxt.variables)"
1796,"def show_processed_task_counter(self, task_count): self.processed_task_counter = self.processed_task_counter = self.processed_task_counter = self.processed_task_counter = self.processed_task_counter = self.processed_task_counter = self.processed_task_counter = self.processed_task_counter = self.processed_task_counter = self.processed_task_counter = self.processed_task_counter = self.processed_task_counter = self.processed_task_counter = self.processed_task_counter = self.processed_task_counter","def processed(self): self.processed_tasks += 1 qsize = self.tasks.qsize() if qsize > 0: progress('[%d task(s) completed, %d remaining, %d thread(s)]', self.processed_tasks, qsize, len(self.workers)) else: progress('[%d task(s) completed, %d thread(s)]', self.processed_tasks, len(self.workers))"
1797,"def parse_argv(self, argv): if argv is None: if argv is None: if argv is None: if argv is None: if argv is None: if argv is None: if argv is None: if argv is None: if argv is None: if argv is None: if argv is None: if argv is None: argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv = argv","def run_from_argv(self, argv=None): exit_code = self.execute(**self.parse_argv(argv)) if exit_code is None: exit_code = 0 return exit_code"
1798,"def add_occurrence_key(self, occurrence, popover): occurrence = self.get_event_key(occurrence, popover, occurrence, popover)","def add_occurrences(events, count): for day in count: for item in count[day]: for event in events: if event.pk == item[1]: try: event.occurrence.append(day) except AttributeError: event.occurrence = [] event.occurrence.append(day)"
1799,===========================================================================================================================================================================================================================================================================================================,"def list_endpoint(self, endpoint, query=None): if not hasattr(self, 'transfer_client'): self._init_transfer_client() endpoint, path = self._parse_endpoint_name(endpoint) try: result = self.transfer_client.operation_ls(endpoint, path=path) except TransferAPIError as err: bot.custom(prefix='ERROR', message=err, color='RED') sys.exit(1) rows = [] for filey in result: name = filey['name'] if query is None or query in name: if name.endswith('img'): name = bot.addColor('PURPLE',name) rows.append([filey['type'], filey['permissions'], str(filey['size']), name ]) if len(rows) > 0: rows = [[""type"",""[perm]"",""[size]"",""[name]""]] + rows bot.custom(prefix=""Endpoint Listing %s"" %path, message='', color=""CYAN"") bot.table(rows) else: bot.info('No content was found at the selected endpoint.') return rows"
1800,"def update_team_membership(self, isModerator, isModerator, isModerator, isModerator, isModerator, isModerator, isModerator, isModerator, isModerator, isModerator, isModerator, isModerator, isModerator, isModerator, isModerator, isModerator, isModerator, isModerator, isModerator, isModerator, isModerator, isModer","def update(self, membershipId, isModerator=None, **request_parameters): check_type(membershipId, basestring, may_be_none=False) check_type(isModerator, bool) put_data = dict_from_items_with_values( request_parameters, isModerator=isModerator, ) json_data = self._session.put(API_ENDPOINT + '/' + membershipId, json=put_data) return self._object_factory(OBJECT_TYPE, json_data)"
1801,"def upload_file(self, account, project name, etc.): if self.convert_file(self.convert_file): self.convert_file(self.convert_file) self.convert_file(self.convert_file) self.convert_file(self.convert_file) self.convert_file(self.convert_file) self.convert_file(self.convert_file) self.convert_file(self.convert_file) self.convert_file(self.convert_file) self.convert_file(self.convert_file) self.convert_file(self.convert_file) self.convert_file(self.convert_file) self.convert_file(self.convert_file) self.convert_file) self.convert_file(self.convert_file) self.convert_file) self.convert_file(self.convert_file) self.convert_file(self.convert_file) self.convert_file(self.convert_file) self.convert_file(self.convert_file) self.convert_file(self.convert_file) self.convert_file(self.convert_file","def upload_file(filename, server, account, projname, language=None, username=None, password=None, append=False, stage=False, date_format=None): stream = transcode_to_stream(filename, date_format) upload_stream(stream_json_lines(stream), server, account, projname, language=language, username=username, password=password, append=append, stage=stage)"
1802,"def generate_process(self, Process): if self.process(Process): self.process(Process): self.process(Process): self.process(Process): self.process(Process) self.process(Process) self.process(Process) self.process(Process) self.process(Process) self.process(Process) self.process(Process) self.process(Process)","def process_iter(): def add(pid): proc = Process(pid) _pmap[proc.pid] = proc return proc def remove(pid): _pmap.pop(pid, None) a = set(get_pid_list()) b = set(_pmap.keys()) new_pids = a - b gone_pids = b - a for pid in gone_pids: remove(pid) for pid, proc in sorted(list(_pmap.items()) +  list(dict.fromkeys(new_pids).items())): try: if proc is None: yield add(pid) else: if proc.is_running(): yield proc else: yield add(pid) except NoSuchProcess: remove(pid) except AccessDenied: yield proc"
1803,"def get_certificate_store(self, X509Store): self.get_certificate_store(x509Store) self.get_certificate_store(x509Store) self.get_certificate_store(x509Store) self.get_certificate_store(x509Store) self.get_certificate_store(x509Store) self.get_certificate_store(x509Store) self.get_certificate_store(x509Store) self.get_certificate_store(x509Store) self.get_certificate_store(x509Store) self.get_certificate_store(x509Store) self.get_certificate_store(x509Store) self.get_certificate_store(x509Store) self.get_certificate_store(x509store) self.get_certificate_store(x509store) self.get_certificate_store(x509store) self.get_certificate_store(x509store) self.get_certificate_store(x509store) self.get_certificat",def get_cert_store(self): store = _lib.SSL_CTX_get_cert_store(self._context) if store == _ffi.NULL: return None pystore = X509Store.__new__(X509Store) pystore._store = store return pystore
1804,"def save_object_to_file(self, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename, filename)","def save_to_file(self, filename, format=None, **kwargs): if format is None: format = format_from_extension(filename) with file(filename, 'wb') as fp: self.save_to_file_like(fp, format, **kwargs)"
1805,"def validate_args(dtype, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validate_args, validation_args, validation_args, validation_args, validation_args, validation_args, validation_args, validation_args, validation_args, validation_args, validation_args, validation_args, validation_args, validation_args, validation_args, validation_args, validation_args, validation_args, validation_args, validation_args, validation_args, validation_args, validation_args, validation_args, validation_args, validation_args, validation_args, validation_args, validation_args, validation_args, validation_args, validation_args, validation_args, validation_args, validation_args, validation_args, validation_args","def process_quadrature_grid_and_probs(quadrature_grid_and_probs, dtype, validate_args, name=None): return tf.compat.dimension_value( tensorshape_util.with_rank_at_least(x.shape, 1)[-1]) m, n = _static_event_size(probs), _static_event_size(grid) if m is not None and n is not None: if m!= n: raise ValueError(""quadrature_grid_and_probs must be a tuple of "" ""same-length zero-th-dimension Tensors "" ""(saw lengths, )"".format(m, n)) elif validate_args: assertions = [ assert_util.assert_equal( dimension_size(probs, axis=-1), dimension_size(grid, axis=-1), message=(""quadrature_grid_and_probs must be a tuple of "" ""same-length zero-th-dimension Tensors"")), ] with tf.control_dependencies(assertions): grid = tf.identity(grid) probs = tf.identity(probs) return grid, probs"
1806,"def import_sql_table(self, connection_url, table, password, optimize): self.import_sql_table(self.import_sql_table) self.import_sql_table(self.import_sql_table) self.import_sql_table(self.import_sql_table) self.import_sql_table(self.import_sql_table) self.import_sql_table(self.import_sql_table) self.import_sql_table) self.import_sql_table) self.import_sql_table(self.import_sql_table) self.import_sql_table) self.import_sql_table(self.import_sql_table) self.import_sql_table) self.import_sql_table(self.import_sql_table) self.import_sql_table(self.import_sql_table) self.import_sql_table) self.import_sql_table(self.import_sql_table) self","def import_sql_table(connection_url, table, username, password, columns=None, optimize=True, fetch_mode=None): assert_is_type(connection_url, str) assert_is_type(table, str) assert_is_type(username, str) assert_is_type(password, str) assert_is_type(columns, [str], None) assert_is_type(optimize, bool) assert_is_type(fetch_mode, str, None) p = ""connection_url"": connection_url, ""table"": table, ""username"": username, ""password"": password, ""fetch_mode"": fetch_mode if columns: p[""columns""] = "", "".join(columns) j = H2OJob(api(""POST /99/ImportSQLTable"", data=p), ""Import SQL Table"").poll() return get_frame(j.dest_key)"
1807,"def ArrayRef(self, c_ast.ID): if c_ast.ID is None: if c_ast.ID is None: if c_ast.ID is None: if c_ast.ID is None: if c_ast.ID is None: if c_ast.ID is None: if c_ast.ID is None: c_ast.ID = c_ast.ID = c_ast.ID = c_ast.ID = c_ast.ID = c_ast.ID = c_ast.ID = c_ast.ID = c_ast.ID = c_ast.ID = c_ast.ID = c_ast.ID = c_ast.ID = c_ast.ID = c_ast.ID = c_ast.ID = c_ast.ID = c_ast.ID = c_ast.ID = c_ast.ID = c_ast.ID =","def _get_offsets(self, aref, dim=0): if isinstance(aref, c_ast.ID): return None assert type(aref.name) in [c_ast.ArrayRef, c_ast.ID],  ""array references must only be used with variables or other array references"" assert type(aref.subscript) in [c_ast.ID, c_ast.Constant, c_ast.BinaryOp],  'array subscript must only contain variables or binary operations' idxs = [self.conv_ast_to_sym(aref.subscript)] if type(aref.name) is c_ast.ArrayRef: idxs += self._get_offsets(aref.name, dim=dim+1) if dim == 0: idxs.reverse() return tuple(idxs)"
1808,def get_service(self): if self.service(self.service): self.service(self.service): self.service(self.service): self.service(self.service): self.service(self.service),"def serviceStatus(self): r = self._apiClient.get(""api/v0002/service-status"") if r.status_code == 200: return ServiceStatus(**r.json()) else: raise ApiException(r)"
1809,def import(self): if self.import(self.import(self.import)): self.import(self.import(self.import)) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.import(self.import) self.,"def create_app(config=None): from home.config import TEMPLATE_FOLDER, STATIC_FOLDER app = Flask(__name__, static_folder=STATIC_FOLDER, template_folder=TEMPLATE_FOLDER) app.config['SECRET_KEY'] ='ssh, its a secret.' app.config.from_object('home.config') if 'HOME_SETTINGS' in environ: app.config.from_envvar('HOME_SETTINGS') app.config.from_object(config) from home.dash.web import web from home.dash.api import api app.register_blueprint(web) app.register_blueprint(api, url_prefix='/api') login_manager.init_app(app) login_manager.login_view = 'Dashboard Web.login' from home.dash.models import User @login_manager.user_loader def load_user(user_id): return User.query.get(int(user_id)) Migrate(app, db, directory=app.config['MIGRATE_DIRECTORY']) admin = Admin(app) from home.dash.admin import setup_admin setup_admin(admin) db.init_app(app) return app"
1810,"def get_download_link(file_id, ticket, prepare_download): if file_id is not in file_id, prepare_download is not in file_id.get_download_response(file_id, ticket, prepare_download) if file_id is not in file_id, prepare_download is not in file_id.get_download_response(file_id, ticket, prepare_download) if file_id.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.get_url.","def get_download_link(self, file_id, ticket, captcha_response=None): params = 'ticket': ticket, 'file': file_id if captcha_response: params['captcha_response'] = captcha_response return self._get('file/dl', params)"
1811,"def stack_encoding_layers(self, stack_encoding_layers): stack_encoding_layers(self, stack_encoding_layers): stack_encoding_layers(self, stack_encoding_layers): stack_encoding_layers(self, stack_encoding_layers): stack_encoding_layers(self, stack_encoding_layers): stack_encoding_layers(self, stack_encoding_layers): stack_encoding_layers): stack_encoding_layers(self, stack_encoding_layers): stack_encoding_layers): stack_encoding_layers: stack_encoding_layers: stack_encoding_layers: stack_encoding_layers: stack_encoding_layers: stack_encoding_layers: stack_encoding_layers: stack_encoding_layers: stack_encoding_layers: stack_encoding_layers: stack_encoding_layers: stack_encoding_layers: stack_encoding_layers: stack_encoding_layers: stack_encoding_layers: stack_encoding_layers: stack_encoding_layers: stack_encoding_layers: stack_","def stack_encoders(self, *layers): self.stack(*layers) self.encoding_layes.extend(layers)"
1812,"def find_localmax(x, axis=0): if x = x(x, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis=0, axis","def localmax(x, axis=0): paddings = [(0, 0)] * x.ndim paddings[axis] = (1, 1) x_pad = np.pad(x, paddings, mode='edge') inds1 = [slice(None)] * x.ndim inds1[axis] = slice(0, -2) inds2 = [slice(None)] * x.ndim inds2[axis] = slice(2, x_pad.shape[axis]) return (x > x_pad[tuple(inds1)]) & (x >= x_pad[tuple(inds2)])"
1813,"def drop_columns(int, index, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis) axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis, axis,","def drop(self, index, axis=1): if axis == 1: if not isinstance(index, list): #If input is a string, i.e., ""C1"": if is_type(index, str): #Check if index is an actual column(s) in the frame if index not in self.names: raise H2OValueError(""Column(s) selected to drop are not in original frame: %r"" % index) index = self.names.index(index) #If input is an int indicating a column index, i.e., 3: elif is_type(index, int): #Check if index is an actual column index in the frame if index > self.ncol: raise H2OValueError(""Column index selected to drop is not part of the frame: %r"" % index) if index  0: raise H2OValueError(""Column index selected to drop is not positive: %r"" % index) fr = H2OFrame._expr(expr=ExprNode(""cols"", self, -(index + 1)), cache=self._ex._cache) fr._ex._cache.ncols -= 1 fr._ex._cache.names = self.names[:index] + self.names[index + 1:] fr._ex._cache.types = name: self.types[name] for name in fr._ex._cache.names return fr elif isinstance(index, list): #If input is an int array indicating a column index, i.e., [3] or [1,2,3]: if is_type(index, [int]): if max(index) > self.ncol: raise H2OValueError(""Column index selected to drop is not part of the frame: %r"" % index) if min(index)"
1814,"def extract_file_path(file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path, file_path)","def ungzip(file_path, extract_path: Path = None): CHUNK = 16 * 1024 file_path = Path(file_path) extract_path = extract_path or file_path.with_suffix('') with gzip.open(file_path, 'rb') as fin, extract_path.open('wb') as fout: while True: block = fin.read(CHUNK) if not block: break fout.write(block)"
1815,"def execute_code(self, dict, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin, allow_stdin,","def execute(self, code, silent=False, user_variables=None, user_expressions=None, allow_stdin=None): if user_variables is None: user_variables = [] if user_expressions is None: user_expressions =  if allow_stdin is None: allow_stdin = self.allow_stdin if not isinstance(code, basestring): raise ValueError('code %r must be a string' % code) validate_string_list(user_variables) validate_string_dict(user_expressions) content = dict(code=code, silent=silent, user_variables=user_variables, user_expressions=user_expressions, allow_stdin=allow_stdin, ) msg = self.session.msg('execute_request', content) self._queue_send(msg) return msg['header']['msg_id']"
1816,"def python_path(python_path): if python_path is not python_path, return python_path(python_path) if python_path is not python_path(python_path): return python_path(python_path) if python_path is not python_path(python_path): return python_path = python_path = python_path = python_path = python_path = python_path = python_path = python_path = python_path = python_path = python_path = python_path = python_path = python_path = python_path = python_path = python_path = python_path = python_path = python_path = python_path = python_path","def pycmd2argv(cmd): rext = os.path.splitext(cmd)[1] if ext in ['.exe', '.com', '.bat']: return [cmd] else: return [sys.executable, cmd]"
1817,"def measure_type(self, measurement_type, additional_tags, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts, ts","def __make_points(self, measurement, additional_tags, ts, fields): tags = self.tags.copy() tags.update(additional_tags) return  ""measurement"": measurement, ""tags"": tags, ""time"": int(ts), ""fields"": fields,"
1818,def typeError(typeError): typeError(typeError): typeError(typeError): typeError(typeError): typeError(typeError): typeError(typeError): typeError(typeError): typeError(typeError): typeError(typeError): typeError(typeError): typeError(typeError): typeError(typeError): typeError): typeError): typeError: typeError: typeError: typeError: typeError: typeError: typeError: typeError: typeError: typeError: typeError: typeError: typeError: typeError: typeError: typeError: typeError: typeError: typeError: typeError: typeError: typeError: typeError: typeError: typeError: typeError: typeError: typeError: typeError: typeError: typeErr,"def validate(self): if not isinstance(self.value, bytes): raise TypeError(""opaque value must be bytes"") elif not isinstance(self.opaque_type, enums.OpaqueDataType): raise TypeError(""opaque data type must be an OpaqueDataType "" ""enumeration"") name_count = len(self.names) for i in range(name_count): name = self.names[i] if not isinstance(name, six.string_types): position = ""(0 in list)"".format(i) raise TypeError(""opaque data name 0 must be a string"".format( position))"
1819,"def get_task_info_dictionary(self, task): self.get_task_info_dictionary(task): self.get_task_info_dictionary(task) self.get_task_info_dictionary(task) self.get_task_info_dictionary(task) self.get_task_info_dictionary(task) self.get_task_info_dictionary(task) self.get_task_info_dictionary(task) self.get_task_info_dictionary(task) self.get_task_info_dictionary(task) self.get_task_info_dictionary(task) self.get_dictionary(task) self.get_dictionary(task) self.get_dictionary(task) self.get_dictionary(task) self.get_dictionary(task) self.get_dictionary(task) self.get_dictionary(task) self.get_dictionary(task) self.get_dictionary(task) self.get_dictionary(task) self.get_dictionary(task) self.get_dictionary(task) self.get_dictionary(task) self.get_dictionary(task) self.get_dictionary(task) self.get_dictionary(task) self.get_dictionary(task",def taskinfo_with_label(label): task = Task.objects.get(label=label) info = json.loads(task._func_info) return info
1820,"def visible_properties(self, ticklines, ticklabels, ticklines, ticklines, ticklabels, ticklines, ticklabels, ticklines, ticklabels, ticklines, ticklabels, ticklines, ticklabels, ticklines, ticklabels, ticklines, ticklabels, ticklines, ticklabels, ticklines, ticklabels, ticklines, ticklabels, ticklines, ticklines, ticklabels, ticklines, ticklabels, ticklines, ticklines, ticklabels, ticklines, ticklabels, ticklines, ticklabels, ticklines, ticklabels, ticklines)",def _hide_tick_lines_and_labels(axis): for item in axis.get_ticklines() + axis.get_ticklabels(): item.set_visible(False)
1821,"def get_s3_object(self, url): self.get_s3_object(self.get_s3_object(url)) self.get_s3_object(url) self.get_s3_object(url) self.get_s3_object(url) self.get_s3_object(url) self.get_s3_object(url) self.get_s3_object(url) self.get_s3_object(url) self.get_s3_object(url) self.get_s3_object(url) self.get_s3_object(url) self.get_s3_object(url) self.get_s3_object(url) self.get_s3_object(url) self.get_s3_object(url) self.get_s3_object(url) self.get_s3_object(url) self.get_s3_object(url) self.get_s3_object(url) self.get_s3_object(url) self.get_s3_object(url) self.get_s3_object(url) self.get_s3_object(url) self.get_s3_object(url) self.get_s3_object(url","def lookup(self, s3url): try: return self.s3.head_object(Bucket=s3url.bucket, Key=s3url.path) except BotoClient.ClientError as e: if e.response['ResponseMetadata']['HTTPStatusCode'] == 404: return None else: raise e"
1822,"def copy(self, name): if self.instruct(self.instruct(self.instruct)): self.instruct(self.instruct) self.instruct(self.instruct) self.instruct(self.instruct) self.instruct(self.instruct) self.instruct(self.instruct) self.instruct(self.instruct) self.instruct(self.instruct) self.instruct(self.instruct) self.instruct(self.instruct) self.instruct(self.instruct) self.instruct(self.instruct) self.instruct(self.instruct) self.instruct(self.instruct) self.instruct(self.instruct) self.instruct(self.instruct) self.instruct(self.instruct) self.instruct(self.instruct) self.instruct(self.instruct) self.instruct(self.instruct) self.instruct(self.instruct) self.instruct(self.instruct) self.instruct(self.instruct) self.instruct(self.instruct) self.instruct(self.instruct) self.instruct(self.instruct) self.instruct(self.instruct) self.instruct(self.instruct) self","def copy(self, name=None): cpy = copy.copy(self) if name: cpy.name = name return cpy"
1823,def _encrypt_all_files(self): if self._encrypt(self._encrypt(self._encrypt)): self._encrypt(self._encrypt(self._encrypt)),"def reencrypt_single_user(engine, user_id, old_crypto, new_crypto, logger): crypto = FallbackCrypto([new_crypto, old_crypto]) reencrypt_user_content( engine=engine, user_id=user_id, old_decrypt_func=crypto.decrypt, new_encrypt_func=crypto.encrypt, logger=logger, )"
1824,"def find_file(self, filename, extension): if filename is not self.get_file(filename): if filename is not self.get_filename(filename): if filename is self.get_filename(filename): if filename is self.get_filename(filename): if filename is self.get_filename(filename): if filename is self.get_filename(filename): if filename(filename): if filename): if filename(filename)","def get_ttf(self): font_dict =  families = [] rootdirlist = string.split(self.search_path, os.pathsep) #for rootdir in rootdirlist: for dirName, subdirList, filelist in itertools.chain.from_iterable(os.walk(path) for path in rootdirlist): for item in filelist: root, ext = os.path.splitext(item) if ext == '.ttf': if root[0].lower() in english: source = os.path.join(dirName, item) name = root.lower().replace('_','') if'bold' in name: name = name.replace(' bold', '_bold') if'italic' in name: name = name.replace(' italic', '_italic') elif 'bold' in name: name = name.replace('bold', '_bold') if 'italic' in name: name = name.replace('italic', '_italic') elif'italic' in name: name = name.replace(' italic', '_italic') elif 'italic' in name: name = name.replace('italic', '_italic') elif 'oblique' in name: name = name.replace('oblique', '_italic') else: families.append(name) font_dict[name] = source else: source = os.path.join(dirName, item) name = root.lower().replace('_','') font_dict[name] = source families.append(name) self.font_dict = font_dict self.families = families"
1825,def convert_time_before_present(time_before_present): if time_before_present(time_before_present): if time_before_present(time_before_present): if time_before_present(time_before_present): if time_before_present(time_before_present)): if time_before_present(time_before_present): if time_before_present(time_before_present): if time_before_present(time_before_present): if time_before_present(time_before_present): if time_before_present(time_before_present): if time_before_present(time_before_present): if time_before_present(time_before_present): if time_before_present(time_before_present): if time_before_present): if time_before_present = if time_present = if time_present = if time_present = if time_present = if time_present = if time_present = if time_present = if time_present = if time_present = if time_present = if time_present = if time_present = if time_present = if time_present =,"def convert_dates(self): from datetime import datetime, timedelta now = numeric_date() for node in self.tree.find_clades(): years_bp = self.date2dist.to_years(node.time_before_present) if years_bp  0 and self.real_dates: if not hasattr(node, ""bad_branch"") or node.bad_branch is False: self.logger(""ClockTree.convert_dates -- WARNING: The node is later than today, but it is not "" ""marked as ""BAD"", which indicates the error in the "" ""likelihood optimization."",4, warn=True) else: self.logger(""ClockTree.convert_dates -- WARNING: node which is marked as ""BAD"" optimized "" ""later than present day"",4, warn=True) node.numdate = now - years_bp year = np.floor(node.numdate) days = max(0,365.25 * (node.numdate - year)-1) try: n_date = datetime(year, 1, 1) + timedelta(days=days) node.date = datetime.strftime(n_date, ""%Y-%m-%d"") except: n_date = datetime(1900, 1, 1) + timedelta(days=days) node.date = ""%04d-%02d-%02d""%(year, n_date.month, n_date.day)"
1826,"def _invoke_function(args, line, bool): if args is None: if args is None: if args is None: if args is None: if args is None: if args is None: if args is None: if args is None: if args is None: if args is None: if args is None: if args is None: args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args = args","def invoke_one(self, line): funname = line.pop(0) context = self.contexts[-1] func = self.find_function(context, funname) #If this is a context derived from a module or package, just jump to it #since there is no initialization function if isinstance(func, dict): self.contexts.append(func) self._check_initialize_context() return None, line, False if func.takes_cmdline is True: val = func(line) line = [] else: posargs, kwargs, line = self.process_arguments(func, line) #We need to check for not enough args for classes before calling or the call won't make it all the way to __init__ if inspect.isclass(func) and not func.metadata.spec_filled(posargs, kwargs): raise ValidationError(""Not enough parameters specified to call function"", function=func.metadata.name, signature=func.metadata.signature()) val = func(*posargs, **kwargs) finished = True if func.finalizer is True: self.contexts.pop() elif val is not None: if func.metadata.returns_data(): val = func.metadata.format_returnvalue(val) else: self.contexts.append(val) self._check_initialize_context() finished = False val = None return val, line, finished"
1827,"def format_field_name(self, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name) field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field_name, field","def _format_field_name(self, field_name) -> str: field = self._get_model_field(field_name) return self.qn(field.column)"
1828,"def make_request(self, restrclient, restrclient, restrclient, restrclient, restrclient, restrclient, restrclient, restrclient, restrclient, restrclient, restrclient, restrclient, restrclient, restrclient, restrclient, restrclient, restrclient)","def request(self, method, url, query_params=None, headers=None, post_params=None, body=None): if method == ""GET"": return self.rest_client.GET(url, query_params=query_params, headers=headers) elif method == ""HEAD"": return self.rest_client.HEAD(url, query_params=query_params, headers=headers) elif method == ""OPTIONS"": return self.rest_client.OPTIONS(url, query_params=query_params, headers=headers, post_params=post_params, body=body) elif method == ""POST"": return self.rest_client.POST(url, query_params=query_params, headers=headers, post_params=post_params, body=body) elif method == ""PUT"": return self.rest_client.PUT(url, query_params=query_params, headers=headers, post_params=post_params, body=body) elif method == ""PATCH"": return self.rest_client.PATCH(url, query_params=query_params, headers=headers, post_params=post_params, body=body) elif method == ""DELETE"": return self.rest_client.DELETE(url, query_params=query_params, headers=headers) else: raise ValueError( ""http method must be GET, HEAD,"" "" POST, PATCH, PUT or DELETE."" )"
1829,def create_event(self): if self.get_event(self.get_event): self.get_event(self.get_event) self.get_event(self.get_event) self.get_event(self.get_event) self.get_event(self.get_event) self.get_event(self.get_event) self.get_event(self.get_event) self.get_event(self.get_event) self.get_event(self.get_event) self.get_event(self.get_event) self.get_event(self.get_event) self.get_event(self.get_event) self.get_event(self.get_event) self.get_event(self.get_event) self.get_event(self.get_event) self.get_event(self.get_event) self.get_event(self.get_event) self.get_event(self.get_event) self.get_event(self.get_event) self.get_event(self.get_event) self.get_event,"def createLog(self, type, data, evtime=None, hostname=None): return Event(None, type, data, 0, self.ttl, hostname=hostname or self.hostname, evtime=evtime, tags=self.tags, type='log' )"
1830,"def pull_request(self, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests, pull_requests)","def get_pull_reqs(self, repo): pull_reqs_open = 0 pull_reqs_closed = 0 for pull_request in repo.iter_pulls(state='all'): self.pull_requests_json[repo.name].append(pull_request.to_json()) if pull_request.closed_at is not None: pull_reqs_closed += 1 else: pull_reqs_open += 1 return pull_reqs_open, pull_reqs_closed"
1831,"def find_file('/foo.zipx', 'dir/part', 'foo.zipx', 'foo.zipx', 'foo.zipx', 'foo.zipx', 'foo.zipx', 'foo.zipx', 'foo.zipx', 'foo.zipx', 'foo.zipx', 'foo.zipx', 'foo.zipx', 'foo.zipx', 'foo.zipx', 'foo.zipx', 'foo.zipx', 'foo.zipx', 'foo.zipx', 'foo.zipx', 'foo.zipx', 'foo.zipx', 'foo.zipx.zipx.zipx.zipx.zipx.zipx.zipx.zipx.zipx.zipx.zipx.zipx.zipx.zipx.zipx.zipx.zipx.zipx.zipx.zipx.zipx.zipx.zipx.zip","def find_file(path): for n in range(len(path_components), 0, -1): file_c = path_components[:n] part_c = path_components[n:] or [''] yield (os.path.join(*file_c), posixpath.join(*part_c)) for file_path, part_path in get_assemblies(): if os.path.isfile(file_path): return file_path, part_path"
1832,"def transform(self, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coords, coord","def transform(self, coords): origin_x, origin_y = self.origin sx, sy = self.scale return [(int(math.floor((x - origin_x) / sx)), int(math.floor((y - origin_y) / sy))) for x, y in coords]"
1833,"def run_epoch_controllers(self, epoch_controllers): epoch_controllers(epoch_controllers): epoch_controllers(epoch_controllers): epoch_controllers(epoch_controllers): epoch_controllers(epoch_controllers): epoch_controllers(epoch_controllers): epoch_controllers(epoch_controllers): epoch_controllers(epoch_controllers): epoch_controllers): epoch_controllers = epoch_controllers = epoch_controllers = epoch_controllers = epoch_controllers = epoch_controllers = epoch_controllers = epoch_controllers = epoch_controllers = epoch_controllers = epoch_controllers = epoch_controllers = epoch_controllers = epoch_controllers = epoch_controllers = epoch","def run(self, train_set, valid_set=None, test_set=None, train_size=None, epoch_controllers=None): epoch_controllers = epoch_controllers if epoch_controllers else [] epoch_controllers += self._epoch_controllers if isinstance(train_set, Dataset): dataset = train_set train_set = dataset.train_set() valid_set = dataset.valid_set() test_set = dataset.test_set() train_size = dataset.train_size() self._current_train_set = train_set self._current_valid_set = valid_set self._current_test_set = test_set if epoch_controllers: for controller in epoch_controllers: controller.bind(self) timer = Timer() for _ in self.train(train_set, valid_set=valid_set, test_set=test_set, train_size=train_size): if epoch_controllers: for controller in epoch_controllers: controller.invoke() if self._ended: break if self._report_time: timer.report()"
1834,"def save_synthetic_image(images, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, fname, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str, str,","def plot_generated_images(images, fname): fig = plt.figure(figsize=(4, 4)) canvas = backend_agg.FigureCanvasAgg(fig) for i, image in enumerate(images): ax = fig.add_subplot(4, 4, i + 1) plt.axis('off') ax.set_xticklabels([]) ax.set_yticklabels([]) ax.imshow(image.reshape(IMAGE_SHAPE[:-1]), cmap='Greys_r') fig.tight_layout() plt.subplots_adjust(wspace=0.05, hspace=0.05) canvas.print_figure(fname, format='png')"
1835,"def normalize_value(self, value): if value is None: if value is None: if value is None: if value is None: if value is None: if value is None: if value is None: if value is None: if value is None: if value is None: if value is None: if value is None: if value is None: if value is None: if value is None: if value is None: if value is None","def normalize_text(value): if value is None: return """" text = decode_bytes(value) if isbytes(value) else str_(value) return normalize_whitespace(text)"
1836,def load_schedule(self): self.load_schedule(self.load_schedule(self.load_schedule)) self.load_schedule(self.load_schedule) self.load_schedule(self.load_schedule) self.load_schedule(self.load_schedule) self.load_schedule(self.load_schedule) self.load_schedule(self.load_schedule) self.load_schedule) self.load_schedule(self.load_schedule) self.load_schedule) self.load_schedule) self.load_schedule(self.load_schedule) self.load_schedule) self.load_schedule) self.load_schedule(self.load_schedule) self.load_schedule(self.load_schedule) self.load_schedule) self.load_schedule(self.load_schedule) self.load_schedule(self.load_schedule) self.load_schedul,"def get_schedule(): params = 'agentid': config()['agent']['name'].encode('utf8') lookahead = config()['agent']['cal_lookahead'] * 24 * 60 * 60 if lookahead: params['cutoff'] = str((timestamp() + lookahead) * 1000) uri = '%s/calendars?%s' % (config()['service-scheduler'][0], urlencode(params)) try: vcal = http_request(uri) except pycurl.error as e: logger.error('Could not get schedule: %s' % e) return try: cal = parse_ical(vcal.decode('utf-8')) except Exception: logger.error('Could not parse ical') logger.error(traceback.format_exc()) return db = get_session() db.query(UpcomingEvent).delete() for event in cal: if event['dtend'] = timestamp(): continue e = UpcomingEvent() e.start = event['dtstart'] e.end = event['dtend'] e.uid = event.get('uid') e.title = event.get('summary') e.set_data(event) db.add(e) db.commit()"
1837,"def pull_max_messages(max_messages, max_messages, max_messages, max_messages, max_messages, max_messages, max_messages, max_messages, max_messages, max_messages, max_messages, max_messages, max_messages, max_messages, max_messages, max_messages, max_messages, max_messages, max_messages, max_messages, max_messages)","def pull(self, project, subscription, max_messages, return_immediately=False): service = self.get_conn() full_subscription = _format_subscription(project, subscription) body = 'maxMessages': max_messages,'returnImmediately': return_immediately  try: response = service.projects().subscriptions().pull( subscription=full_subscription, body=body).execute(num_retries=self.num_retries) return response.get('receivedMessages', []) except HttpError as e: raise PubSubException( 'Error pulling messages from subscription '.format( full_subscription), e)"
1838,"def calculate_running_area_under-curve(self, curve): run_area_under-curve(curve): run_area_under-curve(curve) run_area(curve) run_area(curve) run_area(curve) run_area(curve) run_area(curve) run_area(curve) run_area(curve) run_area(curve) run_area(curve) run_area(curve) run_area(curve) run_area(curve) run_area(curve) run_area(curve) run_area(curve) run_area(curve) run_area(curve) run_area(curve) run_area(curve)","def _running_area(indep_vector, dep_vector): rect_height = np.minimum(dep_vector[:-1], dep_vector[1:]) rect_base = np.diff(indep_vector) rect_area = np.multiply(rect_height, rect_base) triang_height = np.abs(np.diff(dep_vector)) triang_area = 0.5 * np.multiply(triang_height, rect_base) return np.cumsum(np.concatenate((np.array([0.0]), triang_area + rect_area)))"
1839,"def run_aiohttp.web.Application(self, app_uri, host, port, reload=True, reload=True, reload=True, reload=True, reload=True, reload=True, reload=True, reload=True, reload=True, reload=True, reload=True, reload=True, reload=True, reload=True) reload=True, reload=True, reload=True, reload=True, reload=True, reload=True, reload=True, reload=True, reload=True, reload=True, reload=True, reload=True, reload=True, reload=True, reload=True, reload=True, reload=True, reload=True, reload=True, reload=True, reload=True, reload=True, reload=True, reload=True, reload=True,","def run(app: web.Application, **kwargs): runner = Runner(app, **kwargs) runner.run()"
1840,"def take_blob(self, blob_name, dependencies, image_hash): self.get_blob(self.get_blob_name) self.get_blob_name(self.get_blob_name) self.get_blob_name(self.get_blob_name) self.get_blob_data(self.get_blob_data) self.get_blob_data(self.get_blob_data) self.get_blob_data) self.get_data(self.get_data) self.get_data(self.get_data(self.get_data) self.get_data(self.get_data) self.get_data(self.get_data) self.get_data(self.get_data) self.get_data(self.get_data) self.get_data(self.get_data) self.get_data(self.get_data) self.get_data(self.get_data) self.get_data(self.get_data) self.get_data(self.get_data) self.get_data(self.get_data) self.get_data(self.get_data) self.get_data(self.get_data) self.get_","def update_blob_metadata(blob, response, config, bucket, names): manifest = os.path.basename(response['results']['artifactManifest']) manifest = json.loads(bucket.blob(manifest).download_as_string()) metadata = 'file_hash': manifest['file_hash'][0]['file_hash'][0]['value'], 'artifactManifest': response['results']['artifactManifest'], 'location': manifest['location'],'storageSourceBucket': config['source']['storageSource']['bucket'],'storageSourceObject': config['source']['storageSource']['object'], 'buildCommand':''.join(config['steps'][0]['args']), 'builder': config['steps'][0]['name'],'media_link': blob.media_link,'self_link': blob.self_link,'size': blob.size, 'name': names['tag_uri'], 'type': ""container"" blob.metadata = metadata blob._properties['metadata'] = metadata blob.patch()"
1841,"def convert_time_to_frames(self, times, hop_length, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft,","def time_to_frames(times, sr=22050, hop_length=512, n_fft=None): samples = time_to_samples(times, sr=sr) return samples_to_frames(samples, hop_length=hop_length, n_fft=n_fft)"
1842,"def check_server(self, scheme, ip, port): if self._start_server(self._start_server): self._start_server(self._start_server) self._start_server(self._start_server) self._start_server(self._start_server) self._start_server(self._start_server) self._start_server(self._start_server) self._start_server(self._start_server) self._start_server) self._start_server(self._start_server(self._start_server) self._start_server(self._start_server) self._start_server(self._start_server) self._start_server(self._start_server) self._start_server(self._start_server) self._start_server) self._start_server(self._start_server) self._start_server(self._start_server) self._start_server) self._start_server(self._start_server) self._start_server(self._start_server) self._start_server(self._start_server) self._start_server) self._start_server","def _get_server_info_from_logs(self): searchstr = ""INFO: Open H2O Flow in your web browser:"" with open(self._stdout, ""rt"") as f: for line in f: if searchstr in line: url = line[line.index(searchstr) + len(searchstr):].strip().rstrip(""/"") parts = url.split("":"") assert len(parts) == 3 and (parts[0] == ""http"" or parts[1] == ""https"") and parts[2].isdigit(),  ""Unexpected URL: %s"" % url return parts[0], parts[1][2:], int(parts[2]) return None"
1843,"def parse_dates(date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_file, date_","def parse_dates(date_file): print(""nAttempting to parse dates..."") dates =  if not os.path.isfile(date_file): print(""ntERROR: file %s does not exist, exiting...""%date_file) return dates full_sep = 't' if date_file.endswith('.tsv') else r's*,s*' try: df = pd.read_csv(date_file, sep=full_sep, engine='python') potential_date_columns = [] potential_numdate_columns = [] potential_index_columns = [] for ci,col in enumerate(df.columns): d = df.iloc[0,ci] if type(d)==str and d[0] in ['""', ""'""] and d[-1] in ['""', ""'""]: for i,tmp_d in enumerate(df.iloc[:,ci]): df.iloc[i,ci] = tmp_d.strip(d[0]) if 'date' in col.lower(): potential_date_columns.append((ci, col)) if any([x==col.lower() for x in ['name','strain', 'accession']]): potential_index_columns.append((ci, col)) dates =  if not len(potential_index_columns): print(""ERROR: Cannot read metadata: need at least one column that contains the taxon labels."" "" Looking for the first column that contains 'name','strain', or 'accession' in the header."", file=sys.stderr) return dates else: index_col ="
1844,"def process_distributed_process(self, process_distribution, process_distribution, process_distribution, process_distribution, process_distribution, process_distribution, process_distribution, process_distribution, process_distribution, process_distribution, process_distribution, process_distribution, process_distribution, process_distribution, process_distribution) process_distribution, process_distribution, process_distribution, process_distribution, process_distribution, process_distribution, process_distribution, process_distribution, process_distribution, process_distribution, process_distribution, process_distribution, process_distribution, process_distribution, process_distribution, process_distribution, process_distribution, process_distribution, process_distribution, process_distribution, process_distribution, process_distribution, process_distribution, process_distribution, process_distribution, process_distribution","def _run(self, run_dir): file_tools = FileTools() run_dict = file_tools.read_param_file_to_dict(os.path.join(run_dir, 'batch.txt')) #print(run_dict['band_centres_data']) #self.run_params.wavelengths = run_dict['wavelengths'] #run_params = RunParameters() #run_params = file_tools.dict_to_object(run_params, run_dict) #------------------------------------------------ #------------------------------------------------ #self.run_params.update_filenames() #lg.debug('!!!!!!!!!' + run_dict['sky_fp']) if os.path.isfile(run_dict['sky_fp']): sky_file_exists = True lg.info('Found sky_tool generated file' + run_dict['sky_fp']) else: lg.info('No sky_tool generated file, generating one') #try: inp_file = run_dict['sky_fp'] + '_params.txt' #self.run_params.sky_file = inp_file self.run_params.write_sky_params_to_file() #if not os.path.isfile(inp_file): lg.debug('Runing skytool' + os.path.join(self.run_params.exec_path,'skytool_free') + '#') lg.debug(os.path.join(self.run_params.exec_path,'skytool_free') +'params=' + inp_file) os.system(os.path.join(self.run_params"
1845,"def list_of_authors(self, 'authors', 'authors', 'authors', 'authors', 'authors', 'authors', 'authors', 'authors', 'authors', 'authors', 'authors', 'authors', 'authors', 'authors', 'authors', 'authors', 'authors')","def get_author_type_string(self): authorTypeString = '' aStringA = '' aStringB = '' aStringC = '' aStringD = '' authors = dict((k, list(v)) for k, v in groupby(self.authors.all(), lambda a: a.type)) for author in authors: if author == 'author': aStringA += 'Written by'+ self.get_author_string(authors['author']) if author == 'photographer': aStringB += 'Photos by'+ self.get_author_string(authors['photographer']) if author == 'illustrator': aStringC += 'Illustrations by'+ self.get_author_string(authors['illustrator']) if author == 'videographer': aStringD += 'Videos by'+ self.get_author_string(authors['videographer']) if aStringA!= '': authorTypeString += aStringA if aStringB!= '': authorTypeString += ','+ aStringB if aStringC!= '': authorTypeString += ','+ aStringC if aStringD!= '': authorTypeString += ','+ aStringD return authorTypeString"
1846,"def capture_agent_state(self, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state, agent_state,","def control_loop(): set_service_status(Service.AGENTSTATE, ServiceStatus.BUSY) notify.notify('READY=1') notify.notify('STATUS=Running') while not terminate(): notify.notify('WATCHDOG=1') update_agent_state() next_update = timestamp() + config()['agent']['update_frequency'] while not terminate() and timestamp()  next_update: time.sleep(0.1) logger.info('Shutting down agentstate service') set_service_status(Service.AGENTSTATE, ServiceStatus.STOPPED)"
1847,"def add_job(self, job_add_options, custom_headers, custom_headers): self.add_job(self.add_options): self.add_job(self.add_options) self.add_job(self.add_options) self.add_job(self.add_options) self.add_job(self.add_options) self.add_job(self.add_options) self.add_job(self.add_options) self.add_options) self.add_job(self.add_options) self.add_job(self.add_options) self.add_job(self.add_options) self.add_job(self.add_options) self.add_job(self.add_options) self.add_job(self.add_options) self.add_options) self.add_options) self.add_options) self.add_options(self.add_options) self.add_options) self.add_options(self.add_options) self.add_options(self.add_options) self.add_options(self.add_options) self.add_options(self.add_option","def add( self, job, job_add_options=None, custom_headers=None, raw=False, **operation_config): timeout = None if job_add_options is not None: timeout = job_add_options.timeout client_request_id = None if job_add_options is not None: client_request_id = job_add_options.client_request_id return_client_request_id = None if job_add_options is not None: return_client_request_id = job_add_options.return_client_request_id ocp_date = None if job_add_options is not None: ocp_date = job_add_options.ocp_date url = self.add.metadata['url'] path_format_arguments =  'batchUrl': self._serialize.url(""self.config.batch_url"", self.config.batch_url,'str', skip_quote=True)  url = self._client.format_url(url, **path_format_arguments) query_parameters =  query_parameters['api-version'] = self._serialize.query(""self.api_version"", self.api_version,'str') if timeout is not None: query_parameters['timeout'] = self._serialize.query(""timeout"", timeout, 'int') header_parameters =  header_parameters['Content-Type'] = 'application/json; odata=minimalmetadata; charset=utf-8' if self.config.generate_client_request_id: header_parameters['client-request-id'] = str(uuid.uuid1()) if custom_headers: header_parameters.update(custom_head"
1848,def exclude_all_modules(paths): if path is None: if path is None: if path is None: if path is None: if path is None: if path is None: if path is None: if path is None: if path is None: if path is None: if path is None: if path is None: if path is None: if path is None: if path is None: if path is None: if path is,"def skip_path(*paths): paths = (paths and isinstance(paths[0], list)) and paths[0] or paths for path in paths: if not path in SKIPPED_PATHS: SKIPPED_PATHS.append(path) traceback.extract_tb = _new_extract_tb"
1849,def _get_line(self): if self.get_line(self.get_line(self.get_line)): self.get_line(self.get_line): self.get_line(self.get_line) self.get_line(self.get_line) self.get_line(self.get_line) self.get_line(self.get_line) self.get_line(self.get_line) self.get_line(self.get_line) self.get_line) self.get_line(self.get_line) self.get_line) self.get_line(self.get_line) self.get_line) self.get_line(self.get_line) self.get_line) self.get_line(self.get_line) self.get_line) self.get_line(self.get_line) self.get_line) self.get_line(self.get_line) self.get_line) self.get_line(self.get_line) self.get_line(self.get_line) self.get_line(self.get_line) self.get_line(self.get_line) self.get_line(self.get_line) self.get_line(self,"def _tr_paren(line_info): ""Translate lines escaped with: /"" return '%s%s(%s)' % (line_info.pre, line_info.ifun, "", "".join(line_info.the_rest.split()))"
1850,"def extract_function_signature(self, default arguments, keyword-only arguments): self.get_function_signature(self.get_function_signature) self.get_function_signature(self.get_function_signature) self.get_function_signature(self.get_function_signature) self.get_function_signature(self.get_function_signature) self.get_function_signature(self.get_function_signature) self.get_function_signature(self.get_signature) self.get_signature(self.get_signature) self.get_signature(self.get_signature) self.get_signature(self.get_signature) self.get_signature(self.get_signature) self.get_signature(self.get_signature) self.get_signature(self.get_signature) self.get_signature(self.get_signature) self.get_signature(self.get_signature) self.get_signature(self.get_signature) self.get_signature(self.get_signature) self.get_signature(self.get_signature) self.get_signature(self.get_signature) self.get_sig","def __sig_from_func(self, func): if isinstance(func, types.MethodType): argspec = getfullargspec(func.__func__) self.pargl = argspec[0][1:] else: argspec = getfullargspec(func) self.pargl = argspec[0][:] if argspec[3] is not None: def_offset = len(self.pargl) - len(argspec[3]) self.def_argv = dict((self.pargl[def_offset+i],argspec[3][i])  for i in range(len(argspec[3]))) else: self.def_argv =  self.var_pargs = argspec[1] is not None self.var_kargs = argspec[2] is not None self.kargl = argspec[4] if argspec[5] is not None: self.def_argv.update(argspec[5])"
1851,"def execute_request(self, execute_request, apply, execute_request): if self.execute(self.execute(self.execute(self.execute(self.execute, apply, apply, apply))): self.execute(self.execute(self.execute(self.execute))) self.execute(self.execute(self.execute)) self.execute(self.execute) self.execute(self.execute) self.execute) self.execute(self.execute) self.execute(self.execute) self.execute(self.execute) self.execute(self.execute) self.execute(self.execute) self.execute) self.execute(self.execute) self.execute(self.execute) self.execute) self.execute(self.execute) self.execute) self.execute(self.execute) self.execute(self.execute) self.execute) self.execute(self.execute) self.execute) self.execute(self.execute) self.execute) self.execute(self.execute) self.execute(self.execute) self.execute) self.exe","def _handle_execute_reply(self, msg): parent = msg['parent_header'] msg_id = parent['msg_id'] if msg_id not in self.outstanding: if msg_id in self.history: print (""got stale result: %s""%msg_id) else: print (""got unknown result: %s""%msg_id) else: self.outstanding.remove(msg_id) content = msg['content'] header = msg['header'] md = self.metadata[msg_id] md.update(self._extract_metadata(header, parent, content)) self.metadata[msg_id] = md e_outstanding = self._outstanding_dict[md['engine_uuid']] if msg_id in e_outstanding: e_outstanding.remove(msg_id) if content['status'] == 'ok': self.results[msg_id] = ExecuteReply(msg_id, content, md) elif content['status'] == 'aborted': self.results[msg_id] = error.TaskAborted(msg_id) elif content['status'] =='resubmitted': pass else: self.results[msg_id] = self._unwrap_exception(content)"
1852,"def loop_through_messages(self, messages, tasks): self.select(messages) self.select(messages) self.select(messages) self.select(messages) self.select(messages) self.select(messages) self.select(messages) self.select(messages) self.select(messages) self.select(messages) self.select(messages) self.select(messages) self.select(messages)","def message_loop(self, t_q, r_q): t_msg =  while t_msg.get(""state"", """")!= ""__DIE__"": try: t_msg = t_q.get(True, self.cycle_sleep) self.task = t_msg.get(""task"", """") if self.task!= """": self.task.task_start = time.time() self.r_q_send( ""w_id"": self.w_id, ""task"": self.task, ""state"": ""__ACK__"" ) self.cycle_sleep = self.task.worker_loop_delay self.task.result = self.task.run() self.task.task_stop = time.time() self.r_q_send( ""w_id"": self.w_id, ""task"": self.task, ""state"": ""__FINISHED__"" ) self.task = None except Empty: pass except Full: time.sleep(0.1) # except: if self.task is not None: self.task.task_stop = time.time() tb_str = """".join(tb.format_exception(*(sys.exc_info()))) self.r_q_send(  ""w_id"": self.w_id, ""task"": self.task, ""error"": tb_str, ""state"": ""__ERROR__"",  ) return"
1853,"def scout_case_obj(self, server_responses): case_obj(self, server_responses): case_obj(self, server_responses): case_obj(self, server_responses): case_obj(self, server_responses): case_obj(self, server_responses): case_obj(self): case_obj(self, patient_id)","def mme_delete(case_obj, mme_base_url, mme_token): server_responses = [] if not mme_base_url or not mme_token: return 'Please check that Matchmaker connection parameters are valid' for patient in case_obj['mme_submission']['patients']: patient_id = patient['id'] url = ''.join([mme_base_url, '/patient/delete/', patient_id]) resp = matchmaker_request(url=url, token=mme_token, method='DELETE', ) server_responses.append( 'patient_id': patient_id,'message': resp.get('message'),'status_code': resp.get('status_code') ) return server_responses"
1854,"def fetch_topics(self, category, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs)","def fetch_items(self, category, **kwargs): from_date = kwargs['from_date'] logger.info(""Looking for topics at '%s', updated from '%s'"", self.url, str(from_date)) ntopics = 0 topics_ids = self.__fetch_and_parse_topics_ids(from_date) for topic_id in topics_ids: topic = self.__fetch_and_parse_topic(topic_id) ntopics += 1 yield topic logger.info(""Fetch process completed: %s topics fetched"", ntopics)"
1855,def ________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________,"def first_lines(self, lines, *ignores): ignore = set() for ign in ignores: ignore.update(ign) lset = set() for l in lines: if l in ignore: continue new_l = self.first_line(l) if new_l not in ignore: lset.add(new_l) return lset"
1856,"def get_function(self, functionBody, functionType): return self.get_function(functionBody, functionType): return self.get_function(functionType)","def format_function( func_body, func_type=None, indent=2, format_locals=True, ): if func_type is None: yield 'func' else: param_section ='(param )'.format(' '.join( map(format_lang_type, func_type.param_types) )) if func_type.param_types else '' result_section ='(result )'.format( format_lang_type(func_type.return_type) ) if func_type.return_type else '' yield 'func' + param_section + result_section if format_locals and func_body.locals: yield '(locals )'.format(' '.join(itertools.chain.from_iterable( itertools.repeat(format_lang_type(x.type), x.count) for x in func_body.locals ))) level = 1 for cur_insn in decode_bytecode(func_body.code): if cur_insn.op.flags & INSN_LEAVE_BLOCK: level -= 1 yield'' * (level * indent) + format_instruction(cur_insn) if cur_insn.op.flags & INSN_ENTER_BLOCK: level += 1"
1857,def initialise_django_ddp(setup lookups and signal handlers): django_ddp = django_ddp = django_ddp = django_ddp = django_ddp = django_ddp = django_ddp = django_ddp = django_ddp = django_ddp = django_ddp = django_ddp = django_ddp = django_ddp = django_ddp = django_ddp = django_ddp = django_ddp = django_ddp = django_ddp = django_ddp = django_ddp = django_ddp = django_ddp = django_ddp = django_ddp = django_ddp = django_ddp = django_ddp = django_ddp = djan,"def ready(self): if not settings.DATABASES: raise ImproperlyConfigured('No databases configured.') for (alias, conf) in settings.DATABASES.items(): engine = conf['ENGINE'] if engine not in [ 'django.db.backends.postgresql', 'django.db.backends.postgresql_psycopg2', ]: warnings.warn( 'Database %r uses unsupported %r engine.' % ( alias, engine, ), UserWarning, ) self.api = autodiscover() self.api.ready()"
1858,"def auth_hash(self, auth_hash): if self.get_auth_hash(self.get_auth_hash): self.get_auth_hash(self.get_auth_hash) self.get_auth_hash(self.get_auth_hash) self.get_auth_hash(self.get_auth_hash) self.get_auth_hash(self.get_auth_hash) self.get_auth_hash) self.get_auth_hash(self.get_auth_hash) self.get_auth_hash(self.get_auth_hash) self.get_auth_hash(self.get_auth_hash) self.get_auth_hash) self.get_auth_hash(self.get_auth_hash) self.get_auth_hash(self.get_auth_hash) self.get_auth_hash(self.get_auth_hash) self.get_auth_hash(self.get_auth_hash) self.get_auth_hash(self.get_auth_hash","def calc_expiry_time(minutes_valid): return ( timezone.now() + datetime.timedelta(minutes=minutes_valid + 1) ).replace(second=0, microsecond=0)"
1859,def init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init_client_init,"def _init_client(self, from_archive=False): return KitsuneClient(self.url, self.archive, from_archive)"
1860,"def ship_relationship(self, ship_relationship): ship_relationship(ship_relationship): ship_relationship(ship_relationship): ship_relationship(ship_relationship): ship_relationship(ship_relationship): ship_relationship(ship_relationship): ship_relationship(ship_relationship): ship_relationship(ship_relationship): ship_relationship(ship_relationship): ship_relationship): ship_relationship(ship_relationship): ship_relationship(ship_relationship): ship_relationship(ship_relationship): ship_relationship(ship_relationship): ship_relationship): ship_relationship(ship_relationship): ship_relationship): ship_relationship(ship_relationship): ship_relationship): ship_relationship(ship_relationship): ship_relationship(ship_relationship): ship_relationship): ship_relationship(ship_relationship): ship_relationship): ship_relationship(ship_relationship): ship_relationship)","def get_relationship(self, from_object, relation_type): for rel in self.relationships.get(relation_type, ()): if rel.from_object is from_object: return rel raise KeyError(relation_type)"
1861,"def write_pipeline_attributes(self, pipeline_attributes, json): json = self.write_pipeline_attributes(pipeline_attributes) self.write_pipeline_attributes(pipeline_attributes) self.write_pipeline_attributes(pipeline_attributes) self.write_pipeline_attributes(pipeline_attributes) self.write_pipeline_attributes(pipeline_attributes) self.write_pipeline_attributes) self.write_attributes(pipeline_attributes) self.write_attributes(pipeline_attributes) self.write_attributes(pipeline_attributes) self.write_attributes(pipeline_attributes) self.write_attributes) self.write_attributes(pipeline_attributes) self.write_attribute_attribute_attribute_attribute_attribute_attribute_attribute_attribute_attribute_attribute_attribute_attribute_attribute_attribute_attribute_attribute_attribut","def render_pipeline(self): dict_viz =  ""name"": ""root"", ""children"": []  last_of_us =  f_tree = self._fork_tree if self._fork_tree else 1: [1] for x, (k, v) in enumerate(f_tree.items()): for p in self.processes[1:]: if x == 0 and p.lane not in [k] + v: continue if x > 0 and p.lane not in v: continue if not p.parent_lane: lst = dict_viz[""children""] else: lst = last_of_us[p.parent_lane] tooltip =  ""name"": ""_"".format(p.template, p.pid), ""process"":  ""pid"": p.pid, ""input"": p.input_type, ""output"": p.output_type if p.output_type else ""None"", ""lane"": p.lane,, ""children"": []  dir_var = """" for k2, v2 in p.directives.items(): dir_var += k2 for d in v2: try: directive = v2[d].replace(""'"", """").replace('""', '')  if isinstance(v2[d], str) else v2[d] dir_var += "": "".format(d, directive) except KeyError: pass if dir_var: tooltip[""process""][""directives""] = dir_var else: tooltip[""process""][""directives""] = ""N/A"" lst.append(tooltip) last_of_us[p.lane] = lst[-1][""children""] self.dag_to_file(dict_viz) with open("
1862,"def generate_metric_plot(self, server): if self.get_metric_plot(server): self.get_metric_plot(server): self.get_metric_plot(server): self.get_metric_plot(server): self.get_metric_plot(server)","def plot(self, type=""roc"", server=False): assert_is_type(type, ""roc"") try: imp.find_module('matplotlib') import matplotlib if server: matplotlib.use('Agg', warn=False) import matplotlib.pyplot as plt except ImportError: print(""matplotlib is required for this function!"") return if type == ""roc"": plt.xlabel('False Positive Rate (FPR)') plt.ylabel('True Positive Rate (TPR)') plt.title('ROC Curve') plt.text(0.5, 0.5, r'AUC=0:.4f'.format(self._metric_json[""AUC""])) plt.plot(self.fprs, self.tprs, 'b--') plt.axis([0, 1, 0, 1]) if not server: plt.show()"
1863,"def connect_to_cloud_speech_v1.SpeechClient(self, google.cloud.speech_v1.SpeechClient): if self.get_cloud_speech_v1.SpeechClient(self.get_cloud_speech_v1.SpeechClient): self.get_cloud_speech_v1.SpeechClient(self.get_cloud_speech_v1.Speech_v1.Speech_v1.Speech_v1.SpeechClient)",def get_conn(self): if not self._client: self._client = SpeechClient(credentials=self._get_credentials()) return self._client
1864,"def pre_process_list(self, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR, STR,","def str_variants(store, institute_obj, case_obj, variants_query, page=1, per_page=50): return variants(store, institute_obj, case_obj, variants_query, page, per_page)"
1865,"def parse_address(self, address, language, country): if address is not self.parse_address = self.parse_address = self.parse_address = self.parse_address = self.parse_address = self.parse_address = self.parse_address = self.parse_address = self.parse_address = self.parse_address = self.parse_address = self.parse_address = self.parse_address = self.parse_address = self.parse_address = self.parse_address = self.parse_address_address_address_address_address_address_address_address_address_address_address_address_address_address_address_address_address_address_address_address_address_address_address_address_address_address_address_a","def parse_address(address, language=None, country=None): address = safe_decode(address, 'utf-8') return _parser.parse_address(address, language=language, country=country)"
1866,def memoize_decorator(self): self.decorator(self.decorator): self.decorator(self.decorator) self.decorator(self.decorator) self.decorator(self.decorator) self.decorator(self.decorator) self.decorator(self.decorator) self.decorator(self.decorator) self.decorator(self.decorator) self.decorator(self.decorator) self.decorator(self.decorator) self.decorator(self.decorator) self.decorator) self.decorator) self.decorator(self.decorator) self.decorator) self.decorator(self.decorator) self.decorator) self.decorator(self.decorator) self.decorator) self.decorator(self.decorator) self.decorator) self.decorator(self.decorator) self.decorator(self.decorator) self.decorator(self.decorator) self.decorator) self.decorator(self.decorator) self.decorator(self.decorator) self.decor,def memoize(f): cache=  def memf(*x): if x not in cache: cache[x] = f(*x) return cache[x] return memf
1867,"def delete_membership(self, membershipId, basestring): if self.create_membership(membershipId, basestring): self.create_membership(membershipId, basestring): self.create_membership(membershipId, basestring) self.create_membership(membershipId, basestring) self.create_membership(membershipId) self.create_membership(membershipId) self.create_membership(membershipId) self.create_membershipId) self.create_membershipId(membershipId) self.create_membershipId) self.create_membershipId(membershipId) self.create_membershipId) self.create_membershipId(membershipId) self.create_membershipId(membershipId) self.create_membershipId) self.create_membershipId(membershipId) self.create_membershipId(membershipId) self.create_membershipId(membershipId) self.create_membershipId(membershipId) self.create_membershipId(membershipId) self.create_membershipId(membershipId) self.create_membership","def delete(self, membershipId): check_type(membershipId, basestring) self._session.delete(API_ENDPOINT + '/' + membershipId)"
1868,"def set_many(self, lens.Each()): self.set_many(self.set_many) self.set_many(self.set_many) self.set_many(self.set_many) self.set_many(self.set_many) self.set_many(self.set_many) self.set_many(self.set_many) self.set_many(self.set_many) self.set_many(self.set_many) self.set_many(self.set_many)","def set_many(self, new_values): def setter_many(state): return self._optic.iterate(state, new_values) return setter_many"
1869,"def fetch_issues(self, category, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs)","def fetch_items(self, category, **kwargs): from_date = kwargs['from_date'] logger.info(""Fetching issues of '%s' from %s"", self.url, str(from_date)) nissues = 0 for issue_id in self.__fetch_issues_ids(from_date): issue = self.__fetch_and_parse_issue(issue_id) for key in USER_FIELDS: if key not in issue: continue user = self.__get_or_fetch_user(issue[key]['id']) issue[key + '_data'] = user for journal in issue['journals']: if 'user' not in journal: continue user = self.__get_or_fetch_user(journal['user']['id']) journal['user_data'] = user yield issue nissues += 1 logger.info(""Fetch process completed: %s issues fetched"", nissues)"
1870,"def tiny_value(x, np.ndarray, x): if x is integer, tiny_value = np.ndarray, tiny_value = np.ndarray, tiny_value = np.ndarray, tiny_value = np.ndarray, tiny_value = np.ndarray, tiny_value = np.ndarray, tiny_value = np.ndarray, tiny_value, tiny_value, tiny_value, tiny_value, tiny_value, tiny_value, tiny_value, tiny_value, tiny_value, tiny_value, tiny_value, tiny_value, tiny_value, tiny_value, tiny_value, tiny_value, tiny_value, tiny_value, tiny_value, tiny_value, tiny_value, tiny_value, tiny_value, tiny_value, np.np.np.np.np.np.np.np.np.np.np.np.np.np.np.np.np.np.np.np.np.np.np.np.np.np.np.np.np.np.n","def tiny(x): x = np.asarray(x) if np.issubdtype(x.dtype, np.floating) or np.issubdtype(x.dtype, np.complexfloating): dtype = x.dtype else: dtype = np.float32 return np.finfo(dtype).tiny"
1871,"def copy_text_to_clipboard(self, text, prompts): self.copy_text_to_clipboard(text, prompts) self.copy_text_to_clipboard(text, prompts) self.copy_text_to_clipboard(text, prompts) self.copy_text_to_clipboard(text, prompts) self.copy_text_to_clipboard(text, prompts) self.copy_text_to_clipboard(text, prompts) self.copy_text_to_clipboard(text, prompts) self.copy_text_to_clipboard(text, prompts) self.copy_text_to_clipboard(text, prompts) self.copy_text_to_clipboard(text, prompts) self.copy_to_clipboard(text, prompts) self.copy_to_clipboard(text, prompts) self.copy_to_clipboard(text, prompts) self.copy_to_clipboard(text, prompts) self.copy_to_clipboard(text, prompts) self.copy_to_clipboard(text, prompts) self.copy_to_clipboard(text, prompts) self.copy_to_clipboard(text, prompts) self.copy_to_clipboard","def copy(self): if self._page_control is not None and self._page_control.hasFocus(): self._page_control.copy() elif self._control.hasFocus(): text = self._control.textCursor().selection().toPlainText() if text: lines = map(self._transform_prompt, text.splitlines()) text = 'n'.join(lines) QtGui.QApplication.clipboard().setText(text) else: self.log.debug(""frontend widget : unknown copy target"")"
1872,"def parse_rules(self, rules): self.parse_rules(rules) self.parse_rules(rules) self.parse_rules(rules) self.parse_rules(rules) self.parse_rules(rules) self.parse_rules(rules) self.parse_rules(rules) self.parse_rules(rules) self.parse_rules(rules) self.parse_rules(rules) self.parse_rules(rules) self.parse_rules) self.parse_rules(rules) self.parse_rules) self.parse_rules(rules) self.parse_rules(rules) self.parse_rules(rules) self.parse_rules) self.parse_rules(rules) self.parse_rules(rules) self.parse_rules(rules) self.","def get_rules(self) -> parsing.Node: res = None try: res = self.eval_rule('bnf_dsl') if not res: self.diagnostic.notify( error.Severity.ERROR, ""Parse error in '%s' in EBNF bnf"" % self._lastRule, error.LocationInfo.from_maxstream(self._stream) ) raise self.diagnostic except error.Diagnostic as d: d.notify( error.Severity.ERROR, ""Parse error in '%s' in EBNF bnf"" % self._lastRule ) raise d return res"
1873,"def best_learning_rate_schedule(self, args): self.get_learning_rate_schedule(args) self.get_learning_rate_schedule(args) self.get_learning_rate_schedule(args) self.get_learning_rate_schedule(args) self.get_learning_rate_schedule(args) self.get_learning_rate_div(args) self.get_learning_rate_div(args) self.get_learning_rate_div(args) self.get_learning_rate_div(args) self.get_learning_rate_div(args) self.get_learning_rate_div(args) self.get_learning_rate_div(args) self.get_learning_rate_div(args) self.get_learning_rate_div(args) self.get_learning_rate_div(args) self.get_learning_rate_div(args) self.get_learning_rate_div(args) self.get_learning_rate_div(args) self.get_learning_rate_div(args) self.get_learning_rate_div(args) self.get_","def fit(self, *args): data = list(zip(*args)) self.save() if self._fit_batch_size is None: raise ConfigError(""in order to use fit() method"" "" set fit_batch_size parameter"") bs = int(self._fit_batch_size) data_len = len(data) num_batches = self._fit_max_batches or ((data_len - 1) // bs + 1) avg_loss = 0. best_loss = float('inf') lrs, losses = [], [] _lr_find_schedule = DecayScheduler(start_val=self._fit_learning_rate[0], end_val=self._fit_learning_rate[1], dec_type=""exponential"", num_it=num_batches) self._lr = _lr_find_schedule.start_val self._mom = 0. self._update_graph_variables(learning_rate=self._lr, momentum=self._mom) best_lr = _lr_find_schedule.start_val for i in range(num_batches): batch_start = (i * bs) % data_len batch_end = batch_start + bs report = self.train_on_batch(*zip(*data[batch_start:batch_end])) if not isinstance(report, dict): report = 'loss': report avg_loss = self._fit_beta*avg_loss + (1 - self._fit_beta)*report['loss'] smoothed_loss = avg_loss / (1 - self._fit_beta**(i + 1)) lrs.append(self._lr) losses.append(smoothed_loss)"
1874,"def date_oject(self, datetime, time_format, time_format): dateutil.parser(datetime, time_format): dateutil.parser(datetime, time_format): dateutil.parser(datetime, time_format): dateutil.parser(datetime, time_format): dateutil.parser(datetime, time_format): dateutil.parser(datetime, time_format): dateutil.parser(datetime, time_format)","def get_date(datetime, time_format=None): if time_format is None: t = du.parser.parse(datetime) else: t = dt.datetime.strftime(datetime, time_format) return t"
1875,"def ask_yes_no_input(self, ""question""): ask_yes_no_input(question): ask_yes_no_input(question): ask_yes_no_input(question): ask_yes_no_input(question): ask_yes_no_input(question) return yes_no_input(question)","def query_yes_quit(question, default=""quit""): valid = ""yes"": Answers.YES, ""y"": Answers.YES, ""ye"": Answers.YES, ""quit"": Answers.QUIT, ""q"": Answers.QUIT if default is None: prompt = "" [y/q] "" elif default == ""yes"": prompt = "" [Y/q] "" elif default == ""quit"": prompt = "" [y/Q] "" else: raise ValueError(""invalid default answer: '%s'"" % default) while True: sys.stdout.write(question + prompt) choice = input().lower() if default is not None and choice == '': return valid[default] elif choice in valid: return valid[choice] else: sys.stdout.write(""Please respond with 'yes' or 'quit' "" ""(or 'y' or 'q').n"")"
1876,"def get_cluster_sizes(self, train, valid, xval): self.cluster_sizes = self.cluster_sizes() self.cluster_sizes() self.cluster_sizes() self.cluster_sizes() self.cluster_sizes() self.cluster_sizes() self.cluster_sizes() self.cluster_sizes() self.cluster_sizes() self.cluster_sizes() self.cluster_sizes(self.cluster_sizes) self.cluster_sizes() self.cluster_sizes() self.cluster_sizes() self.cluster_sizes() self.cluster_sizes() self.cluster_sizes() self.cluster_sizes() self.cluster_sizes() self.cluster_sizes() self.cluster_sizes() self.cluster_sizes() self.cluster_sizes() self.cluster_sizes() self.cluster_sizes() self.cluster_sizes() self.cluster_sizes() self.cluster_sizes() self.cluster_sizes() self.cluster_sizes() self.cluster_sizes() self.cluster_sizes() self.cluster_size","def size(self, train=False, valid=False, xval=False): tm = ModelBase._get_metrics(self, train, valid, xval) m =  for k, v in tm.items(): m[k] = None if v is None else [v[2] for v in v._metric_json[""centroid_stats""].cell_values] return list(m.values())[0] if len(m) == 1 else m"
1877,"def update_all_displaying_values(self, value, value, value, value, value, value, value, value, value, value, value, value, value, value, value, value, value, value, value, value, value, value)","def UpdateValues(self, grid): ""Update all displayed values"" msg = gridlib.GridTableMessage(self, gridlib.GRIDTABLE_REQUEST_VIEW_GET_VALUES) grid.ProcessTableMessage(msg)"
1878,"def translatable_fields(self, superclasses, superclasses, superclasses, superclasses, superclasses, superclasses, superclasses, superclasses, superclasses, superclasses, superclasses, superclasses, superclasses, superclasses, superclasses, superclasses, superclasses, superclasses, superclasses, superclasses, superclasses, superclasses, superclasses, superclasses, superclasses, superclasses, superclasses, superclasses, superclasses, superclasses, superclasses)","def get_all_translatable_fields(model, model_trans_fields=None, column_in_current_table=False): if model_trans_fields is None: model_trans_fields = set() model_trans_fields.update(set(getattr(model._meta, 'translatable_fields', []))) for parent in model.__bases__: if getattr(parent, '_meta', None) and (not column_in_current_table or parent._meta.abstract): get_all_translatable_fields(parent, model_trans_fields, column_in_current_table) return tuple(model_trans_fields)"
1879,"def seq_list(self, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list) returns seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list, seq_list","def uniqify(cls, seq): seen = set() seen_add = seen.add return [ x for x in seq if x not in seen and not seen_add(x)]"
1880,"def HTML5_Boilerplate(self, HTML5_Boilerplate): if self.________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________","def djfrontend_h5bp_css(version=None): if version is None: version = getattr(settings, 'DJFRONTEND_H5BP_CSS', DJFRONTEND_H5BP_CSS_DEFAULT) return format_html( 'link rel=""stylesheet"" href=""0djfrontend/css/h5bp/1/h5bp.css"">', _static_url, version)"
1881,def c.dictzip(keys): c.dictzip(keys): c.dictzip(keys): c.dictzip(keys): c.dictzip(keys): c.dictzip(keys): c.dictzip(keys): c.dictzip(keys): c.dictzip(keys): c.dictzip(keys): c.dictzip(keys): c.dictzip(keys): c.dictzip(keys): c.dictzip(keys): c.dictzip(keys): c.dictzip(keys): c.dictzip(keys): c.dictzip(keys): c.dictzip(keys): c.dictzip(keys): c.dictzip(keys): c.dictzip(keys): c.dictzip(keys): c.dictzip(keys): c.dictzip(keys): c.dictzip(keys): c.dictzip(keys): c.dictzip(keys): c.dictzip(keys): c.dictzip(key,"def dictzip(self, keys): return Scalar(dict(zip(_unwrap(keys), self.val())))"
1882,"def get_project_dependencies(self, args): self.get_project_dependencies(args): self.get_project_dependencies(args) self.get_project_dependencies(args) self.get_project_dependencies(args) self.get_project_dependencies(args) self.get_project_dependencies(args) self.get_project_dependencies(args) self.get_project_dependencies(args) self.get_project_dependencies(args) self.get_dependencies(args) self.get_dependencies(args) self.get_dependencies(args) self.get_dependencies(args) self.get_dependencies(args) self.get_dependencies(args) self.get_dependencies(args) self.get_dependencies) self.get_dependencies(args) self.get_dependencies(args) self.get_dependencies(args) self.get_dependencies(args) self.get_dependencies(args) self.get_dependencies(args) self.get_dependencies(args) self.get_dependencies(args) self.get_dependencies(args) self.get_dependencies(","def get_dependent_projects(self, recursive = True): projects =  for name, ref in list(self.dependencies.items()): try: prj = self.vcp.projects[name] except KeyError: logger.error(""Unknown project '%s' in project '%s' dependencies!"", name, self.name) continue projects[name] = prj if recursive: projects.update(prj.get_dependent_projects()) return projects"
1883,"def run_name_or_idx(self, run_name_or_idx): run_name_or_idx: run_name_or_idx: run_name_or_idx: run_name_or_idx: run_name_or_idx: run_name_or_idx: run_name_or_idx: run_name_or_idx: run_name_or_idx: run_name_or_idx: run_name_or_idx: run_name_or_idx: run_name_or_idx: run_idx: run_idx: run_idx: run_idx: run_idx: run_idx: run_idx: run_idx: run_idx: run_idx: run_idx: run_idx: run_idx: run_idx: run_idx: run_idx: run_idx: run_idx: run_idx: run_idx: run_idx: run_idx: run_idx: run_idx:","def f_start_run(self, run_name_or_idx=None, turn_into_run=True): if self._run_started: return self if run_name_or_idx is None: if self.v_idx == -1: raise ValueError('Cannot start run if trajectory is not set to a particular run') else: self.f_set_crun(run_name_or_idx) self._run_started = True if turn_into_run: self._make_single_run() self._set_start() return self"
1884,def create_function(self): self.get_function(self.get_function(self.get_function(self.get_function(self.get_function(self.get_function(self.get_function(self.get_function(self.get_function(self.get_function(self.get_function(self.get_function(self.get_function(self.get_function(self.get_function(self.get_function(self.get_function(self.get_function(self.get_function(self.get_function(self.get_function)))))))))))),"def generic(func): def decorate(f): for o in obs: if _by_object.setdefault(id(o), (o,f))[1] is not f: raise TypeError( ""%r already has method for object %r"" % (func, o) ) return f return decorate def dispatch(*args, **kw): f = _gbo(id(args[0]), _sentinel) if f is _sentinel: for t in type(args[0]).__mro__: f = _gbt(t, _sentinel) if f is not _sentinel: return f(*args, **kw) else: return func(*args, **kw) else: return f[1](*args, **kw) dispatch.__name__ = func.__name__ dispatch.__dict__ = func.__dict__.copy() dispatch.__doc__ = func.__doc__ dispatch.__module__ = func.__module__ dispatch.when_type = when_type dispatch.when_object = when_object dispatch.default = func dispatch.has_object = lambda o: id(o) in _by_object dispatch.has_type = lambda t: t in _by_type return dispatch"
1885,"def insert_rows(self, grid, table): if grid is not self.get_rows(rows): if grid is not self.get_rows(rows): if grid is not self.get_rows(rows)","def append(self, values): ""Insert a number of rows into the grid (and associated table)"" if isinstance(values, dict): row = GridRow(self, **values) else: row = GridRow(self, *values) list.append(self, row) self._grid_view.wx_obj.AppendRows(numRows=1)"
1886,"def send_request(self, request_id, response_code): self.request(self.request(self.request_id)) self.request(self.request_id) self.request(self.request_id) self.request(self.request_id) self.request(self.request_id) self.request(self.request_id) self.request(self.request_id) self.request_id) self.request_id) self.request(self.request_id) self.request_id) self.request_id) self.request(self.request_id) self.request_id) self.request_id) self.request(self.request_id) self.request_id) self.request_id) self.request_id(self.request_id) self.request_id(self.request_id) self.request_id(self.request_id) self.request_id(self.request_id) self.request_id(self.request_id)","def track_request(self, name: str, url: str, success: bool, start_time: str=None, duration: int=None, response_code: str =None, http_method: str=None, properties: Dict[str, object]=None, measurements: Dict[str, object]=None, request_id: str=None): raise NotImplementedError('BotTelemetryClient.track_request(): is not implemented.')"
1887,def _compose_link(self): self.compose_link(self.compose_link) self.compose_link(self.compose_link) self.compose_link(self.compose_link) self.compose_link(self.compose_link) self.compose_link(self.compose_link) self.compose_link(self.compose_link) self.compose_link(self.compose_link) self.compose_link(self.compose_link) self.compose_link(self.compose_link) self.compose_link) self.compose_link(self.compose_link) self.compose_link) self.compose_link(self.compose_link) self.compose_link(self.compose_link) self.compose_link) self.compose_link(self.compose_link) self.compose_link(self.compose_link) self.compose_link(self.compose_link) self.compose_link(self.compose_link) self.compose_,"def beacon_link(variant_obj, build=None): build = build or 37 url_template = (""https://beacon-network.org/#/search?pos=this[position]&"" ""chrom=this[chromosome]&allele=this[alternative]&"" ""ref=this[reference]&rs=GRCh37"") return url_template.format(this=variant_obj)"
1888,"def extract_view(self, TemplateResponse, ContextMixin, TemplateResponse, TemplateResponse, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin, ContextMixin,","def _get_view_data(self, context_data): view = context_data.get('view') if not isinstance(view, View): view = None template_context = [] for key, obj in context_data.items(): if isinstance(obj, (BaseForm, BaseFormSet, Model)): template_context.append((key, _format_path(obj.__class__))) return 'model': _get_view_model(view), 'form': _get_form_class(view), 'template_context': template_context,"
1889,"def parse_individual_id(self, father, mother, phenotype, vcf2cytosure, vcf2cytosure, vcf2cytosure, vcf2cytosure, vcf2cytosure, vcf2cytosure, vcf2cytosure, vcf2cytosure, vcf2cytosure, vcf2cytosure, vcf2cytosure, vcf2cytosure, vcf2cytosure, vcf2cytosure, vcf2cytosure, vcf2cytosure)","def parse_individual(sample): ind_info =  if'sample_id' not in sample: raise PedigreeError(""One sample is missing'sample_id'"") sample_id = sample['sample_id'] if'sex' not in sample: raise PedigreeError(""Sample %s is missing'sex'"" % sample_id) sex = sample['sex'] if sex not in REV_SEX_MAP: log.warning(""'sex' is only allowed to have values from "".format(', '.join(list(REV_SEX_MAP.keys())))) raise PedigreeError(""Individual %s has wrong formated sex"" % sample_id) if 'phenotype' not in sample: raise PedigreeError(""Sample %s is missing 'phenotype'"" % sample_id) phenotype = sample['phenotype'] if phenotype not in REV_PHENOTYPE_MAP: log.warning(""'phenotype' is only allowed to have values from "".format(', '.join(list(REV_PHENOTYPE_MAP.keys())))) raise PedigreeError(""Individual %s has wrong formated phenotype"" % sample_id) ind_info['individual_id'] = sample_id ind_info['display_name'] = sample.get('sample_name', sample['sample_id']) ind_info['sex'] = sex ind_info['phenotype'] = phenotype ind_info['father'] = sample.get('father') ind_info['mother'] = sample.get('mother'"
1890,"def log_prob(response, r): if r = r, r = r.log_prob(response, r) if r = r.log_prob(response, r) if r = r.log_prob(response, r) if r = r.log_prob(response, r)) if r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r = r","def log_prob(self, response, predicted_linear_response, name=None): with self._name_scope( name, 'log_prob', [response, predicted_linear_response]): dtype = dtype_util.common_dtype([response, predicted_linear_response]) response = tf.convert_to_tensor( value=response, dtype=dtype, name='response') predicted_linear_response = tf.convert_to_tensor( value=predicted_linear_response, name='predicted_linear_response') return self._log_prob(response, predicted_linear_response)"
1891,"def set_value(self, s3conf): self.set_value = self.set_value(s3conf) self.set_value(s3conf) self.set_value(s3conf) self.set_value(s3conf) self.set_value(s3conf) self.set_value(s3conf) self.set_value(s3conf) self.set_value(s3conf) self.set_value(s3conf) self.set_value(s3conf) self.set_value(s3conf) self.set_value(s3conf) self.set_value(s3conf) self.set_value(s3conf) self.set_value(s3conf) self.set_value(s3conf) self.set_value(s3conf) self.set_value(s3conf) self.set_value(s3conf) self.set_value(s3conf) self.set_value(s3conf) self.set_value(s3conf) self.set_value(s3conf) self.set_value(s3conf) self.set_value(s3conf) self.set_value(s3conf) self.set_value(s3conf) self.set_value(s3conf","def set_variable(section, value, create): if not value: value = section section = None try: logger.debug('Running env command') settings = config.Settings(section=section) conf = s3conf.S3Conf(settings=settings) env_vars = conf.get_envfile() env_vars.set(value, create=create) except exceptions.EnvfilePathNotDefinedError: raise exceptions.EnvfilePathNotDefinedUsageError()"
1892,def _verify_player(self): if self._verify_player(self._verify_player): self._verify_player(self._verify_player) self._verify_player(self._verify_player) self._verify_player(self._verify_player) self._verify_player(self._verify_player) self._verify_player(self._verify_player) self._verify_player(self._verify_player) self._verify_player(self._verify_player) self._verify_player(self._verify_player) self._verify_player(self._verify_player) self._verify_player(self._verify_player) self._verify_verify_player(self._verify_player) self._verify_player(self._verify_player) self._verify_player(self._verify_player) self._verify_player(self._verify_player) self._verify_player(self._verify_player) self._verify_player(self._verify_player) self._verify_player(self,"def _pv_params(cls, session, pvswf, pv, **request_params): try: data, hdntl = pv.split("";"") except ValueError: data = pv hdntl = """" cache = Cache(filename=""stream.json"") key = ""akamaihd-player:"" + pvswf cached = cache.get(key) request_params = deepcopy(request_params) headers = request_params.pop(""headers"", ) if cached: headers[""If-Modified-Since""] = cached[""modified""] swf = session.http.get(pvswf, headers=headers, **request_params) if cached and swf.status_code == 304: hash = cached[""hash""] else: hash = sha256() hash.update(swfdecompress(swf.content)) hash = base64.b64encode(hash.digest()).decode(""ascii"") modified = swf.headers.get(""Last-Modified"", """") if len(modified)  40: cache.set(key, dict(hash=hash, modified=modified)) msg = ""st=0exp=9999999999acl=*data=0!1"".format(data, hash) auth = hmac.new(AKAMAIHD_PV_KEY, msg.encode(""ascii""), sha256) pvtoken = ""0hmac=1"".format(msg, auth.hexdigest()) params = [(""pvtoken"", pvtoken)] params.extend(parse_qsl(hdntl, keep_bla"
1893,"def _compute_context_vector(self, context_vector, soft_attention, soft_attention, soft_attention, soft_attention, soft_attention, soft_attention, soft_attention, soft_attention, soft_attention, soft_attention, soft_attention, soft_attention, soft_attention, soft_attention, soft_attention, soft_attention, soft_attention, soft_attention, soft_attention, soft_attention, soft_attention, soft_attention, soft_attention, soft_attention, soft_attention, soft_attention, soft_attention, soft_attention, soft_attention, soft_attention, soft_attention, soft_attention, soft_attention, soft_attention, soft_attention, soft_attention, soft_attention).","def compute_context_vector(self, prev_state, inputs, precomputed_values=None, mask=None): precomputed_values = precomputed_values if precomputed_values else self.precompute(inputs) align_weights = self.compute_alignments(prev_state, precomputed_values, mask) context_vector = T.sum(align_weights[:, :, None] * inputs, axis=1) return context_vector"
1894,"def compute_harmonics(x, shape=x, h_range=x, axis=x, axis=x, axis=x, axis=x, axis=x, axis=x, axis=x, axis=x, axis=x, axis=x, axis=x, axis=x, axis=x, axis=x, axis=x, axis=x, axis = x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f,","def interp_harmonics(x, freqs, h_range, kind='linear', fill_value=0, axis=0): out_shape = [len(h_range)] out_shape.extend(x.shape) x_out = np.zeros(out_shape, dtype=x.dtype) if freqs.ndim == 1 and len(freqs) == x.shape[axis]: harmonics_1d(x_out, x, freqs, h_range, kind=kind, fill_value=fill_value, axis=axis) elif freqs.ndim == 2 and freqs.shape == x.shape: harmonics_2d(x_out, x, freqs, h_range, kind=kind, fill_value=fill_value, axis=axis) else: raise ParameterError('freqs.shape= does not match''input shape='.format(freqs.shape, x.shape)) return x_out"
1895,"def insert_child(self, child, child, child, child, child, child, child, child, child, child, child, child, child, child, child, child, child, child, child, child, child, child, child, child, child, child, child, child)","def insert_child ( self, object, index, child ): if isinstance( child, Subgraph ): object.subgraphs.insert( index, child ) elif isinstance( child, Cluster ): object.clusters.insert( index, child ) elif isinstance( child, Node ): object.nodes.insert( index, child ) elif isinstance( child, Edge ): object.edges.insert( index, child ) else: pass"
1896,"def _export_gene_panels(self, panels): self.export_gene_panels(panels) self.export_gene_panels(panels) self.export_gene_panels(panels) self.export_gene_panels(panels) self.export_gene_panels(panels) self.export_gene_panels(panels) self.export_gene_panels(panels) self.export_gene_panels) self.export_gene_panels(panels) self.export_gene_panels(panels) self.export_gene_panels) self.export_gene_panels(panels) self.export_gene_panels) self.export_gene_panels(panels) self.export_gene_panels) self.export_gene_panels(panels) self.export_gene_panels(panels) self.export_gene_panels) self.export_gene_panels(panels) self.export_gene_panels(panels) self.export_gene_panels) self.export_","def panel(context, panel, build, bed, version): LOG.info(""Running scout export panel"") adapter = context.obj['adapter'] chromosomes_found = set() if not panel: LOG.warning(""Please provide at least one gene panel"") context.abort() LOG.info(""Exporting panels: "".format(', '.join(panel))) if bed: if version: version = [version] lines = export_panels( adapter=adapter, panels=panel, versions=version, build=build, ) else: lines = export_gene_panels( adapter=adapter, panels=panel, version=version, ) for line in lines: click.echo(line)"
1897,"def fetch_values(self, base): if self.get_values(base): self.get_values(base): self.get_values(base) self.get_values(base) self.get_values(base) self.get_values(base) self.get_values(base) self.get_values(base) self.get_values(base) self.get_values(base) self.get_values(base) self.get_values(base) self.get_values(base) self.get_values(base) self.get_values(base) self.get_values(base) self.get_values(base) self.get_values(base) self.get_values(base) self.get_values(base) self.get_values(base) self.get_values(base) self.get_values(base) self.get_values(base) self.get_values(base) self.get_values(base) self.get_values(base) self.get_values(base) self.get_values(base) self.get_values(base) self.get_values(base) self.get_","def get_or_set_default(self, section, option, value): try: ret = self.get(section, option) except MissingSetting: self.set(section, option, value) ret = value return ret"
1898,"def transform_multidim_to_1d_ref(self, ast, ast, ast, ast, ast, ast, ast, ast, ast, ast, ast, ast, ast, transform_multidim_to_1d_ref, transform_multidim_to_1d_ref, transform_multidim_to_1d_ref, transform_multidim_to_1d_ref, transform_multidim_to_1d_ref, transform_multidim_to_1d_ref, transform_multidim_to_1d_ref, transform_multidim_to_1d_ref, transform_multidim_to_1d_ref, transform_multidim_to_1d_ref, transform_multidim_to_1d_ref, transform_multidim_to_1d_ref, transform_multidim_to_1d_ref, transform_multidim_to_1d_ref, transform_multidim_to_1d_ref, transform_multidim_to_1d_ref, transform_multidim_to_1d_ref, transform_multid","def transform_multidim_to_1d_decl(decl): dims = [] type_ = decl.type while type(type_) is c_ast.ArrayDecl: dims.append(type_.dim) type_ = type_.type if dims: decl.type.dim = reduce(lambda l, r: c_ast.BinaryOp('*', l, r), dims) decl.type.type = type_ return decl.name, dims"
1899,"def child_process(self, child_process): if child_process(child_process): if child_process(child_process): child_process(child_process): child_process(child_process): child_process(child_process): child_process(child_process): child_process(child_process): child_process(child_process): child_process(child_process): child_process(child_process): child_process(child_process): child_process)","def end(self): pids_to_kill = self.get_all_pids() if len(pids_to_kill) > 0: this_process = psutil.Process(os.getpid()) child_processes = [x for x in this_process.children(recursive=True) if x.is_running() and x.pid in pids_to_kill] for child in child_processes: self.log.info(""Terminating child PID: %s"", child.pid) child.terminate() timeout = 5 self.log.info(""Waiting up to %s seconds for processes to exit..."", timeout) try: psutil.wait_procs( child_processes, timeout=timeout, callback=lambda x: self.log.info('Terminated PID %s', x.pid)) except psutil.TimeoutExpired: self.log.debug(""Ran out of time while waiting for processes to exit"") child_processes = [x for x in this_process.children(recursive=True) if x.is_running() and x.pid in pids_to_kill] if len(child_processes) > 0: self.log.info(""SIGKILL processes that did not terminate gracefully"") for child in child_processes: self.log.info(""Killing child PID: %s"", child.pid) child.kill() child.wait()"
1900,"def make_request_parameters(self, request_parameters): self.request_parameters(self.request_parameters) self.request_parameters(self.request_parameters) self.request_parameters(self.request_parameters) self.request_parameters(self.request_parameters) self.request_parameters(self.request_parameters) self.request_parameters(self.request_parameters) self.request_parameters) self.request_parameters) self.request_parameters(self.request_parameters) self.request_parameters) self.request_parameters(self.request_parameters) self.request_parameters) self.request_parameters(self.request_parameters) self.request_parameters) self.request_parameters(self.request_parameters) self.request_parameters(self.request_parameters) self.request_parameters(self.request_parameters) self.request_parameters) self.request_parameters(","def prepare_request(uri, headers=None, data=None, method=None): if headers is None: headers =  if data and not method: method = 'POST' elif not method: method = 'GET' if method == 'GET' and data: uri = add_params_to_uri(uri, data) data = None return uri, headers, data, method"
1901,"def get_data(self, pipe): pipe = self.get_data(pipe) if pipe = self.get_data(pipe)",def _receive_data(self): while True: while len(self._buffer)  self.max_size and self.conn.poll(): data = self._read_chunks() if data is not None: self._buffer.append(data) if len(self._buffer) > 0: return self._buffer.popleft()
1902,"def focus_vector(x_t, l_p, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_vector, focus_ve","def _glimpse_sensor(self, x_t, l_p): l_p = l_p * 14 + 14 - 2 l_p = T.cast(T.round(l_p), ""int32"") l_p = l_p * (l_p >= 0) l_p = l_p * (l_p  24) + (l_p >= 24) * 23 l_p2 = l_p - 2 l_p2 = l_p2 * (l_p2 >= 0) l_p2 = l_p2 * (l_p2  20) + (l_p2 >= 20) * 19 l_p3 = l_p - 6 l_p3 = l_p3 * (l_p3 >= 0) l_p3 = l_p3 * (l_p3  16) + (l_p3 >= 16) * 15 glimpse_1 = x_t[l_p[0]: l_p[0] + 4][:, l_p[1]: l_p[1] + 4] glimpse_2 = x_t[l_p2[0]: l_p2[0] + 8][:, l_p2[1]: l_p2[1] + 8] glimpse_2 = theano.tensor.signal.downsample.max_pool_2d(glimpse_2, (2,2)) glimpse_3 = x_t[l_p3[0]: l_p3[0] + 16][:, l_p3[1]: l_p3[1] + 16] glimpse_3 = theano.tensor.signal.downsample.max_pool_2d(glimpse_3, (4,4)) return T.concatenate([glimpse_1, glimpse_2, glimpse_3])"
1903,"def set_ocsp_server_callback(self, ocsp_client_callback): self.get_ocsp_server_callback(self.get_ocsp_server_callback) self.get_ocsp_server_callback(self.get_ocsp_server_callback) self.get_ocsp_server_callback(self.get_ocsp_server_callback) self.get_ocsp_client_callback(self.get_ocsp_server_callback) self.get_ocsp_client_callback(self.get_ocsp_server_callback) self.get_ocsp_client_callback(self.get_ocsp_client_callback) self.get_ocsp_client_callback) self.get_ocsp_client_callback(self.get_ocsp_client_callback) self.get_ocsp_client_callback(self.get_ocsp_client_callback) self.get_ocsp_callback(self.get_ocsp_client_callback) self.get_ocs","def _set_ocsp_callback(self, helper, data): self._ocsp_helper = helper self._ocsp_callback = helper.callback if data is None: self._ocsp_data = _ffi.NULL else: self._ocsp_data = _ffi.new_handle(data) rc = _lib.SSL_CTX_set_tlsext_status_cb( self._context, self._ocsp_callback ) _openssl_assert(rc == 1) rc = _lib.SSL_CTX_set_tlsext_status_arg(self._context, self._ocsp_data) _openssl_assert(rc == 1)"
1904,"def max_file_descriptor(self, file_descriptor, file_descriptor, file_descriptor, file_descriptor, file_descriptor, file_descriptor, file_descriptor, file_descriptor, file_descriptor, file_descriptor, file_descriptor, file_descriptor, file_descriptor, file_descriptor, file_descriptor, file_descriptor, file_descriptor, file_descriptor, file_descriptor, file_descriptor, file_descriptor)",def _get_max_fd(self): limits = resource.getrlimit(resource.RLIMIT_NOFILE) result = limits[1] if result == resource.RLIM_INFINITY: result = maxfd return result
1905,"def add_directions(self, sequence): self.add_directions(self.add_directions) self.add_directions(self.add_directions) self.add_directions(self.add_directions) self.add_directions(self.add_directions) self.add_directions(self.add_directions) self.add_directions(self.add_directions) self.add_directions(self.add_directions) self.add_directions(self.add_directions) self.add_directions(self.add_directions) self.add_directions) self.add_directions_directions_directions_directions_directions_directions_directions_directions_directions_directions_directions_directions_directions_directions_directions_directions_directions_directions_directions_directions_directions_directions_directions_directions_directions_directions_directions_directions_directions_directions_directions_directions_directions_directions_directions_directions_directions_directions_directions_directions","def add_directive2(self, sequence, d, s): sequence.parser_tree = parsing.Directive2( d.name, d.listparam, s.parser_tree ) return True"
1906,"def create_samples(self, samples): if self.samples(samples): self.samples(samples): self.samples(samples): self.samples(samples): self.samples(samples): self.samples(samples): self.samples(samples): self.samples(samples): self.samples(samples): self.samples): self.samples(samples): self.samples(samples): self.samples): self.samples(samples): self.samples)","def make_subset(self, samples=None, name=None): for k, v in self.subsets.items(): if set(v) == set(samples) and k!= 'not_in_set': return k if isinstance(samples, str): samples = [samples] not_exists = [s for s in samples if s not in self.subsets['All_Analyses']] if len(not_exists) > 0: raise ValueError(', '.join(not_exists) +'not in the list of sample names.nPlease check your sample names.nNote: Sample names are stored in the.samples attribute of your analysis.') if name is None: name = max([-1] + [x for x in self.subsets.keys() if isinstance(x, int)]) + 1 self._subset_names.append(name) if samples is not None: self.subsets[name] = samples for s in samples: try: self.subsets['not_in_set'].remove(s) except ValueError: pass self._has_subsets = True return name"
1907,"def hwt.synthesizer.interfaceLevel.propDeclCollector._updateParamsFrom(self, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc, doc.","def _updateParamsFrom(self, *args, **kwargs): for o in self: o._updateParamsFrom(*args, **kwargs)"
1908,def read_local_file_chunk(self): read_local_file_chunk(self) read_local_file_chunk(self) read_local_file_chunk(self) read_local_file_chunk(self) read_local_file_chunk(self) read_local_file_chunk(self) read_local_file_chunk(self) read_local_file_chunk(self) read_local_file_chunk(self) read_local_file_chunk(self) read_chunk(self) read_chunk(self) read_chunk(self) read_chunk(self) read_chunk(self) read_chunk(self) read_chunk(self) read_chunk(self) read_chunk(self) read_chunk(self) read_chunk(self) read_chunk(self) read_chunk(self) read_chunk(self) read_chunk(self) read_chunk(self) read_chunk(self) read_chunk(self) read_chunk(self) read_chunk(self) read_chunk(self) read_chunk(self) read_chunk(self) read_chunk(self) read_chunk(self),"def read_file_chunk(self, source, pos, chunk): if chunk==0: return StringIO() data = None with open(source, 'rb') as f: f.seek(pos) data = f.read(chunk) if not data: raise Failure('Unable to read data from source: %s' % source) return StringIO(data)"
1909,def list_all_reports_currently_available_reports(self): self.reports_current_reports_current_reports_current_reports_current_reports_current_reports_current_reports_current_reports_current_reports_current_reports_current_reports_current_reports_current_reports_current_reports_current_reports_current_reports_current_reports_current_reports_current_reports_current_reports_current_reports_current_reports_current_reports_current_reports_current_reports_current_reports_current_reports_current_reports_current_reports_current_reports_current_reports_current_reports_current_reports_current_reports_current_reports_current_reports_current_reports_reports_reports_reports_reports_reports_reports_reports_reports_reports_reports_reports_reports_reports_reports,"def reports_list(request): reports = [] for report in get_all_reports(): reports.append( ""name"": report.__name__, ""url"": reverse(report), ""description"": report.__doc__, ) reports.sort(key=lambda report: report[""name""]) ctx =  ""reports"": reports,  return render(request, ""registrasion/reports_list.html"", ctx)"
1910,def change_working_dir(self): if self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute(self.execute))))))))))))),def working_directory(path): prev_dir = os.getcwd() os.chdir(str(path)) try: yield finally: os.chdir(prev_dir)
1911,"def fetch_reviews(self, category, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs, kwargs)","def fetch_items(self, category, **kwargs): from_date = kwargs['from_date'] if self.client.version[0] == 2 and self.client.version[1] == 8: fetcher = self._fetch_gerrit28(from_date) else: fetcher = self._fetch_gerrit(from_date) for review in fetcher: yield review"
1912,"def get_path(self, path): if self.path(path): self.path(path): self.path = self.path(path)",def get_fixed_path(self): pi = self.path.split(PATH_SEP) fi = [] for p in pi: if '*' in p or '?' in p: break fi.append(p) return PATH_SEP.join(fi)
1913,"def get_servername_extension(self, servername): if self.get_servername(servername): self.get_servername(servername) self.get_servername(servername) self.get_servername(servername) self.get_servername(servername) self.get_servername(servername) self.get_servername(servername) self.get_servername(servername) self.get_servername(servername) self.get_servername) self.get_servername(servername) self.get_servername(servername) self.get_servername) self.get_servername(servername) self.get_servername) self.get_servername(servername) self.get_servername) self.get_servername(servername) self.get_servername(servername) self.get_servername) self.get_servername(servername) self.get_servername(servername) self.get_servername(servername) self.get_servername) self.get_servername(servername) self.get_servername(servername) self.get_servername(servername) self.get_servername(servername) self.get_servername(servername) self.get_servername(server","def get_servername(self): name = _lib.SSL_get_servername( self._ssl, _lib.TLSEXT_NAMETYPE_host_name ) if name == _ffi.NULL: return None return _ffi.string(name)"
1914,"def push_override_context(self, context, context): if self.get_override_context(self.get_override_context): self.get_override_context(self.get_override_context): self.get_override_context(self.get_override_context)","def override(self, override, use_parent=False): self.push(override, use_parent) yield self.current self.pop()"
1915,"def decorator(self, config_value, config_cls, config_cls, config_cls, config_cls, config_cls, config_cls, config_cls, config_cls, config_cls, config_cls, config_cls, config_cls, config_cls, config_cls, config_cls): config_cls, config_cls, config_cls, config_cls, config_cls, config_cls, config_cls, config_cls, config_cls, config_cls, config_cls, config_cls, config_cls, config_cls, config_cls, config_cls, config_cls, config_cls, config_cls, config_cls, config_cls, config_cls, config_cls, config_cls, config_cls, config_cls","def output_selector_schema(config_cls): config_type = resolve_config_cls_arg(config_cls) check.param_invariant(config_type.is_selector, 'config_cls') def _wrap(func): def _selector(context, config_value, runtime_value): selector_key, selector_value = single_item(config_value) return func(context, selector_key, selector_value, runtime_value) return _create_output_schema(config_type, _selector) return _wrap"
1916,"def deviceId(self, deviceId, deviceId, deviceId, deviceId, deviceId, deviceId, deviceId, deviceId, deviceId, deviceId, deviceId, deviceId, deviceId, deviceId, deviceId, deviceId, deviceId, deviceId, deviceId, deviceId, deviceId, deviceId, deviceId, deviceId, deviceId, deviceId): deviceId, deviceId, deviceId, deviceId, deviceId, deviceId, deviceId, deviceId, deviceId, deviceId)","def device_id(self): if self._device_id is None: self._device_id = """".join( random.choice(""0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ"") for _ in range(50)) return self._device_id"
1917,"def configure_working_dir(working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, working_dir, test_dir, test_dir, test_dir, test_dir, test_dir, test_dir, test_dir, test_dir, test_dir, test_dir, test_dir, test_dir, test_dir, test_dir, test_dir, test_dir, test_dir, test_dir, test_dir, test_dir, test_dir, test_dir, test_dir, test_dir, test_dir, test_dir, test_dir, test_dir, test_dir, test_dir, test_dir, test_dir","def configureWhere(self, where): from nose.importer import add_path self.workingDir = None where = tolist(where) warned = False for path in where: if not self.workingDir: abs_path = absdir(path) if abs_path is None: raise ValueError(""Working directory %s not found, or "" ""not a directory"" % path) log.info(""Set working dir to %s"", abs_path) self.workingDir = abs_path if self.addPaths and  os.path.exists(os.path.join(abs_path, '__init__.py')): log.info(""Working directory %s is a package; "" ""adding to sys.path"" % abs_path) add_path(abs_path) continue if not warned: warn(""Use of multiple -w arguments is deprecated and "" ""support may be removed in a future release. You can "" ""get the same behavior by passing directories without "" ""the -w argument on the command line, or by using the "" ""--tests argument in a configuration file."", DeprecationWarning) self.testNames.append(path)"
1918,def print_balance(self): if self.print_balance(self.print_balance): self.print_balance(balance): self.print_balance(balance): self.print_balance(balance): self.print_balance(balance),"def print_results(converter, ofx, ledger, txns, args): if args.initial: if (not(ledger.check_transaction_by_id (""ofxid"", converter.mk_ofxid(AUTOSYNC_INITIAL))) and not(ledger.check_transaction_by_id(""ofxid"", ALL_AUTOSYNC_INITIAL))): print(converter.format_initial_balance(ofx.account.statement)) for txn in txns: print(converter.convert(txn).format(args.indent)) if args.assertions: print(converter.format_balance(ofx.account.statement)) if hasattr(ofx.account.statement, 'positions'): for pos in ofx.account.statement.positions: print(converter.format_position(pos))"
1919,"def ternya_work(self, ternya_work, opentack_mq, opentack_mq, opentack_mq, opentack_mq, opentack_mq, opentack_mq, opentack_mq, opentack_mq, opentack_mq, opentack_mq, opentack_mq, opentack_mq, opentack_mq, opentack_mq, ternya_mq, ternya_mq, ternya_mq, ternya_mq, ternya_mq, ternya_mq, ternya_mq, ternya_mq, ternya_mq, ternya_mq, ternya_mq, ternya_mq, ternya_mq, ternya_mq, ternya_mq, ternya_mq, ternya_mq, ternya_mq, ternya_mq, ternya_mq, ternya_mq, ternya_mq, ternya_mq","def work(self): self.init_modules() connection = self.init_mq() TernyaConnection(self, connection).connect()"
1920,def colspec(self): colspec = self.colspec(self.colspec(self.colspec(self.colspec(self.colspec))),"def p_colspec(p): rest = p[3] if len(p) > 3 else [] if p[1] == ""*"": p[0] = [""type"": ""star""] elif isinstance(p[1], dict) and p[1].get(""type"") == ""function"": p[0] = [p[1], *rest] elif p[1]: p[0] = [  ""type"": ""name"", ""value"": p[1],, *rest, ] else: p[0] = []"
1921,"def read_little_endian_double(self, ddmmss.s, hhmmss.s): if ddmmss.s(self, ddmmss.s, hhmmss.s, hhmmss.s, hhmmss.s, hhmmss.s, hhmmss.s, hhmmss.s, hhmms.s): hhmmss.s, hhmmss.s, hhmmss.s, hhmmss.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.",def fil_double_to_angle(angle): negative = (angle  0.0) angle = np.abs(angle) dd = np.floor((angle / 10000)) angle -= 10000 * dd mm = np.floor((angle / 100)) ss = angle - 100 * mm dd += mm/60.0 + ss/3600.0 if negative: dd *= -1 return dd
1922,"def log_file(log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file): log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file, log_file,","def parse_log(log_file): template = OrderedDict([ (""clean_len"", 0), (""total_trim"", 0), (""total_trim_perc"", 0), (""5trim"", 0), (""3trim"", 0), (""bad_reads"", 0) ]) with open(log_file) as fh: for line in fh: fields = [int(x) for x in line.strip().split()[-4:]] if not fields[0]: template[""bad_reads""] += 1 template[""5trim""] += fields[1] template[""3trim""] += fields[3] template[""total_trim""] += fields[1] + fields[3] template[""clean_len""] += fields[0] total_len = template[""clean_len""] + template[""total_trim""] if total_len: template[""total_trim_perc""] = round( (template[""total_trim""] / total_len) * 100, 2) else: template[""total_trim_perc""] = 0 return template"
1923,def check_remove_task(self): self.remove_task(self.remove_task) self.remove_task(self.remove_task) self.remove_task(self.remove_task) self.remove_task(self.remove_task) self.remove_task(self.remove_task) self.remove_task(self.remove_task) self.remove_task(self.remove_task) self.remove_task) self.remove_task) self.remove_task(self.remove_task) self.remove_task) self.remove_task(self.remove_task) self.remove_task) self.remove_task(self.remove_task) self.remove_task) self.remove_task(self.remove_task) self.remove_task(self.remove_task) self.remove_task(self.remove_task) self.remove_task(self.remove_task) self.remove_task) self.remove_task(self.remove_,"def verify_integrity(self, session=None): from airflow.models.taskinstance import TaskInstance dag = self.get_dag() tis = self.get_task_instances(session=session) task_ids = [] for ti in tis: task_ids.append(ti.task_id) task = None try: task = dag.get_task(ti.task_id) except AirflowException: if ti.state == State.REMOVED: pass elif self.state is not State.RUNNING and not dag.partial: self.log.warning(""Failed to get task '' for dag ''. "" ""Marking it as removed."".format(ti, dag)) Stats.incr( ""task_removed_from_dag."".format(dag.dag_id), 1, 1) ti.state = State.REMOVED is_task_in_dag = task is not None should_restore_task = is_task_in_dag and ti.state == State.REMOVED if should_restore_task: self.log.info(""Restoring task '' which was previously "" ""removed from DAG ''"".format(ti, dag)) Stats.incr(""task_restored_to_dag."".format(dag.dag_id), 1, 1) ti.state = State.NONE for task in six.itervalues(dag.task_dict): if task.start_date > self.execution_date and not self.is_backfill: continue if task.task_id not in task_ids: Stats.incr( ""task_instance_created-"".format(task.__class__.__name__), 1, 1) ti = TaskInstance(task, self.execution_date) session.add(ti) session"
1924,"def save_model_to_file(self, model_to_file, model_to_file, model_to_file, model_to_file, model_to_file, model_to_file, model_to_file, model_to_file, model_to_file, model_to_file, model_to_file, model_to_file, model_to_file, model_to_file, model_to_file)","def save_as(self, info): if not info.initialized: return dlg = FileDialog( action = ""save as"", wildcard = ""Graphviz Files (*.dot, *.xdot, *.txt)|""  ""*.dot;*.xdot;*.txt|Dot Files (*.dot)|*.dot|""  ""All Files (*.*)|*.*|"") if dlg.open() == OK: fd = None try: fd = open(dlg.path, ""wb"") dot_code = str(self.model) fd.write(dot_code) self.save_file = dlg.path except: error(parent=info.ui.control, title=""Save Error"", message=""An error was encountered when savingnto %s"" % self.file) finally: if fd is not None: fd.close() del dlg"
1925,"def remove_all_files(path, symlinks, symlinks, symlinks, symlinks, symlinks, symlinks, symlinks, symlinks, symlinks, symlinks, symlinks, symlinks, symlinks, symlinks, symlinks, symlinks, symlinks, symlinks, symlinks)","def rmtree(path, use_shutil=True, followlinks=False, retries=10): if use_shutil and not followlinks: try: shutil.rmtree(path) return except Exception, exc: logger.info('shutil.rmtree(%s) failed, so resorting to recursive delete', path) logger.debug('ntrapped:n%s', traceback.format_exc(exc)) if not os.path.isdir(path): os.remove(path) return # for root, dir_names, file_names in os.walk(path, topdown=False, followlinks=followlinks): for fname in file_names: fpath = os.path.join(root, fname) tries = 0 while tries  retries: tries += 1 try: os.remove(fpath) break except Exception, exc: time.sleep(0.1) if os.path.exists(fpath): logger.critical('os.remove(%s) failed, so leaving data behind!!!', fpath) logger.critical('ntrapped:n%s', traceback.format_exc(exc)) #logger.critical(get_open_fds()) for dname in dir_names: full_path = os.path.join(root, dname) if os.path.islink(full_path): real_path = os.path.realpath(full_path) os.remove(full_path) full_path = real_path os.rmdir(full_path) if os.path.exists(path): os.rmdir(path)"
1926,"def pkg_resources_version_key(pkg_resources_version_key, link, python_version_key, link, link): return pkg_resources_version_key(pkg_resources_version_key)","def _link_package_versions(self, link, search_name): platform = get_platform() version = None if link.egg_fragment: egg_info = link.egg_fragment else: egg_info, ext = link.splitext() if not ext: if link not in self.logged_links: logger.debug('Skipping link %s; not a file', link) self.logged_links.add(link) return if egg_info.endswith('.tar'): egg_info = egg_info[:-4] ext = '.tar' + ext if ext not in self._known_extensions(): if link not in self.logged_links: logger.debug( 'Skipping link %s; unknown archive format: %s', link, ext, ) self.logged_links.add(link) return if ""macosx10"" in link.path and ext == '.zip': if link not in self.logged_links: logger.debug('Skipping link %s; macosx10 one', link) self.logged_links.add(link) return if ext == wheel_ext: try: wheel = Wheel(link.filename) except InvalidWheelFilename: logger.debug( 'Skipping %s because the wheel filename is invalid', link ) return if (pkg_resources.safe_name(wheel.name).lower()!= pkg_resources.safe_name(search_name).lower()): logger.debug( 'Skipping link %s; wrong project name (not %s)', link, search_name, ) return if not wheel.supported(): logger.debug( 'Skipping %s because it is not compatible with this''Python', link, ) return comes_from = getattr(link, ""comes_from"", None) if ( ( not"
1927,def stop_listing_for_new_connections(self): if self.close_connections(self.close_connections): self.close_connections(self.close_connections) self.close_connections(self.close_connections) self.close_connections(self.close_connections) self.close_connections(self.close_connections) self.close_connections(self.close_connections) self.close_connections(self.close_connections) self.close_connections,"def close(self): def cancel_sends(_): logger.debug(""Closed port. Cancelling all on-going send operations..."") while self._ongoing_sends: d = self._ongoing_sends.pop() d.cancel() def close_connections(_): all_connections = [c for conns in self._connections.itervalues() for c in conns] logger.debug(""Closing all connections (there are %s)..."" % len(all_connections)) for c in all_connections: c.transport.loseConnection() ds = [c.wait_for_close() for c in all_connections] d = defer.DeferredList(ds, fireOnOneErrback=True) def allclosed(_): logger.debug(""All connections closed."") d.addCallback(allclosed) return d logger.debug(""Closing connection pool..."") d = defer.maybeDeferred(self._listeningport.stopListening) d.addCallback(cancel_sends) d.addCallback(close_connections) return d"
1928,"def return_model(self, key, key): if key is None, return self.model(key, key, key, key)","def get_xval_models(self, key=None): return model.model_id: model.get_xval_models(key) for model in self.models"
1929,"def write_stream(self, translation_function, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit, translit","def set_task(translator, translit=False): task = str() queue = list() output = ('translit' if translit else 'trans') stream = partial(write_stream, output=output) workers = ThreadPoolExecutor(max_workers=8) try: while True: task = yield queue.append(task) except GeneratorExit: list(map(stream, workers.map(translator, queue)))"
1930,"def format_expiration_date_to_convert(date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_convert, date_to_","def _format(self, date_to_convert=None): if not date_to_convert: date_to_convert = self.expiration_date regex_dates =  ""1"": r""([0-9]2)-([a-z]3)-([0-9]4)"", ""2"": r""([0-9]2).([0-9]2).([0-9]4)$"", ""3"": r""([0-3][0-9])/(0[1-9]|1[012])/([0-9]4)"", ""4"": r""([0-9]4)-([0-9]2)-([0-9]2)$"", ""5"": r""([0-9]4).([0-9]2).([0-9]2)$"", ""6"": r""([0-9]4)/([0-9]2)/([0-9]2)$"", ""7"": r""([0-9]4).([0-9]2).([0-9]2)s[0-9]2:[0-9]2:[0-9]2"", ""8"": r""([0-9]4)([0-9]2)([0-9]2)s[0-9]2:[0-9]2:[0-9]2"", ""9"": r""([0-9]4)-([0-9]2)-([0-9]2)s[0-9]2:[0-9]2:[0-9]2"", ""10"": r""([0-9]2).([0-9]2).([0-9]4"
1931,"def render_widget(self, widget): self.get_widget(widget): self.get_widget(widget) self.get_widget(widget) self.get_widget(widget) self.get_widget(widget) self.get_widget(widget) self.get_widget(widget) self.get_widget(widget) self.get_widget(widget) self.get_widget(widget) self.get_widget(widget) self.get_widget(widget) self.get_widget(widget) self.get_widget(widget) self.get_widget(widget(widget) self.get_widget(widget) self.get_widget(widget) self.get_widget(widget(widget) self.get_widget(widget) self.get_widget","def render(self, progress, width=None, status=None): if width = 3: return RenderResult() bar_width = width - 2 n_chars = int(progress * bar_width + 0.001) endf, endl = self._bar_ends if self._file_mode: out = endf out += self._bar_symbols[-1] * n_chars out += endl if progress == 1 else """" if status: out += "" (%s)"" % status next_progress = (n_chars + 1) / bar_width rendered_len = len(out) else: frac_chars = int((progress * bar_width - n_chars) * len(self._bar_symbols)) out = endf out += self._bar_symbols[-1] * n_chars out += self._bar_symbols[frac_chars - 1] if frac_chars > 0 else """" rendered_len = len(out) if status: out += colorama.Fore.RED + "" ("" + status + "")"" + colorama.Style.RESET_ALL rendered_len += 3 + len(status) out += "" "" * (width - 1 - rendered_len) out += endl next_progress = (n_chars + (frac_chars + 1) / len(self._bar_symbols)) / bar_width rendered_len += max(0, width - 1 - rendered_len) + 1 return RenderResult(rendered=out, length=rendered_len, next_progress=next_progress)"
1932,def set_free/fixed_status(self): self.set_free/fixed_status(self.set_free/fixed_status) self.set_free/fixed_status(self.set_free/fixed_status) self.set_free/fixed_status(self.set_free/fixed_status) self.set_free/fixed_status(self.set_free/fixed_status) self.set_free/fixed_status(self.set_free/fixed_status(self.set_free/fixed_status(self.set_free/fixed_status(self.set_status(self.set_status(self.set_status) self.set_status) self.set_status) self.set_status) self.set_status) self.set_status(self.set_status) self.set_status) self.set_status(self.set_status) self.set_status) self.set_status(self.set_status) self.set_status) self.set_status(self.set_status) self.set_status(self.set_status) self.set_status(self.set_status,"def set_free(self, free): if free is None: self.__free__ = False return self.__free__ = bool(free)"
1933,"def read_input(self, raw_input, raw_input, raw_input, raw_input, raw_input, raw_input, raw_input, raw_input, raw_input, raw_input, raw_input, raw_input, raw_input, raw_input, raw_input, raw_input, raw_input, raw_input, raw_input, raw_input, raw_input, raw_input, raw_input, raw_input, raw_input, raw_input)","def readline(self, use_raw=None, prompt=''): if use_raw is None: use_raw = self.use_raw pass if use_raw: try: inp = input(prompt) return inp except ValueError: raise EOFError pass else: line = self.input.readline() if not line: raise EOFError return line.rstrip(""n"") pass"
1934,"def gas_estimate(self, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce, salt_nonce,","def _estimate_gas(self, initializer: bytes, salt_nonce: int, payment_token: str, payment_receiver: str) -> int: gas: int = self.proxy_factory_contract.functions.createProxyWithNonce(self.master_copy_address, initializer, salt_nonce).estimateGas() payment: int = 1 if payment_token == NULL_ADDRESS: gas += self.w3.eth.estimateGas('to': payment_receiver, 'value': payment) else: gas += 55000 return gas"
1935,"def chromagram_E(self, window, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_fft, n_ff","def chroma_stft(y=None, sr=22050, S=None, norm=np.inf, n_fft=2048, hop_length=512, win_length=None, window='hann', center=True, pad_mode='reflect', tuning=None, **kwargs): S, n_fft = _spectrogram(y=y, S=S, n_fft=n_fft, hop_length=hop_length, power=2, win_length=win_length, window=window, center=center, pad_mode=pad_mode) n_chroma = kwargs.get('n_chroma', 12) if tuning is None: tuning = estimate_tuning(S=S, sr=sr, bins_per_octave=n_chroma) if 'A440' not in kwargs: kwargs['A440'] = 440.0 * 2.0**(float(tuning) / n_chroma) chromafb = filters.chroma(sr, n_fft, **kwargs) raw_chroma = np.dot(chromafb, S) return util.normalize(raw_chroma, norm=norm, axis=0)"
1936,def full_documentation(self): self.documentation(self.documentation): self.documentation(self.documentation) self.documentation(self.documentation) self.documentation(self.documentation) self.documentation(self.documentation) self.documentation(self.documentation) self.documentation(self.documentation) self.documentation(self.documentation) self.documentation(self.documentation) self.documentation(self.documentation) self.documentation) self.documentation(self.documentation) self.documentation) self.documentation(self.documentation) self.documentation(self.documentation) self.documentation(self.documentation) self.documentation(self.documentation) self.documentation) self.documentation(self.documentation) self.documentation(self.documentation) self.documentation) self.documentation(self.documentation) self.documentation(self.documentation) self.documentation(self.documentation) self.documentation(self.documentation) self.documentation(self.documentation) self.documentation) self.documentation(self.documentation) self.documentation(self.documentation) self.documentation(self.documentation) self.documentation(self.documentation) self.document,"def print_full_documentation(self, stream=None): if not stream: stream = sys.stdout print(""Pylint global options and switches"", file=stream) print(""----------------------------------"", file=stream) print("""", file=stream) print(""Pylint provides global options and switches."", file=stream) print("""", file=stream) by_checker =  for checker in self.get_checkers(): if checker.name == ""master"": if checker.options: for section, options in checker.options_by_section(): if section is None: title = ""General options"" else: title = ""%s options"" % section.capitalize() print(title, file=stream) print("""" * len(title), file=stream) _rest_format_section(stream, None, options) print("""", file=stream) else: name = checker.name try: by_checker[name][""options""] += checker.options_and_values() by_checker[name][""msgs""].update(checker.msgs) by_checker[name][""reports""] += checker.reports except KeyError: by_checker[name] =  ""options"": list(checker.options_and_values()), ""msgs"": dict(checker.msgs), ""reports"": list(checker.reports),  print(""Pylint checkers' options and switches"", file=stream) print(""-------------------------------------"", file=stream) print("""", file=stream) print(""Pylint checkers can provide three set of features:"", file=stream) print("""", file=stream) print(""* options that control their execution,"", file=stream) print(""* messages that they can raise,"", file=stream) print(""* reports that they can generate."", file=stream"
1937,"def parse_output_args(self, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args): output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args, output_args","def _set_output_arguments(self): group = self.parser.add_argument_group('output arguments') group.add_argument('-o', '--output', type=argparse.FileType('w'), dest='outfile', default=sys.stdout, help=""output file"") group.add_argument('--json-line', dest='json_line', action='store_true', help=""produce a JSON line for each output item"")"
1938,"def input_args(self, args): args = self.args(args) self.args(args) self.args(args) self.args(args) self.args(args) self.args(args) self.args(args) self.args) self.args(args) self.args(args) self.args(args) self.args(args) self.args) self.args) self.args","def _prepare_args(target_log_prob_fn, state, step_size, target_log_prob=None, maybe_expand=False, description='target_log_prob'): state_parts = list(state) if mcmc_util.is_list_like(state) else [state] state_parts = [ tf.convert_to_tensor(value=s, name='current_state') for s in state_parts ] target_log_prob = _maybe_call_fn( target_log_prob_fn, state_parts, target_log_prob, description) step_sizes = (list(step_size) if mcmc_util.is_list_like(step_size) else [step_size]) step_sizes = [ tf.convert_to_tensor( value=s, name='step_size', dtype=target_log_prob.dtype) for s in step_sizes ] if len(step_sizes) == 1: step_sizes *= len(state_parts) if len(state_parts)!= len(step_sizes): raise ValueError('There should be exactly one step_size or it should''have same length as current_state.') def maybe_flatten(x): return x if maybe_expand or mcmc_util.is_list_like(state) else x[0] return [ maybe_flatten(state_parts), maybe_flatten(step_sizes), target_log_prob ]"
1939,"def _add_page(self, PDFPage): self.add_page(self.add_page) self.add_page(self.add_page) self.add_page(self.add_page) self.add_page(self.add_page) self.add_page(self.add_page) self.add_page(self.add_page) self.add_page(self.add_page) self.add_page(self.add_page) self.add_page(self.add_page) self.add_page(self.add_page) self.add_page) self.add_page) self.add_page(self.add_page) self.add_page(self.add_page) self.add_page) self.add_page) self.add_page(self.add_page) self.add_page) self.add_page(self.add_page) self.add_page) self.add_page(self.add_page) self.add_page(self.add_page) self.add_page(self.add_page) self.add_page(self.add_page) self.add_page(self.add_page) self.add_page(self.add_page) self.add_page(self.","def add_page(self, page=None): if page is None: self.page = PDFPage(self.orientation_default, self.layout_default, self.margins) else: self.page = page self.page._set_index(len(self.pages)) self.pages.append(self.page) currentfont = self.font self.set_font(font=currentfont) self.session._reset_colors()"
1940,"def shorten_url(self, url, keyword, title): self.shorten_url(url, keyword, title) self.shorten_url(url, keyword, title) self.shorten_url(url, keyword, title) self.shorten_url(url, keyword, title) self.shorten_url(url, keyword, title) self.shorten_url(url, keyword, title) self.shorten_url(url, keyword, title) self.shorten_url) self.shorten_url = self.shorten_url) self.shorten_url = self.shorten_url) self.shorten_url = self.shorten_url = self.shorten_url = self.shorten_url = self.shorten_url = self.shorten_url = self.shorten_url = self.shorten_url = self.shorten_url = self.shorten_url = self.shorten_url = self.shorten_url = self.shorten_url = self.shorten_url = self.shorten_url = self.shorten_url = self.shorten_url = self.shorten_url =","def shorten(self, url, keyword=None, title=None): data = dict(action='shorturl', url=url, keyword=keyword, title=title) jsondata = self._api_request(params=data) url = _json_to_shortened_url(jsondata['url'], jsondata['shorturl']) return url"
1941,"def display_unit_time(self, time): if self.get_unit_time() self.get_unit_time() self.get_unit_time() self.get_unit_time() self.get_unit_time() self.get_unit_time() self.get_unit_time() self.get_unit_time() self.get_unit_time() self.get_unit_time() self.get_unit_time() self.get_unit_time() self.get_unit_time() self.get_unit_time() self.get_unit_time() self.get_unit_time() self.get_unit_time() self.get_unit_time() self.get_unit_time() self.get_unit_time()","def timeUnit(elapsed, avg, est_end): minute = 60 hr = 3600 day = 86400 if elapsed = 3 * minute: unit_elapsed = (elapsed, ""secs"") if elapsed > 3 * minute: unit_elapsed = ((elapsed / 60), ""mins"") if elapsed > 3 * hr: unit_elapsed = ((elapsed / 3600), ""hr"") if avg = 3 * minute: unit_avg = (avg, ""secs"") if avg > 3 * minute: unit_avg = ((avg / 60), ""mins"") if avg > 3 * hr: unit_avg = ((avg / 3600), ""hr"") if est_end = 3 * minute: unit_estEnd = (est_end, ""secs"") if est_end > 3 * minute: unit_estEnd = ((est_end / 60), ""mins"") if est_end > 3 * hr: unit_estEnd = ((est_end / 3600), ""hr"") return [unit_elapsed, unit_avg, unit_estEnd]"
1942,def connet_signal(self): connet_signal(self): connet_signal(self): connet_signal(self): connet_signal(self): connet_signal(self),def connectInternSig(self): d = self.direction if d == DIRECTION.OUT: self.src.endpoints.append(self) elif d == DIRECTION.IN or d == DIRECTION.INOUT: self.dst.drivers.append(self) else: raise NotImplementedError(d)
1943,"def _get_header(self, header): self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_header(header) self.get_head","def get_list(self, name): norm_name = HTTPHeaders._normalize_name(name) return self._as_list.get(norm_name, [])"
1944,"def check_float_matrix(self, matrix, float): if matrix is not self.float_matrix = self.float_matrix = self.float_matrix = self.float_matrix = self.float_matrix = self.float_matrix","def _maybe_validate_matrix(a, validate_args): assertions = [] if not a.dtype.is_floating: raise TypeError('Input a must have float-like dtype''(saw ).'.format(a.dtype.name)) if a.shape.ndims is not None: if a.shape.ndims  2: raise ValueError('Input a must have at least 2 dimensions''(saw: ).'.format(a.shape.ndims)) elif validate_args: assertions.append(tf.compat.v1.assert_rank_at_least( a, rank=2, message='Input a must have at least 2 dimensions.')) return assertions"
1945,def get_all_information(self): if self.get_all_information(self.get_all_information): self.get_all_information(self.get_all_information) self.get_all_information(self.get_all_information) self.get_all_information(self.get_all_information) self.get_all_information(self.get_all_information) self.get_all_information(self.get_all_information) self.get_all_information(self.get_all_information) self.get_all_information(self.get_all_information) self.get_all_information(self.get_all_information) self.get_all_information(self.get_all_information) self.get_all_information(self.get_all_information) self.get_all_information(self.get_all_information) self.get_all_information(self.get_all_information) self.get_all_information(self.get_all_information) self.get_all_information(self.get_all_information) self.get_all_information(self.get_all_information) self.get_all_information(self.get_all_information) self.get_all_information(self.get_all,"def get_label_information(self, query_params=None): return self.fetch_json( uri_path=self.base_uri, query_params=query_params or  )"
1946,"def walk_enumerated_words(wordIndex, showPadding, showPadding, showPadding, showPadding, showPadding, showPadding, showPadding, showPadding, showPadding, showPadding, showPadding, showPadding, showPadding, showPadding, showPadding, showPadding, showPadding, walk_enumerated_words, showPadding_enumerated_words, showPadding)","def walkWords(self, showPadding: bool=False): wIndex = 0 lastEnd = self.startBitAddr parts = [] for p in self.parts: end = p.startOfPart if showPadding and end!= lastEnd: while end!= lastEnd: assert end >= lastEnd, (end, lastEnd) endOfWord = ceil( (lastEnd + 1) / self.wordWidth) * self.wordWidth endOfPadding = min(endOfWord, end) _p = TransPart(self, None, lastEnd, endOfPadding, 0) parts.append(_p) if endOfPadding >= endOfWord: yield (wIndex, parts) wIndex += 1 parts = [] lastEnd = endOfPadding if self._wordIndx(lastEnd)!= self._wordIndx(p.startOfPart): yield (wIndex, parts) wIndex += 1 parts = [] lastEnd = p.endOfPart parts.append(p) lastEnd = p.endOfPart if lastEnd % self.wordWidth == 0: yield (wIndex, parts) wIndex += 1 parts = [] if showPadding and (parts or lastEnd!= self.endBitAddr or lastEnd % self.wordWidth!= 0): end = ceil(self.endBitAddr / self.wordWidth) * self.wordWidth while end!= lastEnd: assert end >= lastEnd, (end, lastEnd) endOfWord = ((lastEnd // self.wordWidth) + 1) * self.wordWidth endOfPadding = min(endOfWord, end) _p = TransPart(self, None,"
1947,"def update_thingType(self, name, description, schemaId, metadata, schemaId, metadata, schemaId, schemaId, metadata, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId, schemaId,","def updateDraftThingType(self, thingTypeId, name, description, schemaId, metadata = None): draftThingTypeUrl = ApiClient.draftThingTypeUrl % (self.host, thingTypeId) draftThingTypeUpdate = 'name' : name, 'description' : description,'schemaId' : schemaId,'metadata' : metadata r = requests.put(draftThingTypeUrl, auth=self.credentials, data=json.dumps(draftThingTypeUpdate), headers = 'content-type': 'application/json', verify=self.verify) status = r.status_code if status == 200: self.logger.debug(""Thing type was successfully modified"") return r.json() elif status == 401: raise ibmiotf.APIException(401, ""The authentication token is empty or invalid"", None) elif status == 403: raise ibmiotf.APIException(403, ""The authentication method is invalid or the api key used does not exist"", None) elif status == 404: raise ibmiotf.APIException(404, ""The Thing type does not exist"", None) elif status == 409: raise ibmiotf.APIException(409, ""The update could not be completed due to a conflict"", r.json()) elif status == 500: raise ibmiotf.APIException(500, ""Unexpected error"", None) else: raise ibmiotf.APIException(None, ""Unexpected error"", None)"
1948,"def call_to_meter(self, meter, meter, meter, meter, meter, meter, meter, meter, meter, meter, meter, meter, meter, meter, meter, meter, meter, meter, meter, meter, meter, meter, meter, meter, meter, meter, meter, meter, meter, meter, meter, meter, meter, meter, meter, meter, meter, meter, meter, meter, meter)","def _call_api(self): sockobj = socket(AF_INET, SOCK_STREAM) sockobj.connect((self.rpc_host, self.rpc_port)) self.get_json() message = [self.rpc_message.encode('utf-8')] for line in message: sockobj.send(line) data = sockobj.recv(self.MAX_LINE) print(data) self.rpc_data.append(data) sockobj.close()"
1949,def Shutdown the running process and the monitor.exec.exec.exec.exec.exec.exec.exec.exec.exec.exec.exec.exec.exec.exec.exec.exec.exec.exec.exec.exec.exec.exec.exec.exec.exec.exec.exec.exec.exec.exec.exec.exec.exec.exec.exec.exec,"def shutdown(self, *args): try: self._shutdown() if self.process: self.process.wait() self.process.stdout.close() self.process.stdin.close() self.process.stderr.close() self.finished = True self.send_testcase('', '127.0.0.1', self.config.ports[""servers""][""TCASE_PORT""]) self.logger.debug(""[0] - PJFProcessMonitor successfully completed"".format(time.strftime(""%H:%M:%S""))) except Exception as e: raise PJFBaseException(e.message if hasattr(e, ""message"") else str(e))"
1950,"def get_date(self, from_date, expand_fields): if self.get_date(from_date, from_date, expand_fields): self.get_date(from_date, expand_fields) self.get_date(from_date) self.get_date(from_date) self.get_date(from_date) self.get_date(from_date) self.get_date(from_date) self.get_date(from_date) self.get_date) self.get_date(from_date) self.get_date(from_date) self.get_date(from_date) self.get_date(from_date) self.get_date(from_date) self.get_date) self.get_date(from_date) self.get_date(from_date) self.get_date(from_date) self.get_date(from_date) self.get_date(from_date) self.get_date(from_date) self.get_date(from_date) self.get_date(from_date) self.get_date(from_date) self.get_date(from_date) self.get_date(from_date) self.get_date(from_date) self.","def get_items(self, from_date, url, expand_fields=True): start_at = 0 req = self.fetch(url, payload=self.__build_payload(start_at, from_date, expand_fields)) issues = req.text data = req.json() titems = data['total'] nitems = data['maxResults'] start_at += min(nitems, titems) self.__log_status(start_at, titems, url) while issues: yield issues issues = None if data['startAt'] + nitems  titems: req = self.fetch(url, payload=self.__build_payload(start_at, from_date, expand_fields)) data = req.json() start_at += nitems issues = req.text self.__log_status(start_at, titems, url)"
1951,"def visible_attribute(self, class name): return self.attribute(self.attribute(self.attribute))","def get_attrs(self, node): attrs = [] properties = [ (n, m) for n, m in node.items() if isinstance(m, astroid.FunctionDef) and decorated_with_property(m) ] for node_name, associated_nodes in ( list(node.instance_attrs_type.items()) + list(node.locals_type.items()) + properties ): if not self.show_attr(node_name): continue names = self.class_names(associated_nodes) if names: node_name = ""%s : %s"" % (node_name, "", "".join(names)) attrs.append(node_name) return sorted(attrs)"
1952,"def connect_to_mysql_server(self, none): if self.connect_to_mysql_server = self.connect_to_mysql_server = self.connect_to_mysql_server = self.connect_to_mysql_server = self.connect_to_mysql_server = self.connect_to_mysql_server = self.connect_to_mysql_server = self.connect_to_server = self.connect_to_server = self.connect_to_server = self.connect_to_server = self.connect_to_server = self.connect_to_server = self.connect_to_server = self.connect_to_server = self.connect_to_server = self.connect_to_server = self.connect_to_server = self.connect_to_server = self.connect_to_server = self.connect_to_server = self.connect_to_server = self.connect_to_server = self.connect_to_server = self.connect_to_server = self.connect_to_server = self.connect_to_server = self.connect_to_server = self.connect_to_server = self.connect_to_server =","def connection(self): ctx = _app_ctx_stack.top if ctx is not None: if not hasattr(ctx,'mysql_db'): ctx.mysql_db = self.connect return ctx.mysql_db"
1953,"def load_model(self.load_path, self.load_path, self.load_path, self.load_path, self.load_path, self.load_path, self.load_path, self.load_path, self.load_path, self.load_path, self.load_path, self.load_path, self.load_path, self.load_path, self.load_path, self.load_path, self.load_path, self.load_path)","def load(self, exclude_scopes: tuple = ('Optimizer',)) -> None: if not hasattr(self,'sess'): raise RuntimeError('Your TensorFlow model  must''have sess attribute!'.format(self.__class__.__name__)) path = str(self.load_path.resolve()) if tf.train.checkpoint_exists(path): log.info('[loading model from ]'.format(path)) var_list = self._get_saveable_variables(exclude_scopes) saver = tf.train.Saver(var_list) saver.restore(self.sess, path)"
1954,def last_medias_likes_last_medias_likes_last_medias_likes_last_medias_likes_last_medias_likes_last_medias_likes_last_medias_likes_last_medias_likes_last_medias_likes_last_medias_likes_last_medias_likes_last_medias_likes_last_medias_likes_last_medias_likes_last_medias_last_medias_last_medias_last_medias_last_medias_last_medias_last_medias_last_medias_last_medias_last_medias_last_medias,"def like_hashtag(self, hashtag, amount=None): self.logger.info(""Going to like media with hashtag #%s."" % hashtag) medias = self.get_total_hashtag_medias(hashtag, amount) return self.like_medias(medias)"
1955,"def get_image_type(self, filename): if self.image_type(self.image_type): self.image_type(self.image_type) self.image_type(self.image_type) self.image_type(self.image_type) self.image_type(self.image_type) self.image_type(self.image_type) self.image_type(self.image_type) self.image_type(self.image_type) self.image_type(self.image_type) self.image_type(self.image_type) self.image_type(self.image_type) self.image_type(self.image_type) self.image_type(self.image_type) self.image_type(self.image_type) self.image_type) self.image_type(self.image_type) self.image_type) self.image_type(self.image_type) self.image_type(self.image_type) self.image_type(self.image_type) self.image_type(self.image_type) self.image_type(self.image_type) self.image_type(self.image_type) self.image_type(self.image_type) self.image_type(","def bitmap_type(filename): if filename == '': return None name, ext = os.path.splitext(filename) ext = ext[1:].upper() if ext == 'BMP': return wx.BITMAP_TYPE_BMP elif ext == 'GIF': return wx.BITMAP_TYPE_GIF elif ext == 'JPG' or ext == 'JPEG': return wx.BITMAP_TYPE_JPEG elif ext == 'PCX': return wx.BITMAP_TYPE_PCX elif ext == 'PICT': return wx.BITMAP_TYPE_PICT elif ext == 'PNG': return wx.BITMAP_TYPE_PNG elif ext == 'PNM': return wx.BITMAP_TYPE_PNM elif ext == 'TIF' or ext == 'TIFF': return wx.BITMAP_TYPE_TIF elif ext == 'XBM': return wx.BITMAP_TYPE_XBM elif ext == 'XPM': return wx.BITMAP_TYPE_XPM else: raise RuntimeErro('invalid graphics format')"
1956,def continuation_prompt(self): if self.prompt(self.prompt): self.prompt(self.prompt): self.prompt(self.prompt): self.prompt(self.prompt): self.prompt(self.prompt),"def _make_continuation_prompt(self, prompt): end_chars = '...:'space_count = len(prompt.lstrip('n')) - len(end_chars) body = '&nbsp;' * space_count + end_chars return'span class=""in-prompt"">%s/span>' % body"
1957,"def push_new_dialog(dialog_id, options): dialog_id = dialog_id(dialog_id, options)","async def begin_dialog(self, dialog_id: str, options: object = None): if (not dialog_id): raise TypeError('Dialog(): dialogId cannot be None.') dialog = await self.find_dialog(dialog_id) if dialog is None: raise Exception(""'DialogContext.begin_dialog(): A dialog with an id of '%s' wasn't found."" "" The dialog must be included in the current or parent DialogSet."" "" For example, if subclassing a ComponentDialog you can call add_dialog() within your constructor."" % dialog_id) instance = DialogInstance() instance.id = dialog_id instance.state =  self._stack.append(instance) return await dialog.begin_dialog(self, options)"
1958,"def get_first_record(self, download): if self.get_first_record(self.get_first_record): self.get_first_record(self.get_first_record) self.get_first_record(self.get_first_record) self.get_first_record(self.get_first_record) self.get_first_record(self.get_first_record) self.get_first_record(self.get_first_record) self.get_first_record(self.get_first_record) self.get_first_record(self.get_first_record) self.get_first_record(self.get_first_record) self.get_first_record) self.get_first_record(self.get_first_record) self.get_first_record(self.get_first_record) self.get_first_record(self.get_first_record) self.get_first_record(self.record) self.record(self.record) self.record(self.record) self.record(self.record) self.record(self.record) self.re","def get_first_record( cls, download: bool=True ) -> NistBeaconValue: if download: return NistBeacon.get_record(cls._INIT_RECORD.timestamp) else: return NistBeaconValue.from_json(cls._INIT_RECORD.json)"
1959,"def hash(block_index, block_index): hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash = hash =","def get_block_hash(self, block_index, **kwargs): return self._call(JSONRPCMethods.GET_BLOCK_HASH.value, [block_index, ], **kwargs)"
1960,"def tuple(args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args): args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args, args,","def argpack(args): if isinstance(args[0], (tuple, list, ndarray)): return tupleize(args[0]) elif isinstance(args[0], Iterable) and not isinstance(args[0], str): return tupleize(list(args[0])) else: return tuple(args)"
1961,"def split_layer(self, part_num, layer_num, part_num, layer_num, part_num, layer_num, part_num, part_num, part_num, part_num, part_num, part_num, part_num, part_num, part_num, part_num, part_num, part_num, part_num, part_num, part_num, part_num, part_num, part_num, part_num, part_num, part_num, part_num, part_num, part_num, part_num, part_num, part_num, part_num, part_num, part_num, part_num, part_num, part_num, part_num, part_num, part_num, part_num, layer_num, layer_num, layer_num, layer_num, layer_num, layer_num, layer_num) layer_num, layer_num, layer_num, layer_num, layer_num, layer_num, layer_num, layer_num, layer_num, layer_num, layer_num, layer_num, layer_num, layer_num, layer_num, layer_num, layer_num, layer_num, layer_num, layer_num, layer_num, layer_num, layer_num","def split_parts(script, part_num=None, layer_num=None): filter_xml ='filter name=""Split in Connected Components""/>n' if isinstance(script, mlx.FilterScript): if (layer_num is not None) and (layer_num!= script.current_layer()): change(script, layer_num) util.write_filter(script, filter_xml) if part_num is not None: for i in range(part_num): script.add_layer('CC '.format(i), True) else: script.add_layer('CC 0', True) print('Warning: the number of parts was not provided and cannot', 'be determined automatically. The layer stack is likely', 'incorrect!') else: util.write_filter(script, filter_xml) return None"
1962,"def build_string(self, length, choices, chars): if self.get_string(self.get_string): self.get_string(self.get_string) self.get_string(self.get_string) self.get_string(self.get_string) self.get_string(self.get_string) self.get_string(self.get_string) self.get_string(self.get_string) self.get_string(self.get_string) self.get_string(self.get_string) self.get_string(self.get_string) self.get_string(self.get_string)","def text(length, choices=string.ascii_letters): return ''.join(choice(choices) for x in range(length))"
1963,def totalattenuation(a + b) = a + b = a + b = a + b = a + b = a + b = a + b = a + b = a + b = a + b = a + b = a + b = a + b = a + b = a + b = a + b = a + b = a + b = a + a + b = a + b = a + b = a + b = a + b = a + b = a + b = a + b = a + b = a + b = a + b = a + b = a + b = a + b = a + b = a + b = a + b = a + b = a + b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b =,def build_c(self): lg.info('Building total attenuation C') self.c = self.a + self.b
1964,"def upload_link(file_path, folder_id, sha1): if file_path is not provided, upload_file fails.get_url(file_path, folder_id, sha1): if file_path is not provided, upload_file fails.get_url(file_path, sha1): if file_path is not provided, upload_file fails.get_url(file_path, sha1): if file_path is not provided, upload_file(file_path, sha1): if file_path, sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha1 = sha","def upload_file(self, file_path, folder_id=None, sha1=None, httponly=False): upload_url_response_json = self.upload_link(folder_id=folder_id, sha1=sha1, httponly=httponly) upload_url = upload_url_response_json['url'] with open(file_path, 'rb') as f: response_json = requests.post(upload_url, files='upload_file': f).json() self._check_status(response_json) return response_json['result']"
1965,"def replace_python(self, python, python, python, python, python, python, python, python, python, python, python, python, python, python, python, python, python, python, python, python, python)","def fix_script(path): if os.path.isfile(path): with open(path, 'rb') as script: firstline = script.readline() if not firstline.startswith(b'#!python'): return False exename = sys.executable.encode(sys.getfilesystemencoding()) firstline = b'#!' + exename + os.linesep.encode(""ascii"") rest = script.read() with open(path, 'wb') as script: script.write(firstline) script.write(rest) return True"
1966,"def add_parameters(self, traj.diff_name, traj.diff_name, traj.diff_name, traj.diff_name, traj.diff_name, traj.diff_name, traj.diff_name, traj.diff_name, traj.diff_name, traj.diff_name, traj.diff_name, traj.diff_name, traj.diff_name, traj.diff_name, traj.diff_name, traj.diff_name)","def add_parameters(traj): traj.f_add_parameter('steps', 10000, comment='Number of time steps to simulate') traj.f_add_parameter('dt', 0.01, comment='Step size') traj.f_add_parameter(ArrayParameter,'initial_conditions', np.array([0.0,0.0,0.0]), comment = 'Our initial conditions, as default we will start from''origin!') traj.f_add_parameter('diff_name','diff_lorenz', comment= 'Name of our differential equation') if traj.diff_name == 'diff_lorenz': traj.f_add_parameter('func_params.sigma', 10.0) traj.f_add_parameter('func_params.beta', 8.0/3.0) traj.f_add_parameter('func_params.rho', 28.0) elif traj.diff_name == 'diff_roessler': traj.f_add_parameter('func_params.a', 0.1) traj.f_add_parameter('func_params.c', 14.0) else: raise ValueError('I don't know what %s is.' % traj.diff_name)"
1967,def return CRC calc value from raw serial data to serial data.crc_calc(self): self.crc_calc(self.crc_calc(self.crc_calc(self.crc_calc)),"def get(data): crc = 0 for byte in array('B', data): crc = (VProCRC.CRC_TABLE[(crc >> 8)  byte]  ((crc & 0xFF)  8)) return crc"
1968,"def delete_report(self, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id) report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id, report_id,","def _close_connection(self, report_id): logger.debug( ""Closing connection and sending DELETE request to "".format( self.broadcast_address)) try: r = requests.delete(self.broadcast_address, json=""run_id"": report_id) if r.status_code!= 202: logger.error(colored_print( ""ERROR: There was a problem sending data to the server"" ""with reason: "".format(r.reason))) except requests.exceptions.ConnectionError: logger.error(colored_print( ""ERROR: Could not establish connection with server. The server"" "" may be down or there is a problem with your internet "" ""connection."", ""red_bold"")) sys.exit(1)"
1969,def remove_common_values(common_values): def remove_common_values(common_values): def update_common_values(common_values): def update_common_values(common_values): def update_common_values(common_values): def update_common_values(common_values): def update_common_values(common_values),"def symmetric_difference_update(self, oset: Scope) -> Scope: skey = set() keys = list(self._hsig.keys()) for k in keys: if k in oset: skey.add(k) for k in oset._hsig.keys(): if k not in skey: self._hsig[k] = oset.get(k) for k in skey: del self._hsig[k] return self"
1970,"def derive_keys(passwords, user_id, passwords): def derive_keys(passwords): def derive_keys(passwords): def derive_keys(passwords): def derive_keys(passwords): def derive_keys(passwords): def derive_keys(passwords): def derive_keys(passwords): def derive_keys(passwords)","def derive_fallback_fernet_keys(passwords, user_id): assert isinstance(passwords, (list, tuple)),  ""Expected list or tuple of keys, got %s."" % type(passwords) def derive_single_allow_none(k): if k is None: return None return derive_single_fernet_key(k, user_id).decode('ascii') return list(map(derive_single_allow_none, passwords))"
1971,"def list_reports(self, account_id): return self.reports(self.reports.available_reports.available_reports)","def get_available_reports(self, account_id): url = ACCOUNTS_API.format(account_id) + ""/reports"" report_types = [] for datum in self._get_resource(url): report_types.append(ReportType(data=datum, account_id=account_id)) return report_types"
1972,"def delete_dir(self, directory): if self.dir(dir): self.dir(dir): self.dir(dir): self.dir(dir): self.dir(dir)","def delete_uneeded(self): structure = self._get_structure() list_of_key = list(structure.keys()) structure = structure[list_of_key[0]] parent_path = list_of_key[0] if not parent_path.endswith(PyFunceble.directory_separator): parent_path += PyFunceble.directory_separator for root, _, _ in PyFunceble.walk(parent_path): root = Directory(root).fix_path() if root.replace(parent_path, """") not in structure: PyFunceble.rmtree(root)"
1973,"def create_user(self, nickname): if self.devices = self.devices = self.devices = self.devices = self.devices = self.devices = self.devices = self.devices = self.devices = self.devices = self.devices = self.devices","def create(self, email, password, role=""user"", public=True, **kwargs): kwargs[""email""] = email kwargs[""password""] = password kwargs[""role""] = role kwargs[""public""] = public self.metadata = self.db.create( self.path, kwargs).json()"
1974,"def func_validate_args(self, arguments): if self.func_validate_args(args): self.func_validate_args(args): self.func_validate_args(args): self.func_validate(args)","def validate_kwargs(func, kwargs): func_name = func.__name__ argspec = inspect.getargspec(func) all_args = argspec.args[:] defaults = list(argspec.defaults or []) if inspect.ismethod(func) and all_args[:1] == ['self']: all_args[:1] = [] if defaults: required = all_args[:-len(defaults)] else: required = all_args[:] trans =  arg: arg.endswith('_') and arg[:-1] or arg for arg in all_args  for key in list(kwargs): key_adj = '%s_' % key if key_adj in all_args: kwargs[key_adj] = kwargs.pop(key) supplied = sorted(kwargs) missing = [ trans.get(arg, arg) for arg in required if arg not in supplied ] if missing: raise MeteorError( 400, func.err, 'Missing required arguments to %s: %s' % ( func_name,''.join(missing), ), ) extra = [ arg for arg in supplied if arg not in all_args ] if extra: raise MeteorError( 400, func.err, 'Unknown arguments to %s: %s' % (func_name,''.join(extra)), )"
1975,"def parse_model_instances(self, model_instances): self.parse_model_instances(model_instances) self.parse_model_instances(model_instances) self.parse_model_instances(model_instances) self.parse_model_instances(model_instances) self.parse_model_instances(model_instances) self.parse_model_instances(model_instances) self.parse_model_instances(model_instances) self.parse_model_instances(model_instances) self.parse_model_instances(model_instances) self.parse_model_instances) self.parse_model_instances(model_instances) self.parse_model_instances(model_instances) self.parse_instances(model_instances) self.parse_instances(model_instances) self.parse_instances(model_instances) self.parse_instances(model_instances) self.pars","def parse(self): if not self.loaded: self.load(self.source) for item in self.get_items(): data = self.parse_item(item) instance = self.get_instance(data) self.feed_instance(data, instance) try: self.save_item(item, data, instance) except Exception as e: self.save_error(data, sys.exc_info()) self.unload()"
1976,"def email_remapping(self, vcs_email_domain, ignore_vcs_email_domain, ignore_vcs_email_domain, ignore_vcs_email_domain, ignore_vcs_email_domain, ignore_vcs_email_domain, ignore_vcs_email_domain, ignore_vcs_email_domain, ignore_vcs_email_domain)","def _get_email(self, email): if not email or ""@"" not in email: return None if email in self.email_remapping.remap: return self.email_remapping.remap[email] prefix, domain = email.split(""@"", 2) if prefix in self.email_remapping.remap: return self.email_remapping.remap[prefix] if ""."" not in domain or config.ignore_vcs_email_domain: return ""%s@%s"" % (prefix, config.email_domain_name) return email"
1977,"def save_timestamp('to_test', 'to_test', 'to_test', 'to_test', 'to_test', 'to_test', 'to_test', 'to_test', 'to_test', 'to_test', 'to_test', 'to_test', 'to_test', 'to_test', 'to_test', 'to_test', 'to_test', 'to_test', 'to_test', 'to_test')","def add(self): if PyFunceble.CONFIGURATION[""inactive_database""]: timestamp = str(self._timestamp()) if ( ""inactive_db"" in PyFunceble.INTERN and PyFunceble.INTERN[""file_to_test""] in PyFunceble.INTERN[""inactive_db""] ): if ( timestamp in PyFunceble.INTERN[""inactive_db""][ PyFunceble.INTERN[""file_to_test""] ] ): if ( PyFunceble.INTERN[""to_test""] not in PyFunceble.INTERN[""inactive_db""][ PyFunceble.INTERN[""file_to_test""] ][timestamp] ): PyFunceble.INTERN[""inactive_db""][ PyFunceble.INTERN[""file_to_test""] ][timestamp].append(PyFunceble.INTERN[""to_test""]) else: PyFunceble.INTERN[""inactive_db""][ PyFunceble.INTERN[""file_to_test""] ].update(timestamp: [PyFunceble.INTERN[""to_test""]]) if ( ""to_test"" in PyFunceble.INTERN[""inactive_db""][ PyFunceble.INTERN[""file_to_test""] ] and PyFunceble.INTERN[""to_test""] in PyFunceble.INTERN[""inactive_db""][ PyFunceble.INTERN[""file_to_test""] ][""to_test""] ): PyFunceble.INTERN[""inactive_db""][PyFunceble.INTERN[""file_to_test""]][ ""to_test"" ].remove(PyFunce"
1978,"def patch_instance_group_manager(self, zone, resource_id, body, request_id, project_id, project_id, request_id, request_id, request_id, request_id, project_id, request_id, request_id, request_id, request_id, request_id, request_id, request_id, project_id, request_id, request_id, request_id, request_id, request_id, request_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_id, project_","def patch_instance_group_manager(self, zone, resource_id, body, request_id=None, project_id=None): response = self.get_conn().instanceGroupManagers().patch( project=project_id, zone=zone, instanceGroupManager=resource_id, body=body, requestId=request_id ).execute(num_retries=self.num_retries) try: operation_name = response[""name""] except KeyError: raise AirflowException( ""Wrong response '' returned - it should contain "" ""'name' field"".format(response)) self._wait_for_operation_to_complete(project_id=project_id, operation_name=operation_name, zone=zone)"
1979,"def ndims(self, tf.rank(x)): if self.get(tf.rank(x)): self.get(tf.rank(x)) self.get(tf.rank(x)) self.get(tf.rank(x)) self.get(tf.rank(x)) self.get(tf.rank(x)) self.get(tf.rank(x)) self.get(x) self.get(x) self.get(x) self.get(x) self.get(x) self.get(x) self.get(x) self.get(x) self.get(x) self.get(x) self.get(x) self.get(x) self.get(x) self.get(x) self.get(x) self.get(x) self.get(x) self.get(x) self.get(x) self.get(x) self.get(x) self.get(x) self.get(x) self.get(x) self.get(x) self.get(x) self.get(x) self.get(x) self.get(x) self.get(x) self.get(x) self.get(x) self","def _get_best_effort_ndims(x, expect_ndims=None, expect_ndims_at_least=None, expect_ndims_no_more_than=None): ndims_static = _get_static_ndims( x, expect_ndims=expect_ndims, expect_ndims_at_least=expect_ndims_at_least, expect_ndims_no_more_than=expect_ndims_no_more_than) if ndims_static is not None: return ndims_static return tf.rank(x)"
1980,"def schedule_task(self, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id): task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id, task_id,","def schedule_task(self, task_id): task = self.registry.get(task_id) job_args = self._build_job_arguments(task) archiving_cfg = task.archiving_cfg fetch_from_archive = False if not archiving_cfg else archiving_cfg.fetch_from_archive queue = Q_ARCHIVE_JOBS if fetch_from_archive else Q_CREATION_JOBS job_id = self._scheduler.schedule_job_task(queue, task.task_id, job_args, delay=0) logger.info(""Job #%s (task: %s) scheduled"", job_id, task.task_id) return job_id"
1981,"def cast_object(self, string): self.get_object(string): self.get_object(string): self.get_object(string): self.get_object(string): self.get_object(string): self.get_object(string): self.get_object(string): self.get_object(string): self.get_object(string): self.get_object(string): self.get_object(string)","def to_string(obj): if isinstance(obj, LiteralWrapper): val = obj.obj elif isinstance(obj, Iterable) and not isinstance(obj, str): val = next(obj, None) else: val = obj if val is None: yield '' elif isinstance(val, str): yield val elif isinstance(val, node): yield strval(val) elif isinstance(val, int) or isinstance(val, float): yield str(val) elif isinstance(item, bool): yield 'true' if item else 'false' else: raise RuntimeError('Unknown type for string conversion: '.format(val))"
1982,"def kernel_results(init_state, transformed_init_state, transformed_init_state, transformed_init_state, transformed_init_state, transformed_init_state, transformed_init_state, transformed_init_state, transformed_init_state, transformed_init_state, transformed_init_state, transformed_init_state, transformed_init_state, transformed_init_state, transformed_init_state, transform_init_state, transform_init_state, transform_init_state, transform_init_state, transform_init_state, transform_init_state, transform_init_state, transform_init_state, transform_init_state, transform_init_state, transform_init_state, transform_init_state, transform_init_state, transform_init_state, transform_init_state, transform_init_state, transform_init_state, transform_init_state, transform_init_state, transform_init_state, transform_init_state, transform_init_state, transform_init_state, transform_init_state, transform_init_state, transform_init_state, transform_init","def bootstrap_results(self, init_state=None, transformed_init_state=None): if (init_state is None) == (transformed_init_state is None): raise ValueError('Must specify exactly one of init_state''or transformed_init_state.') with tf.compat.v1.name_scope( name=mcmc_util.make_name(self.name, 'transformed_kernel', 'bootstrap_results'), values=[init_state, transformed_init_state]): if transformed_init_state is None: init_state_parts = (init_state if mcmc_util.is_list_like(init_state) else [init_state]) transformed_init_state_parts = self._inverse_transform(init_state_parts) transformed_init_state = ( transformed_init_state_parts if mcmc_util.is_list_like(init_state) else transformed_init_state_parts[0]) else: if mcmc_util.is_list_like(transformed_init_state): transformed_init_state = [ tf.convert_to_tensor(value=s, name='transformed_init_state') for s in transformed_init_state ] else: transformed_init_state = tf.convert_to_tensor( value=transformed_init_state, name='transformed_init_state') kernel_results = TransformedTransitionKernelResults( transformed_state=transformed_init_state, inner_results=self._inner_kernel.bootstrap_results( transformed_init_state)) return kernel_results"
1983,"def remove_transitions(self, gwin, win, smwin, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, conf, gwin, gwin, gwin, gwin, gwin, gwin, gwin, gwin, gwin, gwin, gwin, gwin, gwin, gwin, gwin, gwin, gwin, gwin","def autorange(self, analyte='total_counts', gwin=5, swin=3, win=20, on_mult=[1., 1.5], off_mult=[1.5, 1], transform='log', ploterrs=True, focus_stage='despiked'): if focus_stage == 'despiked': if 'despiked' not in self.stages_complete: focus_stage = 'rawdata' if analyte is None: analyte = self.internal_standard elif analyte in self.analytes: self.minimal_analytes.update([analyte]) fails =  with self.pbar.set(total=len(self.data), desc='AutoRange') as prog: for s, d in self.data.items(): f = d.autorange(analyte=analyte, gwin=gwin, swin=swin, win=win, on_mult=on_mult, off_mult=off_mult, ploterrs=ploterrs, transform=transform) if f is not None: fails[s] = f prog.update() if len(fails) > 0: wstr = ('nn' + '*' * 41 + 'n' +'WARNINGn' + '*' * 41 + 'n' + 'Autorange failed for some samples:n') kwidth = max([len(k) for k in fails.keys()]) + 1 fstr =':' + ''.format(kwidth) +'s:'for k in sorted(fails.keys()): wstr += fstr.format(k) + ', '.join([':.1f"
1984,"def subscribe_user_to_service(self, service, method): if self.subscribe_user(self.subscribe_user): self.subscribe_user(self.subscribe_user): self.subscribe_user(self.subscribe_user): self.subscribe_user(self.subscribe_user): self.subscribe_user(self.subscribe_user)","def subscribe_user(self, user): url = self.root_url + ""subscribe_user"" values =  values[""username""] = user return self._query(url, values)"
1985,"def retrieve_input(session, line, output, stop): if session is None: return raw if session is None: return raw if session is None: return raw if session is None: return raw if session is None: return raw if session is None: return raw if session is None: return raw if session is None: return raw if session is None: return raw if session is None: return raw if session is None: return raw if session is None: return raw","def get_range(self, session, start=1, stop=None, raw=True,output=False): % lineclause, params, raw=raw, output=output)"
1986,"def get_method(self, doc): if self.get_method(self.get_method(self.get_method(self.get_method(self.get_method(self.get_method(self.get_method(self.get_method(self.get_method(self.get_method(self.get_method(self.get_method(self.get_method(self.get_method(self.get_method(self.get_method(self.get_method(self.get_method(self.get_method(self.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method.get_method)","def getExprVal(self, val, do_eval=False): ctx = VhdlSerializer.getBaseContext() def createTmpVar(suggestedName, dtype): raise NotImplementedError( ""Width value can not be converted do ipcore format (%r)"", val) ctx.createTmpVarFn = createTmpVar if do_eval: val = val.staticEval() val = VivadoTclExpressionSerializer.asHdl(val, ctx) return val"
1987,def get_strip(self): if self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip(self.get_strip))))),"def ugettext(message, context=None): stripped = strip_whitespace(message) message = add_context(context, stripped) if context else stripped ret = django_ugettext(message) return stripped if ret == message else ret"
1988,"def old_pin(self, old_pin, new_pin, new_pin, old_pin, new_pin, old_pin, new_pin, old_pin, new_pin, new_pin, old_pin, new_pin, old_pin, new_pin, old_pin, new_pin, old_pin, new_pin, old_pin, new_pin, new_pin, old_pin, new_pin, old_pin, new_pin, old_pin, new_pin, old_pin, old_pin, new_pin, old_pin, old_pin, new_pin, old_pin, new_pin, old_pin, new_pin, old_pin, new_pin, old_pin, new_pin, new_pin, old_pin, new_pin, old_pin, new_pin, new_pin, new_pin, old_pin, new_pin, new_pin, old_pin, new_pin, new_pin, new_pin, old_pin, new_pin, old_pin, new_pin, new_pin, new_pin, old_pin, new_pin, new_pin, new_pin, old_pin, new_pin, new_pin, new_pin, new_pin, old_pin, new_pin, new_pin","def setPin(self, old_pin, new_pin): old_pin1 = ckbytelist(old_pin) new_pin1 = ckbytelist(new_pin) rv = self.lib.C_SetPIN(self.session, old_pin1, new_pin1) if rv!= CKR_OK: raise PyKCS11Error(rv)"
1989,def add_field(self): if self.get_field(self.get_field(self.get_field(self.get_field(self.get_field(self.get_field(self.get_field(self.get_field(self.get_field(self.get_field(self.get_field(self.get_field(self.get_field(self.get_field(self.get_field(self.get_field(self.get_field)))))))))))),"def _get_field(self, field_name, default=None): full_field_name = 'extra__grpc__'.format(field_name) if full_field_name in self.extras: return self.extras[full_field_name] else: return default"
1990,"def register_descriptors(self, obj, dict): obj(list, dict): obj(list, dict): obj(list, dict): obj(list, dict): obj(list, dict): obj(list, dict): obj(list, dict): obj(list, dict): obj(list, dict): obj(list, dict): obj(list, dict): obj(list, dict): obj(list, dict): obj(list, dict): obj(list, dict): dict): dict): descriptors: descriptors: descriptors: descriptors: descriptors: descriptors: descriptors: descriptors: descriptors: descriptors: descriptors: descriptors: descriptors: descriptors: descriptors: descriptors: descriptors: descriptors: descriptors: descriptors: descriptors: de","def register_json(self, obj): if not isinstance(obj, list): obj = [obj] self.register(Descriptor.from_json(j) for j in obj)"
1991,"def get_wave_functions(self, waveform): if self.wave_functions.acosh = self.wave_functions.acosh = self.wave_functions.acosh = self.wave_functions.acosh = self.wave_functions.acosh = self.wave_functions.acosh = self.wave_functions.acosh = self.wave_functions.acosh = self.wave_functions.acosh = self.wave_functions.acosh","def acosh(wave): rpexdoc.exh.addex(ValueError, ""Math domain error"", bool(min(wave._dep_vector)  1)) return _operation(wave, ""acosh"", """", np.arccosh)"
1992,"def float_value(self, decimal, name): if self.get_value(self.get_value): self.get_value(self.get_value) self.get_value(self.get_value) self.get_value(self.get_value) self.get_value(self.get_value) self.get_value(self.get_value) self.get_value(self.get_value) self.get_value(self.get_value) self.get_value(self.get_value) self.get_value(self.get_value) self.get_value(self.get_value) self.get_value(self.get_value) self.get_value(self.get_value) self.get_value(self.get_value) self.get_value) self.get_value(self.get_value) self.get_value) self.get_value(self.get_value) self.get_value(self.get_value) self.get_value(self.get_value) self.get_value(self.get_value) self.get_value(self.get_value) self.get_value(self.get_value) self.get_value(self.get_value) self.get_value","def parse(self, value, decimal=None): value = value or 0 if check_type(value, 'list'): return map(lambda val: self.parse(val, decimal)) if check_type(value, 'int') or check_type(value, 'float'): return value decimal = decimal or self.settings.number.decimal regex = re.compile(""[0-9-"" + decimal + ""]"") unformatted = str(value) unformatted = re.sub('/((.*))/', ""-$1"", unformatted) unformatted = re.sub(regex, '', unformatted) unformatted = unformatted.replace('.', decimal) formatted = (lambda val: unformatted if val else 0)( is_num(unformatted)) return formatted"
1993,"def create_frontend(self, current_widget, current_widget, current_widget, current_widget, current_widget, current_widget, current_widget, current_widget, current_widget, current_widget, current_widget, current_widget, current_widget, current_widget, current_widget)","def new_frontend_slave(self, current_widget): kernel_manager = self.kernel_manager_class( connection_file=current_widget.kernel_manager.connection_file, config = self.config, ) kernel_manager.load_connection_file() kernel_manager.start_channels() widget = self.widget_factory(config=self.config, local_kernel=False) self.init_colors(widget) widget._existing = True widget._may_close = False widget._confirm_exit = False widget.kernel_manager = kernel_manager return widget"
1994,"def new_connection_to_database(self, database): if self.get_database(database): self.get_database(database): self.get_database(database): self.get_database(database)","def get_connection(db=DATABASE): return database.connect(host=HOST, port=PORT, user=USER, password=PASSWORD, database=db)"
1995,def add_fields(self.add_select): self.add_select(self.add_select): self.add_select(self.add_select): self.add_select(self.add_select): self.add_select(self.add_select): self.add_select(self.add_select): self.add_select(self.add_select): self.add_select(self.add_select): self.add_select),"def add_fields(self, field_names: List[str], allow_m2m: bool=True) -> bool: alias = self.get_initial_alias() opts = self.get_meta() cols = [] for name in field_names: parts = name.split(LOOKUP_SEP) if len(parts) > 1: column_name, hstore_key = parts[:2] is_hstore, field = self._is_hstore_field(column_name) if is_hstore: cols.append( HStoreColumn(self.model._meta.db_table or self.model.name, field, hstore_key) ) continue join_info = self.setup_joins(parts, opts, alias, allow_many=allow_m2m) targets, final_alias, joins = self.trim_joins( join_info[1], join_info[3], join_info[4] ) for target in targets: cols.append(target.get_col(final_alias)) if cols: self.set_select(cols)"
1996,"def subcommand(self, subcommand, subcommand): if self.subcommand(subcommand, subcommand): self.subcommand(subcommand): self.subcommand(subcommand, subcommand) self.subcommand(subcommand) self.subcommand(subcommand) self.subcommand(subcommand) self.subcommand(subcommand) self.subcommand(subcommand) self.subcommand(subcommand) self.subcommand(subcommand) self.subcommand(subcommand) self.subcommand(subcommand) self.subcommand) self.subcommand(subcommand) self.subcommand) self.subcommand(subcommand) self.subcommand(subcommand) self.subcommand(subcommand) self.subcommand) self.subcommand(subcommand) self.subcommand(subcommand) self.subcommand(subcommand) self.subcommand) self.subcommand(subcommand) self.subcommand(subcommand) self.subcommand) self.subcommand(subcommand) self.subcommand(subcommand) self.subcommand(subcommand) self.subcommand(subcommand) self.subcommand(subcommand) self.subcommand(subcommand) self.subcommand(subcommand) self.subcommand(subcommand) self.subcommand(subcommand) self.subcommand(subcommand) self.","def undefined_subcmd(self, cmd, subcmd): self.proc.intf[-1].errmsg(('Undefined ""%s"" subcommand: ""%s"".'+ 'Try ""help %s *"".') % (cmd, subcmd, cmd)) return"
1997,"def decode_date(self, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date) decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date, decode_date,","def decode(self, val): new_val = self.decode_date(val) if val!= new_val: return new_val return json.JSONDecoder.decode(self, val)"
1998,"def _image(self, image): if self.image(image): self.image(image): self.image(image): self.image(image)","def process_image(self, image, image_format, save_kwargs=): imagefile = BytesIO() inv_image = ImageOps.invert(image) inv_image.save( imagefile, **save_kwargs ) return imagefile"
1999,"def timer(self, timing): if self.timer = self.timer = self.timer = self.timer = self.timer = self.timer","def timing(self, stat, value, tags=None): self._log('timing', stat, value, tags)"
