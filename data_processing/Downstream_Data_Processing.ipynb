{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --utf-8---\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import sys\n",
    "import io\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import ast\n",
    "from zipfile import ZipFile\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "from pathlib import Path\n",
<<<<<<< HEAD
    "import autopep8\n",
    "import astunparse\n",
=======
>>>>>>> a6ece6b27f82ac2694c323f86a4c1a2a6a12ee36
    "pd.set_option('max_colwidth',300)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_files = sorted(Path(\"./data/python/python/final/jsonl\").glob('**/*.gz'))"
=======
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_files = sorted(Path(\"./data/python/final/jsonl\").glob('**/*.gz'))"
>>>>>>> a6ece6b27f82ac2694c323f86a4c1a2a6a12ee36
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 6,
>>>>>>> a6ece6b27f82ac2694c323f86a4c1a2a6a12ee36
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "data/python/python/final/jsonl/test/python_test_0.jsonl.gz\n",
      "data/python/python/final/jsonl/train/python_train_0.jsonl.gz\n",
      "data/python/python/final/jsonl/train/python_train_1.jsonl.gz\n",
      "data/python/python/final/jsonl/train/python_train_10.jsonl.gz\n",
      "data/python/python/final/jsonl/train/python_train_11.jsonl.gz\n",
      "data/python/python/final/jsonl/train/python_train_12.jsonl.gz\n",
      "data/python/python/final/jsonl/train/python_train_13.jsonl.gz\n",
      "data/python/python/final/jsonl/train/python_train_2.jsonl.gz\n",
      "data/python/python/final/jsonl/train/python_train_3.jsonl.gz\n",
      "data/python/python/final/jsonl/train/python_train_4.jsonl.gz\n",
      "data/python/python/final/jsonl/train/python_train_5.jsonl.gz\n",
      "data/python/python/final/jsonl/train/python_train_6.jsonl.gz\n",
      "data/python/python/final/jsonl/train/python_train_7.jsonl.gz\n",
      "data/python/python/final/jsonl/train/python_train_8.jsonl.gz\n",
      "data/python/python/final/jsonl/train/python_train_9.jsonl.gz\n",
      "data/python/python/final/jsonl/valid/python_valid_0.jsonl.gz\n"
=======
      "data/python/final/jsonl/test/python_test_0.jsonl.gz\n",
      "data/python/final/jsonl/train/python_train_0.jsonl.gz\n",
      "data/python/final/jsonl/train/python_train_1.jsonl.gz\n",
      "data/python/final/jsonl/train/python_train_10.jsonl.gz\n",
      "data/python/final/jsonl/train/python_train_11.jsonl.gz\n",
      "data/python/final/jsonl/train/python_train_12.jsonl.gz\n",
      "data/python/final/jsonl/train/python_train_13.jsonl.gz\n",
      "data/python/final/jsonl/train/python_train_2.jsonl.gz\n",
      "data/python/final/jsonl/train/python_train_3.jsonl.gz\n",
      "data/python/final/jsonl/train/python_train_4.jsonl.gz\n",
      "data/python/final/jsonl/train/python_train_5.jsonl.gz\n",
      "data/python/final/jsonl/train/python_train_6.jsonl.gz\n",
      "data/python/final/jsonl/train/python_train_7.jsonl.gz\n",
      "data/python/final/jsonl/train/python_train_8.jsonl.gz\n",
      "data/python/final/jsonl/train/python_train_9.jsonl.gz\n",
      "data/python/final/jsonl/valid/python_valid_0.jsonl.gz\n"
>>>>>>> a6ece6b27f82ac2694c323f86a4c1a2a6a12ee36
     ]
    }
   ],
   "source": [
    "for i in python_files:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 7,
>>>>>>> a6ece6b27f82ac2694c323f86a4c1a2a6a12ee36
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Full List:\n",
    "['repo', 'path', 'func_name', 'original_string', 'language', 'code', \n",
    "'code_tokens', 'docstring', 'docstring_tokens', 'sha', 'url', 'partition']\n",
    "\"\"\"\n",
    "\n",
    "columns_long_list = ['repo', 'path', 'url', 'code',\n",
    "                     'code_tokens', 'docstring', 'docstring_tokens', \n",
    "                     'language', 'partition']\n",
    "\n",
    "columns_short_list = [\"func_name\",'code', 'docstring', \n",
    "                      'url', 'partition']"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 8,
>>>>>>> a6ece6b27f82ac2694c323f86a4c1a2a6a12ee36
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonl_list_to_dataframe(file_list, columns=columns_long_list):\n",
    "    \"\"\"Load a list of jsonl.gz files into a pandas DataFrame.\"\"\"\n",
    "    return pd.concat([pd.read_json(f, \n",
    "                                   orient='records', \n",
    "                                   compression='gzip',\n",
    "                                   lines=True)[columns] \n",
    "                      for f in file_list], sort=False)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 9,
>>>>>>> a6ece6b27f82ac2694c323f86a4c1a2a6a12ee36
   "metadata": {},
   "outputs": [],
   "source": [
    "pydf = jsonl_list_to_dataframe(python_files, columns_short_list)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 10,
>>>>>>> a6ece6b27f82ac2694c323f86a4c1a2a6a12ee36
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>func_name</th>\n",
       "      <th>code</th>\n",
       "      <th>docstring</th>\n",
       "      <th>url</th>\n",
       "      <th>partition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YouTube.get_vid_from_url</td>\n",
       "      <td>def get_vid_from_url(url):\\n        \"\"\"Extracts video ID from URL.\\n        \"\"\"\\n        return match1(url, r'youtu\\.be/([^?/]+)') or \\\\n          match1(url, r'youtube\\.com/embed/([^/?]+)') or \\\\n          match1(url, r'youtube\\.com/v/([^/?]+)') or \\\\n          match1(url, r'youtube\\.com/watch/...</td>\n",
       "      <td>Extracts video ID from URL.</td>\n",
       "      <td>https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/youtube.py#L135-L143</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sina_xml_to_url_list</td>\n",
       "      <td>def sina_xml_to_url_list(xml_data):\\n    \"\"\"str-&gt;list\\n    Convert XML to URL List.\\n    From Biligrab.\\n    \"\"\"\\n    rawurl = []\\n    dom = parseString(xml_data)\\n    for node in dom.getElementsByTagName('durl'):\\n        url = node.getElementsByTagName('url')[0]\\n        rawurl.append(url.chil...</td>\n",
       "      <td>str-&gt;list\\n    Convert XML to URL List.\\n    From Biligrab.</td>\n",
       "      <td>https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/miomio.py#L41-L51</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>makeMimi</td>\n",
       "      <td>def makeMimi(upid):\\n    \"\"\"From http://cdn37.atwikiimg.com/sitescript/pub/dksitescript/FC2.site.js\\n    Also com.hps.util.fc2.FC2EncrptUtil.makeMimiLocal\\n    L110\"\"\"\\n    strSeed = \"gGddgPfeaf_gzyr\"\\n    prehash = upid + \"_\" + strSeed\\n    return md5(prehash.encode('utf-8')).hexdigest()</td>\n",
       "      <td>From http://cdn37.atwikiimg.com/sitescript/pub/dksitescript/FC2.site.js\\n    Also com.hps.util.fc2.FC2EncrptUtil.makeMimiLocal\\n    L110</td>\n",
       "      <td>https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/fc2video.py#L11-L17</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fc2video_download</td>\n",
       "      <td>def fc2video_download(url, output_dir = '.', merge = True, info_only = False, **kwargs):\\n    \"\"\"wrapper\"\"\"\\n    #'http://video.fc2.com/en/content/20151021bTVKnbEw'\\n    #'http://xiaojiadianvideo.asia/content/20151021bTVKnbEw'\\n    #'http://video.fc2.com/ja/content/20151021bTVKnbEw'\\n    #'http:...</td>\n",
       "      <td>wrapper</td>\n",
       "      <td>https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/fc2video.py#L46-L57</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dailymotion_download</td>\n",
       "      <td>def dailymotion_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\\n    \"\"\"Downloads Dailymotion videos by URL.\\n    \"\"\"\\n\\n    html = get_content(rebuilt_url(url))\\n    info = json.loads(match1(html, r'qualities\":({.+?}),\"'))\\n    title = match1(html, r'\"video_title\"\\s*:\\s*\"(...</td>\n",
       "      <td>Downloads Dailymotion videos by URL.</td>\n",
       "      <td>https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/dailymotion.py#L13-L35</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  func_name  \\\n",
       "0  YouTube.get_vid_from_url   \n",
       "1      sina_xml_to_url_list   \n",
       "2                  makeMimi   \n",
       "3         fc2video_download   \n",
       "4      dailymotion_download   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                          code  \\\n",
       "0  def get_vid_from_url(url):\\n        \"\"\"Extracts video ID from URL.\\n        \"\"\"\\n        return match1(url, r'youtu\\.be/([^?/]+)') or \\\\n          match1(url, r'youtube\\.com/embed/([^/?]+)') or \\\\n          match1(url, r'youtube\\.com/v/([^/?]+)') or \\\\n          match1(url, r'youtube\\.com/watch/...   \n",
       "1  def sina_xml_to_url_list(xml_data):\\n    \"\"\"str->list\\n    Convert XML to URL List.\\n    From Biligrab.\\n    \"\"\"\\n    rawurl = []\\n    dom = parseString(xml_data)\\n    for node in dom.getElementsByTagName('durl'):\\n        url = node.getElementsByTagName('url')[0]\\n        rawurl.append(url.chil...   \n",
       "2            def makeMimi(upid):\\n    \"\"\"From http://cdn37.atwikiimg.com/sitescript/pub/dksitescript/FC2.site.js\\n    Also com.hps.util.fc2.FC2EncrptUtil.makeMimiLocal\\n    L110\"\"\"\\n    strSeed = \"gGddgPfeaf_gzyr\"\\n    prehash = upid + \"_\" + strSeed\\n    return md5(prehash.encode('utf-8')).hexdigest()   \n",
       "3  def fc2video_download(url, output_dir = '.', merge = True, info_only = False, **kwargs):\\n    \"\"\"wrapper\"\"\"\\n    #'http://video.fc2.com/en/content/20151021bTVKnbEw'\\n    #'http://xiaojiadianvideo.asia/content/20151021bTVKnbEw'\\n    #'http://video.fc2.com/ja/content/20151021bTVKnbEw'\\n    #'http:...   \n",
       "4  def dailymotion_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\\n    \"\"\"Downloads Dailymotion videos by URL.\\n    \"\"\"\\n\\n    html = get_content(rebuilt_url(url))\\n    info = json.loads(match1(html, r'qualities\":({.+?}),\"'))\\n    title = match1(html, r'\"video_title\"\\s*:\\s*\"(...   \n",
       "\n",
       "                                                                                                                                  docstring  \\\n",
       "0                                                                                                               Extracts video ID from URL.   \n",
       "1                                                                               str->list\\n    Convert XML to URL List.\\n    From Biligrab.   \n",
       "2  From http://cdn37.atwikiimg.com/sitescript/pub/dksitescript/FC2.site.js\\n    Also com.hps.util.fc2.FC2EncrptUtil.makeMimiLocal\\n    L110   \n",
       "3                                                                                                                                   wrapper   \n",
       "4                                                                                                      Downloads Dailymotion videos by URL.   \n",
       "\n",
       "                                                                                                                              url  \\\n",
       "0    https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/youtube.py#L135-L143   \n",
       "1       https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/miomio.py#L41-L51   \n",
       "2     https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/fc2video.py#L11-L17   \n",
       "3     https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/fc2video.py#L46-L57   \n",
       "4  https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/dailymotion.py#L13-L35   \n",
       "\n",
       "  partition  \n",
       "0      test  \n",
       "1      test  \n",
       "2      test  \n",
       "3      test  \n",
       "4      test  "
      ]
     },
<<<<<<< HEAD
     "execution_count": 7,
=======
     "execution_count": 10,
>>>>>>> a6ece6b27f82ac2694c323f86a4c1a2a6a12ee36
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pydf.head(5)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracts video ID from URL.\n",
      "************************************************************\n",
      "str->list\n",
      "    Convert XML to URL List.\n",
      "    From Biligrab.\n",
      "************************************************************\n",
      "From http://cdn37.atwikiimg.com/sitescript/pub/dksitescript/FC2.site.js\n",
      "    Also com.hps.util.fc2.FC2EncrptUtil.makeMimiLocal\n",
      "    L110\n",
      "************************************************************\n",
      "wrapper\n",
      "************************************************************\n",
      "Downloads Dailymotion videos by URL.\n",
      "************************************************************\n",
      "http://stackoverflow.com/a/30923963/2946714\n",
      "************************************************************\n",
      "video page\n",
      "************************************************************\n",
      "course page\n",
      "************************************************************\n",
      "Downloads a Sina video by its unique vid.\n",
      "    http://video.sina.com.cn/\n",
      "************************************************************\n",
      "Downloads a Sina video by its unique vkey.\n",
      "    http://video.sina.com/\n",
      "************************************************************\n",
      "Downloads Sina videos by URL.\n",
      "************************************************************\n",
      "wrapper\n",
      "************************************************************\n",
      "Get item_id\n",
      "************************************************************\n",
      "Source: Android mobile\n",
      "************************************************************\n",
      "self, str->None\n",
      "        \n",
      "        Keyword arguments:\n",
      "        self: self\n",
      "        vid: The video ID for BokeCC cloud, something like\n",
      "        FE3BB999594978049C33DC5901307461\n",
      "        \n",
      "        Calls the prepare() to download the video.\n",
      "        \n",
      "        If no title is provided, this method shall try to find a proper title\n",
      "        with the information providin within the\n",
      "        returned content of the API.\n",
      "************************************************************\n",
      "Extracts video ID from live.qq.com.\n",
      "************************************************************\n",
      "Format text with color or other effects into ANSI escaped string.\n",
      "************************************************************\n",
      "Print a log message to standard error.\n",
      "************************************************************\n",
      "Print an error log message.\n",
      "************************************************************\n",
      "What a Terrible Failure!\n",
      "************************************************************\n",
      "Detect operating system.\n",
      "************************************************************\n",
      "Source: Android mobile\n",
      "************************************************************\n",
      "str->None\n",
      "************************************************************\n",
      "str/int->None\n",
      "************************************************************\n",
      "try:\n",
      "        # normal Vimeo video\n",
      "        html = get_content('https://vimeo.com/' + id)\n",
      "        cfg_patt = r'clip_page_config\\s*=\\s*(\\{.+?\\});'\n",
      "        cfg = json.loads(match1(html, cfg_patt))\n",
      "        video_page = get_content(cfg['player']['config_url'], headers=fake_headers)\n",
      "        title = cfg['clip']['title']\n",
      "        info = loads(video_page)\n",
      "    except:\n",
      "        # embedded player - referer may be required\n",
      "        if 'referer' in kwargs:\n",
      "            fake_headers['Referer'] = kwargs['referer']\n",
      "\n",
      "        video_page = get_content('http://player.vimeo.com/video/%s' % id, headers=fake_headers)\n",
      "        title = r1(r'<title>([^<]+)</title>', video_page)\n",
      "        info = loads(match1(video_page, r'var t=(\\{.+?\\});'))\n",
      "\n",
      "    streams = info['request']['files']['progressive']\n",
      "    streams = sorted(streams, key=lambda i: i['height'])\n",
      "    url = streams[-1]['url']\n",
      "\n",
      "    type, ext, size = url_info(url, faker=True)\n",
      "\n",
      "    print_info(site_info, title, type, size)\n",
      "    if not info_only:\n",
      "        download_urls([url], title, ext, size, output_dir, merge=merge, faker=True)\n",
      "************************************************************\n",
      "str->dict\n",
      "    Information for CKPlayer API content.\n",
      "************************************************************\n",
      "Splicing URLs according to video ID to get video details\n",
      "************************************************************\n",
      "Extracts video ID from URL.\n",
      "************************************************************\n",
      "str->list of str\n",
      "        Give you the real URLs.\n",
      "************************************************************\n",
      "Get (branch, commit) from HEAD of a git repo.\n",
      "************************************************************\n",
      "Converts a string to a valid filename.\n",
      "************************************************************\n",
      "Get (width, height) of the current terminal.\n",
      "************************************************************\n",
      "Downloads CBS videos by URL.\n",
      "************************************************************\n",
      "Override the original one\n",
      "        Ugly ugly dirty hack\n",
      "************************************************************\n",
      "str, str, str, bool, bool ->None\n",
      "\n",
      "    Download Acfun video by vid.\n",
      "\n",
      "    Call Acfun API, decide which site to use, and pass the job to its\n",
      "    extractor.\n",
      "************************************************************\n",
      "Main entry point.\n",
      "    you-get-dev\n",
      "************************************************************\n",
      "str, str->True\n",
      "    WARNING: NOT THE SAME PARMS AS OTHER FUNCTIONS!!!!!!\n",
      "    You can basicly download anything with this function\n",
      "    but better leave it alone with\n",
      "************************************************************\n",
      "Scans through a string for substrings matched some patterns (first-subgroups only).\n",
      "\n",
      "    Args:\n",
      "        text: A string to be scanned.\n",
      "        patterns: Arbitrary number of regex patterns.\n",
      "\n",
      "    Returns:\n",
      "        When only one pattern is given, returns a string (None if no match found).\n",
      "        When more than one pattern are given, returns a list of strings ([] if no match found).\n",
      "************************************************************\n",
      "Scans through a string for substrings matched some patterns.\n",
      "\n",
      "    Args:\n",
      "        text: A string to be scanned.\n",
      "        patterns: a list of regex pattern.\n",
      "\n",
      "    Returns:\n",
      "        a list if matched. empty if not.\n",
      "************************************************************\n",
      "Parses the query string of a URL and returns the value of a parameter.\n",
      "\n",
      "    Args:\n",
      "        url: A URL.\n",
      "        param: A string representing the name of the parameter.\n",
      "\n",
      "    Returns:\n",
      "        The value of the parameter.\n",
      "************************************************************\n",
      "Decompresses data for Content-Encoding: gzip.\n",
      "************************************************************\n",
      "Decompresses data for Content-Encoding: deflate.\n",
      "    (the zlib compression is used.)\n",
      "************************************************************\n",
      "Gets the content of a URL via sending a HTTP GET request.\n",
      "\n",
      "    Args:\n",
      "        url: A URL.\n",
      "        headers: Request headers used by the client.\n",
      "        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n",
      "\n",
      "    Returns:\n",
      "        The content as a string.\n",
      "************************************************************\n",
      "Post the content of a URL via sending a HTTP POST request.\n",
      "\n",
      "    Args:\n",
      "        url: A URL.\n",
      "        headers: Request headers used by the client.\n",
      "        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n",
      "\n",
      "    Returns:\n",
      "        The content as a string.\n",
      "************************************************************\n",
      "Parses host name and port number from a string.\n",
      "************************************************************\n",
      "Overload default print function as py (<3.3) does not support 'flush' keyword.\n",
      "    Although the function name can be same as print to get itself overloaded automatically,\n",
      "    I'd rather leave it with a different name and only overload it when importing to make less confusion.\n",
      "************************************************************\n",
      "str->str\n",
      "************************************************************\n",
      "Source: Android mobile\n",
      "************************************************************\n",
      "JSON, int, int, int->str\n",
      "    \n",
      "    Get a proper title with courseid+topicID+partID.\n",
      "************************************************************\n",
      "int->None\n",
      "    \n",
      "    Download a WHOLE course.\n",
      "    Reuse the API call to save time.\n",
      "************************************************************\n",
      "int, int, int->None\n",
      "    \n",
      "    Download ONE PART of the course.\n",
      "************************************************************\n",
      "int, int->list\n",
      "        \n",
      "        Get the height of the videos.\n",
      "        \n",
      "        Since brightcove is using 3 kinds of links: rtmp, http and https,\n",
      "        we will be using the HTTPS one to make it secure.\n",
      "        \n",
      "        If somehow akamaihd.net is blocked by the Great Fucking Wall,\n",
      "        change the \"startswith https\" to http.\n",
      "************************************************************\n",
      "Checks if a task is either queued or running in this executor\n",
      "\n",
      "        :param task_instance: TaskInstance\n",
      "        :return: True if the task is known to this executor\n",
      "************************************************************\n",
      "Returns and flush the event buffer. In case dag_ids is specified\n",
      "        it will only return and flush events for the given dag_ids. Otherwise\n",
      "        it returns and flushes all\n",
      "\n",
      "        :param dag_ids: to dag_ids to return events for, if None returns all\n",
      "        :return: a dict of events\n",
      "************************************************************\n",
      "one method to fetch connection params as a dict\n",
      "        used in get_uri() and get_connection()\n",
      "************************************************************\n",
      "override DbApiHook get_uri method for get_sqlalchemy_engine()\n",
      "************************************************************\n",
      "Returns a snowflake.connection object\n",
      "************************************************************\n",
      "returns aws_access_key_id, aws_secret_access_key\n",
      "        from extra\n",
      "\n",
      "        intended to be used by external import and export statements\n",
      "************************************************************\n",
      "Fetches a field from extras, and returns it. This is some Airflow\n",
      "        magic. The grpc hook type adds custom UI elements\n",
      "        to the hook page, which allow admins to specify scopes, credential pem files, etc.\n",
      "        They get formatted as shown below.\n",
      "************************************************************\n",
      "Executes SQL using psycopg2 copy_expert method.\n",
      "        Necessary to execute COPY command without access to a superuser.\n",
      "\n",
      "        Note: if this method is called with a \"COPY FROM\" statement and\n",
      "        the specified input file does not exist, it creates an empty\n",
      "        file and no data is loaded, but the operation succeeds.\n",
      "        So if users want to be aware when the input file does not exist,\n",
      "        they have to check its existence by themselves.\n",
      "************************************************************\n",
      "Loads a tab-delimited file into a database table\n",
      "************************************************************\n",
      "Dumps a database table into a tab-delimited file\n",
      "************************************************************\n",
      "Uploads the file to Google cloud storage\n",
      "************************************************************\n",
      "Gets the max partition for a table.\n",
      "\n",
      "    :param schema: The hive schema the table lives in\n",
      "    :type schema: str\n",
      "    :param table: The hive table you are interested in, supports the dot\n",
      "        notation as in \"my_database.my_table\", if a dot is found,\n",
      "        the schema param is disregarded\n",
      "    :type table: str\n",
      "    :param metastore_conn_id: The hive connection you are interested in.\n",
      "        If your default is set you don't need to use this parameter.\n",
      "    :type metastore_conn_id: str\n",
      "    :param filter_map: partition_key:partition_value map used for partition filtering,\n",
      "                       e.g. {'key1': 'value1', 'key2': 'value2'}.\n",
      "                       Only partitions matching all partition_key:partition_value\n",
      "                       pairs will be considered as candidates of max partition.\n",
      "    :type filter_map: map\n",
      "    :param field: the field to get the max value from. If there's only\n",
      "        one partition field, this will be inferred\n",
      "    :type field: str\n",
      "\n",
      "    >>> max_partition('airflow.static_babynames_partitioned')\n",
      "    '2015-01-01'\n",
      "************************************************************\n",
      "This function finds the date in a list closest to the target date.\n",
      "    An optional parameter can be given to get the closest before or after.\n",
      "\n",
      "    :param target_dt: The target date\n",
      "    :type target_dt: datetime.date\n",
      "    :param date_list: The list of dates to search\n",
      "    :type date_list: list[datetime.date]\n",
      "    :param before_target: closest before or after the target\n",
      "    :type before_target: bool or None\n",
      "    :returns: The closest date\n",
      "    :rtype: datetime.date or None\n",
      "************************************************************\n",
      "This function finds the date in a list closest to the target date.\n",
      "    An optional parameter can be given to get the closest before or after.\n",
      "\n",
      "    :param table: A hive table name\n",
      "    :type table: str\n",
      "    :param ds: A datestamp ``%Y-%m-%d`` e.g. ``yyyy-mm-dd``\n",
      "    :type ds: list[datetime.date]\n",
      "    :param before: closest before (True), after (False) or either side of ds\n",
      "    :type before: bool or None\n",
      "    :returns: The closest date\n",
      "    :rtype: str or None\n",
      "\n",
      "    >>> tbl = 'airflow.static_babynames_partitioned'\n",
      "    >>> closest_ds_partition(tbl, '2015-01-02')\n",
      "    '2015-01-01'\n",
      "************************************************************\n",
      "Returns a mysql connection object\n",
      "************************************************************\n",
      "Loads a tab-delimited file into a database table\n",
      "************************************************************\n",
      "Checks whether new objects have been uploaded and the inactivity_period\n",
      "        has passed and updates the state of the sensor accordingly.\n",
      "\n",
      "        :param current_num_objects: number of objects in bucket during last poke.\n",
      "        :type current_num_objects: int\n",
      "************************************************************\n",
      "Helps debug deadlocks by printing stacktraces when this gets a SIGQUIT\n",
      "    e.g. kill -s QUIT <PID> or CTRL+\\\n",
      "************************************************************\n",
      "Creates a dag run for the specified dag\n",
      "    :param args:\n",
      "    :return:\n",
      "************************************************************\n",
      "Deletes all DB records related to the specified dag\n",
      "    :param args:\n",
      "    :return:\n",
      "************************************************************\n",
      "Returns the unmet dependencies for a task instance from the perspective of the\n",
      "    scheduler (i.e. why a task instance doesn't get scheduled and then queued by the\n",
      "    scheduler, and then run by an executor).\n",
      "    >>> airflow task_failed_deps tutorial sleep 2015-01-01\n",
      "    Task instance dependencies not met:\n",
      "    Dagrun Running: Task instance's dagrun did not exist: Unknown reason\n",
      "    Trigger Rule: Task's trigger rule 'all_success' requires all upstream tasks\n",
      "    to have succeeded, but found 1 non-success(es).\n",
      "************************************************************\n",
      "Returns the state of a TaskInstance at the command line.\n",
      "    >>> airflow task_state tutorial sleep 2015-01-01\n",
      "    success\n",
      "************************************************************\n",
      "Returns the state of a DagRun at the command line.\n",
      "    >>> airflow dag_state tutorial 2015-01-01T00:00:00.000000\n",
      "    running\n",
      "************************************************************\n",
      "Returns the next execution datetime of a DAG at the command line.\n",
      "    >>> airflow next_execution tutorial\n",
      "    2018-08-31 10:38:00\n",
      "************************************************************\n",
      "Runs forever, monitoring the child processes of @gunicorn_master_proc and\n",
      "    restarting workers occasionally.\n",
      "    Each iteration of the loop traverses one edge of this state transition\n",
      "    diagram, where each state (node) represents\n",
      "    [ num_ready_workers_running / num_workers_running ]. We expect most time to\n",
      "    be spent in [n / n]. `bs` is the setting webserver.worker_refresh_batch_size.\n",
      "    The horizontal transition at ? happens after the new worker parses all the\n",
      "    dags (so it could take a while!)\n",
      "       V ────────────────────────────────────────────────────────────────────────┐\n",
      "    [n / n] ──TTIN──> [ [n, n+bs) / n + bs ]  ────?───> [n + bs / n + bs] ──TTOU─┘\n",
      "       ^                          ^───────────────┘\n",
      "       │\n",
      "       │      ┌────────────────v\n",
      "       └──────┴────── [ [0, n) / n ] <─── start\n",
      "    We change the number of workers by sending TTIN and TTOU to the gunicorn\n",
      "    master process, which increases and decreases the number of child workers\n",
      "    respectively. Gunicorn guarantees that on TTOU workers are terminated\n",
      "    gracefully and that the oldest worker is terminated.\n",
      "************************************************************\n",
      "Retrieves connection to Cloud Translate\n",
      "\n",
      "        :return: Google Cloud Translate client object.\n",
      "        :rtype: Client\n",
      "************************************************************\n",
      "Translate a string or list of strings.\n",
      "\n",
      "        See https://cloud.google.com/translate/docs/translating-text\n",
      "\n",
      "        :type values: str or list\n",
      "        :param values: String or list of strings to translate.\n",
      "\n",
      "        :type target_language: str\n",
      "        :param target_language: The language to translate results into. This\n",
      "                                is required by the API and defaults to\n",
      "                                the target language of the current instance.\n",
      "\n",
      "        :type format_: str\n",
      "        :param format_: (Optional) One of ``text`` or ``html``, to specify\n",
      "                        if the input text is plain text or HTML.\n",
      "\n",
      "        :type source_language: str or None\n",
      "        :param source_language: (Optional) The language of the text to\n",
      "                                be translated.\n",
      "\n",
      "        :type model: str or None\n",
      "        :param model: (Optional) The model used to translate the text, such\n",
      "                      as ``'base'`` or ``'nmt'``.\n",
      "\n",
      "        :rtype: str or list\n",
      "        :returns: A list of dictionaries for each queried value. Each\n",
      "                  dictionary typically contains three keys (though not\n",
      "                  all will be present in all cases)\n",
      "\n",
      "                  * ``detectedSourceLanguage``: The detected language (as an\n",
      "                    ISO 639-1 language code) of the text.\n",
      "                  * ``translatedText``: The translation of the text into the\n",
      "                    target language.\n",
      "                  * ``input``: The corresponding input value.\n",
      "                  * ``model``: The model used to translate the text.\n",
      "\n",
      "                  If only a single value is passed, then only a single\n",
      "                  dictionary will be returned.\n",
      "        :raises: :class:`~exceptions.ValueError` if the number of\n",
      "                 values and translations differ.\n",
      "************************************************************\n",
      "Execute the bash command in a temporary directory\n",
      "        which will be cleaned afterwards\n",
      "************************************************************\n",
      "Retrieves a resource containing information about a Cloud SQL instance.\n",
      "\n",
      "        :param instance: Database instance ID. This does not include the project ID.\n",
      "        :type instance: str\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: A Cloud SQL instance resource.\n",
      "        :rtype: dict\n",
      "************************************************************\n",
      "Creates a new Cloud SQL instance.\n",
      "\n",
      "        :param body: Body required by the Cloud SQL insert API, as described in\n",
      "            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.\n",
      "        :type body: dict\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: None\n",
      "************************************************************\n",
      "Updates settings of a Cloud SQL instance.\n",
      "\n",
      "        Caution: This is not a partial update, so you must include values for\n",
      "        all the settings that you want to retain.\n",
      "\n",
      "        :param body: Body required by the Cloud SQL patch API, as described in\n",
      "            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.\n",
      "        :type body: dict\n",
      "        :param instance: Cloud SQL instance ID. This does not include the project ID.\n",
      "        :type instance: str\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: None\n",
      "************************************************************\n",
      "Deletes a Cloud SQL instance.\n",
      "\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :param instance: Cloud SQL instance ID. This does not include the project ID.\n",
      "        :type instance: str\n",
      "        :return: None\n",
      "************************************************************\n",
      "Retrieves a database resource from a Cloud SQL instance.\n",
      "\n",
      "        :param instance: Database instance ID. This does not include the project ID.\n",
      "        :type instance: str\n",
      "        :param database: Name of the database in the instance.\n",
      "        :type database: str\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: A Cloud SQL database resource, as described in\n",
      "            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.\n",
      "        :rtype: dict\n",
      "************************************************************\n",
      "Creates a new database inside a Cloud SQL instance.\n",
      "\n",
      "        :param instance: Database instance ID. This does not include the project ID.\n",
      "        :type instance: str\n",
      "        :param body: The request body, as described in\n",
      "            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n",
      "        :type body: dict\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: None\n",
      "************************************************************\n",
      "Updates a database resource inside a Cloud SQL instance.\n",
      "\n",
      "        This method supports patch semantics.\n",
      "        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n",
      "\n",
      "        :param instance: Database instance ID. This does not include the project ID.\n",
      "        :type instance: str\n",
      "        :param database: Name of the database to be updated in the instance.\n",
      "        :type database: str\n",
      "        :param body: The request body, as described in\n",
      "            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n",
      "        :type body: dict\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: None\n",
      "************************************************************\n",
      "Deletes a database from a Cloud SQL instance.\n",
      "\n",
      "        :param instance: Database instance ID. This does not include the project ID.\n",
      "        :type instance: str\n",
      "        :param database: Name of the database to be deleted in the instance.\n",
      "        :type database: str\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: None\n",
      "************************************************************\n",
      "Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump\n",
      "        or CSV file.\n",
      "\n",
      "        :param instance: Database instance ID of the Cloud SQL instance. This does not include the\n",
      "            project ID.\n",
      "        :type instance: str\n",
      "        :param body: The request body, as described in\n",
      "            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n",
      "        :type body: dict\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: None\n",
      "************************************************************\n",
      "Waits for the named operation to complete - checks status of the\n",
      "        asynchronous call.\n",
      "\n",
      "        :param project_id: Project ID of the project that contains the instance.\n",
      "        :type project_id: str\n",
      "        :param operation_name: Name of the operation.\n",
      "        :type operation_name: str\n",
      "        :return: None\n",
      "************************************************************\n",
      "Starts Cloud SQL Proxy.\n",
      "\n",
      "        You have to remember to stop the proxy if you started it!\n",
      "************************************************************\n",
      "Stops running proxy.\n",
      "\n",
      "        You should stop the proxy after you stop using it.\n",
      "************************************************************\n",
      "Returns version of the Cloud SQL Proxy.\n",
      "************************************************************\n",
      "Create connection in the Connection table, according to whether it uses\n",
      "        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n",
      "\n",
      "        :param session: Session of the SQL Alchemy ORM (automatically generated with\n",
      "                        decorator).\n",
      "************************************************************\n",
      "Retrieves the dynamically created connection from the Connection table.\n",
      "\n",
      "        :param session: Session of the SQL Alchemy ORM (automatically generated with\n",
      "                        decorator).\n",
      "************************************************************\n",
      "Delete the dynamically created connection from the Connection table.\n",
      "\n",
      "        :param session: Session of the SQL Alchemy ORM (automatically generated with\n",
      "                        decorator).\n",
      "************************************************************\n",
      "Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n",
      "        lifecycle per task.\n",
      "\n",
      "        :return: The Cloud SQL Proxy runner.\n",
      "        :rtype: CloudSqlProxyRunner\n",
      "************************************************************\n",
      "Retrieve database hook. This is the actual Postgres or MySQL database hook\n",
      "        that uses proxy or connects directly to the Google Cloud SQL database.\n",
      "************************************************************\n",
      "Clean up database hook after it was used.\n",
      "************************************************************\n",
      "Reserve free TCP port to be used by Cloud SQL Proxy\n",
      "************************************************************\n"
     ]
    }
   ],
   "source": [
    "for i in pydf[\"docstring\"][:100]:\n",
    "    print(i)\n",
    "    print(\"******\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(autopep8.fix_code(pydf[\"code\"].iloc[35], options={'aggressive': 1, 'ignore': ['W'], \"jobs\":2 }))\n",
    "# content = astunparse.unparse(ast.parse(pydf[\"code\"].iloc[13]))\n",
    "# print(pydf[\"code\"].iloc[3062])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import tokenize\n",
    "\n",
    "\"\"\"\n",
    "Online from https://stackoverflow.com/questions/1769332/script-to-remove-python-comments-docstrings/2962727#2962727\n",
    "\"\"\"\n",
    "\n",
    "def remove_comments_and_docstrings(source):\n",
    "    \"\"\"\n",
    "    Returns 'source' minus comments and docstrings.\n",
    "    \"\"\"\n",
    "    io_obj = StringIO(source)\n",
    "    out = \"\"\n",
    "    prev_toktype = tokenize.INDENT\n",
    "    last_lineno = -1\n",
    "    last_col = 0\n",
    "    for tok in tokenize.generate_tokens(io_obj.readline):\n",
    "        token_type = tok[0]\n",
    "        token_string = tok[1]\n",
    "        start_line, start_col = tok[2]\n",
    "        end_line, end_col = tok[3]\n",
    "        ltext = tok[4]\n",
    "        # The following two conditionals preserve indentation.\n",
    "        # This is necessary because we're not using tokenize.untokenize()\n",
    "        # (because it spits out code with copious amounts of oddly-placed\n",
    "        # whitespace).\n",
    "        if start_line > last_lineno:\n",
    "            last_col = 0\n",
    "        if start_col > last_col:\n",
    "            out += (\" \" * (start_col - last_col))\n",
    "        # Remove comments:\n",
    "        if token_type == tokenize.COMMENT:\n",
    "            pass\n",
    "        # This series of conditionals removes docstrings:\n",
    "        elif token_type == tokenize.STRING:\n",
    "            if prev_toktype != tokenize.INDENT:\n",
    "        # This is likely a docstring; double-check we're not inside an operator:\n",
    "                if prev_toktype != tokenize.NEWLINE:\n",
    "                    # Note regarding NEWLINE vs NL: The tokenize module\n",
    "                    # differentiates between newlines that start a new statement\n",
    "                    # and newlines inside of operators such as parens, brackes,\n",
    "                    # and curly braces.  Newlines inside of operators are\n",
    "                    # NEWLINE and newlines that start new code are NL.\n",
    "                    # Catch whole-module docstrings:\n",
    "                    if start_col > 0:\n",
    "                        # Unlabelled indentation means we're inside an operator\n",
    "                        out += token_string\n",
    "                    # Note regarding the INDENT token: The tokenize module does\n",
    "                    # not label indentation inside of an operator (parens,\n",
    "                    # brackets, and curly braces) as actual indentation.\n",
    "                    # For example:\n",
    "                    # def foo():\n",
    "                    #     \"The spaces before this docstring are tokenize.INDENT\"\n",
    "                    #     test = [\n",
    "                    #         \"The spaces before this string do not get a token\"\n",
    "                    #     ]\n",
    "        else:\n",
    "            out += token_string\n",
    "        prev_toktype = token_type\n",
    "        last_col = end_col\n",
    "        last_lineno = end_line\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_indent_newline(code):\n",
    "    \"\"\"\n",
    "        Replace indent and newline with special symbol § and ø separately.\n",
    "    \"\"\"\n",
    "    code = code.strip()\n",
    "    lines = re.split(r'[\\n;]', code)\n",
    "    for i in range(len(lines)):\n",
    "        lines[i] =  re.sub(\"\\s{4}\",\"§\",lines[i])\n",
    "    code = \"ø\".join(lines)\n",
    "    return code\n",
    "\n",
    "\n",
    "def clean_up_code(content, idx):    \n",
    "    '''\n",
    "    Clean Description, DocString and format\n",
    "    '''\n",
    "    \n",
    "    try:\n",
    "        content_try1 = astunparse.unparse(ast.parse(remove_comments_and_docstrings(content))).strip(\"\\n\")\n",
    "        return replace_indent_newline(content_try1)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        content_try2 = autopep8.fix_code(content, options={'aggressive': 1, 'ignore': ['W'], \"jobs\":2 })\n",
    "        content_try2 = remove_comments_and_docstrings(content_try2)\n",
    "        content_try2 = astunparse.unparse(ast.parse(content_try2)).strip(\"\\n\")\n",
    "        return replace_indent_newline(content_try2)\n",
    "    except:\n",
    "        unvalid_idx.append(idx)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unvalid_idx = []\n",
    "\n",
    "def produced_clean_code(series):\n",
    "    cleaned_code = []\n",
    "    start = time.time()\n",
    "    k = 0\n",
    "    for i in series:\n",
    "        cleaned_code.append(clean_up_code(i, k))\n",
    "        k += 1\n",
    "        if k% 10000 == 0:\n",
    "            print(k)\n",
    "    print(time.time()-start)\n",
    "    return cleaned_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "923.8021256923676\n"
     ]
    }
   ],
   "source": [
    "content = produced_clean_code(pydf[\"code\"])"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_comments(content):\n",
    "    '''\n",
    "    Collect Description and DocString\n",
    "    '''\n",
    "    content = re.sub(r\"\\s*#\\s.*\\n\",\"\\n\", content)\n",
    "    content =  re.sub(r\"(\\\"{3}|\\'{3})[\\s\\S]*(\\\"{3}|\\'{3})[\\n|\\s]*\",\"\", content)\n",
    "    content = re.sub(r\"\\n{2,}\",\"\\n\",content)\n",
    "    return content"
>>>>>>> a6ece6b27f82ac2694c323f86a4c1a2a6a12ee36
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "457461\n",
      "4555\n"
     ]
    }
   ],
   "source": [
    "print(len(content))\n",
    "print(len(unvalid_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4555\n"
     ]
    }
   ],
   "source": [
    "pydf['code'] = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "pydf = pydf.drop(unvalid_idx)\n",
    "# pydf = pydf.reset_index()"
=======
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pydf[\"code\"] = pydf[\"code\"].apply(clean_up_comments)"
>>>>>>> a6ece6b27f82ac2694c323f86a4c1a2a6a12ee36
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "pydf['code'] = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "452906\n"
     ]
    }
   ],
   "source": [
    "# pydf[\"code\"] = pydf[\"code\"].replace(list(range(0,len(pydf[\"code\"]))), content)\n",
    "# print(pydf['code'].iloc[457458])\n",
    "print(len(pydf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
=======
   "execution_count": 13,
>>>>>>> a6ece6b27f82ac2694c323f86a4c1a2a6a12ee36
   "metadata": {},
   "outputs": [],
   "source": [
    "pydf[pydf[\"partition\"]==\"test\"].to_csv(\"./processed_data/finetuning/test.csv\", columns = [\"docstring\",\"code\"])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 118,
=======
   "execution_count": 14,
>>>>>>> a6ece6b27f82ac2694c323f86a4c1a2a6a12ee36
   "metadata": {},
   "outputs": [],
   "source": [
    "pydf[pydf[\"partition\"]==\"train\"].to_csv(\"./processed_data/finetuning/train.csv\", columns = [\"docstring\",\"code\"])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 119,
=======
   "execution_count": 15,
>>>>>>> a6ece6b27f82ac2694c323f86a4c1a2a6a12ee36
   "metadata": {},
   "outputs": [],
   "source": [
    "pydf[pydf[\"partition\"]==\"valid\"].to_csv(\"./processed_data/finetuning/val.csv\", columns = [\"docstring\",\"code\"])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 120,
=======
   "execution_count": 16,
>>>>>>> a6ece6b27f82ac2694c323f86a4c1a2a6a12ee36
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "21948\n",
      "408140\n",
      "22818\n"
=======
      "22176\n",
      "412178\n",
      "23107\n"
>>>>>>> a6ece6b27f82ac2694c323f86a4c1a2a6a12ee36
     ]
    }
   ],
   "source": [
    "print(len(pydf[pydf[\"partition\"]==\"test\"]))\n",
    "print(len(pydf[pydf[\"partition\"]==\"train\"]))\n",
    "print(len(pydf[pydf[\"partition\"]==\"valid\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
=======
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(T5_config)"
   ]
>>>>>>> a6ece6b27f82ac2694c323f86a4c1a2a6a12ee36
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
